Machines Like Me
95
survived the temporary death of the autonomous car was a
dream of redemptive robotic virtue. Adam and his cohort were
its early embodiment, so the user's manual implied. He was
supposed to be my moral superior. I would never meet anyone
better. Had he been my friend, he would have been guilty of
a cruel and terrible lapse. The problem was that I had bought
him, he was my expensive possession and it was not clear what
his obligations to me were, beyond a vaguely assumed helpful-
ness. What does the slave owe to the owner? Also, Miranda did
not "belong" to me. This was clear. I could hear her tell me
that I had no good cause to feel betrayed.
But here was this other matter, which she and I had not yet
discussed. Software engineers from the automobile industry
may have helped with Adam's moral maps. But together we
had contributed to his personality. I didn't know the extent to
which it intruded on, or took priority over, his ethics. How deep
did personality go? A perfectly formed moral system should
float free of any particular disposition. But could it? Confined
to a hard drive, moral software was merely the dry equivalent
of the brain-in-a-dish thought experiment that once littered
philosophical textbooks. Whereas an artificial human had to
get down among us, imperfect, fallen us, and rub along. Hands
assembled in sterile factory conditions must get dirty. To exist
in the human moral dimension was to own a body, a voice,
a pattern of behaviour, memory and desire, experience solid
things and feel pain. A perfectly honest being engaged in such
a way with the world might find Miranda difficult to resist.
Through the night, I'd fantasised Adam's destruction. I saw
my hands tighten around the rope I used to drag him towards