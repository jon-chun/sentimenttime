{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_clean_text_20210607.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2YuKekZRlr5"
      },
      "source": [
        "# **NLP Clean Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiYwTKs74wyE"
      },
      "source": [
        "```\n",
        "\n",
        "    Preprocessing Step I\n",
        "        Lowercasing all the tweets.\n",
        "        Romoval of punctuations\n",
        "        Removal of Stopwords\n",
        "        Lemmatization\n",
        "\n",
        "    Preprocessing Step II\n",
        "        Basic Tf-Idf\n",
        "        Word Embedding\n",
        "\n",
        "    Preparing for Models\n",
        "        Splitting Dataset\n",
        "\n",
        "    Models\n",
        "        Naive Bayes\n",
        "        LSTM\n",
        "```\n",
        "\n",
        "References:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3SOaqHYRtAC"
      },
      "source": [
        "```\n",
        "\n",
        "Some of the common text preprocessing / cleaning steps are:\n",
        "\n",
        "    Lower casing\n",
        "    Removal of Punctuations\n",
        "    Removal of Stopwords\n",
        "    Removal of Frequent words\n",
        "    Removal of Rare words\n",
        "    Stemming\n",
        "    Lemmatization\n",
        "    Removal of emojis\n",
        "    Removal of emoticons\n",
        "    Conversion of emoticons to words\n",
        "    Conversion of emojis to words\n",
        "    Removal of URLs\n",
        "    Removal of HTML tags\n",
        "    Chat words conversion\n",
        "    Spelling correction\n",
        "```\n",
        "References: \n",
        "\n",
        "* https://www.kaggle.com/pocooo/nlp-data-preprocessing-all-you-need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFMSEzzlBcjT"
      },
      "source": [
        "https://www.kaggle.com/jeongwonkim10516/basic-eda-data-cleaning-and-wordclouds\n",
        "```\n",
        "\n",
        "\n",
        "    Importing Neccesary Packages\n",
        "\n",
        "        Most basic stuff for EDA, packages for text processing, Libraries for text preprocessing\n",
        "\n",
        "        We will import basic libraries for data discovery and packages for the Bert model\n",
        "\n",
        "    Road Data\n",
        "\n",
        "        we will need train.csv, test.csv\n",
        "\n",
        "        These include text ID, text, sentiment, selected_text columns that can predict sentiment\n",
        "\n",
        "    EDA\n",
        "\n",
        "        We will see how target values are distributed according to sentiment\n",
        "\n",
        "        Let's look at the train and test data set in general\n",
        "\n",
        "    Cleaning Data\n",
        "\n",
        "        Check for missing values, Removed urls, emojis and punctuations, Removed stopwords etc.\n",
        "\n",
        "        We proceed with data preprocessing for accurate predictive models\n",
        "\n",
        "    Visualizing the Data\n",
        "\n",
        "        Tweet Lengths, Word Lengths, Word Counts, Most Common Words\n",
        "\n",
        "        Increase overall understanding of data through visualization\n",
        "\n",
        "    N-gram Analysis\n",
        "\n",
        "        An approach that considers only a few words and a sequence data representation approach\n",
        "\n",
        "        Let's adjust n in n-gram and look at the changes\n",
        "\n",
        "    Wordclouds\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htekzrFzNpmw"
      },
      "source": [
        "## **Top Notebooks**\n",
        "\n",
        "Overview\n",
        "* https://www.kaggle.com/pocooo/nlp-data-preprocessing-all-you-need#Removal-of-URLs\n",
        "\n",
        "EDA Text\n",
        "* https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data (viz)\n",
        "\n",
        "BERT SA Overview\n",
        "* https://www.kaggle.com/harshjain123/bert-for-everyone-tutorial-implementation\n",
        "\n",
        "SA Viz\n",
        "* https://www.kaggle.com/sarahgonzalez/reddit-news-stock-market-analysis (overlay text/line charts)\n",
        "\n",
        "SA Variation with Time\n",
        "* https://www.kaggle.com/raghav2002sharma/sentiment-analysis-of-covid19-vaccination-tweets\n",
        "\n",
        "SA+Topic Modeling\n",
        "* https://www.kaggle.com/cravingsformeow/imdb-2021-topic-modeling-sentiment-analysis (TextBlob, VADER, LDA on IMDB)\n",
        "\n",
        "Multilingual\n",
        "* https://www.kaggle.com/bond005/multilingual-toxic-bert (Toxic classifier)\n",
        "\n",
        "Real Time \n",
        "* https://www.kaggle.com/albertobellardini/bitcoin-sentiment-analysis-from-yahoo-finance (Yahoo Finance, Google News, bs4, BERT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzOdbvlvU5Wh"
      },
      "source": [
        "**References:**\n",
        "\n",
        "Overivew of Text Cleaning\n",
        "* https://kavita-ganesan.com/text-preprocessing-tutorial/#.YL4JBkwpBhF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e0mfNWXK1-q"
      },
      "source": [
        "## **Configuration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR58RMHTK4qe"
      },
      "source": [
        "### Jupyter Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmix3zBqK4DV"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jbwBsVGK-ul"
      },
      "source": [
        "### Matplotlib Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "groiULCGLBaM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKOYcxgpBk-B"
      },
      "source": [
        "## **Load Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4haUKb3Bkwf"
      },
      "source": [
        "# https://www.kaggle.com/jeongwonkim10516/basic-eda-data-cleaning-and-wordclouds\n",
        "\n",
        "# Most basic stuff for EDA.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Core packages for text processing.\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Libraries for text preprocessing.\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Loading some sklearn packaces for modelling.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# Some packages for word clouds and NER.\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from collections import Counter, defaultdict\n",
        "from PIL import Image\n",
        "import spacy\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n",
        "import en_core_web_sm\n",
        "\n",
        "# Core packages for general use throughout the notebook.\n",
        "\n",
        "import random\n",
        "import warnings\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# For customizing our plots.\n",
        "\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Loading pytorch packages.\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Setting some options for general use.\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set(font_scale=1.5)\n",
        "pd.options.display.max_columns = 250\n",
        "pd.options.display.max_rows = 250\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "#Setting seeds for consistent results.\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ik61Lij5Lty"
      },
      "source": [
        "## **Read In Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoHtmGYe5Ng4"
      },
      "source": [
        "import os\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoxzc_GZ5Ooa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMOgUOfl6nPU"
      },
      "source": [
        "## **Reshape Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMjFHsWY5Ohu"
      },
      "source": [
        "# Select subset of columns\n",
        "\n",
        "xdf = df[['Index','label (depression result)','clean_tweets7']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j68yYKZs6tg6"
      },
      "source": [
        "# Rename columns\n",
        "\n",
        "xdf.columns = ['Index','Labels','Tweets']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ENxveE5PpS"
      },
      "source": [
        "## **EDA Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRyETuTTG3VA"
      },
      "source": [
        "### Pandas Profiling Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWnSK5JpG5z8"
      },
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "ProfileReport(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f85I-GsBHGDT"
      },
      "source": [
        "# https://www.kaggle.com/dtomruk/commonlit-eda-modeling\n",
        "\n",
        "there is not correlation between target and standard error\n",
        "our target has a normal distribution Screen Shot 2021-05-17 at 8 00 29 AM\n",
        "the standard error is skewed left"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTzfRm2SHOSv"
      },
      "source": [
        "### Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB7l1B-3LHZn"
      },
      "source": [
        "rain.describe(include='all').T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owHwq4_cHpa3"
      },
      "source": [
        "# Histogram/Distribution\n",
        "\n",
        "df_tmp[\"sentiment\"].hist();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-VnQZ12HPgO"
      },
      "source": [
        "# Correlation\n",
        "\n",
        "df_tmp.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV2RJz7ZLRJ7"
      },
      "source": [
        "### Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzZpsHBALaQt"
      },
      "source": [
        "Target Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kh9H-pYLWky"
      },
      "source": [
        "# https://www.kaggle.com/rushinaik/bigwords\n",
        "\n",
        "# let's check the distribution of the dataset \n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
        "sns.distplot(train['target'], bins=7, ax=ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKlkO5HpLW68"
      },
      "source": [
        "Standard Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w1Ild9eLSqh"
      },
      "source": [
        "# https://www.kaggle.com/rushinaik/bigwords\n",
        "\n",
        "# let's check the distribution of the dataset \n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
        "sns.distplot(train['standard_error'], bins=100, ax=ax)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2Azy7rpH139"
      },
      "source": [
        "### Scatterplots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXS4b24cH4Lj"
      },
      "source": [
        "# https://www.kaggle.com/dtomruk/commonlit-eda-modeling\n",
        "\n",
        "# Sentiment vs Target (hue=std err)\n",
        "\n",
        "sns.set_palette(\"RdBu\")\n",
        "sns.relplot(x=\"target\", y=\"sentiment\", kind=\"scatter\", hue=\"standard_error\", data=df_tmp, ci=None, height=8.27, aspect=11.7/8.27);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPtE5zCcB4oy"
      },
      "source": [
        "### Language Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkeLiZwIB01q"
      },
      "source": [
        "# Language Detect\n",
        "\n",
        "# language=[]\n",
        "# from langdetect import detect\n",
        "# for index, row in df_comments.iloc[:461812].iterrows():\n",
        "#     try:\n",
        "#         language.append(detect(row['comments']))\n",
        "#     except:\n",
        "#         language.append(\"error\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeOAIFDW_4js"
      },
      "source": [
        "### Word Counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra5MrT4q_52l"
      },
      "source": [
        "# Word Count Histogram\n",
        "\n",
        "# https://www.kaggle.com/jeongwonkim10516/basic-eda-data-cleaning-and-wordclouds\n",
        "\n",
        "def plot_word_number_histogram(textne, textpo, textng):\n",
        "    \n",
        "    \"\"\"A function for comparing word counts\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(18, 6), sharey=True)\n",
        "    sns.distplot(textne.str.split().map(lambda x: len(x)), ax=axes[0], color='#e74c3c')\n",
        "    sns.distplot(textpo.str.split().map(lambda x: len(x)), ax=axes[1], color='#e74c3c')\n",
        "    sns.distplot(textng.str.split().map(lambda x: len(x)), ax=axes[2], color='#e74c3c')\n",
        "    \n",
        "    axes[0].set_xlabel('Word Count')\n",
        "    axes[0].set_ylabel('neutral')\n",
        "    axes[0].set_title('Reliable')\n",
        "    axes[1].set_xlabel('Word Count')\n",
        "    axes[1].set_title('positive')\n",
        "    axes[2].set_xlabel('Word Count')\n",
        "    axes[2].set_title('negative')\n",
        "    \n",
        "    fig.suptitle('Punctuations in tweets', fontsize=24, va='baseline')\n",
        "    \n",
        "    fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkezBDCc_5y_"
      },
      "source": [
        "plot_word_number_histogram(train[train['sentiment'] == 'neutral']['text'],\n",
        "                           train[train['sentiment'] == 'positive']['text'],\n",
        "                           train[train['sentiment'] == 'negative']['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibyZ4bgx_c9x"
      },
      "source": [
        "### Frequent Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmruKADa6FWc"
      },
      "source": [
        "# Frequent Words\n",
        "\n",
        "from collections import Counter\n",
        "cnt = Counter()\n",
        "\n",
        "for text in df['clean_tweets5'].values:\n",
        "  for word in text.split():\n",
        "    cnt[word] += 1\n",
        "\n",
        "cnt.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiiYkvfeYI9p"
      },
      "source": [
        "# Viz most common 50 words\n",
        "\n",
        "# creating dataframe and bar graph of most common 50 words with their frequency\n",
        "word_counts=Counter(words_list).most_common(50)\n",
        "word_df=pd.DataFrame(word_counts)\n",
        "word_df.columns=['word','frq']\n",
        "display(word_df.head(5))\n",
        "# px=import plotly.express\n",
        "#px.bar(word_df,x='word',y='frq',title='Most common words')\n",
        "\n",
        "fig = plt.figure(figsize = (15, 7))\n",
        " \n",
        "# creating the bar plot\n",
        "plt.bar(word_df['word'],word_df['frq'])\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('word')\n",
        "plt.ylabel('frq')\n",
        "plt.title('Most common words')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQT2QYOfAbCR"
      },
      "source": [
        "# https://www.kaggle.com/jeongwonkim10516/basic-eda-data-cleaning-and-wordclouds\n",
        "\n",
        "# Displaying most common words in train data.\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, j in zip(lis, axes):\n",
        "\n",
        "    new = i.str.split()\n",
        "    new = new.values.tolist()\n",
        "    corpus = [word for i in new for word in i]\n",
        "\n",
        "    counter = Counter(corpus)\n",
        "    most = counter.most_common()\n",
        "    x, y = [], []\n",
        "    for word, count in most[:20]:\n",
        "        if (word not in stop):\n",
        "            x.append(word)\n",
        "            y.append(count)\n",
        "\n",
        "    sns.barplot(x=y, y=x, palette='plasma', ax=j)\n",
        "axes[0].set_title('neutral in Tweets')\n",
        "axes[1].set_title('positive in Tweets')\n",
        "axes[2].set_title('negative in Tweets')\n",
        "\n",
        "axes[0].set_xlabel('Count')\n",
        "axes[0].set_ylabel('Word')\n",
        "axes[1].set_xlabel('Count')\n",
        "axes[1].set_ylabel('Word')\n",
        "axes[2].set_xlabel('Count')\n",
        "axes[2].set_ylabel('Word')\n",
        "\n",
        "fig.suptitle('Most Common Unigrams TOP 20 in train data', fontsize=24, va='baseline')\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "# Displaying most common words in test data.\n",
        "\n",
        "lis_test = [\n",
        "    test[test['sentiment'] == 'neutral']['lemma_str_text'],\n",
        "    test[test['sentiment'] == 'positive']['lemma_str_text'],\n",
        "    test[test['sentiment'] == 'negative']['lemma_str_text']\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, j in zip(lis_test, axes):\n",
        "\n",
        "    new = i.str.split()\n",
        "    new = new.values.tolist()\n",
        "    corpus = [word for i in new for word in i]\n",
        "\n",
        "    counter = Counter(corpus)\n",
        "    most = counter.most_common()\n",
        "    x, y = [], []\n",
        "    for word, count in most[:20]:\n",
        "        if (word not in stop):\n",
        "            x.append(word)\n",
        "            y.append(count)\n",
        "\n",
        "    sns.barplot(x=y, y=x, palette='plasma', ax=j)\n",
        "axes[0].set_title('neutral in Tweets')\n",
        "axes[1].set_title('positive in Tweets')\n",
        "axes[2].set_title('negative in Tweets')\n",
        "\n",
        "axes[0].set_xlabel('Count')\n",
        "axes[0].set_ylabel('Word')\n",
        "axes[1].set_xlabel('Count')\n",
        "axes[1].set_ylabel('Word')\n",
        "axes[2].set_xlabel('Count')\n",
        "axes[2].set_ylabel('Word')\n",
        "\n",
        "fig.suptitle('Most Common Unigrams TOP 20 in test data', fontsize=24, va='baseline')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJWhE1xv_bdj"
      },
      "source": [
        "### Rare Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxpQGLmx6FS7"
      },
      "source": [
        "# Rare Words\n",
        "\n",
        "n_rare_words = 10\n",
        "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
        "\n",
        "# Let's see what are the Rarewords\n",
        "\n",
        "RAREWORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk13q5iq_Yr9"
      },
      "source": [
        "### Value Counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCv7c-V9Wp5W"
      },
      "source": [
        "df['Score'].unique()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7ZdRTWaJs99"
      },
      "source": [
        "# The distribution of sentiments\n",
        "data.groupby('sentiment').count().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlPPt8Xb5PaZ"
      },
      "source": [
        "# Value Counts\n",
        "\n",
        "df['label (depression result)'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYCZk8_rB97C"
      },
      "source": [
        "\n",
        "df_lang_stat.iloc[:5].plot.pie(y='count', figsize=(5,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr4VJw4IMWtd"
      },
      "source": [
        "# https://www.kaggle.com/mantej/sentiment-analysis-eda-transformers\n",
        "\n",
        "# Bar graph depicting total sentiment for the different topics\n",
        "\n",
        "df.groupby('Topic').agg('sum')[['SentimentHeadline', 'SentimentTitle']].plot(kind='bar', figsize=(25, 7),\n",
        "                                                          stacked=False, color=['b', 'r', 'g']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RwPZPLPMxzT"
      },
      "source": [
        "### Correlation Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GN4S1b4M2gE"
      },
      "source": [
        "Seaborn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FpCSjPhMzzq"
      },
      "source": [
        "# https://www.kaggle.com/mantej/sentiment-analysis-eda-transformers\n",
        "\n",
        "#Correlation Matrix\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "_ = sns.heatmap(df_notText[['Facebook','GooglePlus','LinkedIn','SentimentTitle','SentimentHeadline']].corr(), square=True, cmap='Blues',linewidths=0.5,linecolor='w',annot=True)\n",
        "plt.title('Correlation matrix ')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbTdaQLfJ6i5"
      },
      "source": [
        "### Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhL_lW5IJ8eG"
      },
      "source": [
        "# https://www.kaggle.com/kritanjalijain/movie-review-sentiment-analysis-eda-bert\n",
        "\n",
        "fig = plt.figure(figsize=(14,7))\n",
        "data['length'] = data.review.str.split().apply(len)\n",
        "ax1 = fig.add_subplot(122)\n",
        "sns.histplot(data[data['sentiment']==1]['length'], ax=ax1,color='green')\n",
        "describe = data.length[data.sentiment==1].describe().to_frame().round(2)\n",
        "\n",
        "ax2 = fig.add_subplot(121)\n",
        "ax2.axis('off')\n",
        "font_size = 14\n",
        "bbox = [0, 0, 1, 1]\n",
        "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
        "table.set_fontsize(font_size)\n",
        "fig.suptitle('Distribution of text length for positive sentiment reviews.', fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eih_p5fzHdTy"
      },
      "source": [
        "### Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-grWxBXTe9l_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df_Text['Topic'].value_counts().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7IVYl78ZcgD"
      },
      "source": [
        "# Visualizing the count of 'Label' column from the dataset\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.countplot(x='Label', data=df)\n",
        "plt.xlabel('Stock Sentiments (0-Down/Same, 1-Up)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLupHQe7HfaS"
      },
      "source": [
        "sns.catplot(x=\"target\", y=\"range\", data=df_tmp, kind=\"bar\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Swbc423Aoi7"
      },
      "source": [
        "### n-Grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na0M8ikqAq0Q"
      },
      "source": [
        "# https://www.kaggle.com/jeongwonkim10516/basic-eda-data-cleaning-and-wordclouds\n",
        "\n",
        "def ngrams(n, title):\n",
        "    \"\"\"A Function to plot most common ngrams\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
        "    axes = axes.flatten()\n",
        "    for i, j in zip(lis, axes):\n",
        "\n",
        "        new = i.str.split()\n",
        "        new = new.values.tolist()\n",
        "        corpus = [word for i in new for word in i]\n",
        "\n",
        "        def _get_top_ngram(corpus, n=None):\n",
        "            #getting top ngrams\n",
        "            vec = CountVectorizer(ngram_range=(n, n),\n",
        "                                  max_df=0.9,\n",
        "                                  stop_words='english').fit(corpus)\n",
        "            bag_of_words = vec.transform(corpus)\n",
        "            sum_words = bag_of_words.sum(axis=0)\n",
        "            words_freq = [(word, sum_words[0, idx])\n",
        "                          for word, idx in vec.vocabulary_.items()]\n",
        "            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "            return words_freq[:15]\n",
        "\n",
        "        top_n_bigrams = _get_top_ngram(i, n)[:15]\n",
        "        x, y = map(list, zip(*top_n_bigrams))\n",
        "        sns.barplot(x=y, y=x, palette='plasma', ax=j)\n",
        "        axes[0].set_title('neutral in Tweets')\n",
        "        axes[1].set_title('positive in Tweets')\n",
        "        axes[2].set_title('negative in Tweets')\n",
        "        \n",
        "        axes[0].set_xlabel('Count')\n",
        "        axes[0].set_ylabel('Words')\n",
        "        axes[1].set_xlabel('Count')\n",
        "        axes[1].set_ylabel('Words')\n",
        "        axes[2].set_xlabel('Count')\n",
        "        axes[2].set_ylabel('Words')\n",
        "        fig.suptitle(title, fontsize=24, va='baseline')\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        \n",
        "def ngrams_test(n, title):\n",
        "    \"\"\"A Function to plot most common ngrams\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
        "    axes = axes.flatten()\n",
        "    for i, j in zip(lis_test, axes):\n",
        "\n",
        "        new = i.str.split()\n",
        "        new = new.values.tolist()\n",
        "        corpus = [word for i in new for word in i]\n",
        "\n",
        "        def _get_top_ngram(corpus, n=None):\n",
        "            #getting top ngrams\n",
        "            vec = CountVectorizer(ngram_range=(n, n),\n",
        "                                  max_df=0.9,\n",
        "                                  stop_words='english').fit(corpus)\n",
        "            bag_of_words = vec.transform(corpus)\n",
        "            sum_words = bag_of_words.sum(axis=0)\n",
        "            words_freq = [(word, sum_words[0, idx])\n",
        "                          for word, idx in vec.vocabulary_.items()]\n",
        "            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "            return words_freq[:15]\n",
        "\n",
        "        top_n_bigrams = _get_top_ngram(i, n)[:15]\n",
        "        x, y = map(list, zip(*top_n_bigrams))\n",
        "        sns.barplot(x=y, y=x, palette='plasma', ax=j)\n",
        "        axes[0].set_title('neutral in Tweets')\n",
        "        axes[1].set_title('positive in Tweets')\n",
        "        axes[2].set_title('negative in Tweets')\n",
        "        \n",
        "        axes[0].set_xlabel('Count')\n",
        "        axes[0].set_ylabel('Words')\n",
        "        axes[1].set_xlabel('Count')\n",
        "        axes[1].set_ylabel('Words')\n",
        "        axes[2].set_xlabel('Count')\n",
        "        axes[2].set_ylabel('Words')\n",
        "        fig.suptitle(title, fontsize=24, va='baseline')\n",
        "        plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKRNdnyzAqxI"
      },
      "source": [
        "# Bigrams\n",
        "\n",
        "ngrams(2, 'Most Common Bigrams in train')\n",
        "ngrams_test(2, 'Most Common Bigrams in test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKdT9QHIAyPN"
      },
      "source": [
        "# Trigrams\n",
        "\n",
        "ngrams(3, 'Most Common Trigrams in train')\n",
        "ngrams_test(3, 'Most Common Bigrams in test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Wgs_25_W3V"
      },
      "source": [
        "### Number of Nulls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyHkK7nS5PTG"
      },
      "source": [
        "# Checking if there is any null values.\n",
        "\n",
        "df.isnull().sum()\n",
        "\n",
        "# Check for missing values\n",
        "\n",
        "train.isnull().value_counts(), test.isnull().value_counts()\n",
        "\n",
        "# Remove missing values\n",
        "\n",
        "train.dropna(inplace=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-ndcHZaIcAb"
      },
      "source": [
        "### Syllable Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlNysCSSIULX"
      },
      "source": [
        "# \n",
        "\n",
        "# from https://www.kaggle.com/jitshil143/submission-score-0-62\n",
        "def syllable_count(word):\n",
        "    count = 0\n",
        "    vowels = \"aeiouy\"\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "            if word.endswith(\"e\"):\n",
        "                count -= 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "# nof syllables\n",
        "df_tmp['nof_syllables'] =  df_tmp['excerpt'].apply(lambda s: syllable_count(s))\n",
        "\n",
        "sns.relplot(x=\"nof_syllables\", y=\"target\", hue=\"standard_error\", data=df_tmp, kind=\"scatter\");"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFPH7p0RAF8G"
      },
      "source": [
        "### Word Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuDGK9WCAHsL"
      },
      "source": [
        "# https://www.kaggle.com/jeongwonkim10516/basic-eda-data-cleaning-and-wordclouds\n",
        "\n",
        "def plot_word_len_histogram(textne, textpo, textng):\n",
        "    \n",
        "    \"\"\"A function for comparing average word length\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(18, 6), sharey=True)\n",
        "    sns.distplot(textne.str.split().apply(lambda x: [len(i) for i in x]).map(\n",
        "        lambda x: np.mean(x)),\n",
        "                 ax=axes[0], color='#e74c3c')\n",
        "    sns.distplot(textpo.str.split().apply(lambda x: [len(i) for i in x]).map(\n",
        "        lambda x: np.mean(x)),\n",
        "                 ax=axes[1], color='#e74c3c')\n",
        "    sns.distplot(textng.str.split().apply(lambda x: [len(i) for i in x]).map(\n",
        "        lambda x: np.mean(x)),\n",
        "                 ax=axes[2], color='#e74c3c')\n",
        "    \n",
        "    axes[0].set_xlabel('Word Count')\n",
        "    axes[0].set_ylabel('neutral')\n",
        "    axes[0].set_title('Reliable')\n",
        "    axes[1].set_xlabel('Word Count')\n",
        "    axes[1].set_title('positive')\n",
        "    axes[2].set_xlabel('Word Count')\n",
        "    axes[2].set_title('negative')\n",
        "    \n",
        "    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n",
        "    fig.tight_layout()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8KD0GRdAHok"
      },
      "source": [
        "plot_word_len_histogram(train[train['sentiment'] == 'neutral']['text'],\n",
        "                        train[train['sentiment'] == 'positive']['text'],\n",
        "                        train[train['sentiment'] == 'negative']['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJIylSlp_T3h"
      },
      "source": [
        "### Sentence Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwSSYzI3_Sx9"
      },
      "source": [
        "# Creating a new feature for the visualization.\n",
        "# text_clean Lengths visualization.\n",
        "\n",
        "# https://www.kaggle.com/jeongwonkim10516/basic-eda-data-cleaning-and-wordclouds\n",
        "\n",
        "train['Character Count'] = train['text_clean'].apply(lambda x: len(str(x)))\n",
        "\n",
        "\n",
        "def plot_dist3(df, feature, title):\n",
        "    # Creating a customized chart. and giving in figsize and everything.\n",
        "    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n",
        "    # Creating a grid of 3 cols and 3 rows.\n",
        "    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n",
        "\n",
        "    # Customizing the histogram grid.\n",
        "    ax1 = fig.add_subplot(grid[:2, :2])\n",
        "    # Set the title.\n",
        "    ax1.set_title('Histogram')\n",
        "    # plot the histogram.\n",
        "    sns.distplot(df.loc[:, feature],\n",
        "                 hist=True,\n",
        "                 kde=True,\n",
        "                 ax=ax1,\n",
        "                 color='#e74c3c')\n",
        "    ax1.set(ylabel='Frequency')\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n",
        "\n",
        "    # Customizing the ecdf_plot.\n",
        "    ax2 = fig.add_subplot(grid[2:, :2])\n",
        "    # Set the title.\n",
        "    ax2.set_title('Empirical CDF')\n",
        "    # Plotting the ecdf_Plot.\n",
        "    sns.distplot(df.loc[:, feature],\n",
        "                 ax=ax2,\n",
        "                 kde_kws={'cumulative': True},\n",
        "                 hist_kws={'cumulative': True},\n",
        "                 color='#e74c3c')\n",
        "    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n",
        "    ax2.set(ylabel='Cumulative Probability')\n",
        "\n",
        "    # Customizing the Box Plot.\n",
        "    ax3 = fig.add_subplot(grid[:, 2])\n",
        "    # Set title.\n",
        "    ax3.set_title('Box Plot')\n",
        "    # Plotting the box plot.\n",
        "    sns.boxplot(y=feature, data=df, ax=ax3, color='#e74c3c')\n",
        "    ax3.yaxis.set_major_locator(MaxNLocator(nbins=20))\n",
        "\n",
        "    plt.suptitle(f'{title}', fontsize=24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al8IpkFX_j9c"
      },
      "source": [
        "plot_dist3(train[train['sentiment'] == 'positive'], 'Character Count',\n",
        "           'Characters Per \"positive\" Tweet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFVyTiYPIMpk"
      },
      "source": [
        "### Paragraph Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBHRtwJKIONE"
      },
      "source": [
        "excerpt_length = df_tmp[\"excerpt\"].apply(lambda x: len(x.split(\" \")))\n",
        "df_tmp[\"excerpt_len\"] = excerpt_length\n",
        "df_tmp[\"excerpt_len\"].max(), df_tmp[\"excerpt_len\"].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFskcX9k_g5f"
      },
      "source": [
        "### Numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-0ElNg5dwg"
      },
      "source": [
        "# Check if there's any number\n",
        "\n",
        "for i in df['message to examine']:\n",
        "  for j in i.split():\n",
        "    if j.isdigit():\n",
        "      s = \"yes\"\n",
        "    else:\n",
        "      s = \"no\"\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVw00iBw_frb"
      },
      "source": [
        "### Word Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSSHh_Yhey8C"
      },
      "source": [
        "# https://www.kaggle.com/mantej/sentiment-analysis-eda-transformers\n",
        "\n",
        "#Creating word cloud (based on title and headline) for each of the topic\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "def wordcloud_draw(data, color = 'black'):\n",
        "    for i,j in enumerate(df_Text.Topic.value_counts().index.tolist()):\n",
        "        words = ' '.join(data[df_Text.Topic==j])\n",
        "        cleaned_word = \" \".join([word for word in words.split()\n",
        "                                if 'obama' not in word.lower()\n",
        "                                    and 'economy' not in word.lower() \n",
        "                                    and 'microsoft' not in word.lower()\n",
        "                                    and 'palestine' not in word.lower()\n",
        "                                    and not word.startswith('@')\n",
        "                                    and not word.startswith('#')\n",
        "                                    \n",
        "                                ])\n",
        "        wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                          background_color=color,\n",
        "                          width=2500,\n",
        "                          height=2000,\n",
        "                          max_words=200\n",
        "                        ).generate(cleaned_word)\n",
        "        plt.figure(1,figsize=(20, 20))\n",
        "        plt.subplot(2,2,i+1)\n",
        "        plt.imshow(wordcloud)\n",
        "        plt.title(j)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "print(\"Word cloud of Title\")\n",
        "wordcloud_draw(df_Text.Title,'white')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1Svai3aKEvX"
      },
      "source": [
        "# https://www.kaggle.com/kritanjalijain/movie-review-sentiment-analysis-eda-bert\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "plt.figure(figsize = (20,20)) # Positive Review Text\n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.sentiment == 1].review))\n",
        "plt.imshow(wc , interpolation = 'bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PNy9qWUA5d9"
      },
      "source": [
        "# https://www.kaggle.com/jeongwonkim10516/basic-eda-data-cleaning-and-wordclouds\n",
        "\n",
        "fig, axes = plt.subplots(1,3, figsize=(24,12))\n",
        "sentiment_list = np.unique(train['sentiment'])\n",
        "\n",
        "for i, sentiment in zip(range(3), sentiment_list):\n",
        "    wc = WordCloud(background_color=\"white\", max_words = 2000, width = 1600, height = 800, mask=mask, colormap=\"Blues\").generate(\" \".join(train[train['sentiment']==sentiment]['lemma_str_text']))\n",
        "    \n",
        "    axes[i].text(0.5,1, \"{} text\".format(sentiment), fontweight=\"bold\", fontfamily='serif', fontsize=17)\n",
        "    axes[i].patch.set_alpha(0)\n",
        "    axes[i].axis('off')\n",
        "    axes[i].imshow(wc)\n",
        "\n",
        "fig.text(0.1,0.8,\"WordCloud by sentiment per selected text in train Tweets\", fontweight=\"bold\", fontfamily='serif', fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87Z5fJEYBBSW"
      },
      "source": [
        "# https://www.kaggle.com/jeongwonkim10516/basic-eda-data-cleaning-and-wordclouds\n",
        "\n",
        "fig, axes = plt.subplots(1,3, figsize=(24,12))\n",
        "sentiment_list = np.unique(test['sentiment'])\n",
        "\n",
        "for i, sentiment in zip(range(3), sentiment_list):\n",
        "    wc = WordCloud(background_color=\"white\", max_words = 2000, width = 1600, height = 800, mask=mask, colormap=\"Blues\").generate(\" \".join(test[test['sentiment']==sentiment]['lemma_str_text']))\n",
        "    \n",
        "    axes[i].text(0.5,1, \"{} text\".format(sentiment), fontweight=\"bold\", fontfamily='serif', fontsize=17)\n",
        "    axes[i].patch.set_alpha(0)\n",
        "    axes[i].axis('off')\n",
        "    axes[i].imshow(wc)\n",
        "\n",
        "fig.text(0.1,0.8,\"WordCloud by sentiment per selected text in test Tweets\", fontweight=\"bold\", fontfamily='serif', fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAbXezWV6y4C"
      },
      "source": [
        "# Plot the word cloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentences = xdf['Tweets'].tolist()\n",
        "\n",
        "len(sentences)\n",
        "\n",
        "# Joining sentences (combining all the sentences that we have)\n",
        "\n",
        "joined_sentences = \" \".join(sentences)\n",
        "\n",
        "plt.figure(figsize = (12,8))\n",
        "plt.imshow(WordCloud().generate(joined_sentences));\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "xdf['Labels'].value_counts()\n",
        "\n",
        "# Let's visualize postive and negative tweets\n",
        "\n",
        "positive_tweets = xdf[xdf['Labels'] == 0]\n",
        "positive_sentences = positive_tweets['Tweets'].tolist()\n",
        "positive_string = \" \".join(positive_sentences)\n",
        "\n",
        "plt.figure(figsize = (12,8))\n",
        "plt.imshow(WordCloud().generate(positive_string));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6wZ9K5W44cU"
      },
      "source": [
        "## **RegEx Filters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOwV601BLzR4"
      },
      "source": [
        "### Pandas Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_anowqQoXleO"
      },
      "source": [
        "Change Types including Datatime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZV48Sy5Xkil"
      },
      "source": [
        "\n",
        "\n",
        "# convert username type from object to category for assigning the numbers after it\n",
        "df.username=df.username.astype('category')\n",
        "df.username=df.username.cat.codes # assign a unique numerical code to each category\n",
        "df.date=pd.to_datetime(df.date).dt.date # it will give only the date\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E_G8HBeMB8F"
      },
      "source": [
        "Drop Nulls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83oHcgj-L3xO"
      },
      "source": [
        "# Drop Nulls\n",
        "\n",
        "df.isna().any()\n",
        "\n",
        "# Dropping NaN values\n",
        "df.dropna(inplace=True)\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8aBISkIL_1w"
      },
      "source": [
        "Drop Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDKQqHoLL0-3"
      },
      "source": [
        "# Check for Duplicates\n",
        "\n",
        "#checking for duplicates\n",
        "df[df.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLkv_HfqME6m"
      },
      "source": [
        "Drop/Rename Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCQ65EThL-E0"
      },
      "source": [
        "df_Text = df[[\"Title\",\"Headline\",\"Source\",\"Topic\"]].copy()\n",
        "\n",
        "df[df.apply(lambda x: x.Topic.lower() not in x.Headline.lower(), axis=1)][['Topic', 'Headline']]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M63cYPoSWLl_"
      },
      "source": [
        "Aggregation and Counting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf54HCsDWLWA"
      },
      "source": [
        "df.groupby(['Score','upvote_perc']).agg('count')\n",
        "\n",
        "df_s= df.groupby(['Score','upvote_perc']).agg({'Id':'count'}).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oHPlwAqZqO0"
      },
      "source": [
        "Filtering by Datetime Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bndCziF2Zp-B"
      },
      "source": [
        "# Splitting the dataset into train an test set\n",
        "train = df_copy[df_copy['Date'] < '20150101']\n",
        "test = df_copy[df_copy['Date'] > '20141231']\n",
        "print('Train size: {}, Test size: {}'.format(train.shape, test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgyZe5v8Zxzn"
      },
      "source": [
        "Filtering by Row Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVLEQKIrZxnE"
      },
      "source": [
        "# Splitting the dataset\n",
        "y_train = train['Label']\n",
        "train = train.iloc[:, 3:28]\n",
        "y_test = test['Label']\n",
        "test = test.iloc[:, 3:28]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYfa_7EVMmsf"
      },
      "source": [
        "Conditionally Filtering Column Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8fMXmXMMl-p"
      },
      "source": [
        "# https://www.kaggle.com/mantej/sentiment-analysis-eda-transformers\n",
        "\n",
        "import numpy as np\n",
        "df_notText['SentimentChange']=np.where((df['SentimentTitle'] <= 0) & (df['SentimentHeadline'] >=0)\n",
        "                     ,'Title -ve Headline +ve' , 'Title +ve Headline -ve')\n",
        "\n",
        "df['helpful_perc']=np.where(df['HelpfulnessDenominator']>0,df['HelpfulnessNumerator']/df['HelpfulnessDenominator'],-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOZ2XTj_Whhb"
      },
      "source": [
        "Remapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LbjAx2OWjPY"
      },
      "source": [
        "## before defining y we need to map the reviews with 0 & 1, where reviews with 1&2 represent 0 and 4&5 represent 1\n",
        "\n",
        "y_dict = {1:0, 2:0, 4:1, 5:1}\n",
        "\n",
        "y = df2['Score'].map(y_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HPBFNgCV9ru"
      },
      "source": [
        "Binning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlOVgJ4SV-kp"
      },
      "source": [
        "# https://www.kaggle.com/danala26/nlp-amazon-sentiment-analysis\n",
        "\n",
        "df['upvote_perc']=pd.cut(df['helpful_perc'],bins=[-1,0,0.2,0.4,0.6,0.8,1],labels=['Empty','0-20%','20-40%','40-60%','60-80%','80-100%'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QNQVkQ6WUKt"
      },
      "source": [
        "Pivoting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmvVQyvTWVif"
      },
      "source": [
        "# https://www.kaggle.com/danala26/nlp-amazon-sentiment-analysis\n",
        "\n",
        "pivot=df_s.pivot(index='upvote_perc',columns='Score')\n",
        "pivot\n",
        "\n",
        "sns.heatmap(pivot,annot=True, cmap='YlGnBu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ernvu_IJZ8zh"
      },
      "source": [
        "Joining all the Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aosf0IZ6Z8Qr"
      },
      "source": [
        "# https://www.kaggle.com/lykin22/sentiment-analysis-djia-stock-news-headlines\n",
        "\n",
        "# Joining all the columns\n",
        "train_headlines = []\n",
        "test_headlines = []\n",
        "\n",
        "for row in range(0, train.shape[0]):\n",
        "  train_headlines.append(' '.join(str(x) for x in train.iloc[row, 0:25]))\n",
        "\n",
        "for row in range(0, test.shape[0]):\n",
        "  test_headlines.append(' '.join(str(x) for x in test.iloc[row, 0:25]))\n",
        "\n",
        "# Creating corpus of train dataset\n",
        "ps = PorterStemmer()\n",
        "train_corpus = []\n",
        "\n",
        "for i in range(0, len(train_headlines)):\n",
        "  \n",
        "  # Tokenizing the news-title by words\n",
        "  words = train_headlines[i].split()\n",
        "\n",
        "  # Removing the stopwords\n",
        "  words = [word for word in words if word not in set(stopwords.words('english'))]\n",
        "\n",
        "  # Stemming the words\n",
        "  words = [ps.stem(word) for word in words]\n",
        "\n",
        "  # Joining the stemmed words\n",
        "  headline = ' '.join(words)\n",
        "\n",
        "  # Building a corpus of news-title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a1YcKrE9gIW"
      },
      "source": [
        "### Contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8crTf5_G6cpm"
      },
      "source": [
        "# Contractions\n",
        "\n",
        "short_words = {\n",
        "\"aint\": \"am not\",\n",
        "\"arent\": \"are not\",\n",
        "\"cant\": \"cannot\",\n",
        "\"'cause\": \"because\",\n",
        "\"couldve\": \"could have\",\n",
        "\"couldnt\": \"could not\",\n",
        "\"didnt\": \"did not\",\n",
        "\"doesnt\": \"does not\",\n",
        "\"dont\": \"do not\",\n",
        "\"hadnt\": \"had not\",\n",
        "\"hasnt\": \"has not\",\n",
        "\"havent\": \"have not\",\n",
        "\"im\": \"I am\",\n",
        "\"em\": \"them\",\n",
        "\"ive\": \"I have\",\n",
        "\"isnt\": \"is not\",\n",
        "\"lets\": \"let us\",\n",
        "\"theyre\": \"they are\",\n",
        "\"theyve\": \"they have\",\n",
        "\"wasnt\": \"was not\",\n",
        "\"well\": \"we will\",\n",
        "\"were\": \"we are\",\n",
        "\"werent\": \"were not\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "def replace_short_words(text):\n",
        "  for word in text.split():\n",
        "    if word in short_words:\n",
        "      text = text.replace(word, short_words[word])\n",
        "  \n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "df[\"clean_tweets7\"] = df['text_lemmatized'].apply(lambda text: replace_short_words(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3c2r41x4rof"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcA2JxECUIAP"
      },
      "source": [
        "### Chat Acronyms to Words\n",
        "\n",
        "This is an important text preprocessing step if we are dealing with chat data. People do use a lot of abbreviated words in chat and so it might be helpful to expand those words for our analysis purposes.\n",
        "\n",
        "Got a good list of chat slang words from this repo. We can use this for our conversion here. We can add more words to this list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ffbDQ6OUH1L"
      },
      "source": [
        "# https://www.kaggle.com/pocooo/nlp-data-preprocessing-all-you-need#Removal-of-URLs\n",
        "\n",
        "chat_words_str = \"\"\"\n",
        "AFAIK=As Far As I Know\n",
        "AFK=Away From Keyboard\n",
        "ASAP=As Soon As Possible\n",
        "ATK=At The Keyboard\n",
        "ATM=At The Moment\n",
        "A3=Anytime, Anywhere, Anyplace\n",
        "BAK=Back At Keyboard\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "    if line != \"\":\n",
        "        cw = line.split(\"=\")[0]\n",
        "        cw_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(cw)\n",
        "        chat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)\n",
        "\n",
        "def chat_words_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMWKQlxOTLOk"
      },
      "source": [
        "### Emoticons to Words\n",
        "\n",
        "his is what we did in the last step right? Nope. We did remove emojis in the last step but not emoticons. There is a minor difference between emojis and emoticons.\n",
        "\n",
        "From Grammarist.com, emoticon is built from keyboard characters that when put together in a certain way represent a facial expression, an emoji is an actual image.\n",
        "\n",
        ":-) is an emoticon\n",
        "\n",
        "😀 is an emoji\n",
        "\n",
        "    Thanks NeelShah\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkaGkB1YTOFK"
      },
      "source": [
        "# \n",
        "\n",
        "EMOTICONS = {\n",
        "    u\":‑\\)\":\"Happy face or smiley\",\n",
        "    u\":\\)\":\"Happy face or smiley\",\n",
        "    u\":-\\]\":\"Happy face or smiley\",\n",
        "    u\":\\]\":\"Happy face or smiley\",\n",
        "    u\":-3\":\"Happy face smiley\",\n",
        "    u\":3\":\"Happy face smiley\",\n",
        "    u\":->\":\"Happy face smiley\",\n",
        "    u\":>\":\"Happy face smiley\",\n",
        "    u\"8-\\)\":\"Happy face smiley\",\n",
        "    u\":o\\)\":\"Happy face smiley\",\n",
        "    u\":-\\}\":\"Happy face smiley\",\n",
        "    u\":\\}\":\"Happy face smiley\",\n",
        "    u\":-\\)\":\"Happy face smiley\",\n",
        "    u\":c\\)\":\"Happy face smiley\",\n",
        "    u\":\\^\\)\":\"Happy face smiley\",\n",
        "    u\"=\\]\":\"Happy face smiley\",\n",
        "    u\"=\\)\":\"Happy face smiley\",\n",
        "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":-\\)\\)\":\"Very happy\",\n",
        "    u\":‑\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑c\":\"Frown, sad, andry or pouting\",\n",
        "    u\":c\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
        "    u\":@\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":'‑\\(\":\"Crying\",\n",
        "    u\":'\\(\":\"Crying\",\n",
        "    u\":'‑\\)\":\"Tears of happiness\",\n",
        "    u\":'\\)\":\"Tears of happiness\",\n",
        "    u\"D‑':\":\"Horror\",\n",
        "    u\"D:<\":\"Disgust\",\n",
        "    u\"D:\":\"Sadness\",\n",
        "    u\"D8\":\"Great dismay\",\n",
        "    u\"D;\":\"Great dismay\",\n",
        "    u\"D=\":\"Great dismay\",\n",
        "    u\"DX\":\"Great dismay\",\n",
        "    u\":‑O\":\"Surprise\",\n",
        "    u\":O\":\"Surprise\",\n",
        "    u\":‑o\":\"Surprise\",\n",
        "    u\":o\":\"Surprise\",\n",
        "    u\":-0\":\"Shock\",\n",
        "    u\"8‑0\":\"Yawn\",\n",
        "    u\">:O\":\"Yawn\",\n",
        "    u\":-\\*\":\"Kiss\",\n",
        "    u\":\\*\":\"Kiss\",\n",
        "    u\":X\":\"Kiss\",\n",
        "    u\";‑\\)\":\"Wink or smirk\",\n",
        "    u\";\\)\":\"Wink or smirk\",\n",
        "    u\"\\*-\\)\":\"Wink or smirk\",\n",
        "    u\"\\*\\)\":\"Wink or smirk\",\n",
        "    u\";‑\\]\":\"Wink or smirk\",\n",
        "    u\";\\]\":\"Wink or smirk\",\n",
        "    u\";\\^\\)\":\"Wink or smirk\",\n",
        "    u\":‑,\":\"Wink or smirk\",\n",
        "    u\";D\":\"Wink or smirk\",\n",
        "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":‑\\|\":\"Straight face\",\n",
        "    u\":\\|\":\"Straight face\",\n",
        "    u\":$\":\"Embarrassed or blushing\",\n",
        "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\"O:‑\\)\":\"Angel, saint or innocent\",\n",
        "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:‑3\":\"Angel, saint or innocent\",\n",
        "    u\"0:3\":\"Angel, saint or innocent\",\n",
        "    u\"0:‑\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
        "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
        "    u\">:‑\\)\":\"Evil or devilish\",\n",
        "    u\">:\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:‑\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:\\)\":\"Evil or devilish\",\n",
        "    u\"3:‑\\)\":\"Evil or devilish\",\n",
        "    u\"3:\\)\":\"Evil or devilish\",\n",
        "    u\">;\\)\":\"Evil or devilish\",\n",
        "    u\"\\|;‑\\)\":\"Cool\",\n",
        "    u\"\\|‑O\":\"Bored\",\n",
        "    u\":‑J\":\"Tongue-in-cheek\",\n",
        "    u\"#‑\\)\":\"Party all night\",\n",
        "    u\"%‑\\)\":\"Drunk or confused\",\n",
        "    u\"%\\)\":\"Drunk or confused\",\n",
        "    u\":-###..\":\"Being sick\",\n",
        "    u\":###..\":\"Being sick\",\n",
        "    u\"<:‑\\|\":\"Dump\",\n",
        "    u\"\\(>_<\\)\":\"Troubled\",\n",
        "    u\"\\(>_<\\)>\":\"Troubled\",\n",
        "    u\"\\(';'\\)\":\"Baby\",\n",
        "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
        "    u\"\\(\\^_-\\)\":\"Wink\",\n",
        "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
        "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
        "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
        "    u\"\\^_\\^\":\"Joyful\",\n",
        "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
        "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
        "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
        "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
        "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;_;\":\"Sad of Crying\",\n",
        "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
        "    u\";_;\":\"Sad or Crying\",\n",
        "    u\";-;\":\"Sad or Crying\",\n",
        "    u\";n;\":\"Sad or Crying\",\n",
        "    u\";;\":\"Sad or Crying\",\n",
        "    u\"Q\\.Q\":\"Sad or Crying\",\n",
        "    u\"T\\.T\":\"Sad or Crying\",\n",
        "    u\"QQ\":\"Sad or Crying\",\n",
        "    u\"Q_Q\":\"Sad or Crying\",\n",
        "    u\"\\(-\\.-\\)\":\"Shame\",\n",
        "    u\"\\(-_-\\)\":\"Shame\",\n",
        "    u\"\\(一一\\)\":\"Shame\",\n",
        "    u\"\\(；一_一\\)\":\"Shame\",\n",
        "    u\"\\(=_=\\)\":\"Tired\",\n",
        "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
        "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
        "    u\"=_\\^=\t\":\"cat\",\n",
        "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
        "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
        "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
        "    u\"\\(\\・\\・?\":\"Confusion\",\n",
        "    u\"\\(?_?\\)\":\"Confusion\",\n",
        "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
        "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
        "    u\"\\^/\\^\":\"Normal Laugh\",\n",
        "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n",
        "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n",
        "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
        "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
        "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
        "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
        "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
        "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
        "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
        "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
        "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
        "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
        "    u'\\(-\"-\\)':\"Worried\",\n",
        "    u\"\\(ーー;\\)\":\"Worried\",\n",
        "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
        "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
        "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
        "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
        "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
        "    u\":O o_O\":\"Surprised\",\n",
        "    u\"o_0\":\"Surprised\",\n",
        "    u\"o\\.O\":\"Surpised\",\n",
        "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
        "    u\"oO\":\"Surprised\",\n",
        "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
        "    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMrnj7B0TVdO"
      },
      "source": [
        "def remove_emoticons(text):\n",
        "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
        "    return emoticon_pattern.sub(r'', text)\n",
        "\n",
        "remove_emoticons(\"Put your text here or call the above function :-)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4StuKGdMTkex"
      },
      "source": [
        "### Emojis to Words\n",
        "\n",
        "Now let us do the same for Emojis as well. Neel Shah has put together a list of emojis with the corresponding words as well as part of his Github repo. We are going to make use of this dictionary to convert the emojis to corresponding words.\n",
        "\n",
        "Again this conversion might be better than emoji removal for certain use cases. Please use the one that is suitable for the use case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m--g7e3uTjuL"
      },
      "source": [
        "# https://www.kaggle.com/pocooo/nlp-data-preprocessing-all-you-need\n",
        "\n",
        "EMO_UNICODE = {\n",
        "    u':1st_place_medal:': u'\\U0001F947',\n",
        "    u':2nd_place_medal:': u'\\U0001F948',\n",
        "    u':3rd_place_medal:': u'\\U0001F949',\n",
        "    u':AB_button_(blood_type):': u'\\U0001F18E',\n",
        "    u':ATM_sign:': u'\\U0001F3E7',\n",
        "    u':A_button_(blood_type):': u'\\U0001F170',\n",
        "    ...\n",
        "\n",
        "UNICODE_EMO = {v: k for k, v in EMO_UNICODE.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w6HOLeJUgUp"
      },
      "source": [
        "### Numbers to Word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy-Uj2A-UlPe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kddb1xJRUiyv"
      },
      "source": [
        "### Spelling Corrections\n",
        "\n",
        "One another important text preprocessing step is spelling correction. Typos are common in text data and we might want to correct those spelling mistakes before we do our analysis.\n",
        "\n",
        "If we are interested in wrinting a spell corrector of our own, we can probably start with famous code from Peter Norvig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA_aCS5aUlox"
      },
      "source": [
        "# https://www.kaggle.com/pocooo/nlp-data-preprocessing-all-you-need#Removal-of-URLs\n",
        "\n",
        "!pip install pyspellchecker\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "def correct_spellings(text):\n",
        "    corrected_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgnJjMm69jBC"
      },
      "source": [
        "### Remove Numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWgb2fQB4wHr"
      },
      "source": [
        "# Remove Numbers\n",
        "\n",
        "def remove_numbers(text):\n",
        "  return ' '.join([i for i in str(text).split() if not i.isdigit()])\n",
        "\n",
        "df['clean_tweets'] = df['message to examine'].apply(lambda x: remove_numbers(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E-zINigJKjx"
      },
      "source": [
        "### Remove Accented Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v64yBTnZJKWR"
      },
      "source": [
        "# remove accented characters\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "for i in range(0, len(df_tmp.excerpt)):\n",
        "    df_tmp.excerpt.loc[i] = remove_accented_chars(df_tmp.excerpt.loc[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-4Ny7HyELV1"
      },
      "source": [
        "### Special Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPlY0ftTEJp9"
      },
      "source": [
        "# Remove special characters\n",
        "\n",
        "def remove_special_chars(text):\n",
        "    re1 = re.compile(r'  +')\n",
        "    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
        "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
        "    return re1.sub(' ', html.unescape(x1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvHRjaDsJSex"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "for i in range(0, len(df_tmp.excerpt)):\n",
        "    df_tmp.excerpt.loc[i] = remove_special_characters(df_tmp.excerpt.loc[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBW8Me-MERvY"
      },
      "source": [
        "### Remove Non-Ascii"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLL2Kd1lEThA"
      },
      "source": [
        "def remove_non_ascii(text):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9hLG3sJEaVl"
      },
      "source": [
        "### Remove Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3bwGr4oEcF-"
      },
      "source": [
        "def remove_whitespaces(text):\n",
        "    return text.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1IYRzoJ9knB"
      },
      "source": [
        "### Lowercase\n",
        "\n",
        "Why should I lower?\n",
        "\n",
        "* This is because words can be saved by changing them to lowercase letters and reducing the number of words through subsequent processes.\n",
        "\n",
        "* Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way\n",
        "\n",
        "* By default, lower casing is done my most of the modern day vecotirzers and tokenizers like sklearn TfidfVectorizer and Keras Tokenizer. So we need to set them to false as needed depending on our use case.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqLNeF5D4wCQ"
      },
      "source": [
        "# Lower casing clean text.\n",
        "\n",
        "train['lower_text'] = train['tokenized_text'].apply(\n",
        "    lambda x: [word.lower() for word in x])\n",
        "\n",
        "---\n",
        "\n",
        "df['clean_tweets'] = df['clean_tweets'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fszvzyLD9men"
      },
      "source": [
        "### Web/URL Links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwnpzuOv4v_J"
      },
      "source": [
        "# Removal of Weblinks\n",
        "\n",
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', text)\n",
        "\n",
        "def remove_html(text):\n",
        "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    return re.sub(html, '', text)\n",
        "---\n",
        "def remove_weblinks(text):\n",
        "  return re.sub(r\"http\\S+\", \"\", text)\n",
        "\n",
        "df['clean_tweets2'] = df['clean_tweets'].apply(lambda x: remove_weblinks(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR1wATbfUBxh"
      },
      "source": [
        "### Remove HTML Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm5W_1LEUDU0"
      },
      "source": [
        "def remove_html(text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ6Kw2FT9pGq"
      },
      "source": [
        "### Emojis\n",
        "\n",
        "With more and more usage of social media platforms, there is an explosion in the usage of emojis in our day to day life as well. Probably we might need to remove these emojis for some of our textual analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5_kT1eYTFOL"
      },
      "source": [
        "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "remove_emoji(\"Put your text or use the function to remove emojis 🔥🔥\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10HxDpgx81HX"
      },
      "source": [
        "# Remove Emojis\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
        "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnOav8CpEoyT"
      },
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yybGHTcY9qLJ"
      },
      "source": [
        "### Twitter @Mentions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuo_uOi04v8T"
      },
      "source": [
        "# Remove Twitter Mentions\n",
        "\n",
        "def remove_twitter(text):\n",
        "  return re.sub('@[\\w]+','',text)\n",
        "\n",
        "df['clean_tweets3'] = df['clean_tweets2'].apply(lambda x: remove_twitter(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjHm5uMM9stz"
      },
      "source": [
        "### Punctuation\n",
        "\n",
        "* Now remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n",
        "\n",
        "* We also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the string.punctuation in python contains the following punctuation symbols\n",
        "\n",
        "* !\"#$%&\\'()*+,-./:;<=>?@[\\]^_{|}~\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPC3DFp0SPga"
      },
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "df[\"column_name\"] = df[\"column_name\"].apply(lambda text: remove_punctuation(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RcmksWg4v5O"
      },
      "source": [
        "# Remove Punctuation\n",
        "\n",
        "\n",
        "def remove_comma(readData):\n",
        "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', readData)\n",
        "    return text\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(table)    \n",
        "---\n",
        "import string\n",
        "\n",
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  return text.translate(str.maketrans('','', PUNCT_TO_REMOVE))\n",
        "\n",
        "df['clean_tweets4'] = df['clean_tweets3'].apply(lambda x: remove_punctuation(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeQu89VF9uLn"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtK98fa-SWnC"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "# It will show all the stopwords in the english library similary we can access stopwords from other languages\n",
        "\", \".join(stopwords.words('english'))\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "df[\"column_name\"] = df[\"column_name\"].apply(lambda text: remove_stopwords(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yAr5ADo52Iy"
      },
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\", \".join(stopwords.words('english'))\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "train['stopwords_removed_text'] = train['lower_text'].apply(\n",
        "    lambda x: [word for word in x if word not in stop])\n",
        "---\n",
        "def remove_stopwords(text):\n",
        "  return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "\n",
        "\n",
        "df['clean_tweets5'] = df['clean_tweets4'].apply(lambda x: remove_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaqPKX1ZX78t"
      },
      "source": [
        "### Add Custom Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9whFklUKX7xR"
      },
      "source": [
        "more_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "stop_words=set(stopwords.words('english')) #nltk package\n",
        "stop_words.update(more_words)\n",
        "\n",
        "remove_words=lambda x: ' '.join([word for word in x.split() if word not in stop_words]) #.join is from package string\n",
        "texts_lr_lc_np_ns=r=texts_lr_lc_np.apply(remove_words)\n",
        "texts_lr_lc_np_ns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unjo0-vO9vqI"
      },
      "source": [
        "### Remove Frequent Words\n",
        "\n",
        "* In the previos preprocessing step, we removed the stopwords based on language information. But say, if we have a domain specific corpus, we might also have some frequent words which are of not so much importance to us.\n",
        "\n",
        "* So this step is to remove the frequent words in the given corpus. If we use something like tfidf, this is automatically taken care of.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyabFvwyShez"
      },
      "source": [
        "from collections import Counter\n",
        "cnt = Counter()\n",
        "for text in df[\"column_name\"].values:\n",
        "    for word in text.split():\n",
        "        cnt[word] += 1\n",
        "        \n",
        "cnt.most_common(10) # gives top 10 most common words with there count\n",
        "\n",
        "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
        "def remove_freqwords(text):\n",
        "    \"\"\"custom function to remove the frequent words\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
        "\n",
        "df[\"column_name\"] = df[\"column_name\"].apply(lambda text: remove_freqwords(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a0B8WmK52FH"
      },
      "source": [
        "# Remove Rare(Freq) Words\n",
        "\n",
        "# Let's remove these\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
        "\n",
        "['clean_tweets6'] = df['clean_tweets5'].apply(lambda x: remove_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4GefB2ASmKf"
      },
      "source": [
        "### Remove Rare Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S74vK9DSnzU"
      },
      "source": [
        "n_rare_words = 10\n",
        "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
        "def remove_rarewords(text):\n",
        "    \"\"\"custom function to remove the rare words\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
        "\n",
        "df[\"column_name\"] = df[\"column_name\"].apply(lambda text: remove_rarewords(text))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HAPPJf0EvEw"
      },
      "source": [
        "## **Clean Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49n3MG8XExpc"
      },
      "source": [
        "def clean_text( text):\n",
        "    text = remove_special_chars(text)\n",
        "    text=remove_html(text)\n",
        "    text=remove_urls(text)\n",
        "    text = remove_non_ascii(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = to_lowercase(text)\n",
        "    \n",
        "\n",
        "    text = replace_numbers(text)\n",
        "    words = text2words(text)\n",
        "    #REMOVE STOPWORDS?\n",
        "    words = remove_stopwords(words, stop_words)\n",
        "    #words = stem_words(words)# Either stem ovocar lemmatize\n",
        "    words = lemmatize_words(words)\n",
        "    words = lemmatize_verbs(words)\n",
        "\n",
        "    return ''.join(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOF3JTxcJl9f"
      },
      "source": [
        "### Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OObf31gJnxp"
      },
      "source": [
        "labeling = {\n",
        "    'positive':1, \n",
        "    'negative':0\n",
        "}\n",
        "\n",
        "data['sentiment'] = data['sentiment'].apply(lambda x : labeling[x])\n",
        "# Output first ten rows\n",
        "data.head(10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcj3FxFoOvn6"
      },
      "source": [
        "### Label Biniarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAVwRF7IOy26"
      },
      "source": [
        "# https://www.kaggle.com/marwanmohammed/imdb-dataset-of-50k-movie-reviews-accuracy-88\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "Corpus = PreProcessing()\n",
        "#Make Catigoral Labels\n",
        "Label =LabelBinarizer().fit_transform(DataSet['sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWOD4Yo59QhL"
      },
      "source": [
        "## **Tokenize**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_njXFmr-ycM"
      },
      "source": [
        "### Sentence Tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2famTZSJ-0D5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Se-taR-w97"
      },
      "source": [
        "### Word Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RpAiOkU9SV5"
      },
      "source": [
        "# Word Tokenize\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenizing the tweet base texts.\n",
        "\n",
        "train['tokenized_text'] = train['text_clean'].apply(word_tokenize)\n",
        "train['tokenized_ST'] = train['ST_clean'].apply(word_tokenize)\n",
        "test['tokenized_text'] = test['text_clean'].apply(word_tokenize)\n",
        "display(train.sample(5))\n",
        "display(test.sample(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACaGSF2D-GIA"
      },
      "source": [
        "### POS Tagging\n",
        "\n",
        "What is Part-of-speech Tagging ?\n",
        "\n",
        "It refers to identifying and tagging the parts of the words in a sentence. It is output in the form of a tuple, and is output in the form of a (word, tag). where tags are POS tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu84Rugp-HmT"
      },
      "source": [
        "# Applying part of speech tags.\n",
        "\n",
        "train['pos_tags_text'] = train['stopwords_removed_text'].apply(nltk.tag.pos_tag)\n",
        "train['pos_tags_ST'] = train['stopwords_removed_ST'].apply(nltk.tag.pos_tag)\n",
        "test['pos_tags_text'] = test['stopwords_removed_text'].apply(nltk.tag.pos_tag)\n",
        "\n",
        "display(train.sample(5))\n",
        "display(test.sample(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy8YnJX6-QDE"
      },
      "source": [
        "### Convert POS to WordNet\n",
        "\n",
        "Why change to Wordnet format?\n",
        "\n",
        "Wordnet is an ontology built by Princeton University about English words in the past, which outlines how they relate to each other. Rather than organizing individual meanings by word like a dictionary, organizing them around the relationship between words can increase their utilization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgGSOS5u-SW7"
      },
      "source": [
        "# Converting part of speeches to wordnet format.\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "\n",
        "train['wordnet_pos_text'] = train['pos_tags_text'].apply(\n",
        "    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbyMlJnQ6UV9"
      },
      "source": [
        "### **Lemmatization**\n",
        "\n",
        "Lemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language.\n",
        "\n",
        "As a result, this one is generally slower than stemming process. So depending on the speed requirement, we can choose to use either stemming or lemmatization.\n",
        "\n",
        "Let us use the WordNetLemmatizer in nltk to lemmatize our sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFuPqlNrS7Lw"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "def lemmatize_words(text):\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "\n",
        "df[\"column_name\"] = df[\"column_name\"].apply(lambda text: lemmatize_words(text))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR9LX0h452Ci"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "train['lemmatized_text'] = train['wordnet_pos_text'].apply(\n",
        "    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
        "---\n",
        "wordnet_map = {\"n\": wordnet.NOUN, \"v\": wordnet.VERB, \"j\": wordnet.ADJ, \"r\": wordnet.ADV}\n",
        "\n",
        "def lemmatize_words(text):\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.VERB)) for word, pos in pos_tagged_text])\n",
        "\n",
        "df[\"text_lemmatized\"] = df['clean_tweets6'].apply(lambda text: lemmatize_words(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9APBEBbV51_W"
      },
      "source": [
        "def lemmatize_words(words):\n",
        "    \"\"\"Lemmatize words in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDP8IyzQ-4Pz"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (From Wikipedia)\n",
        "\n",
        "For example, if there are two words in the corpus history and historical, then stemming will stem the suffix to make them hist. But say in another example, we have two words console and consoling, the stemmer will remove the suffix and make them consol which is not a proper english word.\n",
        "\n",
        "**IF YOU WANT THE TEXT TO PRODUCE SOME MEANINGFUL WORDS THEN USE LEMMATIZATION OVER STEMMING . STEMMING IS USEFUL WHEN WORDS ARE NOT INTERPRETED BY HUMANS WHILE LIKE IN CHATBOTS WORDS MUST READBLE AND UNDERSTANABLE TO USER SO AVOID STEMMING IN SUCH CASES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch0ejrDhSvqX"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Drop the two columns \n",
        "df.drop([\"text_wo_stopfreq\", \"text_wo_stopfreqrare\"], axis=1, inplace=True) \n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "df[\"column_name\"] = df[\"column_name\"].apply(lambda text: stem_words(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZExiJXubEhVx"
      },
      "source": [
        "def stem_words(words):\n",
        "    \"\"\"Stem words in text\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PhM3FwOPB1R"
      },
      "source": [
        "### Keras Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4E0n2-mPBqw"
      },
      "source": [
        "# https://www.kaggle.com/marwanmohammed/imdb-dataset-of-50k-movie-reviews-accuracy-88\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Split Our Data\n",
        "X_train ,X_test , y_train , y_test = train_test_split( Corpus,Label, test_size = 0.25 , random_state = 42)\n",
        "\n",
        "tokenizer = Tokenizer(num_words = Hyper[\"num_words\"] ,oov_token = Hyper[\"oov_token\"])\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "sequance = tokenizer.texts_to_sequences(X_train)\n",
        "pad_training_data = pad_sequences(sequance , maxlen=Hyper[\"maxlen\"] , truncating = Hyper['truncating'])\n",
        "\n",
        "sequance = tokenizer.texts_to_sequences(X_train)\n",
        "pad_training_data = pad_sequences(sequance , maxlen=Hyper[\"maxlen\"], truncating =Hyper['truncating'])\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toEvp14PI15x"
      },
      "source": [
        "## **Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2X8uz-cI4Jx"
      },
      "source": [
        "### Sentence Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W36HpXcnIm8H"
      },
      "source": [
        "# tokenize each word in an excerpt\n",
        "\n",
        "tokenized_sent = []\n",
        "for s in df_tmp.excerpt:\n",
        "    tokenized_sent.append(word_tokenize(s.lower()))\n",
        "tokenized_sent[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLtK3KI0I8Jt"
      },
      "source": [
        "Sentence Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K646okMSItfK"
      },
      "source": [
        "# define the cosine similarity between two words\n",
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl6J8NahVV-q"
      },
      "source": [
        "## **Representations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY-fNmtRVdSl"
      },
      "source": [
        "### **Count Vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teI0KpL0VgFE"
      },
      "source": [
        "# https://www.kaggle.com/pocooo/nlp-data-preprocessing-all-you-need#Removal-of-URLs\n",
        "\n",
        "\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "data_vectors = count_vectorizer.fit_transform(data['text'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uudneYMy7J1M"
      },
      "source": [
        "### **TF/IDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUWbcye_518e"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "cv = TfidfVectorizer()\n",
        "\n",
        "tfidf = cv.fit_transform(xdf['Tweets'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2KPZ2Bk7M_B"
      },
      "source": [
        "# https://www.kaggle.com/rawan21/tabular-text-sentimet-imgfeatures-svd-10\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "bow_tf = TfidfVectorizer( ngram_range=(1,2),max_features=10000)#takeTop10000Freq\n",
        "\n",
        "#bow_tf.get_feature_names()\n",
        "\n",
        "x_text_tfidf = bow_tf.fit_transform(train[\"Description_clean\"])##extTrial\n",
        "\n",
        "x_text_tfidf_test = bow_tf.fit_transform(test[\"Description_clean\"])##extTrial\n",
        "\n",
        "x_text_tfidf\n",
        "\n",
        "x_text_tfidf.toarray()[0].shape\n",
        "\n",
        "#x_text_tfidf=pd.DataFrame(x_text_tfidf.toarray())\n",
        "\n",
        "#x_text_tfidf_test=pd.DataFrame(x_text_tfidf_test.toarray())\n",
        "\n",
        "# VALIDATING TFIDF \n",
        "\n",
        "#X_text=trai_svd[\"Description\"]\n",
        "\n",
        "train.drop([\"Description\", \"Description_clean\"], axis=1,inplace=True)\n",
        "#X_text=trai_svd[\"Description\"]\n",
        "\n",
        "test.drop([\"Description\", \"Description_clean\"], axis=1,inplace=True)\n",
        "train.drop(\"PetID\",axis=1,inplace=True)\n",
        "test.drop(\"PetID\",axis=1,inplace=True)\n",
        "\n",
        "train.drop([\"Name\"], axis=1,inplace=True)\n",
        "#X_text=trai_svd[\"Description\"]\n",
        "\n",
        "test.drop([\"Name\"], axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt1Cipop7M76"
      },
      "source": [
        "# https://www.kaggle.com/cravingsformeow/imdb-2021-topic-modeling-sentiment-analysis\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# define vectorizer parameters\n",
        "# TfidfVectorizer will help us to create tf-idf matrix\n",
        "# max_df : maximum document frequency for the given word\n",
        "# min_df : minimum document frequency for the given word\n",
        "# max_features: maximum number of words\n",
        "# use_idf: if not true, we only calculate tf\n",
        "# stop_words : built-in stop words\n",
        "# tokenizer: how to tokenize the document\n",
        "# ngram_range: (min_value, max_value), eg. (1, 3) means the result will include 1-gram, 2-gram, 3-gram\n",
        "tfidf_model = TfidfVectorizer(max_df=0.99, max_features=1000,\n",
        "                                 min_df=0.01, use_idf=True, stop_words = stopwords,\n",
        "                              tokenizer=tokenization_and_stemming, ngram_range=(1,1))\n",
        "\n",
        "tfidf_matrix = tfidf_model.fit_transform(data) #fit the vectorizer to synopses\n",
        "\n",
        "print (\"In total, there are \" + str(tfidf_matrix.shape[0]) + \\\n",
        "      \" reviews and \" + str(tfidf_matrix.shape[1]) + \" terms.\")\n",
        "\n",
        "\n",
        "\n",
        "# check the parameters\n",
        "tfidf_model.get_params()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYh9X_T3Vi8I"
      },
      "source": [
        "### **Hashing Vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekQjuPTpVnV8"
      },
      "source": [
        "# https://www.kaggle.com/pocooo/nlp-data-preprocessing-all-you-need#Removal-of-URLs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6K_gGqj7Tdm"
      },
      "source": [
        "## **Split Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlyY8BSD7M5I"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tfX_train, tfX_test, tfy_train, tfy_test = train_test_split(tfidf, xdf['Labels'], test_size = 0.2)\n",
        "\n",
        "tfX_train\n",
        "\n",
        "tfX_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CmMPZ7h7M2U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKxpGlNCFkMH"
      },
      "source": [
        "## **Dimensionality Reduction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyd8pXi4FnAN"
      },
      "source": [
        "# https://www.kaggle.com/rawan21/tabular-text-sentimet-imgfeatures-svd-10\n",
        "\n",
        "from sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
        "\n",
        "text_features=[]\n",
        "n_components = 5\n",
        "\n",
        "svd_ = TruncatedSVD(\n",
        "        n_components=n_components, random_state=1337)\n",
        "svd_col = svd_.fit_transform(x_text_tfidf)\n",
        "svd_col = pd.DataFrame(svd_col)\n",
        "  # print(svd_col)\n",
        "svd_col = svd_col.add_prefix('SVD_Description_')\n",
        "\n",
        "#    nmf_col = nmf_.fit_transform(tfidf_col)\n",
        "#    nmf_col = pd.DataFrame(nmf_col)\n",
        "#    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n",
        "\n",
        "text_features.append(svd_col)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRv-IrqwF2ZC"
      },
      "source": [
        "### Explained Variance\n",
        "\n",
        "How do we know that reducing the dimensions of input down to 10 is good or the best we can do? We don’t; 10 was an arbitrary choice.\n",
        "\n",
        "A better approach is to evaluate the same transform and model with different numbers of input features and choose the number of features (amount of dimensionality reduction) that results in the best average performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhCb5CjSFm9j"
      },
      "source": [
        "# https://www.kaggle.com/rawan21/tabular-text-sentimet-imgfeatures-svd-10\n",
        "\n",
        "svd_.explained_variance_ratio_.sum()*100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-6VZOVnFm57"
      },
      "source": [
        "#same4test\n",
        "text_features=[]\n",
        "n_components = 5\n",
        "\n",
        "svd_ = TruncatedSVD(\n",
        "        n_components=n_components, random_state=1337)\n",
        "    #Non-Negative Matrix Factorization (NMF).\n",
        "    #Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. \n",
        "#This factorization can be used for example for dimensionality reduction, source separation or topic extraction.\n",
        "#     nmf_ = NMF(\n",
        "#         n_components=n_components, random_state=1337)\n",
        "svd_col = svd_.fit_transform(x_text_tfidf_test)\n",
        "svd_col = pd.DataFrame(svd_col)\n",
        "  # print(svd_col)\n",
        "svd_col = svd_col.add_prefix('SVD_Description_')\n",
        "\n",
        "#    nmf_col = nmf_.fit_transform(tfidf_col)\n",
        "#    nmf_col = pd.DataFrame(nmf_col)\n",
        "#    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n",
        "\n",
        "text_features.append(svd_col)\n",
        "# Combine all extracted features:\n",
        "text_features = pd.concat(text_features, axis=1)\n",
        "text_features.shape\n",
        "# Concatenate with main DF:\n",
        "test_svd=pd.concat([test, text_features], axis=1)\n",
        "test_svd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV6uLcw6CJek"
      },
      "source": [
        "## **Unsupervised Models and Evalution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReOOzCWvCNO5"
      },
      "source": [
        "### **TextBlob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TjDa5ImYXL8"
      },
      "source": [
        "# https://www.kaggle.com/margaretokon/endsars-eda-sentiment-analysis\n",
        "\n",
        "# getting polarity scores of tweets and storing them in variable 'sentiment_scores'\n",
        "sid=SentimentIntensityAnalyzer()\n",
        "ps=lambda x:sid.polarity_scores(x)\n",
        "sentiment_scores=df.text.apply(ps)\n",
        "sentiment_scores\n",
        "\n",
        "# create the data frame of negative, neutral, positive and compound polarity scroes\n",
        "sentiment_df=pd.DataFrame(data=list(sentiment_scores))\n",
        "display(sentiment_df)\n",
        "\n",
        "# it will label the tweets as neutral if its compound polarity is 0 and positive if its greater than 0 and negative if its less than 0\n",
        "labelize=lambda x:'neutral' if x==0 else('positive' if x>0 else 'negative')\n",
        "sentiment_df['label']=sentiment_df.compound.apply(labelize)\n",
        "display(sentiment_df.head(10))\n",
        "\n",
        "# join the dataframes\n",
        "display(df.head(5))\n",
        "data=df.join(sentiment_df.label)\n",
        "sentiment_df = df.join(sentiment_df)\n",
        "display(data.head(5))\n",
        "\n",
        "# plot\n",
        "counts_df=data.label.value_counts().reset_index()\n",
        "display(counts_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBNvmg7MY71X"
      },
      "source": [
        "Diachronic SA TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSg-fAYlY21v"
      },
      "source": [
        "# https://www.kaggle.com/margaretokon/endsars-eda-sentiment-analysis\n",
        "\n",
        "# Diachromic SA TextBlob\n",
        "\n",
        "# px.line(data_agg,x='date',y='counts',color='label',title='Daily Tweet Sentimental Analysis')\n",
        "fig = plt.figure(figsize = (15, 7))\n",
        "plt.plot(pos['date'],pos['counts'], label='postivie')\n",
        "plt.plot(neg['date'],neg['counts'], label='negative')\n",
        "plt.plot(neu['date'],neu['counts'], label='neutral')\n",
        " \n",
        "# Add labels and title\n",
        "plt.title(\"Daily Tweet Sentimental Analysis\")\n",
        "plt.xlabel(\"date\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "172MYYMPC4BD"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "from textblob import TextBlob\n",
        "from nltk import tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7j16F40CMrI"
      },
      "source": [
        "# TextBlob\n",
        "\n",
        "def get_polarity(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "df_comments_reduced['Polarity'] = df_comments_reduced['comments'].apply(get_polarity)\n",
        "\n",
        "df_comments_reduced['Sentiment_Type Textblob']=''\n",
        "df_comments_reduced.loc[df_comments_reduced.Polarity>0,'Sentiment_Type Textblob']='POSITIVE'\n",
        "df_comments_reduced.loc[df_comments_reduced.Polarity==0,'Sentiment_Type Textblob']='NEUTRAL'\n",
        "df_comments_reduced.loc[df_comments_reduced.Polarity<0,'Sentiment_Type Textblob']='NEGATIVE'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t7cQPsRCphP"
      },
      "source": [
        "df_comments_reduced_sentimental = df_comments_reduced[['listing_id','comments', 'language', 'Sentiment_Type Textblob']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vHPidIKCpVn"
      },
      "source": [
        "df_comments_reduced_sentimental.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovW1TWWQCZX8"
      },
      "source": [
        "pd.set_option(\"display.max_colwidth\", 500)\n",
        "df_comments_reduced_sentimental.loc[(df_comments_reduced_sentimental.language=='en') &(df_comments_reduced_sentimental['Sentiment_Type Textblob']=='NEGATIVE')].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8SFqPVvCwch"
      },
      "source": [
        "### **VADER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0vktnvMCZUu"
      },
      "source": [
        "Vader\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "df_comments_reduced['scores'] = df_comments_reduced['comments'].apply(lambda comments: sid.polarity_scores(comments))\n",
        "df_comments_reduced.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG_f_vX-CZRo"
      },
      "source": [
        "df_comments_reduced['compound'] = df_comments_reduced['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "df_comments_reduced['Sentiment_Type VADER']=''\n",
        "df_comments_reduced.loc[df_comments_reduced.compound>0,'Sentiment_Type VADER']='POSITIVE'\n",
        "df_comments_reduced.loc[df_comments_reduced.compound==0,'Sentiment_Type VADER']='NEUTRAL'\n",
        "df_comments_reduced.loc[df_comments_reduced.compound<0,'Sentiment_Type VADER']='NEGATIVE'\n",
        "\n",
        "df_comments_reduced.loc[df_comments_reduced.compound>0,'Sentiment_Value VADER']=1\n",
        "df_comments_reduced.loc[df_comments_reduced.compound==0,'Sentiment_Value VADER']=0\n",
        "df_comments_reduced.loc[df_comments_reduced.compound<0,'Sentiment_Value VADER']=-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnhUpgvOCZOX"
      },
      "source": [
        "df_comments_reduced.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHp3Tle-DOnx"
      },
      "source": [
        "# VADER performance metrics on 5 language hotel reviews\n",
        "\n",
        "# https://www.kaggle.com/sergiosouza/airbnb-porto-sentimental-analysis "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQwdqSs4Kd_k"
      },
      "source": [
        "### **BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8FhZSH4Kgqt"
      },
      "source": [
        "# https://www.kaggle.com/kritanjalijain/movie-review-sentiment-analysis-eda-bert\n",
        "\n",
        "# Great explaination of details/IMDB w/sup metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF8zSkGP7dHn"
      },
      "source": [
        "## **Supervised Models and Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo2nEfOU7lhW"
      },
      "source": [
        "### Multinomial Naive Bayes (Supervised)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emN8jhKPacln"
      },
      "source": [
        "# https://www.kaggle.com/lykin22/sentiment-analysis-djia-stock-news-headlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdkbHfB27Mza"
      },
      "source": [
        "## NaiveBayes\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "\n",
        "mnb.fit(tfX_train, tfy_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m72zml4r7tpO"
      },
      "source": [
        "# Accuracy\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "y_pred_mnb = mnb.predict(tfX_test)\n",
        "\n",
        "print(f'Accuracy score is : {accuracy_score(tfy_test, y_pred_mnb)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5gYxnvs7tmx"
      },
      "source": [
        "# Confusion Matrix\n",
        "\n",
        "cf = confusion_matrix(tfy_test, y_pred_mnb, labels = [1,0])\n",
        "cf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEHPJrnv7tkK"
      },
      "source": [
        "# Graphical Confusion Matrix\n",
        "\n",
        "x_axis_labels = [\"Positive(1)\",\"Negative(0)\"]\n",
        "y_axis_labels = [\"Positive(1)\",\"Negative(0)\"]\n",
        "\n",
        "plt.figure(figsize = (8,6))\n",
        "sns.set(font_scale=1)\n",
        "sns.heatmap(cf, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, fmt='g',annot_kws = {'size': 16})\n",
        "plt.xlabel(\"Actual Class\", fontsize = 20)\n",
        "plt.ylabel(\"Predicted Class\", fontsize = 20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6jtU8hadDzY"
      },
      "source": [
        "### GaussianNB/MultinomialNB/BernoulliNB (GridSearch/Deploy Pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNLU_DnSdDmZ"
      },
      "source": [
        "# https://www.kaggle.com/sns5154/movie-review-sentiment-analysis-accuracy-84-74"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmyhGgCKXCB8"
      },
      "source": [
        "### Logistic Regression (Amazon GridSearchCV/Oversampling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4rRM2NaXD-Z"
      },
      "source": [
        "# https://www.kaggle.com/danala26/nlp-amazon-sentiment-analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGWgsYP2cwLu"
      },
      "source": [
        "### RNN (Classify Emotions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9LCazf_czrK"
      },
      "source": [
        "# https://www.kaggle.com/anetakovacheva/classifying-emotions-with-a-rnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP_81y14cc46"
      },
      "source": [
        "### RNN and LSTM (Sarcasm)**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByZGOuSjcgC9"
      },
      "source": [
        "# https://www.kaggle.com/shivam017arora/sarcasm-detection-w-lstms-beginner-friendly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-WZfnZ98Aln"
      },
      "source": [
        "### LSTM with Embeddings (Supervised)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mwg-l1aamfp"
      },
      "source": [
        "# Amazon Reviews https://www.kaggle.com/devson/sentimentanalysisbasicandadvanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU6Fmatk7thJ"
      },
      "source": [
        "# DJIA News: https://www.kaggle.com/lykin22/sentiment-analysis-djia-stock-news-headlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUmO_zBOGY4w"
      },
      "source": [
        "### LGB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWOxKRFIGYtV"
      },
      "source": [
        "# https://www.kaggle.com/rawan21/tabular-text-sentimet-imgfeatures-svd-10\n",
        "\n",
        "n_fold = 3\n",
        "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=15)\n",
        "\n",
        "def train_model(X, X_test, y=y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False,\n",
        "                averaging='usual', make_oof=False):\n",
        "    result_dict = {}\n",
        "    if make_oof:\n",
        "        oof = np.zeros((len(X), 5))\n",
        "    prediction = np.zeros((len(X_test), 5))\n",
        "    scores = []\n",
        "    feature_importance = pd.DataFrame()\n",
        "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
        "        gc.collect()\n",
        "        print('Fold', fold_n + 1, 'started at', time.ctime())\n",
        "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "        if model_type == 'lgb':\n",
        "            train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_cols)\n",
        "            valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_cols)\n",
        "\n",
        "            model = lgb.train(params,\n",
        "                              train_data,\n",
        "                              num_boost_round=20000,\n",
        "                              valid_sets=[train_data, valid_data],\n",
        "                              verbose_eval=500,\n",
        "                              early_stopping_rounds=200)\n",
        "\n",
        "            del train_data, valid_data\n",
        "\n",
        "            y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "            del X_valid\n",
        "            gc.collect()\n",
        "            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "\n",
        "        if model_type == 'xgb':\n",
        "            train_data = xgb.DMatrix(data=X_train, label=y_train)\n",
        "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n",
        "\n",
        "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
        "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200,\n",
        "                              verbose_eval=500, params=params)\n",
        "            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n",
        "            y_pred = model.predict(xgb.DMatrix(X_test), ntree_limit=model.best_ntree_limit)\n",
        "            \n",
        "        if model_type == 'lcv':\n",
        "            model = LogisticRegressionCV(scoring='neg_log_loss', cv=3, multi_class='multinomial')\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            y_pred_valid = model.predict_proba(X_valid)\n",
        "            y_pred = model.predict_proba(X_test)\n",
        "\n",
        "        if model_type == 'cat':\n",
        "            model = CatBoostClassifier(iterations=500, loss_function='MultiClass')\n",
        "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[],\n",
        "                      use_best_model=False, verbose=False)\n",
        "\n",
        "            y_pred_valid = model.predict(X_valid)\n",
        "            y_pred = model.predict_proba(X_test)\n",
        "\n",
        "        if make_oof:\n",
        "            oof[valid_index] = y_pred_valid\n",
        "            \n",
        "            \n",
        "        scores.append(kappa(y_valid, y_pred_valid.argmax(1)))\n",
        "        print('Fold kappa:', kappa(y_valid, y_pred_valid.argmax(1)))\n",
        "        print('')\n",
        "\n",
        "        if averaging == 'usual':\n",
        "            prediction += y_pred\n",
        "        elif averaging == 'rank':\n",
        "            prediction += pd.Series(y_pred).rank().values\n",
        "\n",
        "        if model_type == 'lgb':\n",
        "            # feature importance\n",
        "            fold_importance = pd.DataFrame()\n",
        "            fold_importance[\"feature\"] = X.columns\n",
        "            fold_importance[\"importance\"] = model.feature_importance()\n",
        "            fold_importance[\"fold\"] = fold_n + 1\n",
        "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
        "\n",
        "    prediction /= n_fold\n",
        "\n",
        "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
        "\n",
        "    if model_type == 'lgb':\n",
        "\n",
        "        if plot_feature_importance:\n",
        "            feature_importance[\"importance\"] /= n_fold\n",
        "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
        "                by=\"importance\", ascending=False)[:50].index\n",
        "\n",
        "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
        "\n",
        "            plt.figure(figsize=(16, 12))\n",
        "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
        "            plt.title('LGB Features (avg over folds)')\n",
        "\n",
        "            result_dict['feature_importance'] = feature_importance\n",
        "\n",
        "    result_dict['prediction'] = prediction\n",
        "    if make_oof:\n",
        "        result_dict['oof'] = oof\n",
        "\n",
        "    return result_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq_2K5RhGlcj"
      },
      "source": [
        "Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qToymHr_GYqC"
      },
      "source": [
        "params = {'num_leaves': 512,\n",
        "        #  'min_data_in_leaf': 60,\n",
        "         'objective': 'multiclass',\n",
        "         'max_depth': -1,\n",
        "         'learning_rate': 0.01,\n",
        "         \"boosting\": \"gbdt\",\n",
        "         \"feature_fraction\": 0.9,\n",
        "         \"bagging_freq\": 3,#tells LightGBM “re-sample without replacement every 3 iterations, \n",
        "         \"bagging_fraction\": 0.9,#and draw samples of 90% of the training data”.\n",
        "         \"bagging_seed\": 11,\n",
        "         \"random_state\": 42,\n",
        "         \"verbosity\": -1,\n",
        "         \"num_class\": 5}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoMJ0LozaVCb"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbjlp-J2aUFP"
      },
      "source": [
        "# https://www.kaggle.com/lykin22/sentiment-analysis-djia-stock-news-headlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcmbhXZzPcwZ"
      },
      "source": [
        "### Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6S4hEtgPcjc"
      },
      "source": [
        "# https://www.kaggle.com/marwanmohammed/imdb-dataset-of-50k-movie-reviews-accuracy-88\n",
        "\n",
        "# IMDB SA with Keras binary_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5SEKK0YNAHX"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqx_YflkNBVY"
      },
      "source": [
        "# Metric is labeled Sentiment(News headline) \n",
        "\n",
        "# https://www.kaggle.com/mantej/sentiment-analysis-eda-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoIdx8NUfKFq"
      },
      "source": [
        "# News Train then Predict\n",
        "\n",
        "# https://www.kaggle.com/mantej/sentiment-analysis-eda-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_z6BBfnbALg"
      },
      "source": [
        "# Cov19 SA Variance with Time/Vax\n",
        "\n",
        "# https://www.kaggle.com/raghav2002sharma/sentiment-analysis-of-covid19-vaccination-tweets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9eioTGYdo3V"
      },
      "source": [
        "# IMDB SA with TFIDF/NB Baseline\n",
        "\n",
        "# https://www.kaggle.com/houssemayed/bert-for-sentiment-analysis-imdb\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq3k7ibJQQgo"
      },
      "source": [
        "### TRAX (Emotion classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL-IGuo2QR_Q"
      },
      "source": [
        "# https://www.kaggle.com/muhammadtalib/emotions-classification-sequence-model-trax?select=train.txt\n",
        "\n",
        "# https://github.com/google/trax\n",
        "\n",
        "data[\"sentiment\"] = data[\"sentiment\"].map({'joy':0,'anger':1,'love':2,'sadness':3,'fear':4,'surprise':5})\n",
        "\n",
        "def process_sentence(sentence):\n",
        "    sentence_tokens = word_tokenize(sentence)\n",
        "    sentences_clean = []\n",
        "    stemmer = PorterStemmer()\n",
        "    for word in sentence_tokens:\n",
        "        if (word not in stopwords.words('english') and word not in string.punctuation):\n",
        "            stem_word = stemmer.stem(word) # stemming word\n",
        "            sentences_clean.append(stem_word)\n",
        "    return sentences_clean\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUPQ9_drcKQd"
      },
      "source": [
        "## **Visualizations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw-NOrLecMX6"
      },
      "source": [
        "# https://www.kaggle.com/sarahgonzalez/reddit-news-stock-market-analysis\n",
        "\n",
        "# Overlay and Text annotations"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}