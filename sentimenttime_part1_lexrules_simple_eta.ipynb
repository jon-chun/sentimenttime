{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentimenttime_part1_lexrules_simple_eta.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "u932nJxdh0Ac",
        "9wiSBHxoOGZz",
        "8owpM75RILKn",
        "yXwKR4gA8Ouk",
        "PVCkjat0vffd",
        "YFC8GTnw6HrG",
        "j2tIua7tTSRz",
        "QZjqwTvU76AR",
        "NA3dWsnF78mi",
        "SkNZVk128jV9",
        "dUcANLM_8mtT",
        "Cn4KQYpH3glK",
        "jRTjCPLb8cbB",
        "gAEiglIPDfFI",
        "iCN4c-G48e7-",
        "G2blGfVlKb_s",
        "wsaziON_Z263",
        "AIGQgWvyOtg6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jon-chun/sentimenttime/blob/main/sentimenttime_part1_lexrules_simple_eta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibHFmIWoU3Vx"
      },
      "source": [
        "# **An Analytic Methodology to Extract Narratives from Text: Using Sentiment Analysis to find the Arcs and Crux Points in Novels, Social Media and Chat Transcripts**\n",
        "\n",
        "By: Jon Chun\n",
        "12 Jun 2021\n",
        "\n",
        "References:\n",
        "\n",
        "* Coming...\n",
        "\n",
        "TODO:\n",
        "* Demo datafiles\n",
        "* Error detection around Crux points context (out of bounds)\n",
        "* lex_discrete2continous (research binary->gaussian transformation fn)\n",
        "* Text Preprocessing hints/tips/flowchart\n",
        "* Clearly document workflow and partition across notebooks/libraries\n",
        "* Code review and extraction to libraries\n",
        "* Corpus ingestion for any format\n",
        "* XAI (mlm false peak 1717SyuzhetR/1732SentimentR/1797robertalg15 adam watches war argument at dinner) \n",
        "* Centralize and Standardize Model name lists\n",
        "* Normalize model SA Series lengths\n",
        "* Standardize all SA Series with the same method\n",
        "* Seamless report generation/file saving\n",
        "* Get raw text from SentimentR\n",
        "* Filter out non-printable characters\n",
        "* Roll-over Crux-Points (SentNo+Sent/Parag) (plotly)\n",
        "* Label/Roll-over Chapter/Sect No at Boundries\n",
        "* Generate Report PDF/csv\n",
        "* Option to select raw or discrete2continous transformation (Bing)\n",
        "* Annotation functionality + Share/Collaboration of findings/reseearch\n",
        "* clusters, centroids = kmeans1d.cluster(np.array(corpus_sentimentr_df['jockers_rinker']), k)\n",
        "* plotly prefered library to save dynamic images: kaleido\n",
        "* Correlation heatmaps: Justify choice of Spearman, Pearson, or other algo\n",
        "\n",
        "Facts:\n",
        "* SyuzhetR vs SentimentTime Clean/Preprocess\n",
        "* V.Woolf - To The Lighthouse\n",
        "* SyuzhetR Clean: 3511 (SyuzhetR Preprocessed) Sentences (SentimentTime Preprocessed) 3403\n",
        "* SentimentTime Clean: (Raw) 3402  (Clean) 3402\n",
        "\n",
        "\n",
        "Preprocessing of Corpus Textfile\n",
        "* Put headers in ALL CAPS\n",
        "* Put \\n\\n between each CHAPTER/BOOK or SECTION header or Paragraphs\n",
        "* Keep your format/spacing consistent\n",
        "* Try to use utf-8 (not cp1252 (e.g. \\n <- \\n\\r)\n",
        "* No leading blank lines, one trailing blank line at end of textfile\n",
        "* Check for illegal, non-printable or other problematic code (e.g. curly single/double quotes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X05awke6vYdu"
      },
      "source": [
        "# Test\n",
        "\n",
        "# !pip install kmeans1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBWhXS9bxrtd"
      },
      "source": [
        "\"\"\"\n",
        "import kmeans1d\n",
        "\n",
        "k = corpus_sentimentr_df.shape[0]//500  \n",
        "\n",
        "clusters, centroids = kmeans1d.cluster(np.array(corpus_sentimentr_df['jockers_rinker']), k)\n",
        "type(clusters)\n",
        "\n",
        "[[x,clusters.count(x)] for x in set(clusters)]\n",
        "centroids\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u932nJxdh0Ac"
      },
      "source": [
        "# Configuration (Auto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_a9eQyBiiTG"
      },
      "source": [
        "**Global Configuration Constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT2MyyjpihFj"
      },
      "source": [
        "# Hardcoded Sentiment Analysis Models\n",
        "\n",
        "CORPUS_ENCODING = 'utf-8' # Default character/text encoding scheme (others: 'utf-8', but often 'iso-8859-1', 'windows-1252', 'cp1252', or 'ascii')\n",
        "\n",
        "MODELS_LS = ['vader','textblob','stanza','afinn','bing','sentimentr','syuzhet','pattern','sentiword','senticnet','nrc']\n",
        "            \n",
        "# Minimum lengths for Sentences and Paragraphs\n",
        "#   (Shorter Sents/Parags will be deleted)\n",
        "\n",
        "MIN_CHAP_LEN = 50\n",
        "MIN_SECT_LEN = 25  # Minimum char length to be included in section DataFrame\n",
        "MIN_PARAG_LEN = 2\n",
        "MIN_SENT_LEN = 2\n",
        "\n",
        "# Simple Moving Average/Rolling Mean \n",
        "roll_str = \"roll10\" # Default 10% Rolling Mean Window \n",
        "\n",
        "# Min/Max statistics on each lexicon's sentiment values applied to corpus\n",
        "corpus_lexicons_stats_dt = {}\n",
        "corpus_cruxes_dt = {}\n",
        "\n",
        "# Crux Points Dict key:model, value:list of crux point tuples (x,y)\n",
        "corpus_cruxes_all_dt = {}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiapgHzZdm4x"
      },
      "source": [
        "groups_ls = ['models_baseline_ls',\n",
        "                'models_sentimentr_ls',\n",
        "                'models_syuzhetr_ls',\n",
        "                'models_transformer_ls']\n",
        "\n",
        "models_baseline_ls = ['sentimentr',\n",
        "                      'syuzhet',\n",
        "                      'bing',\n",
        "                      'sentiword',\n",
        "                      'senticnet',\n",
        "                      'nrc',\n",
        "                      'afinn',\n",
        "                      'vader',\n",
        "                      'textblob',\n",
        "                      'flair',\n",
        "                      'pattern',\n",
        "                      'stanza']\n",
        "\n",
        "models_sentimentr_ls = ['jockers_rinker',\n",
        "                        'jockers',\n",
        "                        'huliu',\n",
        "                        'senticnet',\n",
        "                        'sentiword',\n",
        "                        'nrc',\n",
        "                        'lmcd']\n",
        "\n",
        "models_syuzhetr_ls = ['syuzhet',\n",
        "                      'bing',\n",
        "                      'afinn',\n",
        "                      'nrc']\n",
        "\n",
        "models_transformer_ls = ['roberta15lg', \n",
        "                         'nlptown', \n",
        "                         'yelp', \n",
        "                         'hinglish',\n",
        "                         'imdb2way', \n",
        "                         'huggingface', \n",
        "                         't5imdb50k', \n",
        "                         'robertaxml8lang']\n",
        "\n",
        "# Temporarily redefine from English to French Transformer Models\n",
        "# models_transformer_ls = ['flaubert', 'nlptown', 'robertaxml8lang']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOPa6HH-OjZp"
      },
      "source": [
        "**Install Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drpZJilASHUN"
      },
      "source": [
        "# fast detection of character set encoding for text/files\n",
        "\n",
        "!pip install cchardet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA7Nw-SA_si1"
      },
      "source": [
        "!pip install pysbd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEjSzsusOOJ-"
      },
      "source": [
        "# common ML code\n",
        "\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ0UVdasuTTS"
      },
      "source": [
        "%pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MbHfUCz6qTQ"
      },
      "source": [
        "!pip install pysbd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENUk4UsK6qTV"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Nis-KA6qTY"
      },
      "source": [
        "import pysbd\n",
        "import spacy\n",
        "from pysbd.utils import PySBDFactory\n",
        "\n",
        "# nlp = spacy.blank('en')\n",
        "\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# explicitly adding component to pipeline\n",
        "# (recommended - makes it more readable to tell what's going on)\n",
        "nlp.add_pipe(PySBDFactory(nlp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxLPTsA_6qTa"
      },
      "source": [
        "\n",
        "\n",
        "# or you can use it implicitly with keyword\n",
        "# pysbd = nlp.create_pipe('pysbd')\n",
        "# nlp.add_pipe(pysbd)\n",
        "\n",
        "doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n",
        "print(list(doc.sents))\n",
        "# [My name is Jonas E. Smith., Please turn to p. 55.]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmtqmvu6OlR9"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7bf4lfgwMEz"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import io\n",
        "import glob\n",
        "import json\n",
        "import contextlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOmyq4h7OOFi"
      },
      "source": [
        "# IMPORT LIBRARIES\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OslLdEsvOuFU"
      },
      "source": [
        "import re\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YelenXz5BcmE"
      },
      "source": [
        "from itertools import cycle  # For plotly\n",
        "\n",
        "import collections\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Suximbjnw8D"
      },
      "source": [
        "# Import libraries for logging\n",
        "\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import time                     # (TODO: check no dependencies and delete)\n",
        "from time import gmtime, strftime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPZmScjVDYyw"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Download for sentence tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download for nltk/VADER sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u34kPKO0_xF_"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm') # Load the English Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMl2mfF8Haw8"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler   # To normalize time series\n",
        "from sklearn.preprocessing import StandardScaler # To Standardize time series: center(sub mean) and rescale within 1 SD (only for well-behaved guassian distributions)\n",
        "from sklearn.preprocessing import RobustScaler   # To Standardize time series: center(sub median) and rescale within 25%-75% (1st-3rd) IQR (better for noisy, outliers distributions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nckwluDXwa1c"
      },
      "source": [
        "minmax_scaler = MinMaxScaler()\n",
        "mean_std_scaler = StandardScaler()\n",
        "median_iqr_scaler = RobustScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U589lvKXmFV-"
      },
      "source": [
        "# Zoom interpolates new datapoints between existing datapoints to expand a time series \n",
        "\n",
        "from scipy.ndimage.interpolation import zoom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wcZfSOuBlW7"
      },
      "source": [
        "from scipy import interpolate\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy import signal\n",
        "from scipy.signal import argrelextrema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY3UyvYjAvDN"
      },
      "source": [
        "from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess\n",
        "from statsmodels import robust"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02LJQlYpgQGs"
      },
      "source": [
        "corpus_sects_df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcSc4jsggSy2"
      },
      "source": [
        "**Define Library-Dependent Objects**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjGN2sN3uRpN"
      },
      "source": [
        "import contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AllIwMngDGC3"
      },
      "source": [
        "# Necessary to define before defining Utility Functions using these DataFrames\n",
        "\n",
        "corpus_sents_df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwl0MBDyOwtX"
      },
      "source": [
        "**Configure Jupyter Notebook**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APCau-T26XQ3"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def my_css():\n",
        "   display(HTML(\"\"\"<style>table.dataframe td{white-space: nowrap;}</style>\"\"\"))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', my_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD1cyqWsfjxA"
      },
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzfybE5kfmE-"
      },
      "source": [
        "# Configure matplotlib and seaborn\n",
        "\n",
        "# Plotting pretty figures and avoid blurry images\n",
        "# %config InlineBackend.figure_format = 'retina'\n",
        "# Larger scale for plots in notebooks\n",
        "# sns.set_context('talk')\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [16, 8]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rc('figure', facecolor='white')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIIjSbyeP2fg"
      },
      "source": [
        "# Configure Jupyter\n",
        "\n",
        "# Enable multiple outputs from one code cell\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets, interactive\n",
        "\n",
        "# Configure Google Colab\n",
        "\n",
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS_El2PiQlyP"
      },
      "source": [
        "# Text wrap\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuM_qnOHUil5"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxQtrH196gl3"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def my_css():\n",
        "   display(HTML(\"\"\"<style>table.dataframe td{white-space: nowrap;}</style>\"\"\"))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', my_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIPzbt5Ikldp"
      },
      "source": [
        "# with pd.option_context('display.max_colwidth', None):\n",
        "#   display(corpus_transformer_df['sent_raw'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDR54Pbg5zqz"
      },
      "source": [
        "# with pd.option_context('display.max_colwidth', None):\n",
        "#   display(corpus_sentimentr_df.iloc[:10]['sent_raw'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dLkfn4KFmDf"
      },
      "source": [
        "**Configuration Details Snapshot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FNPovQBFZky"
      },
      "source": [
        "# Snap Shot of Time, Machine, Data and Library/Version Blueprint\n",
        "# TODO:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wiSBHxoOGZz"
      },
      "source": [
        "# **Delete/Reset Main DataStructure**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZXoa5lANBCM"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3b35AXqNF7w"
      },
      "source": [
        "%reset_selective corpus_sents_df corpus_parags_df corpus_sects_df corpus_chaps_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRiqyCInNOnY"
      },
      "source": [
        "%reset_selective corpus_sentimentr corpus_syuzhetr_df corpus_transformer_df corpus_unified_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP8j6XqSNbhr"
      },
      "source": [
        "%reset_selective temp_baseline_df temp_df temp_sentimentr_df temp_syuzhetr_df temp_transformer_df unified_crux_df corr_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji9GwSuNZRdi"
      },
      "source": [
        "# **Connect to Corpus Text files**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KRfiXXQOZcq"
      },
      "source": [
        "## **Option (a): Connect to Google gDrive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G64etjAUOOSm"
      },
      "source": [
        "# Connect to Google gDrive\n",
        "\n",
        "# Flag to indicate first run through code \n",
        "flag_first_run = True\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0fvFZq-eFaw"
      },
      "source": [
        "# Select the Corpus subdirectory on your Google gDrive\n",
        "\n",
        "# Done\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_mrsdalloway\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_thewaves\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_orlando\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/ddefoe_robinsoncrusoe\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vnabokov_palefire\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/jjoyce_portraitoftheartist\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/fscottfitzgerald_thegreatgatsby\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/jkrowling_harrypotter\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/hjames_portraitofalady\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/emforster_howardsend\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/cdickens_greatexpectations\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/staugustine_confessions\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/fdouglass_narrativelifeslave\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/jconrad_heartofdarkness\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/homer_odyssey\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/tmorrison_beloved\" #@param {type:\"string\"}\n",
        "gdrive_subdir = \"./research/2021/sa_book_code/books_sa/mproust_time\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "# Current\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/homer_odyssey\" #@param {type:\"string\"}\n",
        "\n",
        "# To do\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\" #@param {type:\"string\"}\n",
        "\n",
        "CORPUS_SUBDIR = gdrive_subdir\n",
        "corpus_filename = CORPUS_SUBDIR\n",
        "\n",
        "# Change to working subdirectory\n",
        "if flag_first_run == True:\n",
        "  full_path_str = gdrive_subdir\n",
        "  flag_first_run = False\n",
        "else:\n",
        "  full_path_str = f'/gdrive/MyDrive{gdrive_subdir[1:]}'\n",
        "\n",
        "%cd $full_path_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKV1uMBEO8TR"
      },
      "source": [
        "## **Option (b): Upload Corpus Textfile**\n",
        "\n",
        "***Only do this if your Google subdirectory doesn't already contain a plain text file of your Corpus or you wish to overwrite it and use a newer version***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH6dlB2fO6Ln"
      },
      "source": [
        "# Execute this code cell to upload plain text file of corpus\n",
        "#   Should be *.txt format with paragraphs separated by at least 2 newlines\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3LORQ4fRGBW"
      },
      "source": [
        "# Verify file was uploaded\n",
        "\n",
        "# Get uploaded filename\n",
        "corpus_filename = list(uploaded.keys())[0]\n",
        "print(f'Uploaded Corpus filename is: {corpus_filename}')\n",
        "CORPUS_FILENAME = corpus_filename\n",
        "\n",
        "!ls -al $corpus_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsm8awD4AZ8O"
      },
      "source": [
        "# **Configuration (Manual)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfilGg6Mkxnd"
      },
      "source": [
        "# Verify subdirectory change\n",
        "\n",
        "!pwd\n",
        "!ls -altr *\n",
        "\n",
        "# TODO: Intelligently automate the filling of form based upon directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP3WLEv_g5aq"
      },
      "source": [
        "# CORPUS_TITLE = 'Heart of Darkness' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Joseph Conrad\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"jconrad_heartofdarkness.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/jconrad_heartofdarkness\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Great Expectations' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Charles Dickens\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"cdickens_greatexpectations.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/cdickens_greatexpectations\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Howards End' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"EM Forster\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"emforster_howardsend.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/emforster_howardsend\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Portrait of a Lady' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Henry James\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"hjames_portraitofalady.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/hjames_portraitofalady\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Middlemarch' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"George Eliot\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"geliot_middlemarch.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Robinson Crusoe' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Daniel Defoe\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"ddefoe_robinsoncrusoe.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/ddefoe_robinsoncrusoe\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Middlemarch' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"George Eliot\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"geliot_middlemarch_wprelude.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\"  #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Palefire' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Vladimir Nabokov\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"vnabokov_palefire.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vnabokov_palefire\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'The Great Gatsby' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"F. Scott Fitzgerald\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"fscottfitzgerald_thegreatgatsby.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/fscottfitzgerald_thegreatgatsby\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'The Socerers Stone' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"J.K. Rowling\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"hpotter1_sorcerersstone.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/jkrowling_harrypotter\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Portrait of the Artist as a Young Man' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"James Joyce\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"jjoyce_portraitoftheartist.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/jjoyce_portraitoftheartist\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Confessions' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Saint Augustine\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"staugustine_confessions.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/staugustine_confessions\"  #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Narrative Life of Frederick Douglass' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Frederick Douglass\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"fdouglass_narrative.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/fdouglass_narrativelifeslave\"  #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Orlando' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"vwoolf_orlando.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_orlando\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'The Waves' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"vwoolf_thewaves.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_thewaves\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Mrs. Dalloway' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"vwoolf_mrsdalloway.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_mrsdalloway\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'To The Lighthouse' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"vwoolf_tothelighthouse.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'The Odyssey' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Homer SButler\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"sbutler_odyssey.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/sbutler_odyssey\"  #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'The Odyssey' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Homer EWilson\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"ewilson_odyssey.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/homer_odyssey\"  #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Beloved' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Toni Morrison\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"tmorrison_beloved.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/tmorrison_belovedy\"  #@param {type:\"string\"}\n",
        "\n",
        "CORPUS_TITLE = 'The Guermantes Way - English' #@param {type:\"string\"}\n",
        "CORPUS_AUTHOR = \"Marcel Proust\" #@param {type:\"string\"}\n",
        "CORPUS_FILENAME = \"mproust_guermantes_en.txt\" #@param {type:\"string\"}\n",
        "CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/mproust_time\"  #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "CHAPTER_HEADINGS = \"CHAPTER\" #@param [\"CHAPTER\", \"BOOK\", \"None\"]\n",
        "CHAPTER_NUMBERING = \"Arabic (1,2,...)\" #@param [\"Arabic (1,2,...)\", \"Roman (I,II,...)\"]\n",
        "SECTION_HEADINGS = \"None\" #@param [\"SECTION (ArabicNo)\", \"SECTION (RomanNo)\", \"----- (Hyphens)\", \"None\"]\n",
        "\n",
        "LEXICONS_SUBDIR = \"./research/2021/sa_book_code/books_sa/lexicons\" #@param {type:\"string\"}\n",
        "\n",
        "CORPUS_FULL = f'{CORPUS_TITLE} by: {CORPUS_AUTHOR}'\n",
        "\n",
        "PLOT_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n",
        "\n",
        "FILE_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n",
        "\n",
        "\n",
        "gdrive_subdir = CORPUS_SUBDIR\n",
        "corpus_filename = CORPUS_FILENAME\n",
        "author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "author_abbr_str = (CORPUS_AUTHOR.split(' ')[0][0]+CORPUS_AUTHOR.split(' ')[1]).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "title_str = re.sub(r'[^A-Za-z0-9]','', title_str).lower()\n",
        "\n",
        "print(f'\\nWorking Corpus Datafile: ------------------------------ \\n\\n    {CORPUS_SUBDIR}')\n",
        "print(f'\\nFull Corpus Title/Author: ------------------------------ \\n\\n    {CORPUS_FULL}')\n",
        "\n",
        "\n",
        "if CHAPTER_HEADINGS == 'CHAPTER':\n",
        "  if CHAPTER_NUMBERING == \"Arabic (1,2,...)\":\n",
        "    # pattern_chap = r'CHAPTER [0123456789]{1,2} ' # [\\.]?[^\\n]*'\n",
        "    pattern_chap = r'CHAPTER [0123456789]{1,2}[.]?[^\\n]*' # [os.return]*'\n",
        "  elif CHAPTER_NUMBERING == \"Roman (I,II,...)\":\n",
        "    pattern_chap = r'CHAPTER[\\s]{1,5}[IVXL]{1,10}[.:]?[\\s]+' # [^\\n]+'\n",
        "    # pattern_chap = r'CHAPTER[\\s]{1,}[IVXL]{1,10}[.:]?[^\\n\\r]*'\n",
        "  else:\n",
        "    print(f'ERROR: Illegal CHAPTER_NUMBERING value = {CHAPTER_NUMBERING}')\n",
        "\n",
        "elif CHAPTER_HEADINGS == 'BOOK':\n",
        "  if CHAPTER_NUMBERING == \"Arabic (1,2,...)\":\n",
        "    pattern_chap = r'BOOK [0123456789]{1,2}[.]?[^\\n]*'\n",
        "  elif CHAPTER_NUMBERING == \"Roman (I,II,...)\":\n",
        "    pattern_chap = r'[\\s]*BOOK[\\s]{1,5}[IVXL]{1,10}[.:]?[\\s]+' # [.:]?[\\s]*[^\\n]*[\\n\\r]+' # ]{0,1}[^\\n]*' # [^\\n]*' # Problems with embedded 'Book'\n",
        "  else:\n",
        "    print(f'ERROR: Illegal CHAPTER_NUMBERING value = {CHAPTER_NUMBERING}')\n",
        "\n",
        "elif CHAPTER_HEADINGS == \"None\":\n",
        "  pass\n",
        "\n",
        "else:\n",
        "  print(f'ERROR: Illegal CHAPTER_HEADINGS value = {CHAPTER_HEADINGS}')\n",
        "\n",
        "# Default Section RegEx Pattern\n",
        "pattern_sect = 'SECTION [0123456789]{1,2}[^\\n]*'\n",
        "\n",
        "if SECTION_HEADINGS == 'SECTION (ArabicNo)':\n",
        "  # pattern_sect = r'SECTION [0-9]{1,2} [^\\n]*'\n",
        "  # TODO: [^\\n] gets parsed into [^\\\\n] causing problems, so simplify\n",
        "  pattern_sect = r'SECTION [0123456789]{1,2}[.:]?[^\\n]*'\n",
        "elif SECTION_HEADINGS == 'SECTION (RomanNo)':\n",
        "  pattern_sect = r'SECTION [IVXL]{1,10}[.:]?[^\\n\\r]+' # } [A-Z \\.-:—;-’\\'\"]*[\\n]*'\n",
        "elif SECTION_HEADINGS == '----- (Hyphens)':\n",
        "  pattern_sect = r'^[- ]{3,}[^\\n]*'\n",
        "elif SECTION_HEADINGS == 'None':\n",
        "  pass\n",
        "else:\n",
        "  print(f'ERROR: Illegal SECTION_HEADING value = {SECTION_HEADINGS}')\n",
        "\n",
        "print(f'\\nCHAPTER Headings: ------------------------------ \\n\\n    {CHAPTER_HEADINGS}')\n",
        "\n",
        "print(f'\\nSECTION Headings: ------------------------------ \\n\\n    {SECTION_HEADINGS}')\n",
        "\n",
        "\n",
        "print(f'\\nCorpus file information: ------------------------------ \\n')\n",
        "!ls -al $CORPUS_FILENAME\n",
        "\n",
        "# Verify contents of Corpus File is Correctly Formatted\n",
        "#   \n",
        "# TODO: ./utils/verify_format.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnWBZCGMT-0G"
      },
      "source": [
        "corpus_filename\n",
        "print('\\n')\n",
        "CORPUS_FULL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8owpM75RILKn"
      },
      "source": [
        "# **Utility Functions (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMLbyx6gIPqj"
      },
      "source": [
        "## **File Manipulations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBOvvP-BSIiC"
      },
      "source": [
        "# https://dev.to/bowmanjd/character-encodings-and-detection-with-python-chardet-and-cchardet-4hj7\n",
        "\n",
        "import cchardet as chardet\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "def get_file_encoding(filename):\n",
        "    \"\"\"Detect encoding and return decoded text, encoding, and confidence level.\"\"\"\n",
        "    filepath = Path(filename)\n",
        "\n",
        "    # We must read as binary (bytes) because we don't yet know encoding\n",
        "    blob = filepath.read_bytes()\n",
        "\n",
        "    detection = chardet.detect(blob)\n",
        "    encoding = detection[\"encoding\"]\n",
        "    confidence = detection[\"confidence\"]\n",
        "    text = blob.decode(encoding)\n",
        "\n",
        "    return text, encoding, confidence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09qXAWa0jStn"
      },
      "source": [
        "# re.split(r'SECTION', 'There is one SECTION in this. - can you SECTION string', flags=re.I)\n",
        "\n",
        "re.split(r'SECTION', 'There is one string', flags=re.I)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CEiOZzv5lw9"
      },
      "source": [
        "## **Text Wrangling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQJdrsbpkSSw"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxh27xfMkXDN"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "# Init the Wordnet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize Single Word\n",
        "print(lemmatizer.lemmatize(\"bats\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIjRdm3rlw85"
      },
      "source": [
        "test_str = \"I love eating bats and driving many cars.\" #  around the darken town at night.\"\n",
        "\n",
        "doc = nlp(test_str)\n",
        "\n",
        "# Extract the lemma for each token and join\n",
        "\" \".join([token.lemma_ for token in doc])\n",
        "#> 'the strip bat be hang on -PRON- foot for good'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v374fE0tIrea"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "def stem_str(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return the same string with all tokens stemmed where possible\n",
        "  '''\n",
        "\n",
        "  from nltk.stem import WordNetLemmatizer # For stemming the sentence\n",
        "  from nltk.stem import SnowballStemmer # For stemming the sentence\n",
        "\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZaL4cZbg3td"
      },
      "source": [
        "def lemmatize_str(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return the same string with all tokens lemmatized where possible\n",
        "  '''\n",
        "\n",
        "  # NOTE: depends upon Setup above importing minimal SpaCy pipeline defined as [nlp]\n",
        "\n",
        "  # Lemmaitization MAY help performance, but depends on SA Model and Corpus (see below)\n",
        "  # https://opendatagroup.github.io/data%20science/2019/03/21/preprocessing-text.html \n",
        "  # Which library: NLTK, SpaCy, TextBlob, CLiPS Pattern, gensim, Stanford OpenNLP/Stanza, Flair, etc.\n",
        "  # https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
        "  # Spelling, Contractions, etc\n",
        "  # https://cnvrg.io/sentiment-analysis-python/\n",
        "\n",
        "  text_lemma_ls = []\n",
        "\n",
        "  token_ls = text_str.split()\n",
        "\n",
        "  for atoken in token_ls:\n",
        "    if len(atoken) > 2:\n",
        "      print(f'lemmatizing: {atoken}')\n",
        "      atoken_lemma = lemmatizer.lemmatize(atoken)\n",
        "      print(f'    lemma: {atoken_lemma}')\n",
        "      text_lemma_ls.append(atoken_lemma)\n",
        "    else:\n",
        "      print(f'skip lemmatizing')\n",
        "      text_lemma_ls.append(atoken)\n",
        "\n",
        "  text_lemma_str = ' '.join(text_lemma_ls)\n",
        "\n",
        "  return text_lemma_str.strip()\n",
        "\n",
        "# Test\n",
        "\n",
        "test_str = \"I love eating bats and driving many cars.\" #  around the darken town at night.\"\n",
        "\n",
        "test_lemma_str = lemmatize_str(test_str)\n",
        "print(f'test_lemma_str: {test_lemma_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAl8P6zSkzBM"
      },
      "source": [
        "lemmatizer.lemmatize('eating')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGje9LyN5hx4"
      },
      "source": [
        "def del_leadroman(str_raw):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return the string with any possible leaning roman numerals removed [IVXLCM]\n",
        "  '''\n",
        "\n",
        "\n",
        "  # Rule 1: consecutive 'i' tokens indicate a roman numeral followed by 'I' pronoun\n",
        "  pattern_doublei = re.compile(r'\\b[iI][\\s]+[iI]\\b')\n",
        "  str_clean1 = re.sub(pattern_doublei, \"i\", str_raw)\n",
        "\n",
        "  # Rule 2: any [vxlcm]-only tokens are stray roman numerals\n",
        "  pattern_romanno = re.compile(r'\\b[vxlcm]{1,10}\\b')\n",
        "  str_clean2 = re.sub(pattern_romanno, \"\", str_clean1)\n",
        "\n",
        "  return str_clean2\n",
        "\n",
        "\n",
        "# Test\n",
        "\n",
        "test_str = \"\"\"I I was born in Tuckahoe, near Hillsborough, and about twelve miles from Easton, in Talbot county, Maryland. I have no accurate knowledge of my age, never having seen any authentic record containing it. By far the larger part of the slaves know as little of their ages as horses know of theirs, and it is the wish of most masters within my knowledge to keep their slaves thus ignorant. I do not remember to have ever met a slave who could tell of his birthday. They seldom come nearer to it than planting-time, harvest-time, cherry-time, spring-time, or fall-time. A want of information concerning my own was a source of unhappiness to me even during childhood. The white children could tell their ages. I could not tell why I ought to be deprived of the same privilege. I was not allowed to make any inquiries of my master concerning it. He deemed all such inquiries on the part of a slave improper and impertinent, and evidence of a restless spirit. The nearest estimate I can give makes me now between twenty-seven and twenty-eight years of age. I come to this, from hearing my master say, some time during 1835, I was about seventeen years old.\n",
        "\n",
        "My mother was named Harriet Bailey. She was the daughter of Isaac and Betsey Bailey, both colored, and quite dark. My mother was of a darker complexion than either my grandmother or grandfather.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"\\n\\nRESULT:\\n\\n{del_leadroman(test_str)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rMQwo0cKHS4"
      },
      "source": [
        "# Defined for clean_text()\n",
        "\n",
        "from unicodedata import normalize\n",
        "\n",
        "# prepare regex for char filtering\n",
        "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "# prepare translation table for removing punctuation\n",
        "table = str.maketrans('', '', string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9FDtpQ6QaEJ"
      },
      "source": [
        "# This function converts to lower-case, removes square bracket, removes numbers and punctuation\n",
        "\n",
        "# https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0\n",
        "# https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/ \n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "  # normalize unicode characters\n",
        "  #   library [normalize] imported in Setup above\n",
        "  text = normalize('NFD', text).encode('ascii', 'ignore')\n",
        "  text = text.decode('UTF-8')\n",
        "\n",
        "  # remove non-printable chars form each token\n",
        "  #   regex pattern [re_print] defined in Setup above\n",
        "  text = re_print.sub('', text)\n",
        "\n",
        "  # to lower\n",
        "  text = text.lower()\n",
        "\n",
        "  # Spelling correction\n",
        "  from autocorrect import Speller #correcting the spellings\n",
        "  \n",
        "  # Adjust apostrophes and contractions\n",
        "  # from contractions import contractions_dict # to solve contractions\n",
        "  text = contractions.fix(text)  # Expand contrations\n",
        "  # TODO: Problem with The Great Gatsby [I'm] -> [I' ]\n",
        "  text = re.sub(\"\\\\'s\", \" own\", text)  # After expanding normal apostrophes, expand possessive apostrophes \"Mary's car\" -> \"Mary own car\"\n",
        "\n",
        "  # Join end of line words split by continuation hyphens \n",
        "  text = re.sub(\"-\\n\", \" \", text)       \n",
        "  text = re.sub(\"-\\n\\r\", \" \", text)\n",
        "  text = re.sub(\"-\\r\", \" \", text)\n",
        "  text = re.sub(\"\\[.*?\\]\", \" \", text)\n",
        "\n",
        "  text = re.sub(\"-\", \" \", text)  # Special care for hypenated words well-known: choose option (a)\n",
        "                                  # (a) 'well known', (b) 'wellknown' (c) 'well known' and 'wellknown' cf: https://datascience.stackexchange.com/questions/81072/how-to-process-the-hyphenated-english-words-for-any-nlp-problem\n",
        "\n",
        "  text = re.sub(\"/\", \" \", text)  # sociability/conversation/interesting -> sociability conversation interesting                             \n",
        "\n",
        "  # Split string into tokens\n",
        "  line = text.split()\n",
        "\n",
        "  # remove punctuation from each token\n",
        "  line = [word.translate(table) for word in line]\n",
        "  # OLD string way: text = re.sub(\"[%s]\" % re.escape(string.punctuation), \" \", text)\n",
        "\n",
        "  # remove tokens with numbers in them\n",
        "  line = [word for word in line if word.isalpha()]    \n",
        "  # OLD stirng way: text = re.sub(\"\\w*\\d\\w*\", \" \", text)\n",
        "\n",
        "  # collapse/replace any whitespace(s) with a single hard space\n",
        "  # OLD string way: text = re.sub(\"[\\n]\", \" \", text)  # Replace newline with space\n",
        "  # reassemble tokens into single string to return\n",
        "  text_cleaned = ' '.join(line)\n",
        "\n",
        "  return text_cleaned\n",
        "\n",
        "# Test\n",
        "\n",
        "print(clean_text(\"Le pépiement matinal des oiseaux semblait insipide à Françoise. I'm going to eat at Sloppy Joes's Place tonight.\"))\n",
        "\"\"\"\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_lines(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor line in lines:\n",
        "\t\t# normalize unicode characters\n",
        "\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\tline = line.decode('UTF-8')\n",
        "\t\t# tokenize on white space\n",
        "\t\tline = line.split()\n",
        "\t\t# convert to lower case\n",
        "\t\tline = [word.lower() for word in line]\n",
        "\t\t# remove punctuation from each token\n",
        "\t\tline = [word.translate(table) for word in line]\n",
        "\t\t# remove non-printable chars form each token\n",
        "\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t# remove tokens with numbers in them\n",
        "\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t# store as string\n",
        "\t\tcleaned.append(' '.join(line))\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZFUOC43UL2E"
      },
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQjBZrzCUHo8"
      },
      "source": [
        "test_str = 'what the @#$*(! do you mean!??'\n",
        "print(test_str.strip(string.punctuation))\n",
        "\n",
        "res = re.sub(r'[^\\w\\s]', '', test_str)\n",
        "print(f\"res = {' '.join(res.split())}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxqqtrotUAPI"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "import string\n",
        "p = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n",
        "print(p.sub(\"\", \"\\\"hello world!\\\", he's told me.\"))\n",
        "\n",
        "import string\n",
        "string.punctuation\n",
        "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "'!Hello.'.strip(string.punctuation)\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZSXxliIZOJS"
      },
      "source": [
        "test_str = 'CHAPTER 1. -  HELLO\\n\\n  '\n",
        "# test_str = 'The rain in Spain\\n\\n  '\n",
        "\n",
        "test_str.isupper()\n",
        "\n",
        "test_str.islower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7m6cUZVXuBw"
      },
      "source": [
        "test_str = 'CHAPTER 1. HELLOa'\n",
        "\n",
        "res_str = re.sub(r'[^\\s\\w]','', test_str)\n",
        "print(f'res_str: {res_str}')\n",
        "print(True == re.match(r'[A-Z]', res_str))\n",
        "\n",
        "if (re.match(r'[^A-Z]', re.sub(r'[^\\s\\w]','', test_str))):\n",
        "  print('True')\n",
        "else:\n",
        "  print('False')\n",
        "\n",
        "if (re.match(r'[^A-Z]', re.sub(r'[^\\s\\w]','', test_str))):\n",
        "  print('True')\n",
        "else:\n",
        "  print('False')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2ozSRonDhhV"
      },
      "source": [
        "def del_sectchap_headers(text_raw_ls, text_type='section'):\n",
        "  '''\n",
        "  Given a list of either Chapter or Section texts (with possible embedded SECTION or CHAPTER lines)\n",
        "  Return 2 lists free of all SECTION and CHAPTER lines, with the _clean_ version passed through clean_text(text_raw)\n",
        "  '''\n",
        "\n",
        "  print(f'Entered clean_sectchap() with text_type: {text_type} and text_raw_ls len: {len(text_raw_ls)}')\n",
        "  text_filtered_ls = []\n",
        "  text_headers_ls = []\n",
        "\n",
        "  if text_type == 'chapter':\n",
        "    text_min_len = MIN_CHAP_LEN\n",
        "  elif text_type == 'section':\n",
        "    text_min_len = MIN_SECT_LEN\n",
        "  else:\n",
        "    print(f\"ERROR: In clean_sectchap() with text_type={text_type}, must be ['section'|'chapter']\")\n",
        "    return [-99], [-99], [-99], [-99], [-99], '-99' # Return with ERROR condition\n",
        "\n",
        "\n",
        "  # Strip off whitespace\n",
        "  text_raw_ls = [x.strip() for x in text_raw_ls]\n",
        "\n",
        "  # Filter out chapters that are empty or shorter than MIN_PARAG_LEN\n",
        "  text_raw_ls = [x for x in text_raw_ls if not (len(x.strip()) <= text_min_len)]\n",
        "\n",
        "  # Filter out SECTION/CHAPTER lines (could be embedded or leading)\n",
        "  corpus_segs_filtered_ls = []\n",
        "\n",
        "  if text_type == 'chapter':\n",
        "    # Remove possible SECTION lines within Chapter text segments\n",
        "    print(f'Removing any SECTION headers from Chapters')\n",
        "\n",
        "    for i, achap_raw in enumerate(text_raw_ls):  #  (corpus_chaps_raw_ls):\n",
        "      print(f'  In Chapter #{i}: {achap_raw[:50]}\\n')\n",
        "      achap_nosectheads_ls = []\n",
        "      if bool(re.match(rf\"{pattern_sect}\", achap_raw)):\n",
        "        print(f'    Before filtering SECTION line, len: {len(achap_raw)}')\n",
        "        achap_nosectheads_ls = re.split(rf'{pattern_sect}', achap_raw, flags=re.I) # , flags=re.I)\n",
        "        achap_nosectheads_ls = [x.strip() for x in achap_nosectheads_ls]\n",
        "        achap_nosectheads_ls = [x for x in achap_nosectheads_ls if len(x) > text_min_len]\n",
        "        achap_sects_noheads_str = '\\n\\n'.join([x.strip() for x in achap_nosectheads_ls])\n",
        "        achap_nosectheads_ls = [x.strip() for x in achap_nosectheads_ls]\n",
        "        print(f'    After filtering SECTION line, len: {len(achap_sects_noheads_str)}')\n",
        "        print(f'                                  {len(achap_nosectheads_ls)} SECTION headers in Chapter #{i}')\n",
        "      else:\n",
        "        print(f'    No filtering needed')\n",
        "        achap_sects_noheads_str = achap_raw.strip()\n",
        "      text_filtered_ls.append(achap_sects_noheads_str)\n",
        "\n",
        "    print(f'  Chapter #{i} filtered of any SECTION lines')\n",
        "\n",
        "  elif text_type == 'section':\n",
        "    # Remove possible CHAPTER lines within Section text segments\n",
        "    print(f'Removing any CHAPTER headers from Sections')\n",
        "\n",
        "    for i, asect_raw in enumerate(text_raw_ls):  #  (corpus_sects_raw_ls):\n",
        "      print(f'  In Section #{i}: {asect_raw[:50]}\\n')\n",
        "      asect_nochapheads_ls = []\n",
        "      if bool(re.match(rf\"{pattern_chap}\", asect_raw)):\n",
        "        print(f'    Before filtering CHAPTER line, len: {len(asect_raw)}')\n",
        "        asect_nochapheads_ls = re.split(rf'{pattern_chap}', asect_raw, flags=re.I) # , flags=re.I)\n",
        "        asect_nochapheads_ls = [x for x in asect_nochapheads_ls if not re.escape(x).isupper()]  # TODO: Don't Assume CHAPTER X. TITLE IN ALL CAPS\n",
        "        asect_nochapheads_ls = [x.strip() for x in asect_nochapheads_ls]\n",
        "        asect_nochapheads_ls = [x for x in asect_nochapheads_ls if len(x) > text_min_len]\n",
        "        asect_chaps_noheads_str = '\\n\\n'.join([x.strip() for x in asect_nochapheads_ls])\n",
        "        asect_nochapheads_ls = [x.strip() for x in asect_nochapheads_ls]\n",
        "        print(f'    After filtering CHAPTER line, len: {len(asect_chaps_noheads_str)}')\n",
        "        print(f'                                       {len(asect_nochapheads_ls)} CHAPTER headers in Section #{i}')\n",
        "      else:\n",
        "        print(f'    No filtering needed')\n",
        "        asect_chaps_noheads_str = asect_raw.strip()\n",
        "      text_filtered_ls.append(asect_chaps_noheads_str)\n",
        "\n",
        "    print(f'  Section #{i} filtered of any CHAPTER lines')\n",
        "\n",
        "  # Collapse multiple whitespaces down to one\n",
        "  text_filtered_ls = [re.sub(r'[ ]{2,}',' ', x) for x in text_filtered_ls] # ' '.join(x.split()).strip() for x in corpus_segs_raw_ls]\n",
        "\n",
        "  # Filter out chapters that are empty or shorter than MIN_PARAG_LEN\n",
        "  text_filtered_ls = [x for x in text_filtered_ls if not (len(x.strip()) <= text_min_len)]\n",
        "\n",
        "  # Call clean_text on text sgements (Chapters/Segments)\n",
        "  # text_raw_ls = [clean_text(x) for x in text_filtered_ls]\n",
        "\n",
        "  print(f'  Returning from clean_sectchap() with text_filtered_ls: {len(text_filtered_ls)}')\n",
        "  \n",
        "  return text_filtered_ls, text_raw_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4Fsqx1iXTPY"
      },
      "source": [
        "def delete_multiple_element(list_object, indices):\n",
        "  '''\n",
        "  Given a list of objects and a list of indicies into that list \n",
        "  Remove all the objects in the list and return the resulting shortened list\n",
        "    being careful to remove all objects at the original index positions \n",
        "\n",
        "  Ref: https://thispointer.com/python-remove-elements-from-list-by-index/\n",
        "  '''\n",
        "\n",
        "  indices = sorted(indices, reverse=True)\n",
        "  for idx in indices:\n",
        "    if idx < len(list_object):\n",
        "      list_object.pop(idx)\n",
        "\n",
        "  return list_object\n",
        "\n",
        "# Test\n",
        "list_of_num = [51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
        "list_of_indices = [4, 2, 6]\n",
        "\n",
        "# Remove elements from list_of_num at index 4,2 and 6\n",
        "delete_multiple_element(list_of_num, list_of_indices)\n",
        "print(list_of_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS_zBObiIVoM"
      },
      "source": [
        "test_ls = ['one','two','three','four','five']\n",
        "\n",
        "[i for i, word in enumerate(test_ls) if len(word) == 4 ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzv7H9KXbzRa"
      },
      "source": [
        "def corpus2chapsect(corpus_source, corpus_type='file'):\n",
        "  '''\n",
        "  Given a corpus_source and a corpus_type=['file'|'string'] that tells how to to extract the corpus from corpus_source\n",
        "    (if 'file' type, assume already %cd into correct subdir)\n",
        "  Return a lists of Chapters and Section texts, raw or minimally processed \n",
        "  '''\n",
        "\n",
        "  # corpus_chaps_raw_ls, corpus_chaps_clean_ls, corpus_sects_raw_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str = corpus2chapsect(corpus_filename)\n",
        "\n",
        "  # Return variables\n",
        "  corpus_chaps_raw_ls = []      #\n",
        "  corpus_chaps_filtered_ls = [] # List of raw/filtered/clean Chapter text segments extracted from Corpus\n",
        "  corpus_chaps_clean_ls = []    #      length = 1 if no Chapter structure\n",
        "  corpus_sects_raw_ls = []      # \n",
        "  corpus_sects_filtered_ls = [] # List of raw/filtered/clean Section text segments extracted from Chapters\n",
        "  corpus_sects_clean_ls = []    #      length = 1 if no Chapter or Section structure\n",
        "                                #      length = Chapter segments length if no Section structure\n",
        "  \n",
        "  sect_chapno_ls = []           # List of Chapter numbers sequenced by unique Section number in Corpus\n",
        "  corpus_raw_str = ''           # String with Corpus as raw string\n",
        "  \n",
        "  # This extra layer corpus source (file/string) allows the corpus text to have an\n",
        "  #   additional layer optionally preprocessed after reading from file\n",
        "  #   which could be useful for special text types (e.g. non-Latin encoding, tweets, etc)\n",
        "  if corpus_type == 'file':\n",
        "    # with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    #   corpus_raw_str = infp.read()\n",
        "\n",
        "    encoding_type='cp1252'\n",
        "    encoding_type='utf-8'\n",
        "    encoding_type=''\n",
        "\n",
        "    # with open(corpus_source, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    with open(corpus_source, \"r\", errors='ignore') as infp:\n",
        "      corpus_raw_str = infp.read()\n",
        "  else:\n",
        "    corpus_raw_str = corpus_source\n",
        "\n",
        "\n",
        "\n",
        "  # Strip out non-printing characters\n",
        "  # corpus_clean_str = re.sub(f'[^{re.escape(string.printable)}]', ' ', corpus_raw_str)\n",
        "  # corpus_clean_str = corpus_raw_str\n",
        "\n",
        "  # normalize unicode characters\n",
        "  #   library [normalize] imported in Setup above\n",
        "  corpus_raw_str = normalize('NFD', corpus_raw_str).encode('ascii', 'ignore')\n",
        "  corpus_raw_str = corpus_raw_str.decode('UTF-8')\n",
        "\n",
        "  # remove non-printable chars form each token\n",
        "  #   regex pattern [re_print] defined in Setup above\n",
        "  corpus_clean_str = re_print.sub('', corpus_raw_str)\n",
        "\n",
        "\n",
        "  print(f'BEFORE regex extraction of CHAPTER/SECTION: {len(corpus_clean_str)}')\n",
        "\n",
        "  # Check if a Chapter structure is found in this corpus\n",
        "  print(f'Using RegEx pattern_chap: {pattern_chap}')\n",
        "  corpus_chaps_raw_ls = re.split(rf'{pattern_chap}', corpus_clean_str) # , flags=re.I)\n",
        "  corpus_chap_flag = len(corpus_chaps_raw_ls) > 1\n",
        "  # print(f'Do Chapters exist in Corpus? {corpus_chap_flag} (Chapter count = {len(corpus_chaps_raw_ls)})')\n",
        "\n",
        "  # Check if a Section structure is found in this corpus\n",
        "  print(f'Using RegEx pattern_sect: {pattern_sect}')\n",
        "  corpus_sects_raw_ls = re.split(rf'{pattern_sect}', corpus_clean_str, flags=re.I) # , flags=re.I)\n",
        "  corpus_sect_flag = len(corpus_sects_raw_ls) > 1\n",
        "  # print(f'Do Sections exist in Corpus? {corpus_sect_flag} (Section count = {len(corpus_sects_raw_ls)})')\n",
        "\n",
        "  print(f'The Corpus at {corpus_filename}:')\n",
        "  print(f'    Has Chapters? {corpus_chap_flag} (Count: {len(corpus_chaps_raw_ls)})')\n",
        "  print(f'    Has Sections? {corpus_sect_flag} (Count: {len(corpus_sects_raw_ls)})')\n",
        "\n",
        "\n",
        "\n",
        "  # TEST A: Does Chapter structure exist in Corpus?\n",
        "\n",
        "  if corpus_chap_flag == True:\n",
        "    # If a Chapter structure is found, filter out SECTION headers from all Chapters  corpus_chaps_raw_ls\n",
        "    corpus_chaps_filtered_ls, corpus_chaps_clean_ls = del_sectchap_headers(corpus_chaps_raw_ls, text_type='chapter')\n",
        "    print(f'TEST A (False): {len(corpus_chaps_filtered_ls)} Chapters in Corpus')\n",
        "\n",
        "    # Create list of Chapter Numbers\n",
        "    corpus_chapno_ls = list(range(len(corpus_chaps_filtered_ls)))\n",
        "\n",
        "    # TEST B: Does Section structure exist in Corpus with Chapter struture? (TEST A: Chapter structure exists == True)\n",
        "\n",
        "    if corpus_sect_flag == True:    \n",
        "      # TODO: Verify with manual seeting SECTION_HEADINGS != \"None\":\n",
        "      # TEST B: (True) Yes Section/Yes Chapter\n",
        "      #         Process Sections\n",
        "\n",
        "      # NOTE: Could call del_sectchap_headers to segment original Corpus into Segments, but if Chapter structure exists\n",
        "      #       it is safer to directly segement already parsed Chapters due to edge cases in various Corpora\n",
        "      # corpus_sects_raw_ls, corpus_sects_clean_ls = del_sectchap_headers(corpus_sects_raw_ls, text_type='section')\n",
        "\n",
        "      print(f'  TEST A(True)/TEST B(True): {len(corpus_sects_raw_ls)} Sections in Corpus') # Chapters = True, Sections = True')  del_sectchap_headers\n",
        "      corpus_sectno = 0\n",
        "      corpus_sects_dt = {}       # Dictionary of key=SectionNo, value=SectionText\n",
        "      achap_sectchapno_ls = []  # list of ChapterNo corresponding to counting sequence of SectionNo     \n",
        "      achap_sects_raw_ls = []    # List of raw Sections extracted from current Chapter\n",
        "      # achap_sects_clean_ls = []  # List of clean Sections extracted from current Chapter\n",
        "      for achap_no, achap_filtered_str in enumerate(corpus_chaps_filtered_ls):\n",
        "        # Split current filtered Chapter into multiple Sections\n",
        "        print(f'Calling del_sectchap_headers() with text_type=section and len of Chaps: {len(achap_filtered_str)}')\n",
        "        # Split current Chapter at Section lines\n",
        "        achap_sects_raw_ls = re.split(rf'{pattern_sect}', achap_filtered_str, flags=re.I) # , flags=re.I)\n",
        "        print(f'Calling del_sectchap_headers() with {len(achap_sects_raw_ls)} SECTIONS in Chapter #{achap_no}')\n",
        "        achap_sects_filtered_ls, achap_sects_clean_ls = del_sectchap_headers(achap_sects_raw_ls, text_type='section')\n",
        "        print(f'  Found {len(achap_sects_filtered_ls)} Sections in Chapter #{achap_no}')\n",
        "\n",
        "        # For each Section extracted from the current Chapter, store details\n",
        "        achap_sects_ct = len(achap_sects_filtered_ls)\n",
        "        for achap_asect_no, achap_asect_filtered_str in enumerate(achap_sects_raw_ls):\n",
        "          print(f'Processed: Chapter #{achap_no}, Section #{achap_asect_no}')\n",
        "          corpus_sects_dt[corpus_sectno] = achap_sects_filtered_ls[achap_asect_no]     # Store Section text in Dict indexed by Corpus Section No\n",
        "          corpus_sectno += 1\n",
        "          print(f'Section #{corpus_sectno} is in Chapter #{achap_no}')\n",
        "          sect_chapno_ls.append(achap_no)                              # Store Chapter No corresponding to sequence of Sections in List\n",
        "\n",
        "        corpus_sects_filtered_ls += achap_sects_filtered_ls\n",
        "        corpus_sects_clean_ls += achap_sects_clean_ls\n",
        "        # sect_chapno_ls += achap_sectchapno_ls\n",
        "\n",
        "        # # use index to identify objects to remove from parallel data structures\n",
        "\n",
        "    else:\n",
        "      # corpus_sect_flat == False:\n",
        "      # TEST B: (False) No Section/Yes Chapter stucture\n",
        "\n",
        "      # Create pseudo-Sections by copying Chapters to Sections\n",
        "      # Note, Sections much be at least as fine-grained as Chapters\n",
        "      #       cannot have multiple Chapters with one/no Sections \n",
        "      #       because Sections are used to mark Corpus boundaries in Plots\n",
        "\n",
        "      print(f'  TEST A(True)/TEST B(False): {len(corpus_sects_raw_ls)} Sections in Corpus') # Chapters = True, Sections = True')\n",
        "      # print('Chapters = True, Sections = False')\n",
        "      corpus_sects_filtered_ls = [x for x in corpus_chaps_filtered_ls]\n",
        "      corpus_sects_clean_ls = [x for x in corpus_chaps_clean_ls]\n",
        "      # Create list of Chapter Numbers for each Section\n",
        "      sect_chapno_ls = [x for x in corpus_chapno_ls]\n",
        "\n",
        "\n",
        "  else:\n",
        "    # TEST A: (False) No Chapter structure in Corpus\n",
        "    print(f'TEST A (False): {len(corpus_chaps_raw_ls)} Chapters in Corpus')\n",
        "\n",
        "    # TEST B: Does Section structure exists within Corpus without Chapter structure? (TEST A: Chapter structure exists == False)\n",
        "    if corpus_sect_flag == True:    \n",
        "      # TODO: Verify with manual seeting SECTION_HEADINGS != \"None\":\n",
        "\n",
        "      # TEST B: (True) Yes Section/No Chapter\n",
        "      #         Create Sections with multiple entires, but dummy Chapter with just one row\n",
        "      # If a chapter structure is found, process it\n",
        "      print(f'  TEST A(False)/TEST B(True): {len(corpus_sects_raw_ls)} Sections in Corpus') # Chapters = True, Sections = True')\n",
        "      # print('Chapters = False, Sections = True')\n",
        "      corpus_sects_filtered_ls, corpus_sects_clean_ls = del_sectchap_headers(corpus_sects_raw_ls, text_type='section')\n",
        "      # Create list of Chapter numbers for each Section (trivial since no Chapter structure)\n",
        "      sect_chapno_ls = list(range(len(corpus_sects_filtered_ls)))\n",
        "      # Pad out empty Chapter structure\n",
        "      corpus_chaps_raw_ls = [corpus_raw_str]\n",
        "      corpus_chaps_filtered_ls = [corpus_raw_str]\n",
        "      corpus_chaps_clean_ls = [clean_text(corpus_raw_str)]\n",
        "\n",
        "    else:\n",
        "      # TEST B: (False) No Section/No Chapter\n",
        "      #         Create same dummy Section/Chapter DataFrame with just one row\n",
        "      print(f'  TEST A(False)/TEST B(False): {len(corpus_sects_raw_ls)} Sections in Corpus') # Chapters = True, Sections = True')  achp_no\n",
        "      # print('Chapters = False, Sections = False')\n",
        "      corpus_chaps_raw_ls = [corpus_raw_str]\n",
        "      corpus_chaps_filtered_ls = [corpus_raw_str]\n",
        "      corpus_chaps_clean_ls = [clean_text(corpus_raw_str)]\n",
        "      corpus_sects_raw_ls = [corpus_raw_str]\n",
        "      corpus_sects_filtered_ls = [corpus_raw_str]\n",
        "      corpus_sects_clean_ls = [clean_text(corpus_raw_str)]\n",
        "      sect_chapno_ls = [1]\n",
        "      \n",
        "\n",
        "  print(f'corpus_chaps_raw_ls: {len(corpus_chaps_raw_ls)}')\n",
        "  print(f'corpus_chaps_clean_ls: {len(corpus_chaps_clean_ls)}')\n",
        "  print(f'corpus_sects_raw_ls: {len(corpus_sects_raw_ls)}')\n",
        "  print(f'corpus_sects_clean_ls: {len(corpus_sects_clean_ls)}')\n",
        "  print(f'sect_chapno_ls: {len(sect_chapno_ls)}')\n",
        "\n",
        "\n",
        "  # Last step is to remove strings that are too short/null, \n",
        "  #   removal has to be synchronized with removal across parallel lists containing related data\n",
        "\n",
        "  if corpus_chap_flag == True:\n",
        "    #  get indicies of too short Chapters to delete\n",
        "    chaps_del_idx = [i for i, achap in enumerate(corpus_chaps_raw_ls) if len(achap) < MIN_CHAP_LEN]\n",
        "    print(f'  Deleting {len(chaps_del_idx)} shorth/null Chapters')\n",
        "    # use index to identify objects to remove from parallel data structure\n",
        "    corpus_chaps_raw_ls = delete_multiple_element(corpus_chaps_raw_ls, chaps_del_idx)\n",
        "    print(f'    returned with {len(corpus_chaps_raw_ls)} Chapters left')\n",
        "    # corpus_chaps_clean_ls = delete_multiple_element(corpus_chaps_clean_ls, chaps_del_idx)\n",
        "    # delete_multiple_element(chapno_ls, chaps_del_idx) \n",
        "\n",
        "  if corpus_sect_flag == True:\n",
        "    # get indicies of too short Sections to be removed\n",
        "    sects_del_idx = [i for i, asect in enumerate(corpus_sects_raw_ls) if len(asect) < MIN_SECT_LEN]  \n",
        "    print(f'  Deleting {len(sects_del_idx)} shorth/null Sections')\n",
        "    # use index to identify objects to remove from parallel data structures\n",
        "    corpus_sects_raw_ls = delete_multiple_element(corpus_sects_raw_ls, sects_del_idx)\n",
        "    corpus_sects_clean_ls = delete_multiple_element(corpus_sects_clean_ls, sects_del_idx)\n",
        "    sect_chapno_ls = delete_multiple_element(sect_chapno_ls, sects_del_idx)\n",
        "\n",
        "  # TODO: Redundance check for null/empty elements after last cleaning pass\n",
        "  corpus_sects_clean_ls = [clean_text(x) for x in corpus_sects_clean_ls]\n",
        "  corpus_chaps_clean_ls = [clean_text(x) for x in corpus_chaps_clean_ls]\n",
        "\n",
        "  print(f'corpus_chaps_raw_ls: {len(corpus_chaps_raw_ls)}')\n",
        "  print(f'corpus_chaps_clean_ls: {len(corpus_chaps_clean_ls)}')\n",
        "  print(f'corpus_sects_raw_ls: {len(corpus_sects_raw_ls)}')\n",
        "  print(f'corpus_sects_clean_ls: {len(corpus_sects_clean_ls)}')\n",
        "  print(f'sect_chapno_ls: {len(sect_chapno_ls)}')\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  # Naive method will not work, does not maintain order and synchronization across parellel data structures\n",
        "  corpus_chaps_raw_ls = [x for x in corpus_chaps_raw_ls if len(x) > MIN_CHAP_LEN]    \n",
        "  corpus_chaps_clean_ls = [x for x in corpus_chaps_clean_ls if len(x) > MIN_CHAP_LEN]  \n",
        "  chaps_del_idx = [i for i, achap in enumerate(corpus_chaps_raw_ls) if len(achap) < MIN_CHAP_LEN]\n",
        "  #  get indicies of too short Sections to be removed\n",
        "  corpus_sects_raw_ls = [x for x in corpus_sects_raw_ls if len(x) > MIN_CHAP_LEN]    \n",
        "  corpus_sects_clean_ls = [x for x in corpus_sects_clean_ls if len(x) > MIN_CHAP_LEN] \n",
        "  sects_del_idx = [i for i, asect in enumerate(corpus_sects_raw_ls if len(asect) < MIN_SECT_LEN]\n",
        "  \"\"\";\n",
        "\n",
        "  return corpus_chaps_raw_ls, corpus_chaps_clean_ls, corpus_sects_raw_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AceDrCdU7vLB"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "corpus_chaps_raw_ls, corpus_chaps_clean_ls, corpus_sects_raw_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str = corpus2chapsect(corpus_filename)\n",
        "\n",
        "# Create list of Chapter Numbers\n",
        "corpus_chapno_ls = list(range(len(corpus_chaps_raw_ls)))\n",
        "\n",
        "# print('\\n')\n",
        "# print(f'Chapter #{achap_no} Ch Length: {len(achap_str)}')\n",
        "print(f'\\n             Chapters:  {len(corpus_chaps_raw_ls)}')\n",
        "print('\\n')\n",
        "print(f'        First Chapter:\\n    {corpus_chaps_raw_ls[0][:500]}\\n')\n",
        "print(f'       Second Chapter:\\n    {corpus_chaps_raw_ls[1][:500]}')\n",
        "print('\\n')\n",
        "print(f'  Second-Last Chapter:\\n    {corpus_chaps_raw_ls[-2][:500]}\\n')\n",
        "print(f'         Last Chapter:\\n    {corpus_chaps_raw_ls[-1][:500]}')\n",
        "print('\\n')\n",
        "\n",
        "# print('\\n')\n",
        "# print(f'Chapter #{achap_no} Ch Length: {len(achap_str)}')\n",
        "print(f'\\n             Sections:  {len(corpus_sects_raw_ls)}')\n",
        "print('\\n')\n",
        "print(f'        First Section:\\n    {corpus_sects_raw_ls[0][:500]}\\n')\n",
        "print(f'       Second Section:\\n    {corpus_sects_raw_ls[1][:500]}')\n",
        "print('\\n')\n",
        "print(f'  Second-Last Section:\\n    {corpus_sects_raw_ls[-2][:500]}\\n')\n",
        "print(f'         Last Section:\\n    {corpus_sects_raw_ls[-1][:500]}')\n",
        "print('\\n')\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9ObfIZA_i27"
      },
      "source": [
        "test_raw_str = \"\"\"\n",
        "CHAPTER 0.\n",
        "\n",
        "Many friends have helped me in writing this book. \n",
        "\n",
        "CHAPTER 1.\n",
        "\n",
        "Some are dead and so illustrious that I scarcely dare name them, yet no one can read or write without being perpetually in the debt of Defoe, Sir Thomas Browne, Sterne, Sir Walter Scott, Lord Macaulay, Emily Bronte, De Quincey, and Walter Pater,--to name the first that come to mind. \n",
        "\n",
        "CHAPTER 2.\n",
        "\n",
        "Others are alive, and though perhaps as illustrious in their own way, are less formidable for that very reason. I am specially indebted to Mr C.P. Sanger\n",
        "\"\"\"\n",
        "\n",
        "len(test_raw_str)\n",
        "print('\\n')\n",
        "test_raw_str = re.sub(f'[^{re.escape(string.printable)}]', ' ', test_raw_str)\n",
        "\n",
        "test_chaps_raw_ls, test_chaps_clean_ls, test_sects_raw_ls, test_sects_clean_ls, test_sect_chapno_ls, test_raw_str = corpus2chapsect(test_raw_str, corpus_type='string')\n",
        "\n",
        "\"\"\"\n",
        "pattern = r'CHAPTER [0123456789]{1,2}[.]+[\\w]*[os.return]*'\n",
        "test_raw_ls = re.split(rf'{pattern}', test_raw_str, flags=re.I) # , flags=re.I)\n",
        "print(f'{len(test_raw_ls)} Chapters found in this corpus.\\n')\n",
        "print(f'Chapter 0 len is {len(test_raw_ls[0])}')\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JPk7404_iyz"
      },
      "source": [
        "test_raw_ls[1].strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_nBxtq6-J8l"
      },
      "source": [
        "def filter_nonprintable(text):\n",
        "    import itertools\n",
        "    # Use characters of control category\n",
        "    nonprintable = itertools.chain(range(0x00,0x20),range(0x7f,0xa0))\n",
        "    # Use translate to remove all non-printable characters\n",
        "    return text.translate({character:None for character in nonprintable})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB3OrR5g8VYQ"
      },
      "source": [
        "# build a table mapping all non-printable characters to None\n",
        "\n",
        "NOPRINT_TRANS_TABLE = {\n",
        "    i: None for i in range(0, sys.maxunicode + 1) if not chr(i).isprintable()\n",
        "}\n",
        "\n",
        "def make_printable(s):\n",
        "    \"\"\"Replace non-printable characters in a string.\"\"\"\n",
        "\n",
        "    # the translate method on str removes characters\n",
        "    # that map to None from the string\n",
        "    return s.translate(NOPRINT_TRANS_TABLE)\n",
        "\n",
        "\n",
        "assert make_printable('Café') == 'Café'\n",
        "assert make_printable('\\x00\\x11Hello') == 'Hello'\n",
        "assert make_printable('') == ''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woDWvRZHjU2E"
      },
      "source": [
        "\"\"\"\n",
        "def corpus2chunks(corpus_filename, sent_tok='pysbd'):\n",
        "  '''\n",
        "  Given a corpus filename (assuming already %cd into correct subdir) \n",
        "    and a sentence tokeniziation method in ['pysbd'(default)|'both'|'nltk']\n",
        "  Return 6 lists and a string of the raw corpus:\n",
        "    4 Paragraph length lists -----\n",
        "    parag_raw_ls : list of raw text for each paragraph\n",
        "    parag_clean_ls : list of clean text for each sentence\n",
        "\n",
        "    parag_sentno_start_ls : list of the Sentence Number at the start of every Paragraph\n",
        "    parag_sentno_end_ls : list of the Sentence Number at the end of every Paragraph\n",
        "\n",
        "    2 Sentence length lists -----\n",
        "    sent_raw_ls : list of raw text for each sentence\n",
        "    sent_clean_ls : list of clean text for each sentence\n",
        "  '''\n",
        "\n",
        "  # Load PySBD if necessary\n",
        "  if (sent_tok == 'pysbd') | (sent_tok == 'both'):\n",
        "    from pysbd.utils import PySBDFactory\n",
        "    nlp = spacy.blank('en')\n",
        "    # explicitly adding component to pipeline\n",
        "    # (recommended - makes it more readable to tell what's going on)\n",
        "    nlp.add_pipe(PySBDFactory(nlp))\n",
        "    # pysbd = nlp.create_pipe('pysbd')\n",
        "    # nlp.add_pipe(pysbd)\n",
        "    # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n",
        "    # print(list(doc.sents))\n",
        "\n",
        "  # Read file into raw text string\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  # Split into Raw Paragraphs\n",
        "  print(f'BEFORE stripping out headings len: {len(corpus_raw_str)}')\n",
        "  corpus_parags_raw_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n",
        "  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n",
        "\n",
        "\n",
        "  # Copy/Clean Paragraphs into new list\n",
        "  # Filter out numbers(often footnotes) from Paragraphs\n",
        "  corpus_parags_ls = [re.sub(r'[0-9]','',x) for x in corpus_parags_raw_ls]\n",
        "\n",
        "  # Filter out empty lines Paragraphs\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n",
        "\n",
        "  # Strip out non-printing characters\n",
        "  corpus_parags_ls = [re.sub(f'[^{re.escape(string.printable)}]', '', x) for x in corpus_parags_ls]\n",
        "\n",
        "  print(f'   Parag count before processing sents: {len(corpus_parags_ls)}')\n",
        "  # FIRST PASS at Sentence Tokenization with PySBD\n",
        "  corpus_sents_all_ls = []\n",
        "  for i, aparag in enumerate(corpus_parags_ls):\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMA-040nIypX"
      },
      "source": [
        "def corpus2lines(corpus_filename, pysbd_only=False):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of every line defined by puncutation/NLTK.sent_tokenize or newlines [\\n]{2,}\n",
        "  '''\n",
        "\n",
        "  from pysbd.utils import PySBDFactory\n",
        "  nlp = spacy.blank('en')\n",
        "  # explicitly adding component to pipeline\n",
        "  # (recommended - makes it more readable to tell what's going on)\n",
        "  nlp.add_pipe(PySBDFactory(nlp))\n",
        "  # pysbd = nlp.create_pipe('pysbd')\n",
        "  # nlp.add_pipe(pysbd)\n",
        "  # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n",
        "  # print(list(doc.sents))\n",
        "\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  print(f'BEFORE stripping out headings len: {len(corpus_raw_str)}')\n",
        "\n",
        "  corpus_parags_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n",
        "  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n",
        "\n",
        "  # Strip off whitespace from Paragraphs\n",
        "  corpus_parags_ls = [x.strip() for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out numbers(often footnotes) from Paragraphs\n",
        "  corpus_parags_ls = [re.sub(r'[0-9]','',x) for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out empty lines Paragraphs\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n",
        "\n",
        "  # Strip out non-printing characters\n",
        "  corpus_parags_ls = [re.sub(f'[^{re.escape(string.printable)}]', '', x) for x in corpus_parags_ls]\n",
        "\n",
        "  print(f'   Parag count before processing sents: {len(corpus_parags_ls)}')\n",
        "  # FIRST PASS at Sentence Tokenization with PySBD\n",
        "  corpus_sents_all_ls = []\n",
        "  for i, aparag in enumerate(corpus_parags_ls):\n",
        "\n",
        "    # Generally PySBD outperforms NLTK and SpaCy, \n",
        "    #   but for Samuel Butler's 1900 translation of Homer's Odyssey\n",
        "    #   it failed in many cases so we combine/stack PySBD with NLTK\n",
        "    #   (exception to rule: NLTK > SpaCy for SentTokenization circa 2020\n",
        "    #    https://www.kaggle.com/questions-and-answers/130344)\n",
        "\n",
        "    # NLTK Sentence Tokenization\n",
        "    # 3605 lines with 'To the Lighthouse' by V.Woolf\n",
        "    # aparag_sents_ls = (sent_tokenize(aparag))\n",
        "    \n",
        "    # SpaCy Sentence Tokenization\n",
        "    # TODO: Speed up my specializaing pipe\n",
        "    # 3968 lines for 'To the Lighthouse' by V.Woolf\n",
        "    # doc = nlp(aparag)   \n",
        "    # aparag_sents_ls = [sent for sent in doc.sents]\n",
        "    # aparag_sents_ls = [x for x in doc]\n",
        "\n",
        "    # FIRST, tokenize with PySBD\n",
        "    # PySBD Sentence Tokenization\n",
        "    # 3457 lines for 'To the Lighthouse' by V.Woolf\n",
        "    # using pysbd and SpaCy\n",
        "    # or you can use it implicitly with keyword\n",
        "    \n",
        "\n",
        "    aparag_nonl = re.sub('[\\n]{1,}', ' ', aparag)\n",
        "    doc = nlp(aparag_nonl)\n",
        "    aparag_sents_first_ls = list(doc.sents)\n",
        "    print(f'pysbd found {len(aparag_sents_first_ls)} Sentences in Paragraph #{i}')\n",
        "\n",
        "    # Strip off whitespace from Sentences\n",
        "    aparag_sents_first_ls = [str(x).strip() for x in aparag_sents_first_ls]\n",
        "\n",
        "    # Filter out empty line Sentences\n",
        "    aparag_sents_first_ls = [x for x in aparag_sents_first_ls if (len(x.strip()) > MIN_SENT_LEN)]\n",
        "\n",
        "    print(f'      {len(aparag_sents_first_ls)} Sentences remain after cleaning')\n",
        "\n",
        "    corpus_sents_all_ls += aparag_sents_first_ls\n",
        "\n",
        "  # (OPTIONAL) SECOND PASS as Sentence Tokenization with NLTK\n",
        "  if pysbd_only == True:\n",
        "    # Only do one pass at Sentence tokenization with PySBD above\n",
        "    corpus_sents_all_ls = aparag_sents_first_ls\n",
        "  else:\n",
        "    # Do second pass, tokenize again with NLTK to catch any Sentence tokenization missed by PySBD\n",
        "    corpus_sents_all_second_ls = []\n",
        "    aparag_sents_second_ls = []\n",
        "    for asent_first in corpus_sents_all_ls:\n",
        "      aparag_sents_second_ls = sent_tokenize(asent_first)\n",
        "\n",
        "      # Strip off whitespace from Sentences\n",
        "      aparag_sents_second_ls = [str(x).strip() for x in aparag_sents_second_ls]\n",
        "\n",
        "      # Filter out empty line Sentences\n",
        "      aparag_sents_second_ls = [x for x in aparag_sents_second_ls if (len(x.strip()) > MIN_SENT_LEN)]\n",
        "\n",
        "      corpus_sents_all_second_ls += aparag_sents_second_ls\n",
        "\n",
        "    corpus_sents_all_ls = corpus_sents_all_second_ls\n",
        "\n",
        "  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n",
        "  # parag_before_punctstrip_ct = len(corpus_parags_ls)\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  # print(f'Punctuation only Paragraph Count: {len(corpus_parags_ls) - parag_before_punctstrip_ct}')\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Section separator 'SECTION ' lines\n",
        "  # for i,temp_str in enumerate(corpus_parags_ls):\n",
        "  #   if temp_str.startswith('SECTION '):\n",
        "  #     print(f'Parag #{i}: {temp_str}')\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('SECTION '))]\n",
        "\n",
        "  # Filter out any possible embedded 'SECTION ' lines\n",
        "  # for i,temp_str in enumerate(corpus_parags_ls):\n",
        "  #   if 'SECTION' in temp_str:   # .contains('SECTION '):\n",
        "  #     print(f'Parag #{i}: {temp_str}')\n",
        "  # corpus_parags_ls = del_substrs_list(corpus_parags_ls, pattern_sect) # [re.sub(rf'{pattern_sect}', '', x) for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out the Chapter separator 'CHAPTER ' lines\n",
        "  # for i,temp_str in enumerate(corpus_parags_ls):\n",
        "  #   if temp_str.startswith('CHAPTER '):\n",
        "  #     print(f'Parag #{i}: {temp_str}')\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('CHAPTER '))]\n",
        "\n",
        "  print(f'About to return corpus_sents_all_ls with len = {len(corpus_sents_all_ls)}')\n",
        "  return corpus_sents_all_ls, corpus_raw_str\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5nrHLut8AEl"
      },
      "source": [
        "def corpus2sects(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a 3 lists: First, the list of Section text strings\n",
        "                    Second, a list of tuples that match (Sentence No, Segment No)\n",
        "                    Third, a list of Sentences that not found in any Section \n",
        "  '''\n",
        "\n",
        "  corpus_sects_ls = []\n",
        "\n",
        "\n",
        "  # encoding = CORPUS_ENCODING,  'windows-1252', 'utf-8', 'cp1252', 'iso-8859-1'\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  # pattern_sect = 'SECTION [\\d]{1,2}[.]?[^\\n]*'\n",
        "  # pattern_sect = 'SECTION [0123456789]{1,2}[^\\n]*'\n",
        "  # corpus_sects_ls = re.split(r'SECTION [\\d]{1,2}[.]* [A-Z \\.-:—;-’\\'\"]*[\\n]*', corpus_raw_str flags=re.I) # , flags=re.I)\n",
        "  corpus_sects_ls = re.split(rf'{pattern_sect}', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  # corpus_sects_ls = re.split(rf'{pattern_sect}', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  print(f'len(corpus_raw_str: {len(corpus_raw_str)}')\n",
        "  print(f'len(corpus_sects_ls): {len(corpus_sects_ls)}')\n",
        "  print(f'    First section: Length={len(corpus_sects_ls[0])}\\n    {corpus_sects_ls[0][:500]}')\n",
        "  print(f'    Second section: {corpus_sects_ls[1][:500]}')\n",
        "  print('\\n')\n",
        "  print(f'    Second-Last section: {corpus_sects_ls[-2][:500]}')\n",
        "  print(f'    Last section: {corpus_sects_ls[-1][:500]}')\n",
        "\n",
        "\n",
        "\n",
        "  # Strip off whitespace \n",
        "  corpus_sects_ls = [x.strip() for x in corpus_sects_ls]\n",
        "\n",
        "  # Filter out empty lines\n",
        "  corpus_sects_ls = [x for x in corpus_sects_ls if not (len(x.strip()) <= MIN_SECT_LEN)]\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Section separator 'SECTION ' lines\n",
        "  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('SECTION '))]\n",
        "\n",
        "  # Filter out any possible embedded 'CHAPTER ' lines\n",
        "  # TODO: Zeroing out corpus_sects_ls\n",
        "  # corpus_sects_ls = del_substrs_list(corpus_sects_ls, pattern_chap) # corpus_sects_ls = [re.sub(rf'{pattern_sect}', '', x) for x in corpus_sects_ls]\n",
        "\n",
        "  # Filter out the Chapter separator 'CHAPTER ' lines\n",
        "  # Keep for now, messy but enables proper SECTION assignments to appropraite CHAPTERs\n",
        "  # corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('CHAPTER '))]\n",
        "\n",
        "\n",
        "  print(f'About to process {len(corpus_sects_ls)} Sections')\n",
        "  # Filter out Sentences in Section that don't have a corresponding Sentence in corpus_sents_df \n",
        "  # Old Strategy: Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \n",
        "\n",
        "  sect_sentences_match_ls = []\n",
        "  sect_sentences_unmatch_ls = []\n",
        "  sect_sentences_unmatch_ct = 0\n",
        "  corpus_sentence_current = 0\n",
        "\n",
        "\n",
        "  for asect_no, asection in enumerate(corpus_sects_ls):\n",
        "    # sect_sentences_ls = []\n",
        "\n",
        "    # NLTK Sentence Tokenization\n",
        "    # 3605 lines with 'To the Lighthouse' by V.Woolf\n",
        "    # asect_sents_ls = (sent_tokenize(asect))\n",
        "    \n",
        "    # SpaCy Sentence Tokenization\n",
        "    # TODO: Speed up my specializaing pipe\n",
        "    # 3968 lines for 'To the Lighthouse' by V.Woolf\n",
        "    # doc = nlp(asect)   \n",
        "    # asect_sents_ls = [sent for sent in doc.sents]\n",
        "    # asect_sents_ls = [x for x in doc]\n",
        "\n",
        "    # PySBD Sentence Tokenization\n",
        "    # 3457 lines for 'To the Lighthouse' by V.Woolf\n",
        "    # using pysbd and SpaCy\n",
        "    from pysbd.utils import PySBDFactory\n",
        "    nlp = spacy.blank('en')\n",
        "    # explicitly adding component to pipeline\n",
        "    # (recommended - makes it more readable to tell what's going on)\n",
        "    nlp.add_pipe(PySBDFactory(nlp))\n",
        "    # or you can use it implicitly with keyword\n",
        "    # pysbd = nlp.create_pipe('pysbd')\n",
        "    # nlp.add_pipe(pysbd)\n",
        "    # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n",
        "    # print(list(doc.sents))\n",
        "    doc = nlp(asection)\n",
        "    asect_sents_ls = list(doc.sents)\n",
        "\n",
        "    # Create a normalized/no puncutation list of Corpus sentences for clean test comparisions filtered\n",
        "    corpus_sents_nopunct_ls = [re.sub(r'[^A-Za-z0-9]', ' ',x) for x in corpus_sents_ls]\n",
        "    # corpus_sents_nopunct_ls = [x for x in corpus_sents_nopunct_ls if x.isalnum()]\n",
        "    corpus_sents_nopunct_strip_ls = [x.strip() for x in corpus_sents_nopunct_ls]\n",
        "\n",
        "    for j, asection_sentence_raw in enumerate(asect_sents_ls):\n",
        "      asection_sentence_str = str(asection_sentence_raw)\n",
        "      asection_sentence_nopunct_str = re.sub(r'[^A-Za-z0-9]', ' ', asection_sentence_str)\n",
        "      asection_sentence_nopunct_strip_str = asection_sentence_nopunct_str.strip()\n",
        "      # This 'in' test is not sufficient, need to strip out punctuation/normalize\n",
        "      if asection_sentence_nopunct_strip_str in corpus_sents_nopunct_strip_ls:\n",
        "        sect_sentences_match_ls.append((asect_no, asection_sentence_str))\n",
        "        print(f'  Matched Segment Sent')\n",
        "      else:\n",
        "        sect_sentences_unmatch_ct += 1\n",
        "        print(f'  UNMATCHED Corpus Sentence #[{corpus_sentence_current}]\\n           Segment Sentence #{j}: [{asection_sentence_str}]\\n            [{asection_sentence_nopunct_strip_str}]')\n",
        "        sect_sentences_unmatch_ls.append(asection_sentence_str)\n",
        "\n",
        "      corpus_sentence_current += 1\n",
        "\n",
        "    # section_str = ' '.join(sect_sentences_ls)\n",
        "    # sect_sentences_match_ls.append(section_str)\n",
        "\n",
        "    # if re.search(rf'{pattern_chap}', asect):\n",
        "    #   print(f'In Section #{i} removing embedded CHAPTER:\\n\\n    {asect}')\n",
        "    #   asect = re.sub(rf'{pattern_chap}', ' ', asect)\n",
        "\n",
        "  return corpus_sects_ls, sect_sentences_match_ls, sect_sentences_unmatch_ls\n",
        "\n",
        "\n",
        "# return corpus_sects_ls, corpus_raw_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8z8ZY5n8BRs"
      },
      "source": [
        "def corpus2parags(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of min preprocessed raw paragraphs (corpus_parags_ls)\n",
        "  '''\n",
        "\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  corpus_parags_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n",
        "  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n",
        "\n",
        "  # Strip off whitespace\n",
        "  corpus_parags_ls = [x.strip() for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out numbers(often footnotes) from Paragraphs\n",
        "  corpus_parags_ls = [re.sub(r'[0-9]',' ',x) for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  # Redundant, filed by punctuation only filter above\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Chapter/Section header lines\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('CHAPTER '))]\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('SECTION '))]\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('BOOK '))]\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (re.match(r\"^[ ]*[0-9]{1,3}[\\.]?[ ]*$\", x))]\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (re.match(r\"^[ ]*[IVXLC]{1,10}[\\.]?[ ]*$\", x))]\n",
        "\n",
        "  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n",
        "  parag_before_punctstrip_ct = len(corpus_parags_ls)\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  print(f'Punctuation only Paragraph Count: {len(corpus_parags_ls) - parag_before_punctstrip_ct}')\n",
        "\n",
        "  # Filter out empty lines Paragraphs\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n",
        "\n",
        "  # Made a deepcopy of the original raw paragraphs after simple cleaning while continuing to clean the original\n",
        "  corpus_parags_raw_ls = [x for x in corpus_parags_ls]\n",
        "\n",
        "  # Strip out non-printing characters\n",
        "  corpus_parags_ls = [re.sub(f'[^{re.escape(string.printable)}]', ' ', x) for x in corpus_parags_ls]\n",
        "\n",
        "  # Condense multiple whitespaces down into one\n",
        "  corpus_parags_ls = [' '.join(x.split()).strip() for x in corpus_parags_ls]\n",
        "\n",
        "  # Verify no Chapter/Section header lines remain\n",
        "  for i,temp_str in enumerate(corpus_parags_ls):\n",
        "    if temp_str.startswith('CHAPTER '):\n",
        "      print(f'Parag #{i}: {temp_str}')\n",
        "\n",
        "  return corpus_parags_ls, corpus_parags_raw_ls, corpus_raw_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxTYt6xsD5Iq"
      },
      "source": [
        "def sect2parags(sect_str):\n",
        "  '''\n",
        "  Given a Section as a text string\n",
        "  Return a list of raw Paragraphs and a raw Section text string\n",
        "  '''\n",
        "\n",
        "  sect_clean_str = re.sub(f'[^{re.escape(string.printable)}]', ' ', sect_str)\n",
        "  sect_parags_raw_ls = re.split(r'[\\n]{2,}', sect_clean_str)\n",
        "  # print(f'Section Paragraph Raw Count: {len(sect_parags_raw_ls)}')\n",
        "\n",
        "  # Strip off whitespace\n",
        "  sect_parags_raw_ls = [x.strip() for x in sect_parags_raw_ls]\n",
        "\n",
        "  # Filter out numbers(often footnotes) from Paragraphs\n",
        "  sect_parags_raw_ls = [re.sub(r'[0-9]',' ',x) for x in sect_parags_raw_ls]\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  # Redundant, filed by punctuation only filter above\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Chapter/Section header lines\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('CHAPTER '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('SECTION '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('BOOK '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[0-9]{1,3}[\\.]?[ ]*$\", x))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[IVXLC]{1,10}[\\.]?[ ]*$\", x))]\n",
        "\n",
        "  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n",
        "  parag_before_punctstrip_ct = len(sect_parags_raw_ls)\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  # print(f'Punctuation only Paragraph Count: {len(sect_parags_raw_ls) - parag_before_punctstrip_ct}')\n",
        "\n",
        "  # Filter out Paragraphs that are empty or shorter than MIN_PARAG_LEN\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n",
        "\n",
        "  # Made a clean copy of the original raw paragraphs after simple cleaning while continuing to clean the original\n",
        "  sect_parags_clean_ls = [clean_text(x) for x in sect_parags_raw_ls]\n",
        "\n",
        "  # Verify no Chapter/Section header lines remain\n",
        "  for i,temp_str in enumerate(sect_parags_raw_ls):\n",
        "    if temp_str.startswith('CHAPTER '):\n",
        "      print(f'ERROR: CHAPTERS not filtered\\n    Parag #{i}: {temp_str}')\n",
        "\n",
        "  return sect_parags_raw_ls, sect_clean_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on9yiXYxcnJQ"
      },
      "source": [
        "test_str = \"\"\"\n",
        "CHAPTER 2.\n",
        "\n",
        "The biographer is now faced with a difficulty which it is better perhaps to confess than to gloss over. Up to this point in telling the story of Orlando's life, documents, both private and historical, have made it possible to fulfil the first duty of a biographer, which is to plod, without looking to right or left, in the indelible footprints of truth; unenticed by flowers; regardless of shade; on and on methodically till we fall plump into the grave and write finis on the tombstone above our heads. But now we come to an episode which lies right across our path, so that there is no ignoring it. Yet it is dark, mysterious, and undocumented; so that there is no explaining it. Volumes might be written in interpretation of it; whole religious systems founded upon the signification of it. Our simple duty is to state the facts as far as they are known, and so let the reader make of them what he may.\n",
        "\n",
        "In the summer of that disastrous winter which saw the frost, the flood, the deaths of many thousands, and the complete downfall of Orlando's hopes--for he was exiled from Court; in deep disgrace with the most powerful nobles of his time; the Irish house of Desmond was justly enraged; the King had already trouble enough with the Irish not to relish this further addition--in that summer Orlando retired to his great house in the country and there lived in complete solitude. One June morning--it was Saturday the 18th--he failed to rise at his usual hour, and when his groom went to call him he was found fast asleep. Nor could he be awakened. He lay as if in a trance, without perceptible breathing; and though dogs were set to bark under his window; cymbals, drums, bones beaten perpetually in his room; a gorse bush put under his pillow; and mustard plasters applied to his feet, still he did not wake, take food, or show any sign of life for seven whole days. On the seventh day he woke at his usual time (a quarter before eight, precisely) and turned the whole posse of caterwauling wives and village soothsayers out of his room, which was natural enough; but what was strange was that he showed no consciousness of any such trance, but dressed himself and sent for his horse as if he had woken from a single night's slumber. Yet some change, it was suspected, must have taken place in the chambers of his brain, for though he was perfectly rational and seemed graver and more sedate in his ways than before, he appeared to have an imperfect recollection of his past life. He would listen when people spoke of the great frost or the skating or the carnival, but he never gave any sign, except by passing his hand across his brow as if to wipe away some cloud, of having witnessed them himself. When the events of the past six months were discussed, he seemed not so much distressed as puzzled, as if he were troubled by confused memories of some time long gone or were trying to recall stories told him by another. It was observed that if Russia was mentioned or Princesses or ships, he would fall into a gloom of an uneasy kind and get up and look out of the window or call one of the dogs to him, or take a knife and carve a piece of cedar wood. But the doctors were hardly wiser then than they are now, and after prescribing rest and exercise, starvation and nourishment, society and solitude, that he should lie in bed all day and ride forty miles between lunch and dinner, together with the usual sedatives and irritants, diversified, as the fancy took them, with possets of newt's slobber on rising, and draughts of peacock's gall on going to bed, they left him to himself, and gave it as their opinion that he had been asleep for a week.\n",
        "\n",
        "But if sleep it was, of what nature, we can scarcely refrain from asking, are such sleeps as these? Are they remedial measures--trances in which the most galling memories, events that seem likely to cripple life for ever, are brushed with a dark wing which rubs their harshness off and gilds them, even the ugliest and basest, with a lustre, an incandescence? Has the finger of death to be laid on the tumult of life from time to time lest it rend us asunder? Are we so made that we have to take death in small doses daily or we could not go on with the business of living? And then what strange powers are these that penetrate our most secret ways and change our most treasured possessions without our willing it? Had Orlando, worn out by the extremity of his suffering, died for a week, and then come to life again? And if so, of what nature is death and of what nature life? Having waited well over half an hour for an answer to these questions, and none coming, let us get on with the story.\n",
        "\n",
        "Now Orlando gave himself up to a life of extreme solitude. His disgrace at Court and the violence of his grief were partly the reason of it, but as he made no effort to defend himself and seldom invited anyone to visit him (though he had many friends who would willingly have done so) it appeared as if to be alone in the great house of his fathers suited his temper. Solitude was his choice. How he spent his time, nobody quite knew. The servants, of whom he kept a full retinue, though much of their business was to dust empty rooms and to smooth the coverlets of beds that were never slept in, watched, in the dark of the evening, as they sat over their cakes and ale, a light passing along the galleries, through the banqueting-halls, up the staircase, into the bedrooms, and knew that their master was perambulating the house alone. None dared follow him, for the house was haunted by a great variety of ghosts, and the extent of it made it easy to lose one's way and either fall down some hidden staircase or open a door which, should the wind blow it to, would shut upon one for ever--accidents of no uncommon occurrence, as the frequent discovery of the skeletons of men and animals in attitudes of great agony made evident. Then the light would be lost altogether, and Mrs Grimsditch, the housekeeper, would say to Mr Dupper, the chaplain, how she hoped his Lordship had not met with some bad accident. Mr Dupper would opine that his Lordship was on his knees, no doubt, among the tombs of his ancestors in the Chapel, which was in the Billiard Table Court, half a mile away on the south side. For he had sins on his conscience, Mr Dupper was afraid; upon which Mrs Grimsditch would retort, rather sharply, that so had most of us; and Mrs Stewkley and Mrs Field and old Nurse Carpenter would all raise their voices in his Lordship's praise; and the grooms and the stewards would swear that it was a thousand pities to see so fine a nobleman moping about the house when he might be hunting the fox or chasing the deer; and even the little laundry maids and scullery maids, the Judys and the Faiths, who were handing round the tankards and cakes, would pipe up their testimony to his Lordship's gallantry; for never was there a kinder gentleman, or one more free with those little pieces of silver which serve to buy a knot of ribbon or put a posy in one's hair; until even the Blackamoor whom they called Grace Robinson by way of making a Christian woman of her, understood what they were at, and agreed that his Lordship was a handsome, pleasant, darling gentleman in the only way she could, that is to say by showing all her teeth at once in a broad grin. In short, all his serving men and women held him in high respect, and cursed the foreign Princess (but they called her by a coarser name than that) who had brought him to this pass.\n",
        "\n",
        "But though it was probably cowardice, or love of hot ale, that led Mr Dupper to imagine his Lordship safe among the tombs so that he need not go in search of him, it may well have been that Mr Dupper was right. Orlando now took a strange delight in thoughts of death and decay, and, after pacing the long galleries and ballrooms with a taper in his hand, looking at picture after picture as if he sought the likeness of somebody whom he could not find, would mount into the family pew and sit for hours watching the banners stir and the moonlight waver with a bat or death's head moth to keep him company. Even this was not enough for him, but he must descend into the crypt where his ancestors lay, coffin piled upon coffin, for ten generations together. The place was so seldom visited that the rats made free with the lead work, and now a thigh bone would catch at his cloak as he passed, or he would crack the skull of some old Sir Malise as it rolled beneath his foot. It was a ghastly sepulchre; dug deep beneath the foundations of the house as if the first Lord of the family, who had come from France with the Conqueror, had wished to testify how all pomp is built upon corruption; how the skeleton lies beneath the flesh: how we that dance and sing above must lie below; how the crimson velvet turns to dust; how the ring (here Orlando, stooping his lantern, would pick up a gold circle lacking a stone, that had rolled into a corner) loses its ruby and the eye which was so lustrous shines no more. 'Nothing remains of all these Princes', Orlando would say, indulging in some pardonable exaggeration of their rank, 'except one digit,' and he would take a skeleton hand in his and bend the joints this way and that. 'Whose hand was it?' he went on to ask. 'The right or the left? The hand of man or woman, of age or youth? Had it urged the war horse, or plied the needle? Had it plucked the rose, or grasped cold steel? Had it--' but here either his invention failed him or, what is more likely, provided him with so many instances of what a hand can do that he shrank, as his wont was, from the cardinal labour of composition, which is excision, and he put it with the other bones, thinking how there was a writer called Thomas Browne, a Doctor of Norwich, whose writing upon such subjects took his fancy amazingly.\n",
        "\n",
        "So, taking his lantern and seeing that the bones were in order, for though romantic, he was singularly methodical and detested nothing so much as a ball of string on the floor, let alone the skull of an ancestor, he returned to that curious, moody pacing down the galleries, looking for something among the pictures, which was interrupted at length by a veritable spasm of sobbing, at the sight of a Dutch snow scene by an unknown artist. Then it seemed to him that life was not worth living any more. Forgetting the bones of his ancestors and how life is founded on a grave, he stood there shaken with sobs, all for the desire of a woman in Russian trousers, with slanting eyes, a pouting mouth and pearls about her neck. She had gone. She had left him. He was never to see her again. And so he sobbed. And so he found his way back to his own rooms; and Mrs Grimsditch, seeing the light in the window, put the tankard from her lips and said Praise be to God, his Lordship was safe in his room again; for she had been thinking all this while that he was foully murdered.\n",
        "\n",
        "Orlando now drew his chair up to the table; opened the works of Sir Thomas Browne and proceeded to investigate the delicate articulation of one of the doctor's longest and most marvellously contorted cogitations.\n",
        "\n",
        "For though these are not matters on which a biographer can profitably enlarge it is plain enough to those who have done a reader's part in making up from bare hints dropped here and there the whole boundary and circumference of a living person; can hear in what we only whisper a living voice; can see, often when we say nothing about it, exactly what he looked like; know without a word to guide them precisely what he thought--and it is for readers such as these that we write--it is plain then to such a reader that Orlando was strangely compounded of many humours--of melancholy, of indolence, of passion, of love of solitude, to say nothing of all those contortions and subtleties of temper which were indicated on the first page, when he slashed at a dead nigger's head; cut it down; hung it chivalrously out of his reach again and then betook himself to the windowseat with a book. The taste for books was an early one. As a child he was sometimes found at midnight by a page still reading. They took his taper away, and he bred glow-worms to serve his purpose. They took the glow-worms away, and he almost burnt the house down with a tinder. To put it in a nutshell, leaving the novelist to smooth out the crumpled silk and all its implications, he was a nobleman afflicted with a love of literature. Many people of his time, still more of his rank, escaped the infection and were thus free to run or ride or make love at their own sweet will. But some were early infected by a germ said to be bred of the pollen of the asphodel and to be blown out of Greece and Italy, which was of so deadly a nature that it would shake the hand as it was raised to strike, and cloud the eye as it sought its prey, and make the tongue stammer as it declared its love. It was the fatal nature of this disease to substitute a phantom for reality, so that Orlando, to whom fortune had given every gift--plate, linen, houses, men-servants, carpets, beds in profusion--had only to open a book for the whole vast accumulation to turn to mist. The nine acres of stone which were his house vanished; one hundred and fifty indoor servants disappeared; his eighty riding horses became invisible; it would take too long to count the carpets, sofas, trappings, china, plate, cruets, chafing dishes and other movables often of beaten gold, which evaporated like so much sea mist under the miasma. So it was, and Orlando would sit by himself, reading, a naked man.\n",
        "\n",
        "The disease gained rapidly upon him now in his solitude. He would read often six hours into the night; and when they came to him for orders about the slaughtering of cattle or the harvesting of wheat, he would push away his folio and look as if he did not understand what was said to him. This was bad enough and wrung the hearts of Hall, the falconer, of Giles, the groom, of Mrs Grimsditch, the housekeeper, of Mr Dupper, the chaplain. A fine gentleman like that, they said, had no need of books. Let him leave books, they said, to the palsied or the dying. But worse was to come. For once the disease of reading has laid upon the system it weakens it so that it falls an easy prey to that other scourge which dwells in the inkpot and festers in the quill. The wretch takes to writing. And while this is bad enough in a poor man, whose only property is a chair and a table set beneath a leaky roof--for he has not much to lose, after all--the plight of a rich man, who has houses and cattle, maidservants, asses and linen, and yet writes books, is pitiable in the extreme. The flavour of it all goes out of him; he is riddled by hot irons; gnawed by vermin. He would give every penny he has (such is the malignity of the germ) to write one little book and become famous; yet all the gold in Peru will not buy him the treasure of a well-turned line. So he falls into consumption and sickness, blows his brains out, turns his face to the wall. It matters not in what attitude they find him. He has passed through the gates of Death and known the flames of Hell.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "sect_parags_raw_ls, sect_clean_str =sect2parags(test_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QndUpmJac5xa"
      },
      "source": [
        "len(sect_parags_raw_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWw618b3vQmJ"
      },
      "source": [
        "from pysbd.utils import PySBDFactory\n",
        "nlp = spacy.blank('en')\n",
        "# explicitly adding component to pipeline\n",
        "# (recommended - makes it more readable to tell what's going on)\n",
        "nlp.add_pipe(PySBDFactory(nlp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b38jIZ-7cyQ"
      },
      "source": [
        "# parag_sents_raw_ls, parag_clean_str = parag2sents(aparag_raw)\n",
        "'''\n",
        "test_str = \"\"\"\n",
        "As they neared the shore each bar rose, heaped itself, broke and swept a thin veil of white water across the sand. The wave paused, and then drew out again, sighing like a sleeper whose breath comes and goes unconsciously. Gradually the dark bar on the horizon became clear as if the sediment in an old wine-bottle had sunk and left the glass green. Behind it, too, the sky cleared as if the white sediment there had sunk, or as if the arm of a woman couched beneath the horizon had raised a lamp and flat bars of white, green and yellow spread across the sky like the blades of a fan. Then she raised her lamp higher and the air seemed to become fibrous and to tear away from the green surface flickering and flaming in red and yellow fibres like the smoky fire that roars from a bonfire. Gradually the fibres of the burning bonfire were fused into one haze, one incandescence which lifted the weight of the woollen grey sky on top of it and turned it to a million atoms of soft blue. The surface of the sea slowly became transparent and lay rippling and sparkling until the dark stripes were almost rubbed out. Slowly the arm that held the lamp raised it higher and then higher until a broad flame became visible; an arc of fire burnt on the rim of the horizon, and all round it the sea blazed gold.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "aparag_sents_raw_ls, aparag_clean_str = parag2sents(test_str)\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwuzAJ-A7zuo"
      },
      "source": [
        "!pip install pysbd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KocjF0v8cf1"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0gacUB-8YMa"
      },
      "source": [
        "import pysbd\n",
        "import spacy\n",
        "from pysbd.utils import PySBDFactory\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "\n",
        "# explicitly adding component to pipeline\n",
        "# (recommended - makes it more readable to tell what's going on)\n",
        "nlp.add_pipe(PySBDFactory(nlp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp0i_8PE71aL"
      },
      "source": [
        "\n",
        "\n",
        "# or you can use it implicitly with keyword\n",
        "# pysbd = nlp.create_pipe('pysbd')\n",
        "# nlp.add_pipe(pysbd)\n",
        "\n",
        "doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n",
        "print(list(doc.sents))\n",
        "# [My name is Jonas E. Smith., Please turn to p. 55.]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e0OnfrGJ_GB"
      },
      "source": [
        "def parag2sents(parag_str, sent_tok='pysbd'):\n",
        "  '''\n",
        "  Given a Paragraph as a long string and a Sentence tokenizer engine ['pysbd'(default)|'both'|'nltk']\n",
        "  Return a list of every Sentence within the given Paragraph\n",
        "  '''\n",
        "\n",
        "  # Load PySBD if necessary\n",
        "  \"\"\"\n",
        "  if (sent_tok == 'pysbd') | (sent_tok == 'both'):\n",
        "    from pysbd.utils import PySBDFactory\n",
        "    nlp = spacy.blank('en')\n",
        "    # explicitly adding component to pipeline\n",
        "    # (recommended - makes it more readable to tell what's going on)\n",
        "    nlp.add_pipe(PySBDFactory(nlp))\n",
        "    # pysbd = nlp.create_pipe('pysbd')\n",
        "    # nlp.add_pipe(pysbd)\n",
        "    # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n",
        "    # print(list(doc.sents))\n",
        "  \"\"\";\n",
        "\n",
        "  # Minimally clean Paragraph string of non-printing characters\n",
        "  parag_clean_str = re.sub(f'[^{re.escape(string.printable)}]', ' ', parag_str).strip()\n",
        "\n",
        "  # Replace embedded \\n with spaces\n",
        "  parag_clean_str = re.sub('[\\n]{1,}', ' ', parag_clean_str)\n",
        "\n",
        "  # Replaces mulitple whitespaces with one space\n",
        "  parag_clean_str = ' '.join(parag_clean_str.split())\n",
        "\n",
        "\n",
        "  # TOKENIZE with 1 of 3 ways\n",
        " \n",
        "  # TODO: Try simple/fast RegEx Tokenizer in lieu of NTLK to complement PyBSD\n",
        "\n",
        "  # ONE: Tokenize with PyBSD and NLTK \n",
        "  if (sent_tok == 'both'):\n",
        "    doc = nlp(parag_clean_str)\n",
        "    parag_sents_first_ls = list(doc.sents)\n",
        "    for asent_temp in parag_sents_first_ls:\n",
        "      asent_tokenized_temp = sent_tokenize(asent_temp)\n",
        "      parag_sents_ls.append(asent_tokenized_temp)\n",
        "    # print(f'  BOTH: {len(parag_sents_ls)} Sentences found in Paragraph')\n",
        "\n",
        "  # TWO: Tokenize with PyBSD\n",
        "  elif (sent_tok == 'pysbd'):\n",
        "    doc = nlp(parag_clean_str)\n",
        "    parag_sents_ls = list(doc.sents)\n",
        "    # print(f' PySBD: {len(parag_sents_ls)} Sentences found in Paragraph')\n",
        "\n",
        "  # THREE: Tokenize with NLTK\n",
        "  elif (sent_tok == 'nltk'):\n",
        "    parag_sents_ls = sent_tokenize(parag_clean_str)\n",
        "    # print(f' NLTK: {len(parag_sents_ls)} Sentences found in Paragraph')\n",
        "\n",
        "  # ERROR\n",
        "  else:\n",
        "    print(f'ERROR: sent_tok={sent_tok} but must be [pysbd(default)|both|nltk]')\n",
        "\n",
        "\n",
        "  # CLEAN Sentences\n",
        "\n",
        "  # Strip off whitespace from Sentences\n",
        "  parag_sents_ls = [str(x).strip() for x in parag_sents_ls]\n",
        "\n",
        "  # Copy/Clean Sentences into new list\n",
        "  # Filter out numbers(often footnotes) from Sentences\n",
        "  parag_sents_ls = [re.sub(r'[0-9]',' ',x) for x in parag_sents_ls]\n",
        "\n",
        "  # Filter out the Chapter/Section header lines\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*CHAPTER[\\s]*$\", x))]\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*SECTION[\\s]*$\", x))]\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]BOOK[\\s]*$\", x))]\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*[0-9]{1,3}[\\.]?[\\s]*$\", x))]\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*[IVXLC]{1,10}[\\.]?[\\s]*$\", x))]\n",
        "\n",
        "  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n",
        "  parag_before_punctstrip_ct = len(parag_sents_ls)\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  # print(f'Punctuation only Paragraph Count: {len(parag_sents_ls) - parag_before_punctstrip_ct}')\n",
        "\n",
        "  # Condense multiple consecutive whitespaces down to one whitespace\n",
        "  parag_sents_ls = [' '.join(x.split()) for x in parag_sents_ls]\n",
        "\n",
        "\n",
        "  # Filter Sentences that are empty or shorter than MIN_SENT_LEN\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if (len(x.strip()) >= MIN_SENT_LEN)]\n",
        "\n",
        "  return parag_sents_ls, parag_clean_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX1gjU7S8esY"
      },
      "source": [
        "\"\"\"\n",
        "def parag2sents(parag_str, sent_tok='py'):\n",
        "  '''\n",
        "  Given a Paragraph as a string,\n",
        "  Return a list of raw Sentences and a raw text Paragraph string\n",
        "  '''\n",
        "\n",
        "  parag_clean_str = re.sub(f'[^{re.escape(string.printable)}]', '', parag_str)\n",
        "  parag_parags_raw_ls = re.split(r'[\\n]{2,}', parag_clean_str)\n",
        "  print(f'Section Paragraph Raw Count: {len(parag_parags_raw_ls)}')\n",
        "\n",
        "  # Strip off whitespace\n",
        "  parag_parags_raw_ls = [x.strip() for x in parag_parags_raw_ls]\n",
        "\n",
        "  # Filter out numbers(often footnotes) from Paragraphs\n",
        "  sect_parags_raw_ls = [re.sub(r'[0-9]','',x) for x in sect_parags_raw_ls]\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  # Redundant, filed by punctuation only filter above\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Chapter/Section header lines\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('CHAPTER '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('SECTION '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('BOOK '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[0-9]{1,3}[\\.]?[ ]*$\", x))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[IVXLC]{1,10}[\\.]?[ ]*$\", x))]\n",
        "\n",
        "\n",
        "  parag_no = 0\n",
        "  # sent_base = 0\n",
        "  corpus_sents_ls = []\n",
        "  for parag_no,aparag in enumerate(corpus_parags_ls):\n",
        "    sents_ls = sent_tokenize(aparag)\n",
        "    # Delete (whitespace only) sentences\n",
        "    sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n",
        "    # Delete (punctuation only) sentences\n",
        "    sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n",
        "    # Delete numbers (int or float) sentences\n",
        "    sents_ls = [x for x in sents_ls if not (x.strip().isnumeric())]\n",
        "\n",
        "    # Filter out leading SECTION separator 'SECTION ' lines\n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      if temp_str.startswith('SECTION '):\n",
        "        print(f'Sentence #{i}: {temp_str}')\n",
        "    sents_ls = [x for x in sents_ls if not (x.startswith('SECTION '))]\n",
        "\n",
        "    # Filter out leading Chapter separator 'CHAPTER ' lines\n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      if temp_str.startswith('CHAPTER '):\n",
        "        print(f'Sentence #{i}: {temp_str}')\n",
        "    sents_ls = [x for x in sents_ls if not (x.startswith('CHAPTER '))]\n",
        "    \n",
        "    # Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      # TODO: More specific, robust filtering mechnism \n",
        "      if (re.search(rf'{pattern_sect}', temp_str)):\n",
        "        pass\n",
        "      if (re.search(rf'{pattern_chap}', temp_str)):\n",
        "        pass\n",
        "      else:\n",
        "        corpus_sents_ls.append([sent_no, parag_no, temp_str])\n",
        "        sent_no += 1\n",
        "\n",
        "    # print(f'Returning with corpus_sents_ls length = {len(corpus_sents_ls)}')\n",
        "  \n",
        "  return corpus_sents_ls\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRopU4e3IQ2R"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "def parag2sents(corpus_parags_ls):\n",
        "  '''\n",
        "  Given a list of paragraphs,\n",
        "  Return a list of lists of Sentences [sent_no, parag_no, asent(text)]\n",
        "  '''\n",
        "\n",
        "  sent_no = 0\n",
        "  # sent_base = 0\n",
        "  corpus_sents_ls = []\n",
        "  for parag_no,aparag in enumerate(corpus_parags_ls):\n",
        "    sents_ls = sent_tokenize(aparag)\n",
        "    # Delete (whitespace only) sentences\n",
        "    sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n",
        "    # Delete (punctuation only) sentences\n",
        "    sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n",
        "    # Delete numbers (int or float) sentences\n",
        "    sents_ls = [x for x in sents_ls if not (x.strip().isnumeric())]\n",
        "\n",
        "    # Filter out leading SECTION separator 'SECTION ' lines\n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      if temp_str.startswith('SECTION '):\n",
        "        print(f'Sentence #{i}: {temp_str}')\n",
        "    sents_ls = [x for x in sents_ls if not (x.startswith('SECTION '))]\n",
        "\n",
        "    # Filter out leading Chapter separator 'CHAPTER ' lines\n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      if temp_str.startswith('CHAPTER '):\n",
        "        print(f'Sentence #{i}: {temp_str}')\n",
        "    sents_ls = [x for x in sents_ls if not (x.startswith('CHAPTER '))]\n",
        "    \n",
        "    # Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      # TODO: More specific, robust filtering mechnism \n",
        "      if (re.search(rf'{pattern_sect}', temp_str)):\n",
        "        pass\n",
        "      if (re.search(rf'{pattern_chap}', temp_str)):\n",
        "        pass\n",
        "      else:\n",
        "        corpus_sents_ls.append([sent_no, parag_no, temp_str])\n",
        "        sent_no += 1\n",
        "\n",
        "    # print(f'Returning with corpus_sents_ls length = {len(corpus_sents_ls)}')\n",
        "  \n",
        "  return corpus_sents_ls\n",
        "\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-adBsfM38eo0"
      },
      "source": [
        "# corpus_sents_ls = parag2sents(corpus_parags_ls)\n",
        "# len(corpus_sents_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaRD7RL9IOeg"
      },
      "source": [
        "# Generate full path and timestamp for new filepath/filename\n",
        "\n",
        "def gen_pathfiletime(file_str, subdir_str=''):\n",
        "\n",
        "  # Geenreate compressed author and title substrings\n",
        "  author_raw_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "  title_raw_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "\n",
        "  # Generate current/unique datetime string\n",
        "  datetime_str = str(datetime.now().strftime('%Y%m%d%H%M%S'))\n",
        "\n",
        "  # Built fullpath+filename string\n",
        "  file_base, file_ext = file_str.split('.')\n",
        "\n",
        "  author_str = re.sub('[^A-Za-z0-9]+', '', author_raw_str)\n",
        "  title_str = re.sub('[^A-Za-z0-9]+', '', title_raw_str)\n",
        "\n",
        "  full_filepath_str = f'{subdir_str}{file_base}_{author_str}_{title_str}_{datetime_str}.{file_ext}'\n",
        "\n",
        "  # print(f'Returning from gen_savepath() with full_filepath={full_filepath}')\n",
        "\n",
        "  return full_filepath_str\n",
        "\n",
        "# Test\n",
        "# pathfilename_str = gen_pathfiletime('hist_paraglen.png')\n",
        "# print(pathfilename_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAxQW9IolZjr"
      },
      "source": [
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lYUmVuvl2OK"
      },
      "source": [
        "def save_dataframes(df_ls=['baseline','sentimentr','syuzhetr','transformer','combined','subset','everything']):\n",
        "  '''\n",
        "  Given a list of DataFrame groups to save\n",
        "  Save all DataFrames associated with that group\n",
        "  '''\n",
        "\n",
        "  # Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "  # author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "  # title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "  title_str = ''.join(CORPUS_FILENAME.split('.')[0]).lower()\n",
        "  datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "  if ('baseline' in df_ls) | ('everything' in df_ls):\n",
        "    # Sentences Raw\n",
        "    corpus_sents_filename = f'corpus_text_sents_raw_{title_str}.csv'\n",
        "    print(f'Saving Raw Sentences to file: {corpus_sents_filename}')\n",
        "    corpus_sents_df['sent_raw'].to_csv(corpus_sents_filename)\n",
        "\n",
        "    # Sentences Clean\n",
        "    corpus_sents_filename = f'corpus_text_sents_clean_{title_str}.csv'\n",
        "    print(f'Saving Clean Sentences to file: {corpus_sents_filename}')\n",
        "    corpus_sents_df['sent_clean'].to_csv(corpus_sents_filename)\n",
        "\n",
        "    # Sentence DataFrame\n",
        "    corpus_sents_filename = f'corpus_sents_baseline_{title_str}.csv'\n",
        "    print(f'Saving Sentences DataFrame to file: {corpus_sents_filename}')\n",
        "    corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "    # Paragraph DataFrame\n",
        "    corpus_parags_filename = f'corpus_parags_baseline_{title_str}.csv'\n",
        "    print(f'Saving Paragraph DataFrame to file: {corpus_parags_filename}')\n",
        "    corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "    # if SECTION_HEADINGS != 'None':  # Even if no Sections, save dummy placeholder   \n",
        "    #                                   filled with Chapter data\n",
        "    # Section DataFrame\n",
        "    corpus_sects_filename = f'corpus_sects_baseline_{title_str}.csv'\n",
        "    print(f'Saving Section DataFrame to file: {corpus_sects_filename}')\n",
        "    corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "    # Chapter DataFrame\n",
        "    corpus_chaps_filename = f'corpus_chaps_baseline_{title_str}.csv'\n",
        "    print(f'Saving Chapter DataFrame to file: {corpus_chaps_filename}')\n",
        "    corpus_chaps_df.to_csv(corpus_chaps_filename)\n",
        "\n",
        "  if ('syuzhetr' in df_ls) | ('everything' in df_ls):\n",
        "    # SyuzhetR DataFrame\n",
        "    corpus_sents_filename = f'sum_sentiments_sents_syuzhetr_{title_str}.csv'\n",
        "    print(f'Saving Sentences DataFrame to file: {corpus_sents_filename}')\n",
        "    corpus_syuzhetr_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "  if ('sentimentr' in df_ls) | ('everything' in df_ls):\n",
        "    # SyuzhetR DataFrame\n",
        "    corpus_sents_filename = f'sum_sentiments_sents_sentimentr_{title_str}.csv'\n",
        "    print(f'Saving Sentences DataFrame to file: {corpus_sents_filename}')\n",
        "    corpus_sentimentr_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "  if ('transformer' in df_ls) | ('everything' in df_ls):\n",
        "    # SyuzhetR DataFrame\n",
        "    corpus_sents_filename = f'sum_sentiments_sents_transformer_{title_str}.csv'\n",
        "    print(f'Saving Sentences DataFrame to file: {corpus_sents_filename}')\n",
        "    corpus_transformer_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "  if ('combined' in df_ls) | ('everything' in df_ls):\n",
        "    # Save StandardizedScaled SMA Sentences of ALL Models from the Unified DataFrame\n",
        "    corpus_sents_filename = f'sum_sentiments_all31_sents_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "    print(f'Saving Sentence Unified Models to file: {corpus_sents_filename}')\n",
        "    corpus_unified_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "    # Save Crux Points in Subset of Unified DataFrame (StdScaler/SMA 10% Sentences)\n",
        "    corpus_sents_filename = f'crux_table_unified_subset_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "    print(f'Saving Cruxes found in Subset of Unified Models to file: {corpus_sents_filename}')\n",
        "    unified_crux_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# save_dataframes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D84wkIlCgVn"
      },
      "source": [
        "# Used in function any_alphachar()\n",
        "\n",
        "pattern_alpha = re.compile(r'[A-Za-z]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRrwu2x3_TgB"
      },
      "source": [
        "def any_alphachar(raw_str):\n",
        "  '''\n",
        "  Given a string\n",
        "  Return a boolean True if there is any alpha char [A-Za-z] in it, else False\n",
        "  '''\n",
        "  \n",
        "  result_obj = re.search(pattern_alpha, raw_str)\n",
        "\n",
        "  if str(result_obj) == 'None':\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "# Test\n",
        "  \n",
        "test_str = '$1.00 ---'\n",
        "\n",
        "if (any_alphachar(test_str)):\n",
        "  print(f'alphachar TRUE: {test_str}')\n",
        "else:\n",
        "  print(f'alphachar FALSE: {test_str}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUvKJEybUIeP"
      },
      "source": [
        "## **Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOsfpOS4uHGX"
      },
      "source": [
        "def list2stdscaler(tseries_ls):\n",
        "  '''\n",
        "  Given a list of floating point number\n",
        "  Return a pd.Series that has been Standardized Scaled\n",
        "  '''\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  tseries_np = np.array(tseries_ls)\n",
        "  \n",
        "  tseries_np = tseries_np.reshape((len(tseries_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(tseries_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  tseries_xform_np = scaler.transform(tseries_np)\n",
        "\n",
        "  return pd.Series(tseries_xform_np.flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z0jQoBlfPir"
      },
      "source": [
        "def standardize_tsls(ts_df, col_ls):\n",
        "  '''\n",
        "  Given a DataFrame and list of Columns in that DataFrame\n",
        "  Create 4 new Standardized Columns for each given Columns\n",
        "  '''\n",
        "\n",
        "  # Create 4 new column names for each column provided\n",
        "  for amodel in col_ls:\n",
        "    # col_meanstd = f'{amodel}_meanstd'\n",
        "    col_medianiqr = f'{amodel}_medianiqr'\n",
        "    col_stdscaler = f'{amodel}_stdscaler'\n",
        "    col_lnorm_meanstd = f'{amodel}_lnorm_meanstd'\n",
        "    col_lnorm_medianiqr = f'{amodel}_lnorm_medianiqr'\n",
        "\n",
        "    # Standardize each column provided using Standard Scaler and  MedianIQRScaling\n",
        "    ts_df[col_stdscaler]  = list2stdscaler(ts_df[amodel])\n",
        "    ts_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(ts_df[amodel]).reshape(-1, 1))\n",
        "    # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "    text_len_ls = list(ts_df['token_len'])\n",
        "    text_sentiment_ls = list(ts_df[amodel])\n",
        "    textsentiment_norm_ls = [text_sentiment_ls[i]/text_len_ls[i] for i in range(len(text_len_ls))]\n",
        "    # RobustStandardize Sentence sentiment values\n",
        "    ts_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n",
        "    ts_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLASKwzZF-Lh"
      },
      "source": [
        "def get_sentiments(model_base, sentiment_fn, sentiment_type='lexicon'):\n",
        "  '''\n",
        "  Given a model_base name and sentiment evaluation function\n",
        "  Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "  '''\n",
        "\n",
        "  # Calculate Sentiment Polarities\n",
        "\n",
        "  if sentiment_type == 'lexicon':\n",
        "    print(f'Processing Lexicon Sentiments/Sentences...')\n",
        "    corpus_sents_df[model_base] = corpus_sents_df['sent_raw'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon Sentiments/Paragraphs...')\n",
        "    corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon Sentiments/Sections...')\n",
        "    corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon Sentiments/Chapters...')\n",
        "    corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "  \n",
        "  elif sentiment_type == 'compound':\n",
        "    # VADER\n",
        "\n",
        "    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n",
        "    print(f'Processing Compound Sentiments/Sentences...')\n",
        "    corpus_sents_df['scores'] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Compound Sentiments/Paragraphs...')\n",
        "    corpus_parags_df['scores'] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Compound Sentiments/Sections...')\n",
        "    corpus_sects_df['scores'] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Compound Sentiments/Chapters...')\n",
        "    corpus_chaps_df['scores'] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "\n",
        "    # Extract Compound Sentiment\n",
        "    corpus_sents_df[model_base]  = corpus_sents_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_parags_df[model_base]  = corpus_parags_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_sects_df[model_base]  = corpus_sects_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_chaps_df[model_base]  = corpus_chaps_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "\n",
        "  elif sentiment_type == 'function':\n",
        "    # TextBlob\n",
        "\n",
        "    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean parag_clean\n",
        "    print(f'Processing Function Sentiments/Sentences...')\n",
        "    corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Function Sentiments/Paragraphs...')\n",
        "    corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Function Sentiments/Sections...')\n",
        "    corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Function Sentiments/Chapters...')\n",
        "    corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: sentiment_type={sentiment_type} but must be one of (lexicon, compound, function)')\n",
        "    return\n",
        "\n",
        "  # Create new column names\n",
        "  # col_meanstd = f'{model_base}_meanstd'\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_stdscaler = f'{model_base}_stdscaler'\n",
        "  # col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_stdscaler = f'{model_base}_lnorm_stdscaler'\n",
        "\n",
        "\n",
        "  print('Standardizing Chapters')\n",
        "  # Get Chapter Robust Standardization with Standard Scaler and  MedianIQRScaling\n",
        "  corpus_chaps_df[col_stdscaler]  = list2stdscaler(corpus_chaps_df[model_base])\n",
        "  corpus_chaps_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Chapter Sentiment by dividing by Chapter Length\n",
        "  chaps_len_ls = list(corpus_chaps_df['token_len'])\n",
        "  chaps_sentiment_ls = list(corpus_chaps_df[model_base])\n",
        "  chaps_sentiment_norm_ls = [chaps_sentiment_ls[i]/chaps_len_ls[i] for i in range(len(chaps_len_ls))]\n",
        "  # RobustStandardize Chapter sentiment values\n",
        "  # corpus_chaps_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  print('Standardizing Sections')\n",
        "  # Get Section Robust Standardization with Standard Scaler and  MedianIQRScaling\n",
        "  # corpus_sects_df[col_stdscaler]  = mean_std_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sects_df[col_stdscaler]  = list2stdscaler(corpus_sects_df[model_base])\n",
        "  corpus_sects_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Section Sentiment by dividing by Section Length\n",
        "  sects_len_ls = list(corpus_sects_df['token_len'])\n",
        "  sects_sentiment_ls = list(corpus_sects_df[model_base])\n",
        "  sects_sentiment_norm_ls = [sects_sentiment_ls[i]/sects_len_ls[i] for i in range(len(sects_len_ls))]\n",
        "  # RobustStandardize Section sentiment values\n",
        "  # corpus_sects_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_sects_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  print('Standardizing Paragraphs')\n",
        "  # Get Paragraph Robust Standardization with Standard Scaler and  MedianIQRScaling\n",
        "  corpus_parags_df[col_stdscaler]  = list2stdscaler(corpus_parags_df[model_base])\n",
        "  corpus_parags_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_parags_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Paragraph Sentiment by dividing by Chapter Length\n",
        "  parags_len_ls = list(corpus_parags_df['token_len'])\n",
        "  parags_sentiment_ls = list(corpus_parags_df[model_base])\n",
        "  parags_sentiment_norm_ls = [parags_sentiment_ls[i]/parags_len_ls[i] for i in range(len(parags_len_ls))]\n",
        "  # RobustStandardize Paragraph sentiment values\n",
        "  # corpus_parags_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_parags_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(corpus_parags_df[model_base]).reshape(-1, 1))\n",
        "  corpus_parags_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  print('Standardizing Sentences')\n",
        "  # Get Sentence Robust Standardization with Standard Scaler and  MedianIQRScaling\n",
        "  corpus_sents_df[col_stdscaler]  = list2stdscaler(corpus_sents_df[model_base])\n",
        "  corpus_sents_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "  sents_len_ls = list(corpus_sents_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_sents_df[model_base])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/sents_len_ls[i] for i in range(len(sents_len_ls))]\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  # corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_sents_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sents_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJNepWBIkVjm"
      },
      "source": [
        "# Read in lexicon at given path into Dict[word]=polarity\n",
        "\n",
        "def get_lexicon(lexicon_name, lexicon_format=2):\n",
        "    \"\"\"\n",
        "    Read sentiment lexicon.csv file at lexicon_path\n",
        "    into appropriate Dict[word]=polarity\n",
        "\n",
        "    1. lexicon_dt[word] = <polarity value>\n",
        "\n",
        "    Args:\n",
        "        sa_lib (str, optional): [description]. Defaults to 'syuzhet'.\n",
        "    \"\"\"\n",
        "    \n",
        "    # global lexicon_df\n",
        "\n",
        "    lexicon_df = pd.DataFrame()\n",
        "    \n",
        "    # print(os.getcwd())\n",
        "\n",
        "    \n",
        "    try:\n",
        "      lexicon_df = pd.read_csv(lexicon_name)\n",
        "      lexicon_df.info()\n",
        "      # lexicon_df = lexicon_tmp_df.copy()\n",
        "      # print(lexicon_df.head())\n",
        "      return lexicon_df\n",
        "    except:\n",
        "      print(f'ERROR: Cannot read lexicon.csv at {lexicon_name}')\n",
        "      return -1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWdzLF0jwI-u"
      },
      "source": [
        "# Sentence to Sentiment Polarity according to passed in Lexicon Dictionary\n",
        "\n",
        "def text2sentiment(text_str, lexicon_dt):\n",
        "  '''\n",
        "  Given a text_str and lexicon_dt, calculate \n",
        "  the sentimety polarity.\n",
        "  '''\n",
        "\n",
        "  # Remove all not alphanumeric and whitespace characters\n",
        "  text_str = re.sub(r'[^\\w\\s]', '', text_str) \n",
        "\n",
        "  text_str = text_str.strip().lower()\n",
        "  if (len(text_str) < 1):\n",
        "      print(f\"ERROR: text2sentiment() given empty/null/invalid string: {text_str}\")\n",
        "\n",
        "  text_ls = text_str.split()\n",
        "  # print(f'text_ls: {text_ls}')\n",
        "\n",
        "  # Accumulated Total Sentiment Polarity for entire Sentence\n",
        "  text_sa_tot = 0.0\n",
        "\n",
        "  for aword in text_ls:\n",
        "      # print(f'getting sa for word: {aword}')\n",
        "      try:\n",
        "          word_sa_fl = float(lexicon_dt[aword])\n",
        "          text_sa_tot += word_sa_fl\n",
        "          # print(f\">>{aword} has a sentiment value of {word_sa_fl}\")\n",
        "      except TypeError: # KeyError:\n",
        "          # aword is not in lexicon so it adds 0 to the sentence sa sum\n",
        "          # print(f\"TypeError: cannot convert {lexicon_dt[aword]} to float\")\n",
        "          continue\n",
        "      except KeyError:\n",
        "          # print(f\"KeyError: missing key {aword} in defaultdict syuzhet_dt\")\n",
        "          continue\n",
        "      except:\n",
        "          e = sys.exc_info()[0]\n",
        "          # print(f\"ERROR {e}: sent2lex_sa() cannot catch aword indexing into syuzhet_dt error\")\n",
        "  \n",
        "  # print(f\"Leaving sent2lex_sa() with sentence sa value = {str(text_sa_tot)}\")\n",
        "  \n",
        "  return text_sa_tot\n",
        "\n",
        "\n",
        "# Test\n",
        "\n",
        "# sent2sentiment('I hate and despise and abhor and dislike and am disgusted by Mondays.', lexicon_jockersrinker_dt)\n",
        "# sent2sentiment('hate Mondays.', lexicon_jockersrinker_dt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6AEBZzvEz4a"
      },
      "source": [
        "def plot_smas(section_view=True, model_name='vader', text_unit='sentence', wins_ls=[20], alpha=0.5, subtitle_str='', y_height=0, save2file=False):\n",
        "  '''\n",
        "  Given a model, text_unit\n",
        "  Plot a SMA using default values and wrapping the function get_smas()\n",
        "  '''\n",
        "\n",
        "  if (section_view == True) and not any(x == text_unit for x in ['sentence', 'paragraph']):\n",
        "    print(f'ERROR: You can only plot SMA within a Section with Sentence or Paragraph text units')\n",
        "    return -99\n",
        "\n",
        "  if text_unit == 'sentence':\n",
        "    if section_view == False:\n",
        "      ts_df = corpus_sents_df\n",
        "    else:\n",
        "      ts_df = section_sents_df\n",
        "    wins_ls = [5,10,20]\n",
        "  elif text_unit == 'paragraph':\n",
        "    if section_view == False:\n",
        "      ts_df = corpus_parags_df\n",
        "    else:\n",
        "      ts_df = section_parags_df\n",
        "    wins_ls = [5,10,20]\n",
        "  elif text_unit == 'section':\n",
        "    ts_df = corpus_sects_df\n",
        "    wins_ls=[20]\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n",
        "\n",
        "  sectno_loc = ts_df[model_name].min()\n",
        "\n",
        "  if section_view ==False:\n",
        "    # At Section boundries draw blue vertical lines \n",
        "    section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "    for i, sent_no in enumerate(section_boundries_ls):\n",
        "      plt.text(sent_no, y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n",
        "      plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "      # 'BigNews1', xy=(sent_no, 0.5), xytext=(-10, 25), textcoords='offset points',                   rotation=90, va='bottom', ha='center', annotation_clip=True)\n",
        "\n",
        "      # plt.text(sent_no, -.5, 'goodbye',rotation=90, zorder=0)\n",
        "\n",
        "    # At Chapter boundaries draw red vertical lines\n",
        "    chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n",
        "    for i, sent_no in enumerate(chapter_boundries_ls):\n",
        "      plt.axvline(sent_no, color='navy', alpha=0.1)\n",
        "      # plt.text(sent_no, .5, 'hello', rotation=90, zorder=0)\n",
        "\n",
        "  get_smas(ts_df, model_name=model_name, text_unit=text_unit, wins_ls=wins_ls, alpha=alpha, subtitle_str=subtitle_str, save2file=save2file)\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_sma_sents_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcAMyjyn8ugj"
      },
      "source": [
        "# SMA 5% Sentiment of Sentence Sentiment\n",
        "\n",
        "def get_smas(ts_df, model_name, text_unit='sentence', wins_ls=[5,10], alpha=0.5, scale_factor=1., subtitle_str='', mean_adj=0., do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a model_name and time series DataFrame and list of win_rolls in percentages\n",
        "  Return the rolling means of the time series using the window sizes in win_rolls\n",
        "  '''\n",
        "\n",
        "  temp_roll_df = pd.DataFrame() # TODO: save sma rolling values into temp_df and return this value\n",
        "\n",
        "  win_1per = int(ts_df.shape[0]*0.01)\n",
        "  if text_unit ==  'sentence':\n",
        "    # win_1per = win_s1per\n",
        "    x_idx = 'sent_no'\n",
        "    fname_abbr = 'sents'\n",
        "  elif text_unit == 'paragraph':\n",
        "    # win_1per = win_p1per\n",
        "    x_idx = 'parag_no'\n",
        "    fname_abbr = 'parags'\n",
        "  elif text_unit == 'section':\n",
        "    win_1per = 1\n",
        "    wins_ls = [int(0.1 * corpus_sects_df.shape[0])]  # Edge case to deal with very few Section data points\n",
        "    x_idx = 'sect_no'\n",
        "    fname_abbr = 'sects'\n",
        "  else:\n",
        "    print(f'ERROR: text_unit={text_unit} but must be either sentence, paragraph or section')\n",
        "  \n",
        "  for i, awin_size in enumerate(wins_ls):\n",
        "    if len(str(awin_size)) == 1:\n",
        "      awin_str = '0' + str(awin_size)\n",
        "    else:\n",
        "      awin_str = str(awin_size) \n",
        "    col_roll_str = f'{model_name}_mean_roll{awin_str}'\n",
        "    win_size = awin_size*win_1per\n",
        "    ts_df[col_roll_str] = ts_df[model_name].rolling(window=win_size, center=True).mean()\n",
        "  \n",
        "    if do_plot == True:\n",
        "      alabel = f'{model_name} (win={awin_size})'\n",
        "      ts_df['y_scaled'] = ts_df[col_roll_str]*scale_factor + mean_adj \n",
        "      sns.lineplot(data=ts_df, x=x_idx, y='y_scaled', legend='brief', label=alabel, alpha=alpha)\n",
        "      \n",
        "  plt.title(f'{CORPUS_FULL} (Model: {model_name}: {subtitle_str}) \\nSMA Smoothed {text_unit} Sentiment Plot (windows={wins_ls})')\n",
        "  # plt.legend(loc='best')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_{fname_abbr}_sa_mean_050100sma.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return temp_roll_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULzfHeDK8udN"
      },
      "source": [
        "def get_lexstats(ts_df, model_name, text_unit='sentence'):\n",
        "  '''\n",
        "  Given a model name\n",
        "  calculate, store and return time series stats\n",
        "  '''\n",
        "  \n",
        "  global corpus_lexicons_stats_dt\n",
        "\n",
        "  temp_dt = {}\n",
        "  \n",
        "  if text_unit == 'sentence':\n",
        "    stat_idx = f'{model_name}_sents'\n",
        "  elif text_unit == 'paragraph':\n",
        "    stat_idx = f'{model_name}_parags'\n",
        "  elif text_unit == 'section':\n",
        "    stat_idx = f'{model_name}_sects'\n",
        "  elif text_unit == 'chapter':\n",
        "    stat_idx = f'{model_name}_chaps'\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must either be sentence, paragraph, or section')\n",
        "\n",
        "  sentiment_min = ts_df[model_name].min()\n",
        "  sentiment_max = ts_df[model_name].max()\n",
        "\n",
        "  temp_dt = {'sentiment_min' : sentiment_min,\n",
        "             'sentiment_max' : sentiment_max}\n",
        "\n",
        "  corpus_lexicons_stats_dt[stat_idx] = temp_dt\n",
        "                                     \n",
        "  return \n",
        "\n",
        "# Test\n",
        "# get_lexstats('afinn')\n",
        "# corpus_lexicons_stats_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltdJn-7ePNM9"
      },
      "source": [
        "def lex_discrete2continous_sentiment(text, lexicon):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_tot = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    word_sentiment = text2sentiment(str(aword), lexicon)\n",
        "    text_sentiment_tot += word_sentiment\n",
        "  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.01)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6xMI98l8uaH"
      },
      "source": [
        "\"\"\"\n",
        "def clip_outliers(floats_ser):\n",
        "  '''\n",
        "  Given a pd.Series of float values\n",
        "  Return a list with outliers removed, values limited within 3 median absolute deviations from median\n",
        "  '''\n",
        "  # https://www.statsmodels.org/stable/generated/statsmodels.robust.scale.mad.html#statsmodels.robust.scale.mad\n",
        "\n",
        "  # Old mean/std, less robust\n",
        "  # ser_std = floats_ser.std()\n",
        "  # ser_median = floats_ser.mean() # TODO: more robust: asym/outliers -> median/IQR or median/median abs deviation\n",
        "\n",
        "  floats_np = np.array(floats_ser)\n",
        "  ser_median = floats_ser.median()\n",
        "  ser_mad = robust.mad(floats_np)\n",
        "  print(f'ser_median = {ser_median}')\n",
        "  print(f'ser_mad = {ser_mad}')\n",
        "\n",
        "  if ser_mad == 0:\n",
        "    # for TS with small ranges (e.g. -1.0 to +1.0) Median Abs Deviation = 0\n",
        "    #   so pass back the original time series\n",
        "    floats_clip_ls = list(floats_ser)\n",
        "\n",
        "  else:\n",
        "    ser_oldmax = floats_ser.max()\n",
        "    ser_oldmin = floats_ser.min()\n",
        "    print(f'ser_max = {ser_oldmax}')\n",
        "    print(f'ser_min = {ser_oldmin}')\n",
        "\n",
        "    ser_upperlim = ser_median + 2.5*ser_mad\n",
        "    ser_lowerlim = ser_median - 2.5*ser_mad\n",
        "    print(f'ser_upperlim = {ser_upperlim}')\n",
        "    print(f'ser_lowerlim = {ser_lowerlim}')\n",
        "\n",
        "    # Clip outliers to max or min values\n",
        "    floats_clip_ls = np.clip(floats_np, ser_lowerlim, ser_upperlim)\n",
        "    # print(f'max floast_ls {floats_ls.max()}')\n",
        "\n",
        "    # def map2range(value, low, high, new_low, new_high):\n",
        "    #   '''map a value from one range to another'''\n",
        "    #   return value * 1.0 / (high - low + 1) * (new_high - new_low + 1)\n",
        "\n",
        "    # Map all float values to range [-1.0 to 1.0]\n",
        "    # floats_clip_sig_ls = [map2range(i, ser_oldmin, ser_oldmax, ser_upperlim, ser_lowerlim) for i in floats_clip_ls]\n",
        "\n",
        "    # listmax_fl = float(max(floats_ls))\n",
        "    # floats_ls = [i/listmax_fl for i in floats_ls]\n",
        "    #floats_ls = [1/(1+math.exp(-i)) for i in floats_ls]\n",
        "\n",
        "  return floats_clip_ls  # floats_clip_sig_ls\n",
        "\"\"\";\n",
        "\n",
        "# Test\n",
        "# Will not work on first run as corpus_sents_df is not defined yet\n",
        "'''\n",
        "data = np.array([1, 4, 4, 7, 12, 13, 16, 19, 22, 24])\n",
        "test_ls = clip_outliers(corpus_sents_df['vader'])\n",
        "print(f'new min is {min(test_ls)}')\n",
        "print(f'new max is {max(test_ls)}')\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXwKR4gA8Ouk"
      },
      "source": [
        "## **Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8Hf8nU98uXI"
      },
      "source": [
        "\"\"\"\n",
        "def rename_cols(ts_df, col_old_ls, suffix_str='_raw'):\n",
        "  '''\n",
        "  Given a DataFrame, list of columns in DataFrame and a suffix,\n",
        "  Return a Dictionary mapping old col names to new col name (orig+suffix)\n",
        "  '''\n",
        "\n",
        "  col_new_ls = []\n",
        "  for acol in col_old_ls:\n",
        "    acol_new = acol + suffix_str\n",
        "    col_new_ls.append(acol_new)\n",
        "\n",
        "  # Create dict for col mapping: keys=old col names, value=new col names\n",
        "  col_rename_dt = dict(zip(col_old_ls, col_new_ls))\n",
        "\n",
        "  # ts_df.rename(columns=col_rename_dt, errors=\"raise\")\n",
        "\n",
        "  return col_rename_dt\n",
        "\n",
        "# test_ls = [col for col in corpus_sents_df.columns if not(renaming_fun(col) is None)]\n",
        "# print(f'test_ls: {test_ls}')\n",
        "\n",
        "# Test\n",
        "# col_rename_dt = rename_cols(corpus_sents_df, sentiment_only_cols_ls)\n",
        "# col_rename_dt\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YJJcvDVnUuT"
      },
      "source": [
        "## **Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY-1_xWAGz0q"
      },
      "source": [
        "def process_timeseries(ts_df, col_models_ls, col_mod):\n",
        "  '''\n",
        "  Given a DataFrame, a list of columns to process and a modification to perform on these columns\n",
        "\n",
        "  Return a new DataFrame with the following new columns inserted from each original Model column:\n",
        "\n",
        "    1. Length-Normed (col_mod='lnorm'])\n",
        "       a. {base_model}_lnorm\n",
        "\n",
        "    2. Standardized (col_mod='std-[mean,median,minmix]')\n",
        "       a. {base_model}_stdscaler\n",
        "       b. {base_model}_medianiqr \n",
        "       a. {base_model_lnorm}_stdscaler\n",
        "       b. {base_model_lnorm}_medianiqr \n",
        "\n",
        "    3. Length-Normed Standardized (col_mod='roll[dd]') # where dd = 01 to 20 indicating rolling window width as % of corpus length\n",
        "       a. {base_model}_roll\n",
        "       b. {base_model}_roll \n",
        "       a. {base_model_lnorm}_roll\n",
        "       b. {base_model_lnorm}_roll \n",
        "       a. {base_model_lnorm_stdscaler}_roll\n",
        "       b. {base_model_lnorm_medianiqr}_roll\n",
        " \n",
        "  '''\n",
        "\n",
        "  temp_df = pd.DataFrame()  # ts_df.filter(['sent_no','sent_raw'], axis=1)\n",
        "  # print(f'temp_df is {temp_df}')\n",
        "\n",
        "  # Prerequisite: token_len column must exists\n",
        "  if 'token_len' not in ts_df.columns:\n",
        "    ts_df['token_len'] = ts_df['sent_raw'].apply(lambda x: len(x.strip().split()))\n",
        "    ts_df['char_len'] = ts_df['sent_raw'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "  # OPOTION 1: Apply (token) Length-Normalization to specified time series\n",
        "  if col_mod == 'lnorm':\n",
        "\n",
        "    # Apply lnorm operation on all specificed columns\n",
        "    for anold_model_no, anold_model in enumerate(col_models_ls):\n",
        "\n",
        "      # lnorm (Length normalization of sentiment values using token_len) operation can only be applied to base model time series\n",
        "      # rule: disallowed substrings in anold_model name if applying lnorm (anew_mod)\n",
        "      rule_badsubstr_lnorm = ['lnorm', 'stdscaler', 'medianiqr', 'roll']\n",
        "      if any(map(anold_model.__contains__, rule_badsubstr_lnorm)):\n",
        "        print(f'ERROR: Length-Normalization (lnorm) operation cannot be applied to {anold_model}\\n    any columns containing {rule_badsubstr_lnorm}')\n",
        "        return -99  # Return ERROR condition\n",
        "      else:\n",
        "        # Normalize specified sentiment time series by dividing by text length\n",
        "\n",
        "        # TODO: replace 'sent_raw'/'sent_clean' with 'text_raw'/'text_clean' for Sentence, Paragraph, Section and Chapter DataFrames\n",
        "\n",
        "        print(f'Applying function=[{col_mod}] on time series [{anold_model}]\\n') \n",
        "\n",
        "        text_len_ls = list(ts_df['token_len'])\n",
        "        text_sentiment_ls = list(ts_df[anold_model])\n",
        "        text_sentiment_norm_ls = [text_sentiment_ls[i]/text_len_ls[i] for i in range(len(text_len_ls))]\n",
        "        col_mod_name = f'{anold_model}_{col_mod}'\n",
        "        # ts_df = ts_df.assign(acol_mod_name = pd.Series(text_sentiment_norm_ls).values)\n",
        "        temp_df[col_mod_name] = pd.Series(text_sentiment_norm_ls)\n",
        "\n",
        "\n",
        "  # OPTION 2: Apply (Robust) Standardization to specified time series\n",
        "  if col_mod.startswith('std'):\n",
        "\n",
        "    # Apply lnorm operation on all specificed columns\n",
        "    for anold_model_no, anold_model in enumerate(col_models_ls):\n",
        "\n",
        "      # std (Standardization) operation can only be applied to base model time series\n",
        "      # rule: disallowed substrings in anold_model name if applying std to existing rolling averages or already standardized time series\n",
        "      rule_badsubstr_std = ['minmax', 'stdscaler', 'medianiqr', 'roll']\n",
        "      if any(map(anold_model.__contains__, rule_badsubstr_std)):\n",
        "        print(f'ERROR: Standardization (std) operation cannot be applied to {anold_model}\\n    any columns containing {rule_badsubstr_std}')\n",
        "        return -99  # Return ERROR condition\n",
        "      else:\n",
        "        # Standardize specificied sentiment time series by dividing by applying (Robust)Standization also configured in code below\n",
        "\n",
        "        # TODO: replace 'sent_raw'/'sent_clean' with 'text_raw'/'text_clean' for Sentence, Paragraph, Section and Chapter DataFrames\n",
        "\n",
        "        print(f'Applying function=[{col_mod}] on time series [{anold_model}]\\n')\n",
        "\n",
        "        # Must set any invalid (E.g. NaN cells to 0)\n",
        "      \n",
        "\n",
        "        if col_mod == 'std-minmax':\n",
        "\n",
        "          col_mod_minmax = f'{anold_model}_minmax'\n",
        "          temp_minmax_np = minmax_scaler.fit_transform(np.array(ts_df[anold_model]).reshape(-1, 1))\n",
        "          temp_df[col_mod_minmax] = pd.Series(temp_minmax_np.ravel())\n",
        "\n",
        "        elif col_mod == 'std-stdscaler':\n",
        "\n",
        "          col_mod_minmax = f'{anold_model}_stdscaler'\n",
        "          temp_minmax_np = mean_std_scaler.fit_transform(np.array(ts_df[anold_model]).reshape(-1, 1))\n",
        "          temp_df[col_mod_minmax] = pd.Series(temp_minmax_np.ravel())\n",
        "\n",
        "        elif col_mod == 'std-medianiqr':\n",
        "\n",
        "          col_mod_minmax = f'{anold_model}_medianiqr'\n",
        "          temp_minmax_np = median_iqr_scaler.fit_transform(np.array(ts_df[anold_model]).reshape(-1, 1))\n",
        "          temp_df[col_mod_minmax] = pd.Series(temp_minmax_np.ravel())\n",
        "\n",
        "        else:\n",
        "          print(f\"ERROR: modification fn (col_mod): {col_mod}, but must be one of ['std-minmax','std-stdscaler','std-medianiqr']\\n\")\n",
        "\n",
        "\n",
        "  # OPTION 3: Apply Simple Rolling Mean to specified time series\n",
        "  if col_mod.startswith('roll'):\n",
        "\n",
        "    roll_per = 0\n",
        "\n",
        "    # Check for roll argument against several rules and return detailed error message if fail to pass any combination of rules\n",
        "    roll_err_ls = []\n",
        "    roll_arg_len = len(col_mod)\n",
        "    if (roll_arg_len != 6):\n",
        "      roll_err_ls.append(f'ERROR: argument roll[dd]={col_mod} is too long with {roll_arg_len} characters (must be 6)')\n",
        "    roll_arg_2last = col_mod[-2:]\n",
        "\n",
        "    if (roll_arg_2last.isdigit() == False):\n",
        "      roll_err_ls.append(f'ERROR: argument roll[dd]={col_mod} last 2 chars {roll_arg_2last} must be both be digits [0-9]')\n",
        "    else:\n",
        "      # print(f'BEFORE: roll_arg_2last={roll_arg_2last}')\n",
        "      roll_per = int(roll_arg_2last)\n",
        "      # print(f'AFTER: roll_per={roll_per}')\n",
        "      if (roll_per > 20):\n",
        "        roll_err_ls.append(f'ERROR: argument roll[dd]={col_mod} last 2 chars {roll_arg_2last} must be integers between 01 and 20')\n",
        "\n",
        "    if len(roll_err_ls) > 0:\n",
        "      print(f'ERROR in process_timeseries() due to invalid argument roll[dd] = {col_mod}\\n\\n')\n",
        "      for anerror_str in roll_err_ls:\n",
        "        # print(f'    {anerror_str}')\n",
        "        return -99  # Return ERROR condition\n",
        "\n",
        "    # Apply lnorm operation on all specificed columns\n",
        "    for anold_model_no, anold_model in enumerate(col_models_ls):\n",
        "\n",
        "      # roll (Simple Rolling Average with specificed window size as 2-digit percentage of corpus length (01-20%)\n",
        "      # rule: disallowed substrings in anold_model name applying roll operation more than once to existing time series\n",
        "      rule_badsubstr_roll = ['roll']\n",
        "      if any(map(anold_model.__contains__, rule_badsubstr_roll)):\n",
        "        print(f'ERROR: Rolling Mean (roll) operation cannot be applied to {anold_model}\\n    any columns containing {rule_badsubstr_roll}')\n",
        "        return -99 # Return ERROR condition\n",
        "\n",
        "      else:\n",
        "          # Compute Rolling Mean with the given window size (extracted above as an int in roll_per) for the specificied sentiment time series\n",
        "\n",
        "          # TODO: replace 'sent_raw'/'sent_clean' with 'text_raw'/'text_clean' for Sentence, Paragraph, Section and Chapter DataFrames\n",
        "\n",
        "          print(f'Applying function=[{col_mod}] on time series [{anold_model}]\\n')\n",
        "\n",
        "          col_mod_roll = f'{anold_model}_{col_mod}'\n",
        "          # print(f'  for col_mod: {col_mod} and roll_per: {roll_per}')\n",
        "          roll_win = int(ts_df.shape[0]*roll_per/100)\n",
        "          # print(f'    and roll_win: {roll_win}')\n",
        "          temp_df[col_mod_roll] = ts_df[anold_model].rolling(roll_win, center=True).mean()\n",
        "\n",
        "  return temp_df  # Return SUCCESS condition\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIo6-zGKnZps"
      },
      "source": [
        "\"\"\"\n",
        "def norm2negpos1(data_ser):\n",
        "  '''\n",
        "  Given a series of floating number\n",
        "  Return a a list of same values normed between -1.0 and +1.0\n",
        "  '''\n",
        "  # data_np = np.matrix(data_ser)\n",
        "\n",
        "  scaler=MinMaxScaler(feature_range=(-1.0, 1.0))\n",
        "  temp_ser = scaler.fit_transform(np.matrix(data_ser))\n",
        "  \n",
        "  return temp_ser\n",
        "\"\"\";\n",
        "\n",
        "# Test\n",
        "'''\n",
        "temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n",
        "print(type(temp_np))\n",
        "temp_np.shape\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylt_kLuEFDrj"
      },
      "source": [
        "# This must be defined AFTER the corpus_sects_df DataFrame is created in the Preprocessing Step below\n",
        "# Raw Plot of Section Sentiments (Adjusted for (x-axis) mid-Section Sentence No and (y-axis) Sentiment weighted by Section length )\n",
        "# corpus_sects_df = pd.DataFrame()  # Create empty early as required by some utility functions\n",
        "\n",
        "def plot_crux_sections(model_names_ls, semantic_type='section', subtitle_str='', label_token_ct=0, title_xpos = 0.8, title_ypos=0.2, sec_y_height=0, save2file=False):\n",
        "  '''\n",
        "  Given a Sections DataFrame, model_name and semantic type,\n",
        "  Return a Plot of the Cruxes\n",
        "  '''\n",
        "\n",
        "  crux_points_dt = {}\n",
        "  model_stand_names_ls = []\n",
        "  section_boundries_ls = []\n",
        "\n",
        "\n",
        "  # print(f'Using model_names: {model_names_ls}')\n",
        "\n",
        "  # sns.lineplot(data=ts_df, x='sent_no_mid', y=amodel_stand, markers=['o'], alpha=0.5, label=amodel_stand); # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment (Bing Lexicon)')\n",
        "\n",
        "\n",
        "  # At Section boundries draw blue vertical lines \n",
        "  section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "  for i, sent_no in enumerate(section_boundries_ls):\n",
        "    plt.text(sent_no, sec_y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1);\n",
        "\n",
        "  # At Chapter boundaries draw red vertical lines\n",
        "  chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n",
        "  for i, sent_no in enumerate(chapter_boundries_ls):\n",
        "    plt.axvline(sent_no, color='navy', alpha=0.1);\n",
        "\n",
        "  # Error check and assign DataFrame associated with each semantic_type\n",
        "  if semantic_type == 'section':\n",
        "    # Get midpoints of each Section\n",
        "    ts_df=corpus_sects_df\n",
        "    midpoints_ls = list(corpus_sects_df['sent_no_mid'])\n",
        "  elif semantic_type == 'chapter':\n",
        "    # Get midpoints of each Chapter\n",
        "    ts_df=corpus_chaps_df\n",
        "    midpoints_ls = list(corpus_chaps_df['sent_no_mid'])\n",
        "  else:\n",
        "    print(f\"ERROR: semantic_type={semantic_type} must be either 'section' or 'chapter'\")\n",
        "    return -1\n",
        "\n",
        "  # How many sentiment time series are we plotting?\n",
        "  if len(model_names_ls) == 1:\n",
        "    \n",
        "    # Plotting only one model\n",
        "    model_name_full = str(model_names_ls[0])\n",
        "    model_name_root = model_name_full.split('_')[0]\n",
        "    print(f'model_name_full: {model_name_full} and model_name_root: {model_name_root}')\n",
        "    if model_name_root in MODELS_LS:\n",
        "      # Plot\n",
        "      print(f'about to sns.lineplot model: ') # {ts_df}')\n",
        "      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "      # g._legend.remove()\n",
        "      # print(f'model_name_full={model_name_full}')\n",
        "      # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], markers=\"o\", alpha=0.5, label=model_name_full)\n",
        "    else:\n",
        "      print(f'ERROR: model_names_ls[0]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n",
        "      return -1\n",
        "\n",
        "    # If plotting only one model, add labels\n",
        "    midpoints_sentiment_ls = list(ts_df[model_name_full])\n",
        "    sect_ct = 0\n",
        "    for x,y in zip(midpoints_ls, midpoints_sentiment_ls): \n",
        "      label_token_int = int(label_token_ct)\n",
        "      if label_token_int < 0:\n",
        "        label = ''\n",
        "      elif label_token_int == 0:\n",
        "        # if arg label_token_ct == 0, just print sent_no\n",
        "        label = f\"#{x}({sect_ct})\"\n",
        "      else:\n",
        "        # if arg label_token_ct > 0, print the first label_token_ct words of sentence at crux point\n",
        "        label = f\"#{x}({sect_ct}) {' '.join(corpus_sents_df.iloc[x-1]['sent_raw'].split()[:label_token_int])}\"; # \\nPolarity: {y:.2f}'\n",
        "\n",
        "      # Save Crux point in crux_points_dt Dictionary if plotting Cruxes for a single/specific Model\n",
        "      crux_full_str = ' '.join(corpus_sents_df.iloc[x]['sent_raw'].split())\n",
        "      crux_points_dt[x] = [y, crux_full_str]\n",
        "\n",
        "      plt.annotate(label,\n",
        "                   (x,y),\n",
        "                   textcoords='offset points',\n",
        "                   xytext=(0,10),\n",
        "                   ha='center',\n",
        "                   rotation=90)\n",
        "      sect_ct += 1\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment ({model_name_full.capitalize()})\\n{subtitle_str}', x=title_xpos, y=title_ypos);\n",
        "    # Plot\n",
        "    plt.plot(midpoints_ls, midpoints_sentiment_ls, marker=\"o\", ms=6) # , markevery=[0,1])\n",
        "\n",
        "  else:\n",
        "    # If plotting multiple models\n",
        "    model_names_str = 'Multiple Models'\n",
        "    for i, model_name_full in enumerate(model_names_ls):\n",
        "      # Error check and assign correct model names\n",
        "      model_name_root = model_name_full.split('_')[0]\n",
        "      if model_name_root in MODELS_LS:\n",
        "        # Plot\n",
        "        g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "        # g._legend.remove()\n",
        "        # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], marker=\"o\", alpha=0.5, label=model_name_full)\n",
        "      else:\n",
        "        print(f'ERROR: model_names_ls[]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n",
        "        return -1\n",
        "\n",
        "      # Plot\n",
        "      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "      # g._legend.remove()\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment (Standardized Models)\\n{subtitle_str}', x=title_xpos, y=title_ypos)\n",
        "\n",
        "  # plt.legend(loc='best');\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    models_names_ls = [x[:2] for x in model_names_ls]\n",
        "    models_names_str = ''.join(models_names_ls)\n",
        "    plot_filename = f'plot_cruxes_{semantic_type}_{models_names_str}_{models_names_str}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return crux_points_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUNMIlJKHyz3"
      },
      "source": [
        "def plot_histogram(model_name='vader', text_unit='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a model, text_unit\n",
        "  Plot a Histogram using the default DataFrame\n",
        "  '''\n",
        "\n",
        "  if text_unit == 'sentence':\n",
        "    ts_df = corpus_sents_df\n",
        "\n",
        "  elif text_unit == 'paragraph':\n",
        "    ts_df = corpus_parags_df\n",
        "\n",
        "  elif text_unit == 'section':\n",
        "    ts_df = corpus_sects_df\n",
        "\n",
        "  elif text_unit == 'chapter':\n",
        "    ts_df = corpus_chaps_df\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n",
        "\n",
        "  sns.histplot(ts_df[model_name], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram {text_unit.capitalize()} Sentiment (Model {model_name.capitalize()})')\n",
        "  # get_smas(ts_df, model_name=model_name, text_unit=text_unit, win_ls=wins_def_ls)\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_hist_{text_unit}_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGTkfsSWFCeO"
      },
      "source": [
        "# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n",
        "\n",
        "def plot_raw_sections(ts_df='corpus_sents_df', model_name='vader', semantic_type='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, model_name column, semantic_type \n",
        "  Plot the raw sentiment types\n",
        "  Options to save2file\n",
        "  ''' \n",
        "  \n",
        "  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n",
        "  sns.lineplot(data=ts_df, x='sect_no', y=model_name, alpha=0.5).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_nostand_sects_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmrtifYoIjOT"
      },
      "source": [
        "# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n",
        "\n",
        "def plot_raw_sentiments(model_name='vader', semantic_type='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, model_name column, semantic_type \n",
        "  Plot the raw sentiment types\n",
        "  Options to save2file\n",
        "  ''' \n",
        "  \n",
        "  if semantic_type == 'sentence':\n",
        "    ts_df = corpus_sents_df\n",
        "    x_units = 'sent_no'\n",
        "  elif semantic_type == 'paragraph':\n",
        "    ts_df = corpus_parags_df\n",
        "    x_units = 'parag_no'\n",
        "  elif (semantic_type == 'section') | (semantic_type == 'section_stand'):\n",
        "    ts_df = corpus_sects_df\n",
        "    x_units = 'sect_no'\n",
        "  elif (semantic_type == 'chapter') | (semantic_type == 'chapter_stand'):\n",
        "    ts_df = corpus_chaps_df\n",
        "    x_units = 'chap_no'\n",
        "    \n",
        "  else:\n",
        "    print(f'ERROR: {semantic_type} must be sentence, paragraph or section')\n",
        "\n",
        "\n",
        "  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n",
        "  sns.lineplot(data=ts_df, x=x_units, y=model_name, alpha=0.5, label=model_name).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n",
        "  \n",
        "  plt.legend(loc='best')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_raw_sentiments_{semantic_type}_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FR3G1QH-CAW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_lowess_ts(data, f=2./3., pts=None, itn=3, order=1):\n",
        "    \"\"\"Fits a nonparametric regression curve to a scatterplot.\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : pandas.Series\n",
        "        Data points in the scatterplot. The\n",
        "        function returns the estimated (smooth) values of y.\n",
        "    **Optionals**\n",
        "    f : float\n",
        "        The fraction of the data set to use for smoothing. A\n",
        "        larger value for f will result in a smoother curve.\n",
        "    pts : int\n",
        "        The explicit number of data points to be used for\n",
        "        smoothing instead of f.\n",
        "    itn : int\n",
        "        The number of robustifying iterations. The function will run\n",
        "        faster with a smaller number of iterations.\n",
        "    order : int\n",
        "        The order of the polynomial used for fitting. Defaults to 1\n",
        "        (straight line). Values < 1 are made 1. Larger values should be\n",
        "        chosen based on shape of data (# of peaks and valleys + 1)\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.Series containing the smoothed data.\n",
        "    \"\"\"\n",
        "    # Authors: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n",
        "    #            original\n",
        "    #          Dan Neuman <https://github.com/dneuman>\n",
        "    #            converted to Pandas series and extended to polynomials\n",
        "    # License: BSD (3-clause)\n",
        "\n",
        "    x = np.array(data.index, dtype=float)\n",
        "    # condition x-values to be between 0 and 1 to reduce errors in linalg\n",
        "    x = x - x.min()\n",
        "    x = x / x.max()\n",
        "    y = data.values\n",
        "    n = len(data)\n",
        "    if pts is None:\n",
        "        f = np.min([f, 1.0])\n",
        "        r = int(np.ceil(f * n))\n",
        "    else:  # allow use of number of points to determine smoothing\n",
        "        r = int(np.min([pts, n]))\n",
        "    r = min([r, n-1])\n",
        "    order = max([1, order])\n",
        "    # Create matrix of 1, x, x**2, x**3, etc, by row\n",
        "    xm = np.array([x**j for j in range(order+1)])\n",
        "    # Create weight matrix, one column per data point\n",
        "    h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)]\n",
        "    w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0)\n",
        "    w = (1 - w ** 3) ** 3\n",
        "    # Set up output\n",
        "    yEst = np.zeros(n)\n",
        "    delta = np.ones(n)  # Additional weights for iterations\n",
        "    for iteration in range(itn):\n",
        "        for i in range(n):\n",
        "            weights = delta * w[:, i]\n",
        "            xw = np.array([weights * x**j for j in range(order+1)])\n",
        "            b = xw.dot(y)\n",
        "            a = xw.dot(xm.T)\n",
        "            beta = np.linalg.solve(a, b)\n",
        "            yEst[i] = sum([beta[j] * x[i]**j for j in range(order+1)])\n",
        "        # Set up weights to reduce effect of outlier points on next iteration\n",
        "        residuals = y - yEst\n",
        "        s = np.median(np.abs(residuals))\n",
        "        delta = np.clip(residuals / (6.0 * s), -1, 1)\n",
        "        delta = (1 - delta ** 2) ** 2\n",
        "\n",
        "    return pd.Series(yEst, index=data.index, name='Trend')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l26W_YY2_whD"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "print(*corpus_sents_df.columns, sep='\\n')\n",
        "\n",
        "temp_ls = [f'{x}_stdscaler' for x in models_baseline_ls]\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "for amodel in temp_ls:\n",
        "  corpus_sents_df['temp_ts'] = Lowess(corpus_sents_df[amodel], f=0.1)\n",
        "  g = sns.lineplot(\n",
        "      # data=fmri.query(\"region == 'frontal'\"),\n",
        "      data=corpus_sents_df,\n",
        "      x=\"sent_no\", y='temp_ts', # hue=\"event\", units=\"subject\",\n",
        "      estimator=None, lw=1,\n",
        "      legend=False\n",
        "  )\n",
        "plt.title(f'{CORPUS_FULL}\\n LOWESS')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeSIwArU-Khu"
      },
      "source": [
        "# _ = get_lowess(corpus_sents_df, models_ls=['vader_stdscaler','bing_stdscaler','textblob_stdscaler','flair_stdscaler','stanza_stdscaler'], text_unit='sentence', afrac=1./20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCP2BR-H_XWv"
      },
      "source": [
        "# _ = get_lowess(corpus_sents_df, models_ls=['vader_lnorm_stdscaler','bing_lnorm_stdscaler','textblob_lnorm_stdscaler','flair_lnorm_stdscaler','stanza_lnorm_stdscaler'], text_unit='sentence', afrac=1./20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB8ibstaeiVd"
      },
      "source": [
        "# TODO: must plot in order to save, cannot save without first plotting\n",
        "\n",
        "def get_lowess(ts_df='corpus_parags_df', models_ls=MODELS_LS, text_unit='paragraph', plot_subtitle='', alabel='', afrac=1./10, ait=5, alpha=0.5, do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n",
        "  Return a DataFrame with LOWESS values\n",
        "  If 'plot=True', also output plot\n",
        "  '''\n",
        "\n",
        "  # global corpus_all_df\n",
        "\n",
        "  lowess_df = pd.DataFrame()\n",
        "\n",
        "  # Step 1: Calculate LOWESS smoothed values\n",
        "  for i,acol in enumerate(models_ls):\n",
        "    sm_x, sm_y = sm_lowess(endog=ts_df[acol].values, exog=ts_df.index.values, frac=afrac, it=ait, return_sorted = True).T\n",
        "    col_new = f'{acol}_lowess'\n",
        "    lowess_df[col_new] = pd.Series(sm_y)\n",
        "    # Optionally plot LOWESS for all models\n",
        "    if do_plot:\n",
        "      if alabel == '':\n",
        "        alabel == acol\n",
        "      plt.plot(sm_x, sm_y, label=acol, alpha=alpha, linewidth=2)\n",
        "\n",
        "  lowess_df['median'] = lowess_df.median(axis=1) # sm_y # corpus_all_df[df_cols_ls].median(axis=1)\n",
        "  \n",
        "  # Step 2: Optionally plot LOWESS for median\n",
        "  if do_plot:\n",
        "    # sm_x, sm_y = sm_lowess(endog=lowess_df.median, exog=lowess_df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n",
        "    # plt.plot(sm_x, sm_y, label='median', alpha=0.9, linewidth=2, color='black')\n",
        "    \n",
        "    frac_str = str(round(100*afrac))\n",
        "    plt.title(f'{CORPUS_FULL} \\n {plot_subtitle} {text_unit} Standardized Sentiment Smoothed with LOWESS (frac={frac_str})')\n",
        "    plt.legend() # (title='Sentiment Model', loc='best')\n",
        "\n",
        "  # Step 3: Optionally save to file\n",
        "  if save2file:\n",
        "    # Save Plot to file.\n",
        "    plot_filename = f'plot_{text_unit}_lowess_{plot_subtitle.split()[0].lower()}_{author_abbr_str}_{title_str}.png'\n",
        "    # plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plot_filename, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "\n",
        "  return lowess_df\n",
        "\n",
        "\n",
        "# Test\n",
        "'''\n",
        "new_lowess_col = f'{sa_model}_lowess'\n",
        "my_frac = 1./10\n",
        "my_frac_per = round(100*my_frac)\n",
        "new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n",
        "corpus_all_df[new_lowess_col] = plot_lowess(corpus_all_df, [sa_model], afrac=my_frac)\n",
        "corpus_all_df.head()\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cANCC2iz6nwo"
      },
      "source": [
        "def get_sent2dets(sent_no):\n",
        "  '''\n",
        "  Given a Sentence Number\n",
        "  Return the corresponding Paragraph, Section and Chapter Numbers that contain it\n",
        "  '''\n",
        "\n",
        "  # Get Paragraph No containing given Sentence No\n",
        "  sent_parag_no = int(corpus_sents_df[corpus_sents_df['sent_no']==sent_no]['parag_no'])\n",
        "\n",
        "  # Get Section No containing given Sentence No.\n",
        "  corpus_sects_ls = list(corpus_sects_df['sect_no'])\n",
        "  for asect_no in corpus_sects_ls:\n",
        "    if (int(corpus_sects_df[corpus_sects_df['sect_no'] == asect_no]['sent_no_start']) > sent_no):\n",
        "      break\n",
        "    sent_sect_no = asect_no\n",
        "    # print(f'asect={asect_no}')\n",
        "\n",
        "  # Get Chapter No containing given Sentence No.\n",
        "  corpus_chaps_ls = list(corpus_chaps_df['chap_no'])\n",
        "  for achap_no in corpus_chaps_ls:\n",
        "    if (int(corpus_chaps_df[corpus_chaps_df['chap_no'] == achap_no]['sent_no_start']) > sent_no):\n",
        "      break\n",
        "    sent_chap_no = achap_no\n",
        "    # print(f'achap={achap_no}')\n",
        "\n",
        "\n",
        "  return sent_parag_no, sent_sect_no, sent_chap_no\n",
        "\n",
        "# Test\n",
        "# sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(1408)\n",
        "# print(f'sent_parag_no={sent_parag_no}\\nsent_sect_no={sent_sect_no}\\nsent_chap_no={sent_chap_no}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6liffwhYtSw"
      },
      "source": [
        "# get_sentnocontext_report(the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sijR4OknJive"
      },
      "source": [
        "def get_sentnocontext(timeser_df, sent_no=1, n_sideparags=1, sent_highlight=True):\n",
        "  '''\n",
        "  Given a sentence number in the Corpus\n",
        "  Return the containing paragraph and n-paragraphs on either side\n",
        "  (e.g. if n=2, return 2+1+2=5 paragraphs)\n",
        "  '''\n",
        "\n",
        "  # print(f'just entered get_sentnocontext withsent_no: {sent_no} and  timeser_df:\\n\\n    {timeser_df}')\n",
        "  timeser_indx = timeser_df['sent_no'] == sent_no\n",
        "  parag_target_no = int(timeser_df[timeser_df['sent_no'] == sent_no]['parag_no'])\n",
        "  # print(f'parag_target_no = {parag_target_no} and type: {type(parag_target_no)}')\n",
        "\n",
        "  if n_sideparags == 0:\n",
        "    parags_context_ls = list(corpus_parags_df[corpus_parags_df['parag_no'] == parag_target_no]['parag_raw'])\n",
        "\n",
        "  else:\n",
        "    parag_start = parag_target_no - n_sideparags\n",
        "    parag_end = parag_target_no + n_sideparags + 1\n",
        "    parags_context_ls = list(corpus_parags_df.iloc[parag_start:parag_end]['parag_raw'])\n",
        "\n",
        "\n",
        "  if sent_highlight == True:\n",
        "    parag_match_str = str(parags_context_ls[n_sideparags])\n",
        "    # print(f'parag_match_str:\\n  {parag_match_str}') parag_no\n",
        "    sent_idx = sent_no\n",
        "    sent_str = (timeser_df[timeser_df['sent_no']==sent_idx]['sent_raw'].values)[0]\n",
        "    sent_str_up = sent_str.upper()\n",
        "    # print(f'sent_str:\\n  {sent_str}')\n",
        "    # parags_context_ls[n_sideparags] \n",
        "    parags_context_ls[n_sideparags] = parag_match_str.replace(sent_str, sent_str_up)\n",
        "\n",
        "  return parags_context_ls\n",
        "\n",
        "# Te\n",
        "# context_highlighted = get_sentnoparags(sent_no=1051, n_sideparags=1)\n",
        "# print(context_highlighted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM_I8uDfJztH"
      },
      "source": [
        "def get_sentnocontext_report(ts_df = corpus_sents_df, the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n",
        "  '''\n",
        "  Wrapper function around  get_sentnocontext()   Paragraph(s) Context\n",
        "  Prints a nicely formatted context report\n",
        "  '''\n",
        "\n",
        "  context_noparags = the_n_sideparags*2+1\n",
        "  # tsdf\n",
        "  # print('-------------------------------------------------------------')\n",
        "  print(f'The {context_noparags} Paragraph(s) Context around the Sentence #{the_sent_no} Crux Point:')\n",
        "  # print(f'ts_df = {ts_df}')\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f\"\\nCrux Sentence #{the_sent_no} Raw Text: -------------------------------\\n\\n    {str(ts_df[ts_df['sent_no'] == the_sent_no]['sent_raw'].values[0])}\\n\") # iloc[the_sent_no]['sent_raw']}\")\n",
        "\n",
        "  sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(the_sent_no)\n",
        "  print(f\"\\nCrux Sentence #{the_sent_no} is Contained in: ---------------------------\\n\\n    Paragraph #{sent_parag_no}\\n      Section #{sent_sect_no}\\n      Chapter #{sent_chap_no}\\n\")\n",
        "\n",
        "  print(f\"\\n{context_noparags} Paragraph(s) Context: ------------------------------\")\n",
        "  # print('calling get_sentnocontext')\n",
        "  # context_parags_ls = get_sentnocontext(timeser_df = ts_df, sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_parags_ls = get_sentnocontext(timeser_df = corpus_sents_df, sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_len = len(context_parags_ls)\n",
        "  context_mid = context_len//2\n",
        "  for i, aparag in enumerate(context_parags_ls):\n",
        "    if i==context_mid:\n",
        "      # print(f'\\n>>> Paragraph #{i}: <<< Crux Point Sentence CAPITALIZED within this Paragraph\\n\\n    {aparag}') \n",
        "      print(f'\\n<*> {aparag}')\n",
        "    else:\n",
        "      # print(f'\\n    Paragraph #{i}:\\n\\n    {aparag}')\n",
        "      print(f'\\n    {aparag}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# get_sentnocontext_report(sent_no=1051, n_sideparags=1, sent_highlight=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y04GiohGypNX"
      },
      "source": [
        "def get_section_timeseries(sect_no):\n",
        "  '''\n",
        "  Given a Section No in the current Corpus\n",
        "  Return the start,mid and ending Sent No for this Section as well as the Sentiment Time Series between the start/end Sentence for this Section\n",
        "  '''\n",
        "  \n",
        "  section_count = corpus_sects_df.shape[0]\n",
        "\n",
        "  # Compute the start, mid and end Sentence numbers for the selected Section\n",
        "  if Select_Section_No >= section_count:\n",
        "    print(f'ERROR: You picked Section #{Select_Section_No}.\\n  Section for this Corpus must be between 0 and {section_count-1}')\n",
        "    return -1\n",
        "\n",
        "  else:\n",
        "\n",
        "    # Get the starting and middle Sentence No of this Section\n",
        "    sect_sent_start = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_start'].values)\n",
        "    # sect_sent_mid = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_mid'].values)\n",
        "\n",
        "    # Calculate last Sentence No of this Section\n",
        "    if Select_Section_No == (section_count-1):   \n",
        "      print(f'You selected the last Section of this Corpus')\n",
        "      sect_sent_end = corpus_sents_df.shape[0] - 1\n",
        "    else:\n",
        "      sect_sent_end = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No+1]['sent_no_start'].values) # - 1\n",
        "      \n",
        "    print(f'Section #{sect_no}:----------')\n",
        "    print(f'\\nsect_sent_start: {sect_sent_start}')\n",
        "    # print(f'sect_sent_mid: {sect_sent_mid}')\n",
        "    print(f'sect_sent_end: {sect_sent_end}')\n",
        "\n",
        "\n",
        "  # Comput the start, and end Paragraph numbers for the selected Section\n",
        "  sect_parag_start = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_start]['parag_no'].values)\n",
        "  sect_parag_end = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_end]['parag_no'].values)\n",
        "\n",
        "  print(f'\\nsect_parag_start: {sect_parag_start}')\n",
        "  print(f'sect_parag_end: {sect_parag_end}')\n",
        "\n",
        "\n",
        "  # Extract and Return both a Sentence and Paragraph DataFrame for this Section \n",
        "\n",
        "  section_sents_df = corpus_sents_df.iloc[sect_sent_start:sect_sent_end]\n",
        "\n",
        "  section_parags_df = corpus_parags_df.iloc[sect_parag_start:sect_parag_end]\n",
        "\n",
        "\n",
        "  return section_sents_df, section_parags_df\n",
        "\n",
        "# Test\n",
        "\n",
        "# section_sents_df, section_parags_df = get_section_timeseries(Select_Section_No)\n",
        "\n",
        "# section_sents_df.head()\n",
        "\n",
        "# print(f'\\nsection_sents_df.shape: {section_sents_df.shape}')\n",
        "# print(f'section_parags_df.shape: {section_parags_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgFMgdQ3X33F"
      },
      "source": [
        "def get_lowess_cruxes(ts_df, col_series, text_type='sentence', win_lowess=5, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "\n",
        "  sent_no_min = ts_df.sent_no.min()\n",
        "  sent_no_max = ts_df.sent_no.max()\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values\n",
        "\n",
        "  half_win = int((win_lowess/100)*series_len)\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    if text_type == 'sentence':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(sent_no, sec_y_height, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "    elif text_type == 'paragraph':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(aparag_no, sec_y_height, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(aparag_no, color='blue', alpha=0.1)    \n",
        "    else:\n",
        "      print(f\"ERROR: text_type is {text_type} but must be either 'sentence' or 'paragarph'\")\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} Raw Sentence Crux Detection in Section #{Select_Section_No}\\nLOWESS Smoothed {subtitle_str} and SciPy find_peaks')\n",
        "    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig('argrelextrema.png')\n",
        "\n",
        "  return crux_coord_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv376c5_bfrg"
      },
      "source": [
        "def crux_sortsents(crux_ls, corpus_df=corpus_sents_df, atop_n=3, get_peaks=True, sort_key='sent_no'):\n",
        "  '''\n",
        "  Given a list of tuples (sent_no, sentiment value), atop_n cruxes to retrieve and bool flag get_peaks\n",
        "  Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n",
        "  '''\n",
        "  # print(f'Entered crux_sortsents with crux_ls={crux_ls}\\natop_n={atop_n}')\n",
        "\n",
        "  crux_old_ls = []\n",
        "  crux_new_ls = []\n",
        "\n",
        "  # TODO: Error check for null/invalid corpus_df/crus_ls sent_no\n",
        "\n",
        "  if sort_key == 'sent_no':\n",
        "    crux_old_ls = sorted(crux_ls, key=lambda tup: (tup[0]))\n",
        "  else:\n",
        "    crux_old_ls = sorted(crux_ls, key=lambda tup: (tup[1]), reverse=get_peaks)\n",
        "\n",
        "  \"\"\"\n",
        "  if get_peaks == True:\n",
        "    crux_old_ls = [x for x in crux_old_ls if x[1] > 0]\n",
        "  else:\n",
        "    crux_old_ls = [x for x in crux_old_ls if x[1] < 0]\n",
        "  \"\"\";\n",
        "\n",
        "  # Return only the n_top cruxes if more cruxes than n_top else return all cruxes\n",
        "  if (sort_key != 'sent_no') & (len(crux_old_ls) >= atop_n):\n",
        "    # trim crux list if user asked for less than total number\n",
        "    crux_old_ls = crux_old_ls[:atop_n]\n",
        "\n",
        "  # Retrieve the Sentence raw text for each Crux and add as Tuple(sent_no, sentiment_val, raw_text) to return List\n",
        "  for asent_no, asentiment_val in crux_old_ls:\n",
        "    # print(f'  Retrieving Sentence #{asent_no} with Sentiment Value {asentiment_val} from DataFrame {corpus_df}')\n",
        "    asent_int = int(asent_no)\n",
        "    # print(f\"                      asent_int is type: {type(asent_int)} and Sentence Text:\\n\\n     {corpus_df.iloc[asent_int]['sent_raw']}\")\n",
        "\n",
        "    asent_raw = str(corpus_df[corpus_df['sent_no'] == asent_int]['sent_raw'].values[0])\n",
        "    crux_new_ls.append((int(asent_no), float(f'{asentiment_val:.3f}'), str(asent_raw),)) # Append a Tuple to return List\n",
        "\n",
        "  return crux_new_ls\n",
        "\n",
        "# Test\n",
        "# crux_n_top_ls = crux_sortsents(section_crux_ls, atop_n=3, get_peaks=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjb60CVnz5SF"
      },
      "source": [
        "## **crux_sortsents_report**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_7wlJjq2QUo"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFDjK1o8gnQ0"
      },
      "source": [
        "def crux_sortsents_report(crux_ls, ts_df=corpus_sents_df, library_type='baseline', top_n=3, get_peaks=True, sort_by='sent_no', n_sideparags=1, sentence_highlight=True):\n",
        "  '''\n",
        "  Wrapper function to produce report based upon 'crux_sortsents() described as:\n",
        "    Given a list of tuples (sent_no, sentiment value), top_n cruxes to retrieve and bool flag get_peaks\n",
        "    Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n",
        "\n",
        "    # get_sentnocontext_report\n",
        "  '''\n",
        "\n",
        "  if get_peaks == True:\n",
        "    crux_label = 'Peak'\n",
        "  else:\n",
        "    crux_label = 'Valley'\n",
        "\n",
        "  # Filter and keep only the desired crux type in List crux_subset_ls\n",
        "  crux_subset_ls = []\n",
        "  for acrux_tup in crux_ls:\n",
        "    crux_type, crux_x_coord, crux_y_coord = acrux_tup\n",
        "    if crux_type.lower() == crux_label.lower():\n",
        "      crux_subset_ls.append((crux_x_coord,crux_y_coord)) # Append a Tuple to List\n",
        "\n",
        "  flag_2few_cruxes = False\n",
        "\n",
        "  # Check to see if asked for more Cruxes than were found \n",
        "  top_n_found = len(crux_subset_ls)\n",
        "  if top_n_found < top_n:\n",
        "    flag_2few_cruxes = True\n",
        "    print(f'\\n\\nWARNING: You asked for {top_n} {crux_label}s\\n         but there only {top_n_found} were found above.\\n')\n",
        "    print(f'             Displaying as many {crux_label}s as possible,')\n",
        "    print(f'             to retrieve more, go back to the previous code cells and re-run with wider Crux Window.\\n\\n')\n",
        "\n",
        "\n",
        "  # Get Sentence no and raw text for appropriate Crux subset\n",
        "  # print(f'Calling crux_n_top_ls with crux_subset_ls={crux_subset_ls}\\ntop_n={top_n}\\nget_peaks={get_peaks}')\n",
        "  crux_n_top_ls = crux_sortsents(corpus_df = ts_df, crux_ls=crux_subset_ls, atop_n=top_n, get_peaks=get_peaks, sort_key=sort_by)\n",
        "  # print(f'Returning crux_n_top_ls = {crux_n_top_ls}')\n",
        "\n",
        "  # Print appropriate header Select_Section_No sent_no\n",
        "  print('------------------------------')\n",
        "  # print(f'library_type: {library_type}')\n",
        "  if library_type in ['baseline','sentimentr','syuzhetr','transformer','unified']:\n",
        "    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n",
        "      print(f'Library: {library_type.capitalize()} ALL Top {top_n} {crux_label}s Found\\n')\n",
        "    else:\n",
        "      print(f'Library #{library_type.capitalize()} ONLY Top {top_n_found} {crux_label}s Found\\n')\n",
        "  else:\n",
        "    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n",
        "      print(f'Section #{Select_Section_No} ALL Top {top_n} {crux_label}s Found\\n')\n",
        "    else:\n",
        "      print(f'Section #{Select_Section_No} ONLY Top {top_n_found} {crux_label}s Found\\n')\n",
        "\n",
        "  # Print summary of subset Cruxes\n",
        "  for i,crux_sent_tup in enumerate(crux_n_top_ls):\n",
        "    # crux_type, crux_x_coord, crux_y_coord = crux_sent_tup\n",
        "    crux_x_coord, crux_y_coord, crux_sent_raw = crux_sent_tup\n",
        "    print(f'   {crux_label} #{i} at Sentence #{crux_x_coord} with Sentiment Value {crux_y_coord}')\n",
        "  # print('------------------------------\\n')\n",
        "  # print('Sent_No  Sentiment   Sentence (Raw Text)\\n')\n",
        "  \n",
        "  # Print details of each Crux in subset\n",
        "  for sent_no, sent_pol, sent_raw in crux_n_top_ls: \n",
        "    sent_no = int(sent_no)\n",
        "    print('\\n\\n-------------------------------------------------------------')\n",
        "    print(f'Sentence #{sent_no}   Sentiment: {sent_pol:.3f}\\n') #     {sent_raw}\\n')\n",
        "    # print('------------------------------')\n",
        "    get_sentnocontext_report(ts_df=ts_df, the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n",
        "    # get_sentnocontext(sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJw2WDlwHH5y"
      },
      "source": [
        "# For the selected Section, create an expanded Paragraph DataFrame to match the number of Sentences in the Section\n",
        "\n",
        "def expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df', model_name='vader_lnorm_medianiqr'):\n",
        "  '''\n",
        "  Given a Corpus Paragraph DataFrame and a longer Sentence DataFrame that cover the same Section of a Corpus\n",
        "  Return an expanded version of the Paragraph DataFrame of equal length to the Sentence DataFrame so they can be plotted/compared along the same x-axis\n",
        "  '''\n",
        "\n",
        "  parag_sentiment_expanded_ls = []\n",
        "  parags_midpoint_ls = []\n",
        "  sent_sum = 0\n",
        "  parag_start = section_parags_df.parag_no.min()\n",
        "  print(f'parag_start: {parag_start}')\n",
        "  parag_end = section_parags_df.parag_no.max() + 1 # shape[0] + 3\n",
        "  print(f'parag_end: {parag_end}')\n",
        "  parags_range_ls = list(range(parag_start, parag_end, 1))\n",
        "  print(f'parags_range_ls: {parags_range_ls}')\n",
        "  for i, aparag_no in enumerate(parags_range_ls):\n",
        "    aparag_sentiment_fl = float(corpus_parags_df[corpus_parags_df['parag_no']==aparag_no][model_name])\n",
        "    sent_ct = len(corpus_sents_df[corpus_sents_df.parag_no == aparag_no])\n",
        "    parag_midpoint_int = int(sent_ct//2 + sent_sum)\n",
        "    parags_midpoint_ls.append(parag_midpoint_int)\n",
        "    for asent in range(sent_ct):\n",
        "      parag_sentiment_expanded_ls.append(aparag_sentiment_fl)\n",
        "    sent_sum += sent_ct\n",
        "    print(f'#{i}: Paragraph #{aparag_no} has {sent_ct} Sentences and Avg Sentiment: {aparag_sentiment_fl:.3f}')\n",
        "\n",
        "  print(f'\\nSentence Total: {sent_sum} vs Original section_sents_df: {section_sents_df.shape[0]}')\n",
        "  print(f'  Paragraph Sentiment length: {len(parag_sentiment_expanded_ls)}')\n",
        "\n",
        "  # section_sents_parags_df = section_sents_df.copy()\n",
        "  \n",
        "  # section_sents_parags_df.head(1);\n",
        "\n",
        "  # corpus_sents_df['']\n",
        "\n",
        "  return parag_sentiment_expanded_ls, parags_midpoint_ls\n",
        "\n",
        "# Test\n",
        "# section_sents_df['vader_lnorm_medianiqr_parag'] = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXRq54NQwI7D"
      },
      "source": [
        "def get_crux_points(ts_df, col_series, text_type='sentence', win_per=5, sec_y_labels=True, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  # print('entered get_crux_points') \n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "  # print(f'series_len = {series_len}')\n",
        "\n",
        "  sent_no_min = ts_df.sent_no.min()\n",
        "  sent_no_max = ts_df.sent_no.max()\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values.flatten()\n",
        "\n",
        "  half_win = int((win_per/100)*series_len)\n",
        "  # print(f'half_win = {half_win}')\n",
        "  # print(f'sm_y type = {type(sm_y)}')\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  # print(f'peak_indexes type = {type(peak_indexes)}') # sent_no_start sent\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_x_adj_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  peak_label_ls = ['peak'] * len(peak_y_ls)\n",
        "  peak_coord_ls = tuple(zip(peak_label_ls, peak_x_adj_ls, peak_y_ls))\n",
        "\n",
        "  # peak_y_all_ls = peak_y_ls + valley_y_ls\n",
        "  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_x_adj_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  valley_label_ls = ['valley'] * len(valley_y_ls)\n",
        "  valley_coord_ls = tuple(zip(valley_label_ls, valley_x_adj_ls, valley_y_ls))\n",
        "\n",
        "  # Combine Peaks and Valley Coordinates into List of Tuples(label, x_coord, y_coord)\n",
        "  crux_coord_ls = peak_coord_ls + valley_coord_ls\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  #  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  #  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  #  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    if sec_y_labels == True:\n",
        "      section_sent_no_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "      section_no_ls = list(corpus_sects_df['sect_no'])\n",
        "      for i, asect_no in enumerate(section_sent_no_boundries_ls):\n",
        "        # Plot vertical lines for section boundries\n",
        "        plt.text(asect_no, sec_y_height, f'Section #{section_no_ls[i]}', alpha=0.2, rotation=90)\n",
        "        plt.axvline(asect_no, color='blue', alpha=0.1)    \n",
        "\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, annotation_clip=True) # xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} SMA Smoothed Sentence Sentiment Arcs Crux Detection\\n{subtitle_str} Models: {col_series}')\n",
        "    plt.xlabel(f'Sentence No') # within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n SMA Smoothed Sentence Sentiment Arcs Crux Points')\n",
        "    # plt.legend(loc='best')\n",
        "    plt.savefig(f\"{CORPUS_FILENAME.split('.')[0]}_find_peaks.png\")\n",
        "\n",
        "  return crux_coord_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNdJQGtghS_X"
      },
      "source": [
        "def get_standardscaler(series_name, values_ser):\n",
        "  '''\n",
        "  Given a Series of values\n",
        "  Return a list of StandardSclar transformations on that input Series\n",
        "  '''\n",
        "\n",
        "  scaler = StandardScaler()  \n",
        "\n",
        "  # Convert to np.array\n",
        "  values_np = np.array(values_ser)\n",
        "  \n",
        "  values_flat_np = values_np.reshape((len(values_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(values_flat_np)\n",
        "  print(f'Model: {series_name}\\n       Mean: {scaler.mean_}, StandardDeviation: {np.sqrt(scaler.var_)}') # % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  values_flat_xform_np = scaler.transform(values_flat_np)\n",
        "\n",
        "  return values_flat_xform_np.flatten().tolist()\n",
        "\n",
        "# Test\n",
        "# stdscaler_series_ls = get_standardscaler('vader_lnorm_medianiqr_roll100', corpus_sents_df['vader_lnorm_medianiqr_roll100'])\n",
        "# corpus_sents_df['vader_roll100_stdscaler'] = pd.Series(stdscaler_series_ls)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABW4oQ4xJT4R"
      },
      "source": [
        "def plot_models(models_subset_ls, models_type='baseline', text_unit='sent_no', win_per=10):\n",
        "  '''\n",
        "  Given a DataFrame and list of correponding Models in that DataFrame\n",
        "  Plot the Sentiment Arcs for the stdscaler with the rolling window \n",
        "  '''\n",
        "\n",
        "  Mean_All_Arc = True\n",
        "\n",
        "  if models_type == 'baseline':\n",
        "    models_ls = models_baseline_ls\n",
        "  elif models_type == 'sentimentr':\n",
        "    models_ls = models_sentimentr_ls\n",
        "  elif models_type == 'syuzhetr':\n",
        "    models_ls = models_syuzhetr_ls\n",
        "  elif models_type == 'transformer':\n",
        "    models_ls = models_transformer_ls\n",
        "  else:\n",
        "    print(f'ERROR: model_type={model_type} must be one of: baseline, sentimentr, syuzhetr or transformer')\n",
        "\n",
        "\n",
        "  if (models_type == 'baseline') | (models_type == 'transformers'):\n",
        "    # Have Corpus data in 4 DataFrames: corpus_[sents|parags|sects|chaps]_df\n",
        "    if text_unit == 'sent_no':\n",
        "      ts_df = corpus_sents_df\n",
        "      text_raw = 'sent_raw'\n",
        "      text_type = 'Sentence'\n",
        "      win_roll = int(corpus_sents_df.shape[0]* win_per/100)\n",
        "    elif text_unit == 'parag_no':\n",
        "      ts_df = corpus_parags_df\n",
        "      text_raw = 'parag_raw'\n",
        "      text_type = 'Paragraph'\n",
        "      win_roll = int(corpus_parags_df.shape[0]* win_per/100)\n",
        "    elif text_unit == 'sect_no':\n",
        "      ts_df = corpus_sects_df\n",
        "      text_raw = 'sect_raw'\n",
        "      text_type = 'Section'\n",
        "      win_roll = int(corpus_sects_df.shape[0]* win_per/100)\n",
        "    elif text_unit == 'chap_no':\n",
        "      ts_df = corpus_chaps_df\n",
        "      text_raw = 'chap_raw'\n",
        "      text_type = 'Chapter'\n",
        "      win_roll = int(corpus_chaps_df.shape[0]* win_per/100)\n",
        "    else:\n",
        "      print(f'ERROR: text_unit={text_unit} must be one of: sent_no, parag_no, sect_no or chap_no')\n",
        "\n",
        "  elif (models_type == 'sentimentr') | (models_type == 'syuzhetr'):\n",
        "\n",
        "    # Only have Corpus Sentence data in one DataFrame: corpus_sentimentr_df or corpus_syuzhetr_df\n",
        "    text_raw = 'sent_raw'\n",
        "    text_type = 'Sentence'\n",
        "\n",
        "    if models_type == 'sentimentr':\n",
        "      ts_df = corpus_sentimentr_df\n",
        "      win_roll = int(corpus_sentimentr_df.shape[0]*win_per/100)\n",
        "    else:\n",
        "      ts_df = corpus_syuzhetr_df\n",
        "      win_roll = int(corpus_syuzhetr_df.shape[0]*win_per/100)\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: model_type = {model_type} but must be one of 4: [baseline|transformer|sentimentr|syuzhet]')\n",
        "\n",
        "\n",
        "  # Get Rolling Window String\n",
        "  if len(str(win_per)) == 1:\n",
        "    roll_str = 'roll' + '0' + str(win_per)\n",
        "  else:\n",
        "    roll_str = 'roll' + str(win_per%100)\n",
        "\n",
        "\n",
        "  # Translate base model name into _stdscaler_rollxxx derivative\n",
        "  all_stdscaler_roll_ls = []\n",
        "  subset_stdscaler_roll_ls = []\n",
        "  for amodel in models_ls:\n",
        "    # Create a Rolling SMA Series from the stdscaler version of model sentiment values\n",
        "    amodel_stdscaler = f'{amodel}_stdscaler'\n",
        "    col_stdscaler_rollwin = f'{amodel}_stdscaler_{roll_str}'\n",
        "    ts_df[col_stdscaler_rollwin] = ts_df[amodel_stdscaler].rolling(win_roll, center=True).mean()\n",
        "    all_stdscaler_roll_ls.append(col_stdscaler_rollwin)\n",
        "    if amodel in models_subset_ls:\n",
        "      subset_stdscaler_roll_ls.append(col_stdscaler_rollwin)\n",
        "    # col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "\n",
        "  # Compute the Mean of All\n",
        "  mean_all_col = 'mean_stdscaler_' + roll_str\n",
        "  col_stdscaler_roll_ls = [f'{x}_stdscaler_{roll_str}' for x in models_ls] #  if ('mean' not in x)]\n",
        "  # print(f'Computing Mean based upon:\\n    {col_stdscaler_roll_ls}')\n",
        "  ts_df[mean_all_col] = ts_df[all_stdscaler_roll_ls].mean(axis=1)\n",
        "\n",
        "\n",
        "  palette = cycle(px.colors.qualitative.Safe)\n",
        "  # palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "  my_layout = go.Layout(\n",
        "      autosize=False,\n",
        "      width=1600,\n",
        "      height=800,\n",
        "      margin=go.layout.Margin(\n",
        "          l=10,\n",
        "          r=50,\n",
        "          b=100,\n",
        "          t=100,\n",
        "          pad = 1\n",
        "      )\n",
        "  )\n",
        "\n",
        "\n",
        "  fig = go.Figure(layout=my_layout)\n",
        "\n",
        "  # add traces\n",
        "  for i,amodel_stdscaler_roll in enumerate(subset_stdscaler_roll_ls):\n",
        "    # print(f'adding trace: {amodel_stdscaler_roll}')\n",
        "    fig.add_traces(go.Line(x = ts_df[text_unit],\n",
        "                          y = ts_df[amodel_stdscaler_roll],\n",
        "                          text = ts_df[text_raw],\n",
        "                          name = amodel_stdscaler_roll,\n",
        "                          hovertemplate = \"Model: <b>\"+amodel_stdscaler_roll+\"</b><br>\"+text_type+\" #<b>%{x}</b><br>Polarity <b>%{y}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                          marker_color=next(palette)))\n",
        "  \"\"\"\n",
        "  if Mean_Subset_Arc == True:\n",
        "    mean_subset_col = 'mean_subset_'+roll_str\n",
        "    corpus_sents_df[mean_subset_col] = corpus_sents_df[model_baseline_subset_ls].mean(axis=1)\n",
        "    fig.add_traces(go.Line(x=corpus_sents_df['sent_no'],\n",
        "                          y = corpus_sents_df[mean_subset_col],\n",
        "                          line=dict(\n",
        "                                # color='#000000',\n",
        "                                width=5\n",
        "                                ),\n",
        "                          text = 'NA', # corpus_sents_df['sent_raw'],\n",
        "                          name = 'Mean of Selected Models',\n",
        "                          hovertemplate = \"Model <b>%{mean_subset_col}</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                          marker_color=next(palette)))\n",
        "\n",
        "  \"\"\";\n",
        "\n",
        "  if Mean_All_Arc == True:\n",
        "    # mean_all_col = 'mean_all_stdscaler_'+roll_str\n",
        "    # ts_df[mean_all_col] = ts_df[col_stdscaler_rollwin_ls].mean(axis=1)\n",
        "    fig.add_traces(go.Line(x=ts_df[text_unit],\n",
        "                          y = ts_df[mean_all_col],\n",
        "                          line=dict(\n",
        "                                color='#000000',\n",
        "                                width=5,\n",
        "                                dash='dot',\n",
        "                                ),\n",
        "                          text = 'NA', # ts_df['sent_raw'],\n",
        "                          name = 'Mean of All Models',\n",
        "                          hovertemplate = \"Model <b>%{mean_all_col}</b><br>Paragraph #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                          marker_color=next(palette)))\n",
        "\n",
        "\n",
        "  fig.update_layout(\n",
        "      title=f\"{CORPUS_FULL}\\n{text_type} {models_type.capitalize()} Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "      xaxis_title=text_type + \" Number\",\n",
        "      yaxis_title=\"StdScaler Sentiment Value\",\n",
        "      hoverlabel=dict(\n",
        "          bgcolor=\"white\",\n",
        "          font_size=16,\n",
        "          font_family=\"Rockwell\"\n",
        "      ),\n",
        "      font=dict(\n",
        "          family=\"Courier New, monospace\",\n",
        "          size=18,\n",
        "          color=\"RebeccaPurple\"\n",
        "      )\n",
        "  )\n",
        "\n",
        "  fig.show();\n",
        "\n",
        "# Test\n",
        "\n",
        "# plot_models(models_subset_ls = ['vader','stanza'], models_type='baseline', text_unit='sent_no', win_per=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4ehet9JjOsK"
      },
      "source": [
        "def standardize_ts_ls(ts_df, col_ls):\n",
        "  '''\n",
        "  Given a DataFrame and list of Columns in that DataFrame\n",
        "  Create 4 new Standardized Columns for each given Columns\n",
        "  '''\n",
        "\n",
        "  # Create 4 new column names for each column provided\n",
        "  for amodel in col_ls:\n",
        "    # col_meanstd = f'{amodel}_meanstd'\n",
        "    col_medianiqr = f'{amodel}_medianiqr'\n",
        "    col_stdscaler = f'{amodel}_stdscaler'\n",
        "    col_lnorm_stdscaler = f'{amodel}_lnorm_stdscaler'\n",
        "    col_lnorm_medianiqr = f'{amodel}_lnorm_medianiqr'\n",
        "\n",
        "    # Standardize each column provided using Standard Scaler and  MedianIQRScaling\n",
        "    ts_df[col_stdscaler]  = list2stdscaler(ts_df[amodel])\n",
        "    ts_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(ts_df[amodel]).reshape(-1, 1))\n",
        "    # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "    text_len_ls = list(ts_df['token_len'])\n",
        "    text_sentiment_ls = list(ts_df[amodel])\n",
        "    text_sentiment_norm_ls = [text_sentiment_ls[i]/text_len_ls[i] for i in range(len(text_len_ls))]\n",
        "    # RobustStandardize Sentence sentiment values\n",
        "    ts_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n",
        "    ts_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daEzvNu6huM-"
      },
      "source": [
        "# **Either (a) Load Precomputed DataFrames or (b) Create Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnWJCkqDhA5L"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "len(sections_ls)\n",
        "min(sections_ls, key=len) \n",
        "\n",
        "# TODO: Spell check and correct common OCR errors\n",
        "\n",
        "# SymSpellPy\n",
        "# JamSpell\n",
        "# OCR - https://github.com/Alvant/MIL-OCR\n",
        "\n",
        "# !pip install -U symspellpy\n",
        "\n",
        "# Did not need these\n",
        "# dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# term_index is the column of the term and count_index is the\n",
        "# column of the term frequency\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# lookup suggestions for single-word input strings\n",
        "input_term = \"memebers\"  # misspelling of \"members\"\n",
        "input_term = \"summermorning\"\n",
        "# max edit distance per lookup\n",
        "# (max_edit_distance_lookup <= max_dictionary_edit_distance)\n",
        "suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST,\n",
        "                               max_edit_distance=2)\n",
        "# display suggestion term, term frequency, and edit distance\n",
        "for suggestion in suggestions:\n",
        "    print(suggestion)\n",
        "\n",
        "\n",
        "\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell\n",
        "\n",
        "# Set max_dictionary_edit_distance to avoid spelling correction\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# term_index is the column of the term and count_index is the\n",
        "# column of the term frequency\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# a sentence without any spaces\n",
        "input_term = \"thequickbrownfoxjumpsoverthelazydog\"\n",
        "input_term = \"summermorning\"\n",
        "result = sym_spell.word_segmentation(input_term)\n",
        "print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n",
        "                          result.log_prob_sum))\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI5Tg1bFiERD"
      },
      "source": [
        "### **(a) Load Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg4NGPlCGffp"
      },
      "source": [
        "!ls -altr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL_i7AkGGjHB"
      },
      "source": [
        "title_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxCBdYNTiDrC"
      },
      "source": [
        "# Read Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "# title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "title_str = ''.join(CORPUS_FILENAME.split('.')[0]).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "# Sentence DataFrame\n",
        "corpus_sents_filename = f'corpus_sents_baseline_{title_str}.csv'\n",
        "print(f'Reading from file: {corpus_sents_filename}')\n",
        "corpus_sents_df = pd.read_csv(corpus_sents_filename)\n",
        "\n",
        "# Paragraph DataFrame\n",
        "corpus_parags_filename = f'corpus_parags_baseline_{title_str}.csv'\n",
        "print(f'Reading from file: {corpus_parags_filename}')\n",
        "corpus_parags_df = pd.read_csv(corpus_parags_filename)\n",
        "\n",
        "# Section DataFrame\n",
        "corpus_sects_filename = f'corpus_sects_baseline_{title_str}.csv'\n",
        "print(f'Reading from file: {corpus_sects_filename}')\n",
        "corpus_sects_df = pd.read_csv(corpus_sects_filename)\n",
        "\n",
        "# Chapter DataFrame\n",
        "corpus_chaps_filename = f'corpus_chaps_baseline_{title_str}.csv'\n",
        "print(f'Reading from file: {corpus_chaps_filename}')\n",
        "corpus_chaps_df = pd.read_csv(corpus_chaps_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqsUHiF9K_5L"
      },
      "source": [
        "# Verify Sentences\n",
        "\n",
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3fD5CpfKJnw"
      },
      "source": [
        "# corpus_sents_df = corpus_sents_df.loc[:, ~corpus_sents_df.columns.str.contains('^Unnamed')]\n",
        "corpus_sents_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "corpus_sents_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl3hwS0wLEeR"
      },
      "source": [
        "# Verify Paragraphs\n",
        "\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzP_CrO3J0gd"
      },
      "source": [
        "# corpus_parags_df = corpus_parags_df.loc[:, ~corpus_parags_df.columns.str.contains('^Unnamed')]\n",
        "corpus_parags_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "corpus_parags_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZR54xIPJvm-"
      },
      "source": [
        "# Verify Sections\n",
        "\n",
        "# corpus_sects_df.head(2)\n",
        "corpus_sects_df.info(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmlXbKnbL1N7"
      },
      "source": [
        "corpus_sects_df.rename(columns={'Unnamed: 0':'sect_no','chap_raw':'sect_raw','chap_clean':'sect_clean'}, inplace=True)\n",
        "# corpus_sects_df = corpus_sects_df.loc[:, ~corpus_sects_df.columns.str.contains('^Unnamed')]\n",
        "# corpus_sects_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "corpus_sects_df = corpus_sects_df.loc[:, ~corpus_sects_df.columns.duplicated()]\n",
        "# corpus_sects_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "corpus_sects_df.info () # [: corpus_sects_df.columns.like('_no')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb2DYmbqMEF-"
      },
      "source": [
        "# Verfiy Chapters\n",
        "\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmfsseAz2F8T"
      },
      "source": [
        "# corpus_chaps_df = corpus_chaps_df.loc[:, ~corpus_chaps_df.columns.str.contains('^Unnamed')]\n",
        "corpus_chaps_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9X5gfDoi_d0"
      },
      "source": [
        "# Verify all 4 semantic unit DataFrame shapes\n",
        "\n",
        "print(f'corpus_sents_df.shape: {corpus_sents_df.shape}')\n",
        "print(f'corpus_parags_df.shape: {corpus_parags_df.shape}')\n",
        "print(f'corpus_sects_df.shape: {corpus_sects_df.shape}')\n",
        "print(f'corpus_chaps_df.shape: {corpus_chaps_df.shape}')\n",
        "\n",
        "\"\"\"\n",
        "SButler Odyssey\n",
        "\n",
        "corpus_sents_df.shape: (2445, 8)\n",
        "corpus_parags_df.shape: (1051, 8)\n",
        "corpus_sects_df.shape: (24, 8)\n",
        "corpus_chaps_df.shape: (24, 8)\n",
        "\"\"\";\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZizvsnBrhAvh"
      },
      "source": [
        "corpus_sents_df.iloc[1618]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQYjye-v9XMY"
      },
      "source": [
        "### **(b) Create Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6qZH_qAVPQD"
      },
      "source": [
        "#### **Try to Automatically Detected File/Text Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtWVf1cJUeSW"
      },
      "source": [
        "!pwd\n",
        "!ls -altr *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW3HwuvBv1un"
      },
      "source": [
        "# files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4TH13cFUErD"
      },
      "source": [
        "# Try to automatically discover Corpus text Encoding scheme (default to 'utf-8', but often 'iso-8859-1', 'windows-1252', 'cp1252', or 'ascii')\n",
        "\n",
        "CORPUS_ENCODING = 'utf-8' # Python3 default encoding\n",
        "\n",
        "corpus_str, corpus_encode, encoding_confidence = get_file_encoding(CORPUS_FILENAME)\n",
        "CORPUS_ENCODING = str(corpus_encode).lower()\n",
        "\n",
        "if encoding_confidence > 0.8:\n",
        "  print(f'Setting file/text encoding to {CORPUS_ENCODING}\\n')\n",
        "  print(f\"    {encoding_confidence*100:.2f}% confidence Encoding = '{CORPUS_ENCODING}' for '{CORPUS_FILENAME}'\")\n",
        "else:\n",
        "  print(f\"WARNING: Less than 80% confidence estimating Encoding scheme for '{CORPUS_FILENAME}'\\n\")\n",
        "  print(f\"         Only {encoding_confidence*100:.2f}% confidence Encoding = '{CORPUS_ENCODING}'\")\n",
        "  print(f\"         Manually verify corpus file '{CORPUS_FILENAME}' encoding, set as GLOBAL_CONSTATANT and rerun\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMkihxoY6T6U"
      },
      "source": [
        "#### **Create Chapter and Section DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaAZSbN8t537"
      },
      "source": [
        "corpus_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ydj8ITnTE1D"
      },
      "source": [
        "!head -n 10 $corpus_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qULnmeVG0Le"
      },
      "source": [
        "corpus_chaps_filtered_ls, corpus_chaps_clean_ls, corpus_sects_filtered_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str = corpus2chapsect(corpus_filename, corpus_type='file')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz0uqmK8G1lj"
      },
      "source": [
        "print(f'Corpus Filtered:\\n    {corpus_chaps_filtered_ls[0][:500]}')\n",
        "print('\\n')\n",
        "print(f'Corpus Cleaned:\\n    {corpus_chaps_clean_ls[0][:500]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUQ5-o3HbrMq"
      },
      "source": [
        "# Parse out raw/clean Chapters/Sections from Corpus text file\n",
        "\n",
        "corpus_chaps_filtered_ls, corpus_chaps_clean_ls, corpus_sects_filtered_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str = corpus2chapsect(corpus_filename, corpus_type='file')\n",
        "\n",
        "# Remove problematic leading ROMAN numerials if they exist\n",
        "corpus_chaps_filtered_ls = [del_leadroman(x) for x in corpus_chaps_filtered_ls]\n",
        "corpus_chaps_clean_ls =  [del_leadroman(x) for x in corpus_chaps_clean_ls]\n",
        "corpus_sects_filtered_ls = [del_leadroman(x) for x in corpus_sects_filtered_ls]\n",
        "corpus_sects_clean_ls = [del_leadroman(x) for x in corpus_sects_clean_ls]\n",
        "\n",
        "corpus_raw_str = del_leadroman(corpus_raw_str)\n",
        "\n",
        "len(corpus_chaps_filtered_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhTAWxFUrg6V"
      },
      "source": [
        "corpus_chaps_filtered_ls[0][:500]\n",
        "print('\\n')\n",
        "corpus_chaps_clean_ls[0][:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilQv-883wl6_"
      },
      "source": [
        "# Test\n",
        "\n",
        "# test_ls = [x for x in corpus_chaps_filtered_ls if 'I have no' in x]\n",
        "# test_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcx-zqsXSKSE"
      },
      "source": [
        "# Verify Chapter/Section count and sample\n",
        "\n",
        "if type(corpus_chaps_filtered_ls[0]) is not str:\n",
        "\n",
        "  print(f'\\nERROR: Could not parse Corpus file into Chapters and Sections correctly\\n       Edit Corpus file and re-run')\n",
        "\n",
        "else:\n",
        "\n",
        "  print(f'\\n{len(corpus_chaps_filtered_ls)} Chapters found in this Corpus')\n",
        "\n",
        "  print(f'\\n{len(corpus_sects_filtered_ls)} Sections found in this Corpus')\n",
        "\n",
        "\n",
        "  print(f'\\n\\n----------{len(corpus_chaps_filtered_ls)} CHAPTERS ----------')\n",
        "  chap_sample_no = 0\n",
        "  print(f'First 500 character sample from Chapter #{chap_sample_no}\\n\\n     {corpus_chaps_filtered_ls[chap_sample_no][:500]}')\n",
        "\n",
        "  print(f'\\n\\n----------{len(corpus_sects_filtered_ls)} SECTIONS ----------')\n",
        "  sect_sample_no = 0\n",
        "  print(f'First 500 character sample from Section #{sect_sample_no}\\n\\n    {corpus_sects_filtered_ls[sect_sample_no][:500]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roEQ9qkWbLew"
      },
      "source": [
        "!lsb_release -a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyY6gwR4Goy6"
      },
      "source": [
        "# Verify shortest Chapter\n",
        "\n",
        "min(corpus_chaps_filtered_ls, key=len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPgOUT6-Urs2"
      },
      "source": [
        "len(corpus_chaps_filtered_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhKXKE-vTzSD"
      },
      "source": [
        "# BUGFIX: Second Pass CHAPTER split on Chapter text segments\n",
        "#         Fix Chapters thats re.split() couldn't split apart with manual second pass\n",
        "#\n",
        "#  re.split does not work for 4 of 55 Chapters\n",
        "#     on Henry James, Portrait of a Lady, Cannot re.split(rf'{pattern_chap}',..)\n",
        "#     CHAPTER [V|X|XVIII|L]\n",
        "#  Attempts: cut-paste identical working CHAPTER text, encoding/ignore, RegEx minimization, etc\n",
        "# \n",
        "#  Colab Pro: No GPU/TPU, High-RAM\n",
        "#     Python: 3.7.11\n",
        "#    Unbuntu: 18.04.5 LTS (bionic) (!lsb_release -a)\n",
        "#\n",
        "# WORKAROUND: Have to use <string>.split('CHAPTER ') method and clean up variable parts after CHAPTER (e.g. V, X, XVIII, L)\n",
        "\n",
        "\n",
        "corpus_chaps_exp_ls = []\n",
        "for i, achap in enumerate(corpus_chaps_filtered_ls):\n",
        "  print(f'Chapter #{i}: Length: {len(achap)}\\n\\n')\n",
        "  if len(achap) > 50:\n",
        "    print(f'     {achap[:50]}\\n\\n')\n",
        "  if 'CHAPTER' in achap:\n",
        "    print(f'\\n\\n\\n============================\\n')\n",
        "    print(f'      FOUND: Chapter with remaining/embedded CHAPTER Heading\\n')\n",
        "\n",
        "    temp_chap_ls = achap.split('CHAPTER ')\n",
        "    print(f'      string.split(CHAPTER)')\n",
        "\n",
        "    # temp_chap_ls = re.split(r'^CHAPTER ', achap) #  [IVXL]{1,10}[\\s]*', achap)\n",
        "    # print(f'    RegEx = {pattern_chap}')\n",
        "\n",
        "    # temp_chap_ls = re.split(rf'{pattern_chap}', achap)\n",
        "    # print(f'    RegEx = [^CHAPTER [IVXL]{1,10}[\\s]*]')\n",
        "\n",
        "    print(f'      Split into {len(temp_chap_ls)}')\n",
        "    print('\\n=============================\\n\\n\\n')\n",
        "    # temp_chap_ls = [re.sub(r'[IVLX]{1,10}[\\s]{1,}','',x) for x in temp_chap_ls]\n",
        "    # temp_chap_ls = [re.sub(r'CHAPTER ','',x) for x in temp_chap_ls]\n",
        "    temp_chap_ls = [re.sub(rf'{pattern_chap}','',x) for x in temp_chap_ls]\n",
        "    temp_chap_ls = [x.strip() for x in temp_chap_ls]\n",
        "    temp_chap_ls = [del_leadroman(x).strip() for x in temp_chap_ls] \n",
        "    corpus_chaps_exp_ls.extend(temp_chap_ls)\n",
        "  else:\n",
        "    corpus_chaps_exp_ls.append(achap)\n",
        "\n",
        "  corpus_chaps_exp_ls = [x for x in corpus_chaps_exp_ls if len(x) > MIN_CHAP_LEN]\n",
        "\n",
        "print(f'         Old Chapter List had {len(corpus_chaps_filtered_ls)} Chapters')\n",
        "print(f'New Expanded Chapter List has {len(corpus_chaps_exp_ls)} Chapters')\n",
        "\n",
        "# Copy expanded Chapter structure to base/reference list\n",
        "corpus_chaps_filtered_ls = corpus_chaps_exp_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw_-X2MftagF"
      },
      "source": [
        "pattern_chap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEtHYm_Ytksg"
      },
      "source": [
        "corpus_chaps_exp_ls[0][:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKn3hT_axHdA"
      },
      "source": [
        "# Test\n",
        "\n",
        "# test_ls = [x for x in corpus_chaps_filtered_ls if 'I have no' in x]\n",
        "# test_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG9tnKz_dEVI"
      },
      "source": [
        "corpus_chaps_exp_ls[0][:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pwfl-0axB1X"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# Parse out raw/clean Chapters/Sections from Corpus text file\n",
        "\n",
        "corpus_chaps_filtered_ls, corpus_chaps_clean_ls, corpus_sects_filtered_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str = corpus2chapsect(corpus_filename)\n",
        "\n",
        "# Verify Paragraph count and sample\n",
        "\n",
        "if type(corpus_chaps_raw_ls[0]) is not str:\n",
        "\n",
        "  print(f'\\nERROR: Could not parse Corpus file into Chapters and Sections correctly\\n       Edit Corpus file and re-run')\n",
        "\n",
        "else:\n",
        "\n",
        "  print(f'\\n{len(corpus_chaps_raw_ls)} Chapters found in this Corpus')\n",
        "\n",
        "  print(f'\\n{len(corpus_sects_raw_ls)} Sections found in this Corpus')\n",
        "\n",
        "\n",
        "  print(f'\\n\\n----------{len(corpus_chaps_filtered_ls)} CHAPTERS ----------')\n",
        "  chap_sample_no = 0\n",
        "  print(f'First 500 character sample from Chapter #{chap_sample_no}\\n\\n     {corpus_chaps_filtered_ls[chap_sample_no][:500]}')\n",
        "\n",
        "  print(f'\\n\\n----------{len(corpus_sects_filtered_ls)} SECTIONS ----------')\n",
        "  sect_sample_no = 0\n",
        "  print(f'First 500 character sample from Section #{sect_sample_no}\\n\\n    {corpus_sects_filtered_ls[sect_sample_no][:500]}')\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19iueVUIg4Sh"
      },
      "source": [
        "# Check for extraneous CHAPTER or SECTION headers in either Chapter or Section lists\n",
        "\n",
        "print(f'\\n\\nChecking {len(corpus_chaps_filtered_ls)} Chapters for extraneous CHAPTER and SECTION headers')\n",
        "\n",
        "chaps_wchapheads_ls = [x for x in corpus_chaps_filtered_ls if 'CHAPTER ' in x]\n",
        "print(f'{len(chaps_wchapheads_ls)} Sections may still have a CHAPTER heading')\n",
        "\n",
        "chaps_wsectheads_ls = [x for x in corpus_chaps_filtered_ls if 'SECTION ' in x]\n",
        "print(f'{len(chaps_wsectheads_ls)} Sections may still have a SECTION heading')\n",
        "\n",
        "\n",
        "print(f'\\n\\n Checking {len(corpus_sects_filtered_ls)} Sections for extraneous CHAPTER and SECTION headers')\n",
        "\n",
        "sects_wchapheads_ls = [x for x in corpus_sects_filtered_ls if 'CHAPTER ' in x]\n",
        "print(f'{len(sects_wchapheads_ls)} Sections may still have a CHAPTER heading')\n",
        "\n",
        "sects_wsectheads_ls = [x for x in corpus_sects_filtered_ls if 'SECTION ' in x]\n",
        "print(f'{len(sects_wsectheads_ls)} Sections may still have a SECTION heading')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p7xiakRvHgq"
      },
      "source": [
        "# Verify Chapters and manually delete extraneous CHAPTER or SECTION Headers\n",
        "\n",
        "# TODO: Need to check for all 4 combinations of Chapter/Section existing/missing in Corpus\n",
        "\n",
        "corpus_chap_ct = len(corpus_chaps_filtered_ls)\n",
        "corpus_chaps_clean_ls = []\n",
        "\n",
        "# corpus_chaps_filtered_ls[0]\n",
        "\n",
        "if corpus_chap_ct > 1:\n",
        "  \n",
        "  # Remove the extraneous Chapter by Hand if necessary\n",
        "  corpus_chaps_filtered_ls = [x for x in corpus_chaps_filtered_ls if len(x.strip()) > MIN_CHAP_LEN]\n",
        "\n",
        "  # Clean the text\n",
        "  corpus_chaps_clean_ls = [clean_text(x) for x in corpus_chaps_filtered_ls]\n",
        "\n",
        "else:\n",
        "\n",
        "  print(f'WARNING: Corpus contains no CHAPTER structure')\n",
        "\n",
        "  if len(corpus_sects_filtered_ls) > 1:\n",
        "    # If Sections exists, copy Section structure to Chapter structure\n",
        "    print('         Copying SECTION structure to CHAPTER\\n\\n')\n",
        "    corpus_chaps_filtered_ls = corpus_sects_filtered_ls\n",
        "    corpus_chaps_clean_ls = corpus_sects_clean_ls\n",
        "    chapno_ls = list(range(len(corpus_chaps_filtered_ls)))\n",
        "\n",
        "  else:\n",
        "    # No Chapter nor Section structure, just plain text in Corpus without any headings\n",
        "    print('         No CHAPTER nor SECTION structure in this Corpus\\n\\n')\n",
        "    chapno_ls = [1]\n",
        "\n",
        "\n",
        "if len(corpus_chaps_filtered_ls) > 1:\n",
        "\n",
        "  # Verify\n",
        "  print(f'First Section of {len(corpus_chaps_filtered_ls)}:\\n\\n  Beginning: {corpus_chaps_filtered_ls[0][:50]}\\n\\n  Ending: {corpus_chaps_filtered_ls[0][-50:]}\\n\\n')\n",
        "\n",
        "  print('----------')\n",
        "\n",
        "  print(f'{corpus_chap_ct} Chapters found in this Corpus:\\n\\n')\n",
        "\n",
        "  print(f'First Section of {len(corpus_chaps_filtered_ls)}:\\n\\n  Beginning: {corpus_chaps_filtered_ls[0][:50]}\\n\\n  Ending: {corpus_chaps_filtered_ls[0][-50:]}\\n\\n')\n",
        "\n",
        "  print(f'Second Section of {len(corpus_chaps_filtered_ls)}:\\n\\n  Beginning: {corpus_chaps_filtered_ls[1][:50]}\\n\\n  Ending: {corpus_chaps_filtered_ls[1][-50:]}\\n\\n')\n",
        "\n",
        "  print(f'Second Last Section of {len(corpus_chaps_filtered_ls)}:\\n\\n  Beginning: {corpus_chaps_filtered_ls[-2][:50]}\\n\\n  Ending: {corpus_chaps_filtered_ls[-2][-50:]}\\n\\n')\n",
        "\n",
        "  print(f'Last Section of {len(corpus_chaps_filtered_ls)}:\\n\\n  Beginning: {corpus_chaps_filtered_ls[-1][:50]}\\n\\n  Ending: {corpus_chaps_filtered_ls[-1][-50:]}\\n\\n')\n",
        "\n",
        "  print('----------')\n",
        "\n",
        "else:\n",
        "\n",
        "  print('\\n\\nCorpus has no CHAPTER nor SECTION headers:')\n",
        "  print('  handle as one block to text above Paragraph/Sentence structural level')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "import re\n",
        "\n",
        "mylist = [\"dog\", \"cat\", \"wildcat\", \"thundercat\", \"cow\", \"hooo\"]\n",
        "r = re.compile(\"CHAPTER [IVXL]{1,10}\")\n",
        "newlist = list(filter(r.match, mylist)) # Read Note below\n",
        "# print(newlist)\n",
        "\"\"\";\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LScTOSIVRE95"
      },
      "source": [
        "# Verify Sections and manually delete extraneous CHAPTER or SECTION Headers\n",
        "\n",
        "corpus_sect_ct = len(corpus_sects_filtered_ls)\n",
        "corpus_sects_clean_ls = []\n",
        "\n",
        "if corpus_sect_ct > 1:\n",
        "\n",
        "  # Remove the extraneous Section by Hand if necessary\n",
        "  corpus_sects_filtered_ls = [x for x in corpus_sects_filtered_ls if not x.isupper()]\n",
        "\n",
        "  # Clean the text\n",
        "  corpus_sects_clean_ls = [clean_text(x) for x in corpus_sects_filtered_ls]\n",
        "\n",
        "else:\n",
        "\n",
        "  print(f'WARNING: Corpus contains no SECTION structure')\n",
        "\n",
        "  if len(corpus_chaps_filtered_ls) > 1:\n",
        "    print('         Copying CHAPTER structure to SECTION\\n\\n')\n",
        "    # If Chapters exists, copy Chapter structure to Section structure\n",
        "    corpus_sects_filtered_ls = corpus_chaps_filtered_ls\n",
        "    corpus_sects_clean_ls = corpus_chaps_clean_ls\n",
        "    sect_chapno_ls = list(range(len(corpus_sects_filtered_ls)))\n",
        "\n",
        "  else:\n",
        "    # No Chapter nor Section structure, just plain text in Corpus without any headings\n",
        "    print('         No CHAPTER nor SECTION structure in this Corpus\\n\\n')\n",
        "    sect_chapno_ls = [1]\n",
        "\n",
        "\n",
        "if len(corpus_sects_filtered_ls) > 1:\n",
        "\n",
        "  # Verify\n",
        "  print(f'First Section of {len(corpus_sects_filtered_ls)}:\\n\\n  Beginning: {corpus_sects_filtered_ls[0][:50]}\\n\\n  Ending: {corpus_sects_filtered_ls[0][-50:]}\\n\\n')\n",
        "\n",
        "  print('----------')  \n",
        "\n",
        "  print(f'{corpus_sect_ct} Chapters found in this Corpus:\\n\\n')\n",
        "\n",
        "  print(f'First Section of {len(corpus_sects_filtered_ls)}:\\n\\n  Beginning: {corpus_sects_filtered_ls[0][:50]}\\n\\n  Ending: {corpus_sects_filtered_ls[0][-50:]}\\n\\n')\n",
        "\n",
        "  print(f'Second Section of {len(corpus_sects_filtered_ls)}:\\n\\n  Beginning: {corpus_sects_filtered_ls[1][:50]}\\n\\n  Ending: {corpus_sects_filtered_ls[1][-50:]}\\n\\n')\n",
        "\n",
        "  print(f'Second Last Section of {len(corpus_sects_filtered_ls)}:\\n\\n  Beginning: {corpus_sects_filtered_ls[-2][:50]}\\n\\n  Ending: {corpus_sects_filtered_ls[-2][-50:]}\\n\\n')\n",
        "\n",
        "  print(f'Last Section of {len(corpus_sects_filtered_ls)}:\\n\\n  Beginning: {corpus_sects_filtered_ls[-1][:50]}\\n\\n  Ending: {corpus_sects_filtered_ls[-1][-50:]}\\n\\n')\n",
        "\n",
        "  print('----------')\n",
        "\n",
        "else:\n",
        "\n",
        "  print('\\n\\nCorpus has no CHAPTER nor SECTION headers:')\n",
        "  print('  handle as one block to text above Paragraph/Sentence structural level')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJgZpAJtxYJO"
      },
      "source": [
        "# Test\n",
        "\n",
        "test_ls = [x for x in corpus_chaps_filtered_ls if 'I have no' in x]\n",
        "len(test_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovxAu0w1cRou"
      },
      "source": [
        "# TODO: Temp fix\n",
        "\n",
        "corpus_chaps_clean_ls = [clean_text(x) for x in corpus_chaps_filtered_ls]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohieoJ_ehb2e"
      },
      "source": [
        "# Verify that Chapter DataFrame inputs are all same length\n",
        "\n",
        "print(f'chap_no: {list(range(len(corpus_chaps_filtered_ls)))}')\n",
        "print(f'chap_raw: {len(corpus_chaps_filtered_ls)}')\n",
        "print(f'chap_clean: {len(corpus_chaps_clean_ls)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvljymc6cDee"
      },
      "source": [
        "# corpus_chaps_filtered_ls[4][:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Coq6ZS6aMkvd"
      },
      "source": [
        "# Create Chapter DataFrame\n",
        "\n",
        "chap_no_ls = list(range(len(corpus_chaps_filtered_ls)))\n",
        "corpus_chaps_df = pd.DataFrame({'chap_no':chap_no_ls, 'chap_raw':corpus_chaps_filtered_ls, 'chap_clean':corpus_chaps_clean_ls})\n",
        "corpus_chaps_df['chap_raw'] = corpus_chaps_df['chap_raw'].astype('string')\n",
        "corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_clean'].astype('string')\n",
        "# corpus_chaps_df.head(1)\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdKAEsWt-y_w"
      },
      "source": [
        "# Calculate length statistics for Chapters\n",
        "\n",
        "corpus_chaps_df['char_len'] = corpus_chaps_df['chap_raw'].apply(lambda x : len(x))\n",
        "corpus_chaps_df['token_len'] = corpus_chaps_df['chap_raw'].apply(lambda x : len(x.split()))\n",
        "# corpus_chaps_df.head()\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTDdHkUa45rB"
      },
      "source": [
        "# Correct if lengths don't match due to Section != Chapter structures\n",
        "\n",
        "if corpus_chaps_df.shape[0] < corpus_sects_df.shape[0]:\n",
        "  sect_chapno_ls = [0]*corpus_sects_df.shape[0]\n",
        "\n",
        "len(sect_chapno_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKmMapiBuzKN"
      },
      "source": [
        "corpus_raw_str[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYG8knXZtpHm"
      },
      "source": [
        "# TODO: Temp fix\n",
        "\n",
        "\"\"\"\n",
        "corpus_tokens_ls = corpus_raw_str.split()\n",
        "sect_chapno_ls = []\n",
        "chap_ptr = 0\n",
        "sect_ptr = 0\n",
        "len(corpus_tokens_ls)\n",
        "for i, atoken in enumerate(corpus_tokens_ls):\n",
        "  if atoken == 'CHAPTER':\n",
        "    chap_ptr += 1\n",
        "  if atoken == 'SECTION':\n",
        "    sect_ptr += 1\n",
        "    sect_chapno_ls.append(chap_ptr)\n",
        "\n",
        "print(f'Section ChapNo Length: {len(sect_chapno_ls)}\\n    {sect_chapno_ls}')\n",
        "\n",
        "# If OK\n",
        "\n",
        "# sect_chapno_ls = sect_chapno_ls\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK7mAwMVirjE"
      },
      "source": [
        "len(sect_chapno_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJRG7cXI5T10"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "len(sect_chapno_ls)\n",
        "print('\\n')\n",
        "len(sect_no_ls)\n",
        "print('\\n')\n",
        "len(corpus_sects_filtered_ls)\n",
        "print('\\n')\n",
        "\n",
        "# TODO: Fix earlier\n",
        "corpus_sects_clean_ls = [clean_text(x) for x in corpus_sects_filtered_ls]\n",
        "\n",
        "len(corpus_sects_clean_ls)\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvUvQ1WMjEqd"
      },
      "source": [
        "# TODO: To fix\n",
        "\n",
        "sect_no_ls = sect_no_ls = chap_no_ls\n",
        "\n",
        "sect_chapno_ls = list(range(len(sect_no_ls)))\n",
        "\n",
        "# corpus_sects_clean_ls = [clean_text(x) for x in corpus_sects_filtered_ls]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNYDPkO7izxE"
      },
      "source": [
        "# Verify that Section DataFrame inputs are all same length\n",
        "\n",
        "print(f'sect_no: {len(sect_no_ls)}')\n",
        "print(f'sect_chapno_ls: {len(sect_chapno_ls)}')\n",
        "print(f'setc_raw: {len(corpus_sects_filtered_ls)}')\n",
        "print(f'sect_clean: {len(corpus_sects_clean_ls)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiBMyYDD88z5"
      },
      "source": [
        "# Create Section DataFrame\n",
        "\n",
        "sect_no_ls = list(range(len(corpus_sects_filtered_ls)))\n",
        "corpus_sects_df = pd.DataFrame({'sect_no':sect_no_ls, 'chap_no':sect_chapno_ls, 'sect_raw':corpus_sects_filtered_ls, 'sect_clean':corpus_sects_clean_ls})\n",
        "corpus_sects_df['sect_raw'] = corpus_sects_df['sect_raw'].astype('string')\n",
        "corpus_sects_df['sect_clean'] = corpus_sects_df['sect_clean'].astype('string')\n",
        "# corpus_sects_df.head(1)\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnBFWwX62RZ7"
      },
      "source": [
        "len(sect_no_ls)\n",
        "print('\\n')\n",
        "len(sect_chapno_ls)\n",
        "print('\\n')\n",
        "len(corpus_sects_filtered_ls)\n",
        "print('\\n')\n",
        "len(corpus_sects_clean_ls)\n",
        "print('\\n')\n",
        "\n",
        "# TODO: Make more robust solution\n",
        "sect_chapno_len = len(sect_no_ls)\n",
        "print(f'sect_chapno_ls: {sect_chapno_len}')\n",
        "if len(sect_chapno_ls) == 0:\n",
        "  sect_chapno_ls = [0]*len(sect_no_ls)\n",
        "\n",
        "print('\\n')\n",
        "len(sect_no_ls)\n",
        "print('\\n')\n",
        "len(sect_chapno_ls)\n",
        "print('\\n')\n",
        "len(corpus_sects_filtered_ls)\n",
        "print('\\n')\n",
        "len(corpus_sects_clean_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9jpNBIGmyBT"
      },
      "source": [
        "# Calculate length statistics for Chapters\n",
        "\n",
        "corpus_sects_df['char_len'] = corpus_sects_df['sect_raw'].apply(lambda x : len(x))\n",
        "corpus_sects_df['token_len'] = corpus_sects_df['sect_raw'].apply(lambda x : len(x.split()))\n",
        "# corpus_sects_df.head()\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpNdgPL_4rbQ"
      },
      "source": [
        "corpus_sects_df.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFj2CQPD6l01"
      },
      "source": [
        "#### **Create Paragraph and Sentence DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td-w3TyEmSn1"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui_fYtR4G34A"
      },
      "source": [
        "# The spacy/nlp/pipe need to be recreated each time a large Corpus is parsed below\n",
        "\n",
        "# import pysbd\n",
        "# import spacy\n",
        "# from pysbd.utils import PySBDFactory\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "\n",
        "# explicitly adding component to pipeline\n",
        "# (recommended - makes it more readable to tell what's going on)\n",
        "nlp.add_pipe(PySBDFactory(nlp))\n",
        "\n",
        "# Test\n",
        "\n",
        "doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n",
        "print(list(doc.sents))\n",
        "# [My name is Jonas E. Smith., Please turn to p. 55.]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P34vN8P-9TiV"
      },
      "source": [
        "corpus_sects_df.iloc[0]['sect_raw'].split('\\n')[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weu1ZRsC6XCP"
      },
      "source": [
        "corpus_sectno = -1\n",
        "\n",
        "corpus_paragno = -1\n",
        "corpus_parags_ls = []\n",
        "\n",
        "corpus_sentno = -1\n",
        "corpus_sents_ls = []\n",
        "\n",
        "sects_filtered_ls = list(corpus_sects_df.sect_raw)\n",
        "\n",
        "# For every Section in the Corpus\n",
        "for asectno, asect_raw in enumerate(sects_filtered_ls):\n",
        "  # print(f'Section #{asectno}')\n",
        "  corpus_sectno += 1\n",
        "\n",
        "  # Split into a list of Paragraphs\n",
        "  sect_parags_filtered_ls, sect_clean_str = sect2parags(asect_raw)\n",
        "  # For every Paragraph in Section \n",
        "  for aparagno, aparag_raw in enumerate(sect_parags_filtered_ls):\n",
        "    print(f'  Paragraph #{aparagno}')\n",
        "    print(f'Split Section #{asectno} in {len(sect_parags_filtered_ls)} Paragraphs')\n",
        "    corpus_paragno += 1\n",
        "\n",
        "    # Split into list of Sentences\n",
        "    print(f'\\nPassing Paragraph:\\n\\n    {aparag_raw} to parag2sents')\n",
        "    parag_sents_filtered_ls, parag_clean_str = parag2sents(aparag_raw)\n",
        "    print(f'Split Paragraph into {len(parag_sents_filtered_ls)} Sentences')\n",
        "    aparag_clean = clean_text(aparag_raw)\n",
        "    if any_alphachar(aparag_clean):\n",
        "      # Only add Paragraphs that have at least one alpha char after clean_text() (e.g. skip Paragraph = '$1.00!!! ---' )\n",
        "      corpus_parags_ls.append((corpus_paragno, corpus_sectno, aparag_raw, aparag_clean))\n",
        "\n",
        "    # For every Sentence in Paragraph\n",
        "    for asentno, asent_raw in enumerate(parag_sents_filtered_ls):\n",
        "      print(f'Section #{asectno}, Paragraph #{aparagno}, Section Sentence #{asentno} Corpus Sentence #{corpus_sentno}')\n",
        "      corpus_sentno += 1\n",
        "      asent_clean = clean_text(asent_raw)\n",
        "      if any_alphachar(asent_clean):\n",
        "        # Only add Sentences that have at least one alpha char after clean_text() (e.g. skip Sentence = '$1.00!!! ---' )\n",
        "        corpus_sents_ls.append((corpus_sentno, corpus_paragno, corpus_sectno, asent_raw, asent_clean))\n",
        "\n",
        "    # if asectno > 2:\n",
        "    #   break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5eu0IKxORTS"
      },
      "source": [
        "# Test (The Great Gatsby) ensure iloc[2844] raw_sent '$2.11' removed\n",
        "\n",
        "# corpus_sents_df.shape\n",
        "# corpus_sents_df.iloc[2843]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCvhDPyHZUni"
      },
      "source": [
        "# Verfiy Section counts\n",
        "\n",
        "for i, asect in enumerate(sects_filtered_ls):\n",
        "  print(f'Length of Section #{i}: {len(asect)}\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc5-8GDOd8rF"
      },
      "source": [
        "# Verify the first 500 chars of first few Sections\n",
        "\n",
        "char_ct = 500\n",
        "\n",
        "for i, asect in enumerate(sects_filtered_ls):\n",
        "  print(f'Start of Section #{i} ------------------------------\\n\\n    {asect[:char_ct]}\\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhXKvuQPevmM"
      },
      "source": [
        "# Verify first raw Section text\n",
        "\n",
        "sects_filtered_ls[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4721-er6W96"
      },
      "source": [
        "# Verify Sentence and Paragraph Counts\n",
        "\n",
        "sent_ct = len(corpus_sents_ls)\n",
        "print(f'{sent_ct} Sentences were found in the Corpus')\n",
        "\n",
        "parag_ct = len(corpus_parags_ls)\n",
        "print(f'{parag_ct} Paragraphs were found in the Corpus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0gltS-r0QPv"
      },
      "source": [
        "print(corpus_parags_ls[0][:50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm9fDtVJziE3"
      },
      "source": [
        "# Create Paragraph DataFrame\n",
        "\"\"\"\n",
        "parag_no_ls = list(range(len(corpus_parags_ls)))\n",
        "corpus_parags_df = pd.DataFrame({'sect_no':sect_no_ls, 'chap_no':sect_chapno_ls, 'sect_raw':corpus_sects_raw_ls, 'sect_clean':corpus_sects_clean_ls})\n",
        "corpus_parags_df['sect_raw'] = corpus_parags_df['sect_raw'].astype('string')\n",
        "corpus_parags_df['sect_clean'] = corpus_parags_df['sect_clean'].astype('string')\n",
        "corpus_parags_df.info()\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ7Xdmyt0Asb"
      },
      "source": [
        "# Create Paragraph DataFrame\n",
        "\n",
        "corpus_parags_df = pd.DataFrame(corpus_parags_ls,columns=['parag_no','sect_no','parag_raw','parag_clean'])\n",
        "corpus_parags_df['parag_raw'] = corpus_parags_df['parag_raw'].astype('string')\n",
        "corpus_parags_df['parag_clean'] = corpus_parags_df['parag_clean'].astype('string')\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arm9ZlvSl-42"
      },
      "source": [
        "# Calculate length statistics for Paragraphs\n",
        "\n",
        "corpus_parags_df['char_len'] = corpus_parags_df['parag_raw'].apply(lambda x : len(x))\n",
        "corpus_parags_df['token_len'] = corpus_parags_df['parag_raw'].apply(lambda x : len(x.split()))\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqN4aFeT3RMq"
      },
      "source": [
        "# Verify Paragraph DataFrame start and end\n",
        "\n",
        "corpus_parags_df.head(5)\n",
        "corpus_parags_df.tail(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T27_raCzWk6"
      },
      "source": [
        "# Create Sentences DataFrame\n",
        "\n",
        "corpus_sents_df = pd.DataFrame(corpus_sents_ls,columns=['sent_no', 'parag_no','sect_no','sent_raw','sent_clean'])\n",
        "corpus_sents_df['sent_raw'] = corpus_sents_df['sent_raw'].astype('string')\n",
        "corpus_sents_df['sent_clean'] = corpus_sents_df['sent_clean'].astype('string')\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glceM5TylaA8"
      },
      "source": [
        "# Compute length statistics for Sentences\n",
        "\n",
        "corpus_sents_df['char_len'] = corpus_sents_df['sent_raw'].apply(lambda x : len(x))\n",
        "corpus_sents_df['token_len'] = corpus_sents_df['sent_raw'].apply(lambda x : len(x.split()))\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4WWrd1J2uz_"
      },
      "source": [
        "# Verify Sentence DataFrame start and end\n",
        "\n",
        "corpus_sents_df.head(20)\n",
        "corpus_sents_df.tail(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSzNKPacx9Nd"
      },
      "source": [
        "# Test\n",
        "\n",
        "search_str = 'I have no'\n",
        "corpus_sents_df[corpus_sents_df['sent_raw'].str.find(search_str) != -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgJKnHHk4f3F"
      },
      "source": [
        "#### **For each Section, insert [start|mid|end] Sentence numbers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnwy-rFW4swr"
      },
      "source": [
        "# Calculate the start, mid and end Sentence No for each Section\n",
        "\n",
        "sect_sent_no_start_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].min())\n",
        "sect_sent_no_end_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].max())\n",
        "\n",
        "def my_mid(anum, bnum):\n",
        "  mid_no = (anum - bnum)//2 + bnum\n",
        "\n",
        "  return mid_no\n",
        "\n",
        "print('\\nSection start sentence no: -----')\n",
        "print(sect_sent_no_start_ls)\n",
        "\n",
        "sect_sent_no_mid_ls = list(map(my_mid, sect_sent_no_end_ls, sect_sent_no_start_ls))\n",
        "print('\\nSection mid-sentence no: -----')\n",
        "print(sect_sent_no_mid_ls)\n",
        "\n",
        "print('\\nSection end sentence no: -----')\n",
        "print(sect_sent_no_end_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MNNcN_d48xa"
      },
      "source": [
        "# Insert 3 new columns on start,mid and end Sentence No for each Section\n",
        "\n",
        "corpus_sects_df.insert(2, 'sent_no_start', sect_sent_no_start_ls)\n",
        "corpus_sects_df.insert(3, 'sent_no_mid', sect_sent_no_mid_ls)\n",
        "corpus_sects_df.insert(4, 'sent_no_end', sect_sent_no_end_ls)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuxMQpOY2uwF"
      },
      "source": [
        "# Verfiy Section \n",
        "\n",
        "corpus_sects_df.filter(like='_no')\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z48WKV1W6KdA"
      },
      "source": [
        "#### **For Each Chapter, Insert [start|mid|end] Sentence numbers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77oGMj4v6pZZ"
      },
      "source": [
        "corpus_chaps_filtered_ls[0][:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGNlqV5G7Tk-"
      },
      "source": [
        "corpus_chaps_clean_ls[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70ntj-6X8PbU"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWFbBuEGQ4fH"
      },
      "source": [
        "search_str = 'goose'\n",
        "\n",
        "corpus_sents_df[corpus_sents_df['sent_clean'].str.contains(search_str)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWvddQwkCjKY"
      },
      "source": [
        "corpus_sents_df.iloc[1056]['sent_clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DylkEa8cCwG"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vrZVqCRVOwF"
      },
      "source": [
        "achap_sentstart_clean_ls = []\n",
        "\n",
        "for indx, achap_tup in corpus_chaps_df.iterrows():\n",
        "  achap_no, achap_raw, achap_clean, achap_charlen, achap_tokenlen = achap_tup\n",
        "  achap_sentstart_clean_ls.append(achap_clean[:100])\n",
        "\n",
        "print(achap_sentstart_clean_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2HcU3WLaHpV"
      },
      "source": [
        "!pip install fuzzywuzzy[speedup]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU_11lWzaI-I"
      },
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbxpUygQaL3g"
      },
      "source": [
        " fuzz.ratio(\"this is a test\", \"this is a test!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6B0n3hdbLyp"
      },
      "source": [
        "corpus_sects_df.info()\n",
        "\n",
        "type(corpus_sects_df['sect_clean'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCarKELJ-S2x"
      },
      "source": [
        "# Optional\n",
        "\n",
        "# corpus_chaps_df.drop(columns=['sect_no'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDKC_kucCYih"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ytgSzv5DCjM"
      },
      "source": [
        "corpus_parags_df.iloc[:5]['parag_clean'][:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oXrE-d0C3Ik"
      },
      "source": [
        "corpus_sects_df.iloc[:5]['sect_clean'][:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaoY4ACmDdEN"
      },
      "source": [
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qn8CySv9byQ"
      },
      "source": [
        "corpus_chaps_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSqva3DLwu4H"
      },
      "source": [
        "# Clean Chapters DataFrame\n",
        "\n",
        "corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_clean'].apply(lambda x: re.sub(r'SECTION [IVXL]{1,10}[.]','',x))\n",
        "corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_clean'].apply(lambda x: clean_text(x).strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDOUcy8fyGBM"
      },
      "source": [
        "corpus_chaps_df.iloc[0]['chap_clean'][:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtGurlnd6-Qt"
      },
      "source": [
        "# Calculate the start, mid and end Sentence No for each Section\n",
        "\n",
        "min_charlen = 80  # How many characters to match at the start of start/end Sentences\n",
        "\n",
        "# If there is no Chapter structure\n",
        "if corpus_chaps_df.shape[0] == 1:\n",
        "  \n",
        "  chaps_sentend = corpus_sents_df.shape[0] - 1\n",
        "  chaps_sentmid = chaps_sentend//2\n",
        "\n",
        "  chaps_sentstart_ls = [0]\n",
        "  chaps_sentmid_ls = [chaps_sentmid]\n",
        "  chaps_sentend_ls = [chaps_sentend]\n",
        "\n",
        "else:\n",
        "\n",
        "  # First, find which Sections align with each Chapter and save Section's sent_no_start as the same for Chapter\n",
        "  chaps_sentstart_ls = []\n",
        "  sects_sentstart_ls = list(corpus_sects_df['sect_clean'].apply(lambda x: x[:min_charlen].strip()))\n",
        "  for achap_indx, achap_tup in corpus_chaps_df.iterrows():\n",
        "    # TODO: Insert deal with 5 or 6 returned items in tuple depending on Corpus Chapter/Section structure\n",
        "    # 6 args returned\n",
        "    # achap_no, achap_sentno_start, achap_raw, achap_clean, achap_charlen, achap_tokenlen = achap_tup\n",
        "    # 5 args returned\n",
        "    achap_no, achap_raw, achap_clean, achap_charlen, achap_tokenlen = achap_tup\n",
        "    achap_sentstart_str = achap_clean[:min_charlen].strip()\n",
        "    achap_sentstart_str = ' '.join(achap_sentstart_str.split())\n",
        "\n",
        "    sent_startno_found = False\n",
        "    # Loop over all the Sections to see if the starting text matches the Chapter starting text\n",
        "    for asect_indx, asect_sentstart_str in enumerate(sects_sentstart_ls):\n",
        "      asect_sentstart_str = ' '.join(asect_sentstart_str.split())\n",
        "      achap_sentstart_len = len(achap_sentstart_str)\n",
        "      asect_sentstart_len = len(asect_sentstart_str)\n",
        "      if (achap_sentstart_len > asect_sentstart_len):\n",
        "        len_diff = achap_sentstart_len - asect_sentstart_len\n",
        "        achap_sentstart_str = achap_sentstart_str[:-len_diff]\n",
        "      elif (achap_sentstart_len < asect_sentstart_len):\n",
        "        len_diff = asect_sentstart_len - achap_sentstart_len\n",
        "        asect_sentstart_str = asect_sentstart_str[:-len_diff]\n",
        "      else:\n",
        "        # Both Chapter and Section starting Sentence strings are equal length\n",
        "        pass\n",
        "\n",
        "      print(f'Fuzzy Compare:\\n\\n    Chapter Start: {achap_sentstart_str}\\n    Section Start: {asect_sentstart_str}')\n",
        "      if fuzz.ratio(achap_sentstart_str, asect_sentstart_str) > 97:\n",
        "        sent_startno_ls = list(corpus_sects_df[corpus_sects_df['sect_no'] == asect_indx]['sent_no_start'])\n",
        "        # print(f'MATCH!!! type(sent_startno): {sent_startno_ls[0]}')\n",
        "        chaps_sentstart_ls.append(sent_startno_ls[0])\n",
        "        sent_startno_found = True\n",
        "        break\n",
        "    # If no match found, enter ERROR code -1 in the Sentence start no for the current Chapter\n",
        "    if sent_startno_found == False:\n",
        "      chaps_sentstart_ls.append((achap_indx, -1))\n",
        "\n",
        "  print(f'\\n\\nchaps_sentstart_ls: {chaps_sentstart_ls}')\n",
        "\n",
        "\n",
        "\n",
        "  # Second, get the end Sentence for Each Chapter by rotating Sentence Start No left and pushing on the last Sentence No\n",
        "  chaps_sentend_ls = []\n",
        "  chaps_sentend_ls = [x-1 for x in chaps_sentstart_ls]\n",
        "  corpus_sentlast = corpus_sents_df.shape[0] - 1\n",
        "\n",
        "  chaps_sentend_ls.pop(0)\n",
        "  chaps_sentend_ls.append(corpus_sentlast)\n",
        "\n",
        "  print(f'\\n\\nchaps_sentend_ls: {chaps_sentend_ls}')\n",
        "\n",
        "\n",
        "  # Third, calculate the Sentence No in the middle of each Chapter\n",
        "\n",
        "  chaps_sentmid_ls = [(((chaps_sentend_ls[i] - chaps_sentstart_ls[i])//2)+chaps_sentstart_ls[i]) for i in range(len(chaps_sentend_ls))]\n",
        "\n",
        "  print(f'\\n\\nchaps_sentmid_ls: {chaps_sentmid_ls}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpOzOahxhgvU"
      },
      "source": [
        "# Verify boundry cases\n",
        "\n",
        "corpus_sents_df.iloc[268]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN_GXkBekH0P"
      },
      "source": [
        "# corpus_chaps_df.drop(columns=['sent_no_start','sent_no_mid','sent_no_end'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrfK9MVAEuYp"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZTduIsl6KdB"
      },
      "source": [
        "# Insert 3 new columns on start,mid and end Sentence No for each Chapter\n",
        "\n",
        "corpus_chaps_df.insert(1, 'sent_no_start', chaps_sentstart_ls)\n",
        "corpus_chaps_df.insert(2, 'sent_no_mid', chaps_sentmid_ls)\n",
        "corpus_chaps_df.insert(3, 'sent_no_end', chaps_sentend_ls)\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTW3Gxmf6KdC"
      },
      "source": [
        "# Verfiy Chapter \n",
        "\n",
        "corpus_chaps_df.filter(like='_no')\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDSyIfqDQCMI"
      },
      "source": [
        "# Verify\n",
        "\n",
        "print(f'Corpus Sentence DataFrame shape: {corpus_sents_df.shape}')\n",
        "print(f'Corpus Paragraph DataFrame shape: {corpus_parags_df.shape}')\n",
        "print(f'Corpus Section DataFrame shape: {corpus_sects_df.shape}')\n",
        "print(f'Corpus Chapter DataFrame shape: {corpus_chaps_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjfAWFJ1xl8d"
      },
      "source": [
        "corpus_sects_df.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtOOm2UQd1o"
      },
      "source": [
        "#### **Save Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39_ehy97zB1Z"
      },
      "source": [
        "# Test\n",
        "\n",
        "search_str = 'I have no'\n",
        "corpus_sents_df[corpus_sents_df['sent_raw'].str.find(search_str) != -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hf92HlV9--1"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGkvSy4HQdCe"
      },
      "source": [
        "# Save Corpus DataFrames\n",
        "\n",
        "save_dataframes(df_ls=['baseline'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDEqASdRUfVF"
      },
      "source": [
        "corpus_sents_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK27hN0KkvBI"
      },
      "source": [
        "### **END HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVCkjat0vffd"
      },
      "source": [
        "#### **Get All (non-null) Raw Lines**\n",
        "\n",
        "* NOTE: Corpus textfile needs to be Preprocessed before running this\n",
        "- Remove all Curve Parenthesis that span multiple sentences or paragraphs\n",
        "- Remove all Square Parenthesis\n",
        "- Filter out non-printing characters if they exist (or use proper encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgqqpwagWPS7"
      },
      "source": [
        "len(corpus_lines_ls)\n",
        "print('\\n')\n",
        "print(corpus_lines_ls[11])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBl_hrhPZo29"
      },
      "source": [
        "min(corpus_lines_ls, key=lambda word: len(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthUz3kpZ8_K"
      },
      "source": [
        "corpus_lines_ls.sort(key=lambda s: len(s))\n",
        "corpus_lines_ls[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an7SzlAuvXA0"
      },
      "source": [
        "# NOTE: ~3-15 minutes (one pass with PySBD)\n",
        "#             minutes (two passes with PySBD+NLTK)\n",
        "\n",
        "# Time consuming so only set pysbd_only=True (second NLTK sentence tokenizer pass) \n",
        "#   if necessary (e.g. Samuel Butler's 1900 trans. of Homer's Odyssey)\n",
        "#                 PySBD: 1075 lines, PySBD+NLTK: 3905 lines, NLTK: 3109 lines\n",
        "#                        Why? a 3 Sentence Paragraph enclosed in double quotes is counted as one Sentence by PySBD\n",
        "#           w/spec char strip: PySBD+NLTK: 3926\n",
        "#           w/o digits/footnotes: PySBD+NLTK: 3925\n",
        "\n",
        "corpus_lines_ls, lines_raw_str = corpus2lines(corpus_filename, pysbd_only=False)\n",
        "\n",
        "# Verify\n",
        "print(f'\\n\\nRead raw corpus lines: character count: {len(lines_raw_str)}')\n",
        "print(f'                        raw line count:  {len(corpus_lines_ls)}\\n\\n')\n",
        "\n",
        "line_ct = 10\n",
        "print(f'First {line_ct} raw lines: --------------------\\n')\n",
        "for i,aline in enumerate(corpus_lines_ls[:line_ct]):\n",
        "  print(f'Line #{i}:\\n    {aline}')\n",
        "print(f'\\n\\nLast {line_ct} raw lines: -------------------\\n')\n",
        "for i,aline in enumerate(corpus_lines_ls[-line_ct:]):\n",
        "  print(f'Line #{i}: {aline}')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "BEFORE stripping out headings len: 610949 (605835 w/2nd pass)\n",
        "Corpus Paragraph Raw Count: 1075\n",
        "   Parag count before processing sents: 1075\n",
        "About to return corpus_sents_raw_ls with len = 2469\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b6ORN6A08Dv"
      },
      "source": [
        "#### **Create Sentence DataFrame: [corpus_sents_df]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BeGt1ot0_dK"
      },
      "source": [
        "# Filter out all the structural/metatag and blank/punctuation only lines\n",
        "#    and save all semantically meaningful Sentences in corpus_sents_ls\n",
        "\n",
        "corpus_sents_ls = []\n",
        "\n",
        "for i, aline in enumerate(corpus_lines_ls):\n",
        "\n",
        "    # print(f'Examing line #{i}: {aline}')\n",
        "\n",
        "    aline_clean = aline.strip()\n",
        "    # Skip/delete whitespace only sentences\n",
        "    if len(aline_clean) == 0:\n",
        "      continue\n",
        "    \n",
        "    # Skip/delete any sentences starting with CHAPTER RegEx Pattern\n",
        "    if aline_clean.startswith('CHAPTER '):\n",
        "      continue\n",
        "    \n",
        "    # Skip/delete any sentences starting with SECTION RegEx Pattern\n",
        "    if aline_clean.startswith('SECTION '):\n",
        "      continue\n",
        "\n",
        "    # Skip/delete any sentences starting with SECTION RegEx Pattern\n",
        "    if aline_clean.startswith('BOOK '):\n",
        "      continue\n",
        "\n",
        "    # Skip/delete any sentence no alpha/numeric charcters (e.g. only punctuation)\n",
        "    if (re.match('^[^a-zA-Z]+$', aline_clean)):\n",
        "      print(f'No alnum line #{i}: {aline}')\n",
        "      continue\n",
        "\n",
        "    # If passed through all previous filters, save as genuine Sentence\n",
        "    corpus_sents_ls.append(aline_clean)\n",
        "\n",
        "# Test\n",
        "print(f'Raw Lines length: {len(corpus_lines_ls)}')\n",
        "print(f' Clean Sentences: {len(corpus_sents_ls)}')\n",
        "\n",
        "line_ct = 10\n",
        "print(f'First {line_ct} clean Sentences : --------------------\\n')\n",
        "for i,aline in enumerate(corpus_sents_ls[:line_ct]):\n",
        "  print(f'Line #{i}:\\n    {aline}')\n",
        "print(f'\\n\\nLast {line_ct} clean Sentences: -------------------\\n')\n",
        "for i,aline in enumerate(corpus_sents_ls[-line_ct:]):\n",
        "  print(f'Line #{i}: {aline}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8ERYqQW7U9q"
      },
      "source": [
        "# Create Sentence DataFrame\n",
        "\n",
        "sent_no_ls = list(range(len(corpus_sents_ls)))\n",
        "\n",
        "corpus_sents_df = pd.DataFrame({'sent_no': sent_no_ls, 'sent_raw': corpus_sents_ls})\n",
        "corpus_sents_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70VO_w4Oql1a"
      },
      "source": [
        "# Compute Sentence text statistics\n",
        "\n",
        "corpus_sents_df['sent_clean'] = corpus_sents_df['sent_raw'].apply(lambda x: clean_text(x))\n",
        "corpus_sents_df['token_len'] = corpus_sents_df['sent_clean'].apply(lambda x: len(x.split()))\n",
        "corpus_sents_df['char_len'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x))\n",
        "corpus_sents_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFC8GTnw6HrG"
      },
      "source": [
        "#### **Create Paragraph DataFrame: [corpus_parags_df]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGog8ZW16bbr"
      },
      "source": [
        "# Read Corpus into a single string then split into raw Paragraphs\n",
        "\n",
        "corpus_parags_ls, corpus_parags_raw_ls, corpus_raw_str = corpus2parags(CORPUS_FILENAME)\n",
        "print(f'Found #{len(corpus_parags_ls)} paragraphs\\n')\n",
        "\n",
        "print('\\nThe first 5 Paragraphs of the Corpus (first 10 chars):')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_ls[:5][:10]\n",
        "print('\\n')\n",
        "print('\\n\\nThe last 5 Paragraphs of the Corpus (first 10 chars):')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_ls[-5:][:10]\n",
        "print('\\n')\n",
        "\n",
        "n_shortest = 10\n",
        "print(f'The {n_shortest} shortest Paragraphs in the Corpus are:')\n",
        "print('--------------------------------------------')\n",
        "temp_parags_ls = sorted(corpus_parags_ls, key=lambda x: (len(x), x))\n",
        "for i, asent in enumerate(temp_parags_ls[:n_shortest]):\n",
        "  print(f'Shortest Paragraph #{i}: {asent}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZdfH-rTXwwT"
      },
      "source": [
        "# Verify Paragraphs found and Sentence-Paragraph matches\n",
        "\n",
        "# TODO: Upgrade if warranted (requires updating x2parags functions)\n",
        "\"\"\"\n",
        "print(f'{len(corpus_parags_ls)} Paragraphs found in Corpus')\n",
        "print(f'{len(sentences_section_ls)} Sentences found in a Section')\n",
        "\n",
        "sentences_nosection_ct = len(sentences_nosection_ls)\n",
        "\n",
        "if (sentences_nosection_ct > 0):\n",
        "  print(f'\\n    WARNING: The following {sentences_nosection_ct} Sentences were NOT FOUND in any Section')\n",
        "  print(f'             If these are important/numerous, go back and edit/correct source Corpus text file and rerun this notebook\\n')\n",
        "\n",
        "  for i, asent in enumerate(sentences_nosection_ls):\n",
        "    print(f'    #{i}: {asent}\\n')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAwfcbYJnwtq"
      },
      "source": [
        "# Verify Paragraph count and sample\n",
        "\n",
        "len(corpus_parags_ls)\n",
        "print('\\n')\n",
        "\n",
        "parag_no_ls = range(len(corpus_parags_ls))\n",
        "len(corpus_parags_ls)\n",
        "print('\\n')\n",
        "\n",
        "corpus_parags_ls[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnJQvKkyHfVc"
      },
      "source": [
        "# Create DataFrame from list of Paragraphs extracted from Corpus\n",
        "\n",
        "corpus_parags_df = pd.DataFrame({'parag_no':parag_no_ls, 'parag_raw':corpus_parags_raw_ls, 'parag_clean':[clean_text(x) for x in corpus_parags_ls]})\n",
        "corpus_parags_df['parag_raw'] = corpus_parags_df['parag_raw'].astype('string')\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.tail(2)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOvIFAsgdgK2"
      },
      "source": [
        "corpus_parags_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrQ4YmkK6HrH"
      },
      "source": [
        "# For each Paragraph, compute the start, mid and end Sentence Number\n",
        "\n",
        "# NOTE: ~5 minutes runtime\n",
        "\n",
        "# NOTE: May fail on any paragraph/sentence with dirty text and require\n",
        "#       multiple iterations of fix/run cycles\n",
        "\n",
        "# Used raw paragraph data to update Master Corpus DataFrame with Paragraph Info\n",
        "\n",
        "# Filter out all the structural/metatag and blank/punctuation only lines\n",
        "#    and save all semantically meaningful Sentences in corpus_sents_ls\n",
        "\n",
        "corpus_sents2parag_ls = []\n",
        "corpus_sents2parag_reject_ls = []\n",
        "parag_sents_tup_ls = []\n",
        "sent_no_current = 0\n",
        "\n",
        "parag_no_current = 0\n",
        "sent_no_current = 0\n",
        "\n",
        "flag_previous_miss = False\n",
        "\n",
        "corpus_parag_ct = len(corpus_parags_ls)\n",
        "\n",
        "def get_sent2paragno(asent_no, asent_raw_str):\n",
        "  '''\n",
        "  Given a sent_no and sent_raw_str\n",
        "  Search and return the corresponding parag_no that contains the sent_raw_str\n",
        "      (return -1 if not found)\n",
        "  '''\n",
        "\n",
        "  global parag_no_current\n",
        "  global flag_previous_miss\n",
        "  # NOTE: Dependencies on local vars outside this def and global vars corpus_parags_ls\n",
        "  # global parag_no_current\n",
        "\n",
        "  # Loop over every paragraph until we find matching sentence (or fail)\n",
        "  while parag_no_current < corpus_parag_ct:\n",
        "\n",
        "    print(f'Sentence #{asent_no}, Text: {asent_raw_str}')\n",
        "    print(f'    Starting search at Paragraph #{parag_no_current}')\n",
        "    print(f'    Paragraph Text:\\n    {corpus_parags_ls[parag_no_current]}')\n",
        "    parag_str = corpus_parags_ls[parag_no_current]\n",
        "\n",
        "    # Search for Sentence string in current Paragraph string\n",
        "    # if re.search(asent_raw_str, re.escape(parag_str)):\n",
        "    # problems with embedded parenthesis\n",
        "    # noparen_table = str.maketrans({'(':' ', ')':' ', '[':' ', ']':' ', '?':' ', '\"':' ', \"'\":\" \"})\n",
        "    # asent_noparens_str = asent_raw_str.translate(noparen_table)\n",
        "    # parag_noparens_str = parag_str.translate(noparen_table)\n",
        "    asent_noparens_str = re.sub('[^0-9a-zA-Z]+', ' ', asent_raw_str)\n",
        "    parag_noparens_str = re.sub('[^0-9a-zA-Z]+', ' ', parag_str)\n",
        "\n",
        "    if re.search(asent_noparens_str, parag_noparens_str):\n",
        "      print(f'    Found it!')\n",
        "      flag_previous_miss = False\n",
        "      return parag_no_current\n",
        "    else:\n",
        "      if flag_previous_miss == True:\n",
        "        # Sentence not found in 2 consecutive Paragraphs, skip this Sentence\n",
        "        print(f'    Miss: Skip this Sentence and go set current paragraph back 1')\n",
        "        parag_no_current -= 1\n",
        "        flag_previous_miss = False\n",
        "        return -1\n",
        "      else:\n",
        "        # Sentence not found in current Paragraph, so try next one\n",
        "        print(f'     Miss: Sentence not found in current Paragraph, try next one')\n",
        "        parag_no_current += 1\n",
        "        flag_previous_miss = True\n",
        "\n",
        "\n",
        "  # At this point we searched both the current and next Paragraphs for Sentence \n",
        "  #     without success so return error code\n",
        "  return -1 \n",
        "\n",
        "# Step through every Sentence and find the Paragraph Number it lies within\n",
        "# corpus_sents_df['parag_no'] = corpus_sents_df.apply(lambda x: get_sent2paragno(x.sent_no, x.sent_raw), axis=1)\n",
        "parag_no_last_match = 0\n",
        "for idx, row in corpus_sents_df.iterrows():\n",
        "  parag_no_current = parag_no_last_match\n",
        "  asent_no = row['sent_no']\n",
        "  asent_str = row['sent_raw']\n",
        "  print(f'idx #{idx}, sent_no: {asent_no}\\n    {asent_str}')\n",
        "  \n",
        "  asent_parag_no = get_sent2paragno(asent_no, asent_str)\n",
        "  print(f'back from searching for Sentence in all Paragraphs with result: {asent_parag_no}')\n",
        "  if asent_parag_no < 0:\n",
        "    # print(f'FAIL: Did not find current Sentence so skip to next Sentence\\n\\n')\n",
        "    corpus_sents2parag_reject_ls.append((asent_no, corpus_sents_ls[asent_no]))\n",
        "    parag_no_current = parag_no_last_match\n",
        "  else:\n",
        "    # print(f'SUCCESS: Sentence #{asent_no} found in Paragraph #{asent_parag_no}\\n\\n')\n",
        "    corpus_sents2parag_ls.append((asent_no, asent_parag_no))\n",
        "    parag_no_last_match = parag_no_current\n",
        "\n",
        "  # if idx > 20:\n",
        "  #   break\n",
        "\n",
        "# Test\n",
        "\"\"\"\n",
        "print(f'Raw Lines length: {len(corpus_lines_ls)}')\n",
        "print(f' Clean Sentences: {len(corpus_sents_ls)}')\n",
        "\n",
        "line_ct = 10\n",
        "print(f'First {line_ct} clean Sentences : --------------------\\n')\n",
        "for i,aline in enumerate(corpus_sents_ls[:line_ct]):\n",
        "  print(f'Line #{i}:\\n    {aline}')\n",
        "print(f'\\n\\nLast {line_ct} clean Sentences: -------------------\\n')\n",
        "for i,aline in enumerate(corpus_sents_ls[-line_ct:]):\n",
        "  print(f'Line #{i}: {aline}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJxi6BUWqj-l"
      },
      "source": [
        "# Any Sentences not found in Corpus Paragraphs?\n",
        "#  If not empty list, manually check and edit corpus if many/significant Sentences rejected\n",
        "\n",
        "sentences_noparag_ct = len(corpus_sents2parag_reject_ls)\n",
        "\n",
        "if sentences_noparag_ct > 0:\n",
        "  print(f'\\n    WARNING: The following {sentences_noparag_ct} Sentences were NOT FOUND in any Paragraph')\n",
        "  print(f'             If these are important/numerous, go back and edit/correct source Corpus text file and rerun this notebook\\n')\n",
        "\n",
        "  for i, asent in enumerate(corpus_sents2parag_reject_ls):\n",
        "    print(f'    #{i}: {asent}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9FrWNj80PkD"
      },
      "source": [
        "corpus_sents_df.iloc[200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UAnLu0k1PAr"
      },
      "source": [
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGqcTKBtF4QU"
      },
      "source": [
        "# Create a list of the Sentence Nos associated with each Paragraph in the Corpus\n",
        "\n",
        "sent_parag_no_ls = [sentnoparagno_tp[1] for sentnoparagno_tp in corpus_sents2parag_ls]\n",
        "\n",
        "corpus_sent_ct = corpus_sents_df.shape[0]\n",
        "parags_sent_ct = len(sent_parag_no_ls)\n",
        "if (corpus_sent_ct == parags_sent_ct):\n",
        "  print(f'GOOD, all {corpus_sent_ct} Sentences were matched in one of the {corpus_parags_df.shape[0]} Paragraphs\\n')\n",
        "else:\n",
        "  print(f'WARNING: only {parags_sent_ct} Sentences were matched in one of the {corpus_parags_df.shape[0]} Paragraphs')\n",
        "  print(f'         {corpus_sent_ct - parags_sent_ct} Sentences were not matched\\n')\n",
        "\n",
        "print(f'First 10 Sentences belong to these Paragraph:\\n    {sent_parag_no_ls[:10]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7-1oBkJKeJH"
      },
      "source": [
        "# Verfiy the data about to be used to update the master corpus_all_df DataFrame with Paragraph No data\n",
        "\n",
        "corpus_sents2parag_ls[3233:3237]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2vZk07BkeNw"
      },
      "source": [
        "# corpus_all_df.drop(columns=['parag_no'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29tMZKR4KERa"
      },
      "source": [
        "# Update the master corpus_all_df DataFrame with Paragraph No for each Sentence\n",
        "\n",
        "# WARNING: Only execute once (insert Column/Series into DataFrame)\n",
        "\n",
        "parag_no_ser = pd.Series(parag_no_ls)\n",
        "corpus_sents_df.insert(loc=1, column='parag_no', value=sent_parag_no_ls)\n",
        "\n",
        "corpus_sents_df.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsOgtjM71eKE"
      },
      "source": [
        "corpus_sents_df.iloc[300:320]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK5kKZbVl3jR"
      },
      "source": [
        "# corpus_parags_df.drop(columns=['sent_no_start', 'sent_no_mid','sent_no_end'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmLLzsNtECDt"
      },
      "source": [
        "corpus_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVkStzTrlxll"
      },
      "source": [
        "# Verify the Paragraph only DataFrame\n",
        "\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkHIEWqEKz66"
      },
      "source": [
        "# Calculate the start, mid and end Sentence No for each Paragraph\n",
        "\n",
        "parag_sent_no_start_ls = np.array(corpus_sents_df.groupby('parag_no')['sent_no'].min())\n",
        "parag_sent_no_end_ls = np.array(corpus_sents_df.groupby('parag_no')['sent_no'].max())\n",
        "\n",
        "def my_mid(anum, bnum):\n",
        "  mid_no = (anum - bnum)//2 + bnum\n",
        "\n",
        "  return mid_no\n",
        "\n",
        "print('\\nParagraph Start Sentence no: -----')\n",
        "print(parag_sent_no_start_ls)\n",
        "\n",
        "parag_sent_no_mid_ls = list(map(my_mid, parag_sent_no_end_ls, parag_sent_no_start_ls))\n",
        "print('\\nParagraph Mid-Sentence no: -----')\n",
        "print(parag_sent_no_mid_ls)\n",
        "\n",
        "print('\\nParagraph End Sentence no: -----')\n",
        "print(parag_sent_no_end_ls)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbSi16c1Kz6_"
      },
      "source": [
        "# If necessary, delete prior columns to update DataFrame with new data\n",
        "\n",
        "# corpus_parags_df.drop(columns=['parag_no_start'], inplace=True)\n",
        "# corpus_parags_df.drop(columns=['parag_no_mid'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkdeR-S_Kz7B"
      },
      "source": [
        "# Insert 3 new columns on start,mid and end Sentence No for each Paragraph\n",
        "\n",
        "corpus_parags_df.insert(1, 'sent_no_start', parag_sent_no_start_ls)\n",
        "corpus_parags_df.insert(2, 'sent_no_mid', parag_sent_no_mid_ls)\n",
        "corpus_parags_df.insert(3, 'sent_no_end', parag_sent_no_end_ls)\n",
        "\n",
        "corpus_parags_df.head()\n",
        "                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9uIWYnCLSr7"
      },
      "source": [
        "# Create a clean text version of each Paragraph\n",
        "\n",
        "# corpus_parags_df = pd.DataFrame({'parag_no':parag_no_ls, 'parag_raw':corpus_parags_ls})\n",
        "corpus_parags_df['parag_clean'] = corpus_parags_df['parag_raw'].apply(lambda x: clean_text(x))\n",
        "corpus_parags_df['parag_clean'] = corpus_parags_df['parag_clean'].astype('string')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gza5pqW3LSsF"
      },
      "source": [
        "# Compute Paragraph text Statistics\n",
        "\n",
        "corpus_parags_df['token_len'] = corpus_parags_df['parag_clean'].apply(lambda x: len(x.split()))\n",
        "corpus_parags_df['char_len'] = corpus_parags_df['parag_raw'].apply(lambda x: len(x))\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOjESpbc6Izc"
      },
      "source": [
        "#### **Create Section DataFrame: [corpus_sects_df]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vITL5uo1ABR"
      },
      "source": [
        "# If Corpus has Sections, read Corpus and split into raw Sections\n",
        "#   else just copy Chapter data as pseudo Sections\n",
        "\n",
        "corpus_sects_ls = []    # List of all Section Numbers\n",
        "sent_sectno_ls = []     # Section Number for EVERY Matching Sentence in Corpus found in a Section\n",
        "sent_no_sectno_ls = []  # Sentence Number for for ANY Unmatched Sentence in Corpus NOT found in any Section\n",
        "\n",
        "\n",
        "if SECTION_HEADINGS == 'None':\n",
        "  # Just copy Chapter info\n",
        "  corpus_sects_ls = [x for x in corpus_chaps_ls]\n",
        "  print(f'No Sections in {CORPUS_FULL},\\n    so using Chapters as pseudo-Sections.')\n",
        "  sent_sectno_ls = sent_chap_no_ls # Every Sentence in Corpus belongs to the same SectionNo as CorpusNo\n",
        "\n",
        "else:\n",
        "\n",
        "  # Read corpus into a single string then split into raw Section\n",
        "\n",
        "  corpus_sects_ls, sent_sectno_ls, sent_no_sectno_ls = corpus2sects(CORPUS_FILENAME)\n",
        "  print(f'Found #{len(sent_sectno_ls)} Section\\n')\n",
        "\n",
        "  print('\\nThe first 5 Section of the Corpus (first 10 chars):')\n",
        "  print('-----------------------------------\\n')\n",
        "  sent_sectno_ls[:5][:10]\n",
        "  print('\\n')\n",
        "  print('\\n\\nThe last 5 Section of the Corpus (first 10 chars):')\n",
        "  print('-----------------------------------\\n')\n",
        "  sent_sectno_ls[-5:][:10]\n",
        "  print('\\n')\n",
        "\n",
        "  n_shortest = 10\n",
        "  print(f'The {n_shortest} shortest Section in the Corpus are:')\n",
        "  print('--------------------------------------------')\n",
        "  temp_sects_ls = sorted(sent_sectno_ls, key=lambda x: (len(x), x))\n",
        "  for i, asent in enumerate(temp_sects_ls[:n_shortest]):\n",
        "    print(f'Shortest Section #{i}: {asent}')\n",
        "\n",
        "\n",
        "  # Calculate Section Informationif SECTION_HEADINGS != 'None':\n",
        "  # encoding = 'windows-1252', 'utf-8', 'cp1252', 'iso-8859-1'\n",
        "  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n",
        "  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n",
        "  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n",
        "\n",
        "  \"\"\"\n",
        "  with open(corpus_filename, \"r\", encoding='utf-8') as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  len(corpus_raw_str)\n",
        "\n",
        "  # Extract and process Sections from Corpus\n",
        "  corpus_sects_ls, corpus_str_raw = corpus2sects(corpus_filename)\n",
        "\n",
        "  print('\\n\\nAFTER ----------')\n",
        "  print(f'len(corpus_raw_str): {len(corpus_raw_str)}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'len(corpus_sects_ls): {len(corpus_sects_ls)}\\n\\n')\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[0]:\\n\\n    {corpus_sects_ls[0]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[1]:\\n\\n    {corpus_sects_ls[1]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[2]:\\n\\n    {corpus_sects_ls[2]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[-2]:\\n\\n    {corpus_sects_ls[-2]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[-1]:\\n\\n    {corpus_sects_ls[-1]}')\n",
        "  \"\"\";\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1MJfVfpKOU-"
      },
      "source": [
        "# Create a list of the Sentence Nos associated with each Section in the Corpus\n",
        "\n",
        "# sent_chap_no_ls = [sentnochapno_tp[1] for sentnochapno_tp in corpus_sents2chap_ls]\n",
        "\n",
        "corpus_sent_ct = corpus_sents_df.shape[0]\n",
        "sect_sent_ct = len(sent_sectno_ls)\n",
        "if (corpus_sent_ct == sect_sent_ct):\n",
        "  print(f'GOOD, all {corpus_sent_ct} Sentences were matched in one of the {corpus_sects_df.shape[0]} Sections\\n')\n",
        "else:\n",
        "  print(f'WARNING: only {sect_sent_ct} Sentences were matched in one of the {corpus_sects_df.shape[0]} Sections')\n",
        "  print(f'         {corpus_sent_ct - sect_sent_ct} Sentences were not matched\\n')\n",
        "\n",
        "\n",
        "# print(f'There are {corpus_sents_df.shape[0]} Sentences in the Corpus')\n",
        "# print(f'{len(sent_chap_no_ls)} Sentences have been associated with {corpus_chaps_df.shape[0]} Sections\\n')\n",
        "\n",
        "print(f'First 10 Sentences belong to these Sections:\\n    {sent_sectno_ls[:10]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqKwpPs9JC0R"
      },
      "source": [
        "# Verify Sections found and Sentence-Section matches\n",
        "\n",
        "print(f'{len(corpus_sects_ls)} Sections found in Corpus')\n",
        "print(f'{len(sent_sectno_ls)} Sentences found in a Section')\n",
        "\n",
        "sentences_nosection_ct = len(sent_no_sectno_ls)\n",
        "\n",
        "if (sentences_nosection_ct > 0):\n",
        "  print(f'\\n    WARNING: The following {sentences_nosection_ct} Sentences were NOT FOUND in any Section')\n",
        "  print(f'             If these are important/numerous, go back and edit/correct source Corpus text file and rerun this notebook\\n')\n",
        "\n",
        "  for i, asent in enumerate(sentences_nosection_ls):\n",
        "    print(f'    #{i}: {asent}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JFQTmEgK66I"
      },
      "source": [
        "# Create DataFrame from list of Sections extracted from Corpus\n",
        "\n",
        "sect_no_ls = list(range(len(corpus_sects_ls)))\n",
        "corpus_sects_df = pd.DataFrame({'sect_no':sect_no_ls, 'sect_raw':corpus_sects_ls})\n",
        "corpus_sects_df['sect_raw'] = corpus_sects_df['sect_raw'].astype('string')\n",
        "corpus_sects_df.head(1)\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjN6omWrifWm"
      },
      "source": [
        "# Create a list of the Sentence Nos associated with each Section in the Corpus\n",
        "\n",
        "# sentno_sectno_ls = [sentno2sectno_tp[0] for sentno2sectno_tp in sentences_section_ls]\n",
        "# sentstr_sectno_ls = [sentno2sectno_tp[1] for sentno2sectno_tp in sentences_section_ls]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KM-JEKECfSa"
      },
      "source": [
        "# corpus_sents_df.drop(columns=['sect_no'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpAlcbpQru0t"
      },
      "source": [
        "# Add Section No for each Sentence in Master DataFrame corpus_all_df\n",
        "# ONLY RUN THIS CODE CELL ONCE\n",
        "\n",
        "# Test if already exists, if not execute\n",
        "corpus_sents_df.insert(3, 'sect_no', sent_sectno_ls)  # This can only be run once\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "353_Nn7pM1FX"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.tail(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpE0UiORYZy6"
      },
      "source": [
        "# Verfiy correct Section Nos using the Sentence No boundaries found in previous code cell\n",
        "#   Search for iloc index ranges containing 1 or more Section boundaries to check correctness\n",
        "\n",
        "corpus_sents_df.iloc[300:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVNIhlQpNZiV"
      },
      "source": [
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yG53MVwkNHB"
      },
      "source": [
        "# Calculate the start, mid and end Sentence No for each Section\n",
        "\n",
        "sect_sent_no_start_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].min())\n",
        "sect_sent_no_end_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].max())\n",
        "\n",
        "def my_mid(anum, bnum):\n",
        "  mid_no = (anum - bnum)//2 + bnum\n",
        "\n",
        "  return mid_no\n",
        "\n",
        "print('\\nSection start sentence no: -----')\n",
        "print(sect_sent_no_start_ls)\n",
        "\n",
        "sect_sent_no_mid_ls = list(map(my_mid, sect_sent_no_end_ls, sect_sent_no_start_ls))\n",
        "print('\\nSection mid-sentence no: -----')\n",
        "print(sect_sent_no_mid_ls)\n",
        "\n",
        "print('\\nSection end sentence no: -----')\n",
        "print(sect_sent_no_end_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUofWkm0kNHC"
      },
      "source": [
        "# If necessary, delete prior columns to update DataFrame with new data\n",
        "\n",
        "# corpus_chaps_df.drop(columns=['parag_no_start'], inplace=True)\n",
        "# corpus_chaps_df.drop(columns=['parag_no_mid'], inplace=True)\n",
        "# corpus_chaps_df.drop(columns=['parag_no_end'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HxCqi2XkNHC"
      },
      "source": [
        "# Verify DataFrame before update\n",
        "\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhfgZuiPkNHD"
      },
      "source": [
        "# Insert 3 new columns on start,mid and end Sentence No for each Section\n",
        "\n",
        "corpus_sects_df.insert(1, 'sent_no_start', sect_sent_no_start_ls)\n",
        "corpus_sects_df.insert(2, 'sent_no_mid', sect_sent_no_mid_ls)\n",
        "corpus_sects_df.insert(3, 'sent_no_end', sect_sent_no_end_ls)\n",
        "\n",
        "corpus_sects_df.loc[:, corpus_sects_df.columns != 'sect_raw']\n",
        "# corpus_sects_df.loc[:, ['sect_no', 'sect_no_start', 'sect_no_mid', 'sect_no_end']]\n",
        "corpus_sects_df.info()\n",
        "\n",
        "                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iObHkdhmkNHF"
      },
      "source": [
        "# Create a clean text version of each Paragraph\n",
        "\n",
        "corpus_sects_df['sect_clean'] = corpus_sects_df['sect_raw'].apply(lambda x: clean_text(x))\n",
        "corpus_sects_df['sect_clean'] = corpus_sects_df['sect_clean'].astype('string')\n",
        "\n",
        "# corpus_sects_df.loc[:, corpus_sects_df.columns != 'sect_raw']\n",
        "corpus_sects_df.filter(like='_no')\n",
        "# corpus_sects_df.loc[:, ['sect_no', 'sect_no_start', 'sect_no_mid', 'sect_no_end']]\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFDwD6M-kNHH"
      },
      "source": [
        "# Compute Paragraph text Statistics\n",
        "\n",
        "corpus_sects_df['token_len'] = corpus_sects_df['sect_clean'].apply(lambda x: len(x.split()))\n",
        "corpus_sects_df['char_len'] = corpus_sects_df['sect_raw'].apply(lambda x: len(x))\n",
        "corpus_sects_df.loc[:, list(set(corpus_sects_df.columns) - set(['sect_raw', 'sect_clean']))]\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SdZ4e3ALYdD"
      },
      "source": [
        "### **Add Descriptive Statistics and Clean Raw Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWu8_C6PQ4iE"
      },
      "source": [
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7au9Zy2kR0NK"
      },
      "source": [
        "# TODO: Verfiy and eal with any NaN entries\n",
        "#   all Sentences with NaN or '' Raw Text\n",
        "\"\"\"\n",
        "\n",
        "# Sentences\n",
        "# Let's take a look at the updated text\n",
        "corpus_sents_df['sent_clean'] = corpus_sents_df['sent_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Textcorpus_sents_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_sents_df.dropna(how='any', axis=0, subset=['sent_raw'], inplace=True)\n",
        "corpus_sents_df.dropna(how='any', axis=0, subset=['sent_clean'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Sentences:')\n",
        "print('--------------------------------------')\n",
        "corpus_sents_df.head(2)\n",
        "\n",
        "\n",
        "# Paragraphs\n",
        "# Let's take a look at the updated text\n",
        "corpus_parags_df['parag_clean'] = corpus_parags_df['parag_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Text\n",
        "corpus_parags_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_parags_df.dropna(how='any', axis=0, subset=['parag_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Paragraphs:')\n",
        "print('--------------------------------------')\n",
        "corpus_parags_df.head(2)\n",
        "\n",
        "\n",
        "# Sections\n",
        "# Let's take a look at the updated text\n",
        "corpus_sects_df['sect_clean'] = corpus_sects_df['sect_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Text\n",
        "corpus_sects_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_sects_df.dropna(how='any', axis=0, subset=['sect_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Sections:')\n",
        "print('--------------------------------------')\n",
        "# corpus_sects_df.head(2)\n",
        "\n",
        "\n",
        "# Chapters\n",
        "# Let's take a look at the updated text\n",
        "corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Chapters with NaN or '' Raw Text\n",
        "corpus_chaps_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_chaps_df.dropna(how='any', axis=0, subset=['chap_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Chapters:')\n",
        "print('--------------------------------------')\n",
        "# corpus_sects_df.head(2)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf99apfwKPAO"
      },
      "source": [
        "# Verify shapes of all 4 Baseline 4 Models\n",
        "\n",
        "print(f'corpus_sents_df.shape: {corpus_sents_df.shape}')\n",
        "print(f'corpus_parags_df.shape: {corpus_parags_df.shape}')\n",
        "print(f'corpus_sects_df.shape: {corpus_sects_df.shape}')\n",
        "print(f'corpus_chaps_df.shape: {corpus_chaps_df.shape}')\n",
        "\n",
        "\"\"\"\n",
        "SButler Odyssey\n",
        "\n",
        "corpus_sents_df.shape: (2445, 8)\n",
        "corpus_parags_df.shape: (1051, 8)\n",
        "corpus_sects_df.shape: (24, 8)\n",
        "corpus_chaps_df.shape: (24, 8)\n",
        "\"\"\";\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHscLkclSYqN"
      },
      "source": [
        "##**Save Preprocess Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEzo8eltSvWS"
      },
      "source": [
        "# Save Corpus DataFrames\n",
        "\n",
        "save_dataframes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8R_ANtfYG6"
      },
      "source": [
        "# (Optional) EDA Raw Text Features: Interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1njcRD-jgGJh"
      },
      "source": [
        "**(Optional) Can Skip Ahead to: 'EDA of Raw Text and Extracted Features'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ti9jQK7grxO"
      },
      "source": [
        "# Review Cleaned Up Sentences\n",
        "\n",
        "corpus_sents_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TN5ksy55kXy"
      },
      "source": [
        "# Summary Statistics\n",
        "\n",
        "corpus_sents_df.describe()\n",
        "corpus_sents_df['token_len'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxvxkbUGzkfy"
      },
      "source": [
        "# Create histogram of Paragraph lengths\n",
        "\n",
        "sns.histplot(data=corpus_sents_df['char_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Lengths');\n",
        "\n",
        "if (PLOT_OUTPUT == 'All'):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'hist_paraglen.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqUtGz8UjY2b"
      },
      "source": [
        "# Plot histogram of Sentence lengths\n",
        "\n",
        "sns.histplot(data=corpus_sents_df['token_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Lengths')\n",
        "\n",
        "if (PLOT_OUTPUT == 'All'):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'hist_sentlen.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OoBrucYR9Xc"
      },
      "source": [
        "# SELECT CORPUS TYPE\n",
        "# TODO: Customized Preprocessing (e.g. Tweets) by Corpus Type\n",
        "\n",
        "# Novel, Tweets, Chat Transcript\n",
        "\n",
        "# Processing Options\n",
        "\n",
        "# Apply first level cleaning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2tIua7tTSRz"
      },
      "source": [
        "# (Optional) Manually Create Sentiment Arc Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU11XOZIPalR"
      },
      "source": [
        "***Can skip to Section [Load Sentiment Polarities...] or [Calculate VADER...]***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cke3OowdWk6I"
      },
      "source": [
        "**Interactively Enter Cruxes and Edge Cases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv7Foe3oTMmz"
      },
      "source": [
        "# Setup data structures for endpoints of Sentiment Time Series\n",
        "\n",
        "#   [-1.0 to +1.0] = [v.neg, neg, neutral, pos, v.pos]\n",
        "\n",
        "corpus_man_crux_ols = []  # working datastructure to dynamically build ordered list of manually selected Crux Points\n",
        "corpus_man_cruxes_odt = OrderedDict() # Once all manual Crux points selected, this will be working data structure\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0] - 1\n",
        "\n",
        "corpus_parags_len = corpus_sents_df.parag_no.max() # make sure no omissions/repeats/skips\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-_ylgiAc6Aw"
      },
      "source": [
        "# <INPUT> Set the Begining and Ending Sentiment Values (Manual Versions)\n",
        "\n",
        "# Start of Corpus Sentiment Analysis Time Series\n",
        "Corpus_Starting_Sentiment = -0.1 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "# corpus_sa_begin = Corpus_Starting_Sentiment\n",
        "\n",
        "# End of Corpus Sentiment Analysis Time Series\n",
        "Corpus_Ending_Sentiment = -1 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "# corpus_sa_end = Corpus_Ending_Sentiment\n",
        "\n",
        "corpus_man_crux_ols = [tuple((0, Corpus_Starting_Sentiment)), tuple((corpus_sents_len, Corpus_Ending_Sentiment))]\n",
        "# corpus_man_cruxes_dt[0.] = corpus_sa_begin\n",
        "# corpus_man_cruxes_dt[float(corpus_sents_len)] = corpus_sa_end\n",
        "\n",
        "print(f'Manual Cruxes with Start/End: {corpus_man_crux_ols}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmcLrzyUw93d"
      },
      "source": [
        "**Seach for Key Words that suggest Min/Max Sentiment Crux**\n",
        "* Specific to the Novel: Introduction of Pivotal Character, Scene, Factual Reveal, McGuffin, etc...\n",
        "* General to Events/Themes: Death, Birth, Fight, Accident, Money, Sex, etc... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UESgwwIAY28T"
      },
      "source": [
        "# <INPUT> Search Corpus for Line No of Peaks/Valleys\n",
        "# TODO: Better Vis\n",
        "Search_String = \"Death\" #@param {type:\"string\"}\n",
        "if (Search_String == \"\"):\n",
        "  search_str = \"accident\"\n",
        "else:\n",
        "  search_str = Search_String.lower()\n",
        "\n",
        "# search the list of cleaned paragraphs\n",
        "# results_ls = [x for x in search_match_ls if re.search(subs, x)]\n",
        "\n",
        "# creating and passsing series to new column\n",
        "match_sents_ser = corpus_sents_df[\"sent_clean\"].str.find(search_str)\n",
        "\n",
        "# print(f'Found #{len(match_index>0)} Matches')\n",
        "match_sents_df = corpus_sents_df.loc[match_sents_ser > 0]\n",
        "print(f'Found #{match_sents_df.shape[0]} Matching Sentences')\n",
        "print('------------------------------------')\n",
        "# print(f'  {match_sents_df}')\n",
        "match_sents_df[['sent_no', 'parag_no', 'sent_raw', 'token_len']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oVYBfz6-EVo"
      },
      "source": [
        "**Get Context for Matched Sentence by Retrieving Surrounding Paragraph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AvYQqT5TsdV"
      },
      "source": [
        "# Extract Surrounding Paragraphs for context on matching Sentences\n",
        "\n",
        "def get_parag4sentno(asent_no):\n",
        "  '''\n",
        "  Return the original raw paragraph containing a \n",
        "  given sentence number.\n",
        "  '''\n",
        "  # parag_df = pd.DataFrame()\n",
        "  # print(f'Passed in sent_no: {asent_no}')\n",
        "  aparag_no = int(corpus_sents_df.loc[corpus_sents_df['sent_no'] == asent_no]['parag_no'])\n",
        "  # print(f'  This sent_no {asent_no} is in parag_no: {aparag_no}')\n",
        "  aparag_str = corpus_sents_df.loc[corpus_sents_df['parag_no'] == aparag_no]['sent_raw'].str.cat() # ['sent_clean']\n",
        "  # sentno_parag_df = corpus_sents_df[corpus_sents_df['sent_no']==asent_no]\n",
        "  # print(f'Sent #{asent_no} is in the paragraph: ')\n",
        "  # print(aparag)\n",
        "  # print(f'returning aparag_no: [{aparag_no}]: {aparag}')\n",
        "  return aparag_no, aparag_str\n",
        "\n",
        "'''\n",
        "# Testing\n",
        "asent_no = 7\n",
        "print(f'Searching for paragraph containing Sentence #{asent_no}')\n",
        "\n",
        "aparag_no, aparag_str = get_parag4sentno(asent_no)\n",
        "print(f'\\n  Found in Paragraph #{aparag_no} \\n\\n{aparag_str}')\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnHz08NyDroK"
      },
      "source": [
        "# Extract Surrounding Paragraphs for context on matching Sentences\n",
        "\n",
        "def get_parag_str(aparag_no):\n",
        "  '''\n",
        "  Return the original raw paragraph containing a \n",
        "  given sentence number.\n",
        "  '''\n",
        "  # parag_df = pd.DataFrame()\n",
        "  # print(f'Passed in sent_no: {asent_no}')\n",
        "  # aparag_no = int(corpus_sents_df.loc[corpus_sents_df['sent_no'] == asent_no]['parag_no'])\n",
        "  # print(f'  This sent_no {asent_no} is in parag_no: {aparag_no}')\n",
        "  aparag_str = corpus_sents_df.loc[corpus_sents_df['parag_no'] == aparag_no]['sent_raw'].str.cat() # ['sent_clean']\n",
        "  # sentno_parag_df = corpus_sents_df[corpus_sents_df['sent_no']==asent_no]\n",
        "  # print(f'Sent #{asent_no} is in the paragraph: ')\n",
        "  # print(aparag)\n",
        "  # print(f'returning aparag_no: [{aparag_no}]: {aparag}')\n",
        "  return aparag_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByKLYZDsK123"
      },
      "source": [
        "# Summarize current status of manually selected Crux Points\n",
        "# TODO:\n",
        "\n",
        "def crux_sum_short():\n",
        "  print(f'\\nOrdered list of all manually selected Crux Points')\n",
        "  print('---------------------------------------')\n",
        "  for i, acrux_tp in enumerate(corpus_man_crux_ols):\n",
        "    asent_no, asent_pol = acrux_tp\n",
        "    asent_str = corpus_sents_df[corpus_sents_df.sent_no==asent_no].sent_raw.str.cat()\n",
        "    # print(f'Type: {type(asent_str)}')\n",
        "    print(f'Sent No {asent_no:4d}: Polarity: {asent_pol}\\nText: {asent_str}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz0FLzt2UeSe"
      },
      "source": [
        "# Summarize current manually selected Crux Points\n",
        "\n",
        "def crux_summary():\n",
        "  print(f'\\nOrdered list of all manually selected Crux Points')\n",
        "  print('---------------------------------------\\n\\n')\n",
        "  for i, acrux_tp in enumerate(corpus_man_crux_ols):\n",
        "    asent_no, asent_pol = acrux_tp\n",
        "    asent_str = corpus_sents_df[corpus_sents_df.sent_no==asent_no].sent_raw.str.cat()\n",
        "    # print(f'Type: {type(asent_str)}')\n",
        "    print(f'Sent No {asent_no:4d}: Polarity: {asent_pol}')\n",
        "    print('------------------------------')\n",
        "    print(f'Text: {asent_str}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiUmseNw3CkN"
      },
      "source": [
        "# View the Paragraph containing your Matching Sentence:\n",
        "\n",
        "def get_nparags_context(crux_sent_no, parag_ct):\n",
        "\n",
        "  parag_win = int(parag_ct)\n",
        "  parag_crux_str = ''\n",
        "\n",
        "  parag_crux_no = 0\n",
        "\n",
        "  if (crux_sent_no < 0) | (crux_sent_no > corpus_sents_len):\n",
        "    print(f'ERROR: Pick a Sentence No between 0-{corpus_sents_len-1}')\n",
        "  else:\n",
        "    # get_sent_no = crux_sent_no\n",
        "    # print(f'Retrieving Sentence No: {get_sent_no}')\n",
        "    # print('----------')\n",
        "\n",
        "    parag_crux_no, aparag_str = get_parag4sentno(crux_sent_no)\n",
        "    if parag_win == 1:\n",
        "      print(f'Match #{i}: Sentence No. {asent_no} found in Paragraph No. {parag_crux_no}')\n",
        "      print('----------------------------')\n",
        "      print(f'Sentence:\\n')\n",
        "      # print(f'     {corpus_sents_df[corpus_sents_df.sent_no == crux_sent_no]}\\n\\n')\n",
        "      corpus_sents_df[corpus_sents_df.sent_no == crux_sent_no]\n",
        "      print('----------------------------')\n",
        "      print(f'Paragraph Context:\\n')\n",
        "      print(f'     {aparag_str}\\n\\n')\n",
        "    else:\n",
        "      parag_half_win = int((parag_win-1)/2)\n",
        "      parag_start = parag_crux_no - parag_half_win\n",
        "      parag_end = parag_crux_no + parag_half_win\n",
        "      print(f'Retrieving {parag_ct} Contextual Paragraphs Nos {parag_start} to {parag_end}')\n",
        "      print(f'  for Crux Point centered on Sentence No {crux_sent_no}')\n",
        "      for i in range(parag_start, parag_end + 1, 1):\n",
        "        if i == parag_crux_no:\n",
        "          print(f'\\n   ---------------------------------------------------------')\n",
        "          print(f'** Crux Point Paragraph #{i} with Sentence No. {crux_sent_no} **')\n",
        "          print(f'   ---------------------------------------------------------')\n",
        "          parag_crux_str = get_parag_str(i)\n",
        "          print(parag_crux_str)\n",
        "        else:\n",
        "          print(f'\\n   ----------------------')\n",
        "          print(f'   Regular Paragraph #{i}')\n",
        "          print(f'   ----------------------')\n",
        "          print(get_parag_str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjoXC8Fg3lBP"
      },
      "source": [
        "# Insert new crux point into ordered list: corpus_man_crux_ls\n",
        "\n",
        "# NOTE: For very long lists, use Python simple bisect library (at cost of additional dependency)\n",
        "\n",
        "\n",
        "def insert_ord_tp_list(crux_ord_ols, crux_tp):\n",
        "  '''\n",
        "  Insert new crux tuple: crux_tp = (sent_no, sentiment_polarity)\n",
        "  into ordered list of tuples while maintaining sent_no order\n",
        "  '''\n",
        "  sent_no, senti_pol = crux_tp\n",
        "\n",
        "  # Searching for the position\n",
        "  for i in range(len(crux_ord_ols)):\n",
        "    if crux_ord_ols[i][0] == sent_no:\n",
        "      # Attempting to insert duplicate\n",
        "      return crux_ord_ols\n",
        "    elif crux_ord_ols[i][0] < sent_no:\n",
        "      insert_idx = i\n",
        "    else:\n",
        "      break\n",
        "      \n",
        "  # Inserting n in the list\n",
        "  list = crux_ord_ols[:i] + [crux_tp] + crux_ord_ols[i:]\n",
        "  return list\n",
        "\n",
        "'''\n",
        "# Test\n",
        "crux_test_ls = [(1,0), (5,1), (10,-1)]\n",
        "crux_test_tp = (3,10)\n",
        "  \n",
        "print(insert_ord_tp_list(crux_test_ls, crux_test_tp))\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PPOLJDZ1wHc"
      },
      "source": [
        "**Start of Human in the Loop Manual Crux Point Identification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1MLexX57Guc"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "print('Enter a Sentence number based upon your search above to see the ')\n",
        "print('  surrounding Paragraph context.')\n",
        "print('----------------------------------------')\n",
        "print(f'(Enter an integer between 0 and {corpus_sents_len-1})\\n\\n')\n",
        "\n",
        "print('\\n')\n",
        "print('Enter an ODD NUMBER for the Number of surrounding Paragraphs ')\n",
        "print('  around the Sentence No to give Context.')\n",
        "print('----------------------------------------')\n",
        "print(f'(Enter an integer: 3, 5, 7\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exxWtaUPC30l"
      },
      "source": [
        "# Input your Context Retrieval Parameters\n",
        "\n",
        "Sentence_No =  2692#@param {type:\"integer\"}\n",
        "No_Paragraphs_Context = \"3\" #@param [\"1\", \"3\", \"5\"]\n",
        "\n",
        "get_nparags_context(Sentence_No, No_Paragraphs_Context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30fBikIrrgMe"
      },
      "source": [
        "**Add Crux to Manually Generated Sentiment Arc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgljeT6ypNbo"
      },
      "source": [
        "# Instructions to add current sentence as a Crux Point\n",
        "\n",
        "crux_summary()\n",
        "\n",
        "print('--------------------------------------------------------')\n",
        "print(f'INSTRUCTIONS To current Sentence No: {Sentence_No} as a Crux Point')\n",
        "print('--------------------------------------------------------')\n",
        "\n",
        "print(\"\\nCheck this box if you want to add the Sentence/Paragraph above \")\n",
        "print(\"  as a new Min/Max Crux Point with your approximation \")\n",
        "print(\"  for a Sentiment Polarity value between -1.0 to +1.0\\n\\n\")\n",
        "\n",
        "print(f\"Crux Sentence No: {Sentence_No} in Paragraph No: {parag_crux_no}\\n\")\n",
        "print(parag_crux_str)\n",
        "\n",
        "print(\"\\n\\nLeave Add_Sentence_Crux 'unchecked' to not add\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia1IXChRmyr3"
      },
      "source": [
        "# <INPUT> Option to add this Sentence/Paragraph as a Min/Max Crux Point\n",
        "\n",
        "Sentiment_Polarity = -0.4 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "Add_Sentence_Crux = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "# Add Crux if selected and give current summary status\n",
        "\n",
        "if Add_Sentence_Crux == True:\n",
        "  crux_new_tp = tuple((Sentence_No, Sentiment_Polarity))\n",
        "  corpus_man_crux_ols = insert_ord_tp_list(corpus_man_crux_ols, crux_new_tp)\n",
        "  if (corpus_man_crux_ols):\n",
        "    print(f'Successfully inserted new Crux = {crux_new_tp}')\n",
        "    print(f'Added Crux at Sentence No={Sentence_No} with Polarity={Sentiment_Polarity}')\n",
        "    # corpus_man_cruxes_dt[Sentence_No] = Sentiment_Polarity\n",
        "  else:\n",
        "    print(f'ERROR: Could not insert new Crux = {crux_new_tp}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lps67T-lUTi2"
      },
      "source": [
        "# Summary of current Crux Points after addition\n",
        "\n",
        "print('\\n------------------------------------------------------------')\n",
        "print(f'After addition of new Crux Point (Sentence No {Sentence_No})')\n",
        "print('------------------------------------------------------------\\n')\n",
        "\n",
        "crux_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjZT9_0CrzFk"
      },
      "source": [
        "**Delete Manually Selected Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTX30nokNPBp"
      },
      "source": [
        "len(corpus_man_crux_ols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxS-J_hPOfkI"
      },
      "source": [
        "crux_tp = (1, 2)\n",
        "a, b = crux_tp\n",
        "print(f'a is {a} and b is {b}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi7Cu1iErvzc"
      },
      "source": [
        "# Insert new crux point into ordered list: corpus_man_crux_ls\n",
        "\n",
        "# FIX: 20210616 and move to utility functions\n",
        "\n",
        "# NOTE: For very long lists, use Python simple bisect library (at cost of additional dependency)\n",
        "\n",
        "\n",
        "def del_ord_tp_list(acorpus_man_crux_ols, crux_tp):\n",
        "  '''\n",
        "  Insert new crux tuple: crux_tp = (sent_no, sentiment_polarity)\n",
        "  into ordered list of tuples while maintaining sent_no order\n",
        "  '''\n",
        "  crux_ct = len(acorpus_man_crux_ols)\n",
        "  sent_no = crux_tp[0]\n",
        "  print(f'Deleting sent_no: {sent_no} over crux_ls len={len(acorpus_man_crux_ols)}')\n",
        "\n",
        "  # Searching for the positionk\n",
        "  del_idx = -1\n",
        "  for i in range(len(acorpus_man_crux_ols)):\n",
        "    acrux_sent_no = acorpus_man_crux_ols[i][0]\n",
        "    print(f'Crux #{i} is sent_no={acrux_sent_no}')\n",
        "    if acrux_sent_no == sent_no:\n",
        "      print(f'Matching index at {i}')\n",
        "      del_idx = i\n",
        "      \n",
        "  # Delete n in the list\n",
        "  print(f'Deletion index = {del_idx}')\n",
        "  if del_idx == 0:\n",
        "    # Delete the first Crux\n",
        "    list = acorpus_man_crux_ols[1:]\n",
        "    return list\n",
        "  elif del_idx == crux_ct -1:\n",
        "    # Delete the last Crux\n",
        "    list = acorpus_man_crux_ols[:-1]\n",
        "    return list    \n",
        "  elif (del_idx > 0) & (del_idx < crux_ct):\n",
        "    # Delete an interior Crux\n",
        "    before_idx = i - 1\n",
        "    after_idx = i\n",
        "    list = acorpus_man_crux_ols[:before_idx] + acorpus_man_crux_ols[after_idx:]\n",
        "    print(f'Returning list: {list}')\n",
        "    return list\n",
        "  else:\n",
        "    print('No matching Crux tuple found')\n",
        "    return acorpus_man_crux_ols\n",
        "  \n",
        "\n",
        "# Test\n",
        "crux_test_ls = [(1,0), (5,1), (10,-1)]\n",
        "crux_test_tp = (5,5)\n",
        "  \n",
        "print(del_ord_tp_list(crux_test_ls, crux_test_tp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI5hZqmSLg5R"
      },
      "source": [
        "# Instructions to Delete a Crux Point\n",
        "\n",
        "crux_summary()\n",
        "\n",
        "print('--------------------------------------------------------')\n",
        "print('INSTRUCTIONS To Delete a Crux Point')\n",
        "print('--------------------------------------------------------')\n",
        "\n",
        "print(\"\\nEnter the Sentence No of a Crux you want to delete.\\n\")\n",
        "print(f'     Current Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n",
        "print(\" Skip this if you want to keep all manually selected Crux Points.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzowFY-I8U1_"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "print(\"\\nEnter the Sentence No of a Crux you want to delete.\\n\")\n",
        "print(f'     Current Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n",
        "print(\" Skip this if you want to keep all manually selected Crux Points.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ShFCfOsrvsv"
      },
      "source": [
        "Delete_Sent_No =  777#@param {type:\"integer\"}\n",
        "# Select a Crux to Delete\n",
        "# TODO: Drop down list\n",
        "\n",
        "# corpus_man_crux_ols\n",
        "corpus_man_crux_temp_ols = []\n",
        "\n",
        "crux_sent_set = set([x[0] for x in corpus_man_crux_ols])\n",
        "if not(Delete_Sent_No in crux_sent_set):\n",
        "  print(f'ERROR: {Delete_Sent_No} is not a Crux Point Sentence No')\n",
        "else:\n",
        "  # Keep the same tuple format for uniformity and future features\n",
        "  crux_del_tp = tuple((Delete_Sent_No, 'dummy_sentence'))\n",
        "  print(f'Selected {(crux_del_tp)} to delete')\n",
        "  # corpus_man_crux_temp_ols = \n",
        "  print(f'WTF: {del_ord_tp_list(corpus_man_crux_ols, crux_del_tp)}')\n",
        "  corpus_man_crux_old = del_ord_tp_list(corpus_man_crux_ols, crux_del_tp)\n",
        "  print(f\"corpus_man_crux_ols: {corpus_man_crux_ols}\")\n",
        "  # get_sent_no = Sentence_No\n",
        "  # print(f'Retrieving Sentence No: {get_sent_no}')\n",
        "  # print('----------')\n",
        "  print(f'Updated Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJdNu-oYrwHv"
      },
      "source": [
        "**Review Summary of all Manually Selected Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmpggAWuvMb1"
      },
      "source": [
        "# Generate Report Summary of All Manually Selected Cruxes\n",
        "\n",
        "f = io.StringIO()\n",
        "with contextlib.redirect_stdout(f):\n",
        "    crux_summary()\n",
        "crux_summary = f.getvalue()\n",
        "\n",
        "# Print Manual Crux Report Summary to Screen\n",
        "\n",
        "# print(crux_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ULFfHbxwDfu"
      },
      "source": [
        "# Save Manual Crux Summary Report\n",
        "\n",
        "plot_filename = 'man_cruxes.txt'\n",
        "plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "\n",
        "with open(plotpathfilename_str, 'a+') as outfp:\n",
        "  outfp.write(crux_summary)\n",
        "\n",
        "# Verify \n",
        "\n",
        "!ls -alt $plotpathfilename_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG_hGPgjxY7X"
      },
      "source": [
        "# Verify Report Content\n",
        "\n",
        "!cat man_cruxes_fscottfitzgerald_thegreatgatsby_20210616214050.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zPYRKCex8-U"
      },
      "source": [
        "**Clean and Organize Manual Crux Points into new Data Structures**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YICJNt3AXI2d"
      },
      "source": [
        "print(corpus_sents_df[corpus_sents_df['sent_no']==5]['sent_raw'].squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2bLtpweWQt4"
      },
      "source": [
        "# Convert and assemble all the Crux values in lists to save in a new Crux DataFrame\n",
        "\n",
        "\n",
        "pol_val_ls = [x[1] for x in corpus_man_crux_ols]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "parag_no_ls = [(get_parag4sentno(x))[0] for x in sent_no_ls]\n",
        "parag_str_ls = [get_parag_str(x) for x in parag_no_ls]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "sent_raw_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_raw'].squeeze() for x in sent_no_ls]\n",
        "sent_raw_ls\n",
        "sent_clean_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_clean'].squeeze() for x in sent_no_ls]\n",
        "sent_clean_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFEYg7Gfbj9i"
      },
      "source": [
        "# Create a Dict of Crux Points to Tuples (Polarity, Raw Sentence)\n",
        "\n",
        "# First Create the Tuples for each Sentence No (Sentiment Polarity, Raw Text)\n",
        "def merge(list1, list2):\n",
        "    merged_list = tuple(zip(list1, list2)) \n",
        "    return merged_list\n",
        "      \n",
        "crux_tp_ls = merge(pol_val_ls, sent_raw_ls)\n",
        "\n",
        "# Second, Create the Dictionary C\n",
        "corpus_man_cruxes_dt = {sent_no_ls[i]: crux_tp_ls[i] for i in range(len(crux_tp_ls))}\n",
        "\n",
        "# Verify\n",
        "corpus_man_cruxes_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtIHecpMdbXa"
      },
      "source": [
        "corpus_sents_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8qmpFRaTfu2"
      },
      "source": [
        "**Plot Interpolated Manual Sentiment Arc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ASg7uUZWf3"
      },
      "source": [
        "corpus_man_sa_df = pd.DataFrame({'sent_no':xn, 'sentiment':yn, 'sent_raw':corpus_sents_df.sent_raw.values})\n",
        "corpus_man_sa_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36VfGi4a47Yl"
      },
      "source": [
        "# Hermite Interpolation with SciPy\n",
        "\n",
        "pol_val_ls = [x[1] for x in corpus_man_crux_ols]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "parag_no_ls = [(get_parag4sentno(x))[0] for x in sent_no_ls]\n",
        "parag_str_ls = [get_parag_str(x) for x in parag_no_ls]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "sent_raw_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_raw'].squeeze() for x in sent_no_ls]\n",
        "sent_raw_ls\n",
        "sent_clean_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_clean'].squeeze() for x in sent_no_ls]\n",
        "sent_clean_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElJKawWfZWbv"
      },
      "source": [
        "sent_no_ls\n",
        "pol_val_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xufA403s3lbF"
      },
      "source": [
        "corpus_man_crux_np = np.asarray(corpus_man_crux_ols)\n",
        "corpus_man_crux_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdgKbjpi63a9"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMkgU-JhwudG"
      },
      "source": [
        "x2 = np.array(sent_no_ls)\n",
        "y2 = np.array(pol_val_ls)\n",
        "\n",
        "xn = np.linspace(0, corpus_sents_len, corpus_sents_len)\n",
        "yn = interpolate.pchip_interpolate(x2, y2, xn)\n",
        "\n",
        "crux_man_df = pd.DataFrame(\n",
        "    {'sent_no': sent_no_ls,\n",
        "     'pol_val': pol_val_ls\n",
        "     }\n",
        ")\n",
        "\n",
        "# plt.plot(x2, y2, 'ok', label='True values')\n",
        "# plt.plot(xn, yn, label='Hermite Interpolation')\n",
        "\n",
        "# plt.plot(xn, yn4, label='Spline order 4')\n",
        "# plt.plot(xn, yn5, label='Spline order 5')\n",
        "# plt.plot(xn, yn6, label='Spline order 6')\n",
        "# plt.plot(xn, yn7, label='Spline order 7')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# sns.histplot(data=corpus_sents_df['char_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Lengths')\n",
        "sns.histplot(data=crux_man_df, x='sent_no', y='pol_val', kde=True).set_title(f'{CORPUS_FULL} \\n Manual Cruxes with Hermite Smoothing')\n",
        "\n",
        "\n",
        "# Save graph to file.\n",
        "plot_filename = 'man_crux_plot.png'\n",
        "plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "plt.savefig(plotpathfilename_str, format='png', dpi=300)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW-VkS2s_ogy"
      },
      "source": [
        "**Gaussian Process Regression**\n",
        "\n",
        "* https://blog.dominodatalab.com/fitting-gaussian-process-models-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLFFYDfYpPjn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0BukxX9ZKYA"
      },
      "source": [
        "# (Optional) Load Sentiment Polarities: Interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhPJ9e-V9NMu"
      },
      "source": [
        "***If you upload a file of Sentiment Values you don't have to Calculate them in the following sections***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpx7viqf9AAJ"
      },
      "source": [
        "!ls -altr *.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw8_mUHI9sdr"
      },
      "source": [
        "# Test\n",
        "\n",
        "files.download('sentiments_raw_all_virginiawoolf_tothelighthouse_20210618161224.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90axHClm9cGZ"
      },
      "source": [
        "# Upload your precomputed Sentiment Values\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02NH5nnto2AC"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL9JCyCI-Sic"
      },
      "source": [
        "# Verify the file was uploaded correctly\n",
        "\n",
        "newest_csvfile = get_recentfile().split('/')[-1]\n",
        "print(f'The most recently updated *.csv file is: {newest_csvfile}')\n",
        "\n",
        "!head -n 10 $newest_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTfBj8qdi_vZ"
      },
      "source": [
        "%whos DataFrame\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgjz-E-mYgzI"
      },
      "source": [
        "# Upload file into DataFrame\n",
        "\n",
        "corpus_test_df = pd.read_csv(newest_csvfile)\n",
        "corpus_test_df['sent_clean'] = corpus_test_df['sent_clean'].astype('string')\n",
        "corpus_test_df['sent_raw'] = corpus_test_df['sent_raw'].astype('string')\n",
        "corpus_test_df.head()\n",
        "corpus_test_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ajyb8Jpmzw"
      },
      "source": [
        "***Skip to Section <Calculate Median of All...> if SA Loaded***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGA6sQy6RPDC"
      },
      "source": [
        "corpus_lexicons_stats_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7CfH00OFkQV"
      },
      "source": [
        "# **Either (a) Load Precomputed Sentiment Series or (b) Calculate Sentiment Values**\n",
        "\n",
        "Sentiment Models\n",
        "\n",
        "* VADER [-1.0 to 1.0] zero peak\n",
        "* TextBlob [-1.0 to 1.0] zero peak\n",
        "* Stanza outliers [-1.0 to 199.0] pos, outliers(+peak)\n",
        "* AFINN [-14 (-8 to 8) 20] discrete\n",
        "* SentimentR 11,710 [-5.4 to 8.8] norm\n",
        "* Syuzhet [-5.4 to 8.8] norm\n",
        "* Bing [-100.0 (-20.0 to 20.0) 100] discrete, outliers\n",
        "* Pattern [-1.0 to 1.0] norm\n",
        "* SentiWord [-3.8 to 4.4] norm\n",
        "* SenticNet [-3.8 to 10] norm\n",
        "* NRC [-100.0 (-5.0 to 5.0) 100] zero, outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ueo8fTSqqaak"
      },
      "source": [
        "## **(a) Load Previously Computed Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEq8eWslgu28"
      },
      "source": [
        "!ls -altr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RH8TY-LgCVf"
      },
      "source": [
        "### **Baseline 12 Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVX1gXQajWNG"
      },
      "source": [
        "# Read previous Computed 12 Model Baseline sentiment data saved from previous run of this notebook\n",
        "\n",
        "corpus_sents_df = pd.read_csv('corpus_sents_baseline_jconrad_heartofdarkness.csv')\n",
        "corpus_parags_df = pd.read_csv('corpus_parags_baseline_jconrad_heartofdarkness.csv')\n",
        "corpus_sects_df = pd.read_csv('corpus_sects_baseline_jconrad_heartofdarkness.csv')\n",
        "corpus_chaps_df = pd.read_csv('corpus_chaps_baseline_jconrad_heartofdarkness.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZC4--5TojdI"
      },
      "source": [
        "print(f'Corpus Sentences shape: {corpus_sents_df.shape}')\n",
        "print(f'Corpus Paragraphs shape: {corpus_parags_df.shape}')\n",
        "print(f'Corpus Sections shape: {corpus_sects_df.shape}')\n",
        "print(f'Corpus Chapters shape: {corpus_chaps_df.shape}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfl-NIhZeNzT"
      },
      "source": [
        "# Clean Sentences DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_sents_df.columns:\n",
        "  corpus_sents_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  \n",
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WnLPxjefkhE"
      },
      "source": [
        "# Clean Paragraphs DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_parags_df.columns:\n",
        "  corpus_parags_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  \n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMNQI76jfkdu"
      },
      "source": [
        "# Clean Section DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_sects_df.columns:\n",
        "  corpus_sects_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  \n",
        "corpus_sects_df.head(2)\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFfc4HTPfy5c"
      },
      "source": [
        "# Clean Chapter DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_chaps_df.columns:\n",
        "  corpus_chaps_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  \n",
        "corpus_chaps_df.head(2)\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWDYFJFHxXum"
      },
      "source": [
        "# Derive Normalized/Standaradized Time Series\n",
        "\n",
        "# Once applied modification function\n",
        "\"\"\"\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=models_baseline_ls, col_mod='lnorm')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=models_baseline_ls, col_mod='std-minmax')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\"\"\"\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=models_baseline_ls, col_mod='std-stdscaler')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\"\"\"\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=models_baseline_ls, col_mod='std-medianiqr')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=models_baseline_ls, col_mod='roll10')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\n",
        "\n",
        "# Twice applied modification function (ends with roll)\n",
        "lnorm_ls = [f'{x}_lnorm' for x in models_baseline_ls]\n",
        "stdminmax_ls = [f'{x}_minmax' for x in models_baseline_ls]\n",
        "stdstdscaler_ls = [f'{x}_stdscaler' for x in models_baseline_ls]\n",
        "stdmedianiqr_ls = [f'{x}_medianiqr' for x in models_baseline_ls]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=lnorm_ls, col_mod='std-minmax')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=lnorm_ls, col_mod='std-stdscaler')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=lnorm_ls, col_mod='std-medianiqr')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=lnorm_ls, col_mod='roll10')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=stdminmax_ls, col_mod='roll10')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\"\"\"\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\"\"\"\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=stdmedianiqr_ls, col_mod='roll10')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\n",
        "\n",
        "# Thrice applied modification function (starts with lnorm and ends with roll)\n",
        "lnorm_stdminmax_ls = [f'{x}_lnorm_minmax' for x in models_senmodels_baseline_lstimentr_ls]\n",
        "lnorm_stdstdscaler_ls = [f'{x}_lnorm_stdscaler' for x in models_baseline_ls]\n",
        "lnorm_stdmedianiqr_ls = [f'{x}_lnorm_medianiqr' for x in models_baseline_ls]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=lnorm_stdminmax_ls, col_mod='roll10')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=lnorm_stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=lnorm_stdmedianiqr_ls, col_mod='roll10')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.info()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdYoS50pgJtz"
      },
      "source": [
        "### **SentimentR 7 Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT1kNw36DhgV"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFy2LZzBC_XC"
      },
      "source": [
        "!ls -altr *csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBk7sJltaE43"
      },
      "source": [
        "# SentimentR 7 Models: Read Computed sentiment data saved from previous run of this notebook\n",
        "\n",
        "corpus_sentimentr_df = pd.read_csv('sum_sentiments_sentimentR_7models_sentimenttimeraw_mproust_guermantes_en.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5RkpbLrgWNQ"
      },
      "source": [
        "# Clean DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_sentimentr_df.columns:\n",
        "  # corpus_sentimentr_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  corpus_sentimentr_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n",
        "\n",
        "corpus_sentimentr_df.head(2)\n",
        "corpus_sentimentr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXotYX02g-Zf"
      },
      "source": [
        "# Derive Normalized/Standaradized Time Series\n",
        "\n",
        "# Once applied modification function\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=models_sentimentr_ls, col_mod='lnorm')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=models_sentimentr_ls, col_mod='std-minmax')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=models_sentimentr_ls, col_mod='std-stdscaler')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=models_sentimentr_ls, col_mod='std-medianiqr')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=models_sentimentr_ls, col_mod='roll10')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "# Twice applied modification function (ends with roll)\n",
        "lnorm_ls = [f'{x}_lnorm' for x in models_sentimentr_ls]\n",
        "stdminmax_ls = [f'{x}_minmax' for x in models_sentimentr_ls]\n",
        "stdstdscaler_ls = [f'{x}_stdscaler' for x in models_sentimentr_ls]\n",
        "stdmedianiqr_ls = [f'{x}_medianiqr' for x in models_sentimentr_ls]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=lnorm_ls, col_mod='std-minmax')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=lnorm_ls, col_mod='std-stdscaler')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=lnorm_ls, col_mod='std-medianiqr')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=lnorm_ls, col_mod='roll10')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=stdminmax_ls, col_mod='roll10')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=stdmedianiqr_ls, col_mod='roll10')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "# Thrice applied modification function (starts with lnorm and ends with roll)\n",
        "lnorm_stdminmax_ls = [f'{x}_lnorm_minmax' for x in models_sentimentr_ls]\n",
        "lnorm_stdstdscaler_ls = [f'{x}_lnorm_stdscaler' for x in models_sentimentr_ls]\n",
        "lnorm_stdmedianiqr_ls = [f'{x}_lnorm_medianiqr' for x in models_sentimentr_ls]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=lnorm_stdminmax_ls, col_mod='roll10')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=lnorm_stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=lnorm_stdmedianiqr_ls, col_mod='roll10')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "\n",
        "corpus_sentimentr_df.head(2)\n",
        "corpus_sentimentr_df.info()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDIq7gP7Ow94"
      },
      "source": [
        "corpus_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVqjH8G_O9An"
      },
      "source": [
        "# Derive Rolling Means Time Series\n",
        "\n",
        "temp_ls = [f'{x}_roll10' for x in models_sentimentr_ls]\n",
        "# corpus_sentimentr_df[models_sentimentr_ls].plot()\n",
        "corpus_sentimentr_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} Baseline 12 Models\\nSMA 10%')\n",
        "\n",
        "temp_ls = [f'{x}_minmax_roll10' for x in models_sentimentr_ls]\n",
        "# corpus_sentimentr_df[models_sentimentr_ls].plot()\n",
        "corpus_sentimentr_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} Baseline 12 Models\\nMinMax SMA 10%')\n",
        "\n",
        "temp_ls = [f'{x}_stdscaler_roll10' for x in models_sentimentr_ls]\n",
        "# corpus_sentimentr_df[models_sentimentr_ls].plot()\n",
        "corpus_sentimentr_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} Baseline 12 Models\\nStdScaler SMA 10%')\n",
        "\n",
        "temp_ls = [f'{x}_lnorm_stdscaler_roll10' for x in models_sentimentr_ls]\n",
        "# corpus_sentimentr_df[models_sentimentr_ls].plot()\n",
        "corpus_sentimentr_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} Baseline 12 Models\\nlnorm StdScaler SMA 10%')\n",
        "\n",
        "temp_ls = [f'{x}_medianiqr_roll10' for x in models_sentimentr_ls]\n",
        "# corpus_sentimentr_df[models_sentimentr_ls].plot()\n",
        "corpus_sentimentr_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} Baseline 12 Models\\nMedianIQR SMA 10%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krB0DGVMgNbW"
      },
      "source": [
        "### **SyuzhetR 4 Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI_U-nwPG_ur"
      },
      "source": [
        "!ls -altr *.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9GOO2EseGTV"
      },
      "source": [
        "# SyuzhetR 4 Models: Read Computed sentiment data saved from previous run of this notebook\n",
        "\n",
        "# corpus_syuzhetr_df = pd.read_csv('sum_sentiments_syuzhetR_4models_vwoolf_tothelighthouse.csv')\n",
        "\n",
        "corpus_syuzhetr_df = pd.read_csv('sum_sentiments_syuzhetR_4models_sentimenttimeraw_mproust_guermantes_en.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EAxGBWwgdPz"
      },
      "source": [
        "# Clean Chapter DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_syuzhetr_df.columns:\n",
        "  corpus_syuzhetr_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n",
        "  \n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bPKx_55M_iM"
      },
      "source": [
        "# Derive Normalized/Standardized Time Series\n",
        "\n",
        "# Once applied modification function\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=models_syuzhetr_ls, col_mod='lnorm')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=models_syuzhetr_ls, col_mod='std-minmax')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=models_syuzhetr_ls, col_mod='std-stdscaler')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=models_syuzhetr_ls, col_mod='std-medianiqr')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=models_syuzhetr_ls, col_mod='roll10')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "# Twice applied modification function (ends with roll)\n",
        "lnorm_ls = [f'{x}_lnorm' for x in models_syuzhetr_ls]\n",
        "stdminmax_ls = [f'{x}_minmax' for x in models_syuzhetr_ls]\n",
        "stdstdscaler_ls = [f'{x}_stdscaler' for x in models_syuzhetr_ls]\n",
        "stdmedianiqr_ls = [f'{x}_medianiqr' for x in models_syuzhetr_ls]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=lnorm_ls, col_mod='std-minmax')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=lnorm_ls, col_mod='std-stdscaler')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=lnorm_ls, col_mod='std-medianiqr')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=lnorm_ls, col_mod='roll10')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=stdminmax_ls, col_mod='roll10')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=stdmedianiqr_ls, col_mod='roll10')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "# Thrice applied modification function (starts with lnorm and ends with roll)\n",
        "lnorm_stdminmax_ls = [f'{x}_lnorm_minmax' for x in models_syuzhetr_ls]\n",
        "lnorm_stdstdscaler_ls = [f'{x}_lnorm_stdscaler' for x in models_syuzhetr_ls]\n",
        "lnorm_stdmedianiqr_ls = [f'{x}_lnorm_medianiqr' for x in models_syuzhetr_ls]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=lnorm_stdminmax_ls, col_mod='roll10')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=lnorm_stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=lnorm_stdmedianiqr_ls, col_mod='roll10')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVcqz_baLcMe"
      },
      "source": [
        "corpus_syuzhetr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpP3mwFLaFQi"
      },
      "source": [
        "# Derive Rolling Means Time Series\n",
        "\n",
        "temp_ls = [f'{x}_roll10' for x in models_syuzhetr_ls]\n",
        "# corpus_syuzhetr_df[models_syuzhetr_ls].plot()\n",
        "corpus_syuzhetr_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} SyuzhetR 4 Models\\nSMA 10%')\n",
        "\n",
        "temp_ls = [f'{x}_minmax_roll10' for x in models_syuzhetr_ls]\n",
        "# corpus_syuzhetr_df[models_syuzhetr_ls].plot()\n",
        "corpus_syuzhetr_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} SyuzhetR 4 Models\\nMinMax SMA 10%')\n",
        "\n",
        "temp_ls = [f'{x}_stdscaler_roll10' for x in models_syuzhetr_ls]\n",
        "# corpus_syuzhetr_df[models_syuzhetr_ls].plot()\n",
        "corpus_syuzhetr_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} SyuzhetR 4 Models\\nStdScaler SMA 10%')\n",
        "\n",
        "temp_ls = [f'{x}_lnorm_stdscaler_roll10' for x in models_syuzhetr_ls]\n",
        "# corpus_syuzhetr_df[models_syuzhetr_ls].plot()\n",
        "corpus_syuzhetr_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} SyuzhetR 4 Models\\nlnorm StdScaler SMA 10%')\n",
        "\n",
        "temp_ls = [f'{x}_medianiqr_roll10' for x in models_syuzhetr_ls]\n",
        "# corpus_syuzhetr_df[models_syuzhetr_ls].plot()\n",
        "corpus_syuzhetr_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} SyuzhetR 4 Models\\nMedianIQR SMA 10%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPuuUo3aZMWD"
      },
      "source": [
        "test_df = corpus_syuzhetr_df.filter(['sent_no', 'sent_raw'], axis=1)\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXdacd0FXWTq"
      },
      "source": [
        "corpus_syuzhetr_df.info()\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "corpus_syuzhetr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "933q5T5HasEW"
      },
      "source": [
        "plt.clf()\n",
        "corpus_syuzhetr_df[['syuzhet_roll10', 'bing_roll10', 'syuzhet_lnorm_roll10','bing_lnorm_roll10']].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is7GsGTCgQik"
      },
      "source": [
        "### **Transformer 8 Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBNrmdEIxYs"
      },
      "source": [
        "!ls -altr *.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuGaejHXeGP2"
      },
      "source": [
        "# Transformer 8 Models: Read Computed sentiment data saved from previous run of this notebook\n",
        "\n",
        "# corpus_transformer_df = pd.read_csv('sum_sentiments_sents_trans_mproust_theguermantesway-french.csv')\n",
        "\n",
        "corpus_transformer_df = pd.read_csv('sum_sentiments_sents_transformer_mproust_guermantes_en.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm2Lsk4zgoce"
      },
      "source": [
        "# Clean Chapter DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_transformer_df.columns:\n",
        "  corpus_transformer_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  \n",
        "corpus_transformer_df.head(2)\n",
        "corpus_transformer_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPJyxE-1JMcZ"
      },
      "source": [
        "# Derive Normalized/Standardized Time Series\n",
        "\n",
        "# Once applied modification function\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=models_transformer_ls, col_mod='lnorm')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=models_transformer_ls, col_mod='std-minmax')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=models_transformer_ls, col_mod='std-stdscaler')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=models_transformer_ls, col_mod='std-medianiqr')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=models_transformer_ls, col_mod='roll10')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "# Twice applied modification function (ends with roll)\n",
        "lnorm_ls = [f'{x}_lnorm' for x in models_transformer_ls]\n",
        "stdminmax_ls = [f'{x}_minmax' for x in models_transformer_ls]\n",
        "stdstdscaler_ls = [f'{x}_stdscaler' for x in models_transformer_ls]\n",
        "stdmedianiqr_ls = [f'{x}_medianiqr' for x in models_transformer_ls]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=lnorm_ls, col_mod='std-minmax')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=lnorm_ls, col_mod='std-stdscaler')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=lnorm_ls, col_mod='std-medianiqr')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=lnorm_ls, col_mod='roll10')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=stdminmax_ls, col_mod='roll10')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=stdmedianiqr_ls, col_mod='roll10')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "# Thrice applied modification function (starts with lnorm and ends with roll)\n",
        "lnorm_stdminmax_ls = [f'{x}_lnorm_minmax' for x in models_transformer_ls]\n",
        "lnorm_stdstdscaler_ls = [f'{x}_lnorm_stdscaler' for x in models_transformer_ls]\n",
        "lnorm_stdmedianiqr_ls = [f'{x}_lnorm_medianiqr' for x in models_transformer_ls]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=lnorm_stdminmax_ls, col_mod='roll10')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=lnorm_stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=lnorm_stdmedianiqr_ls, col_mod='roll10')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "corpus_transformer_df.head(2)\n",
        "corpus_transformer_df.info()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHoF3nhvZejR"
      },
      "source": [
        "print(*corpus_transformer_df.columns, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1R2P40IgnKA"
      },
      "source": [
        "# roberta15lg_roll10_stdscaler\n",
        "# robertaxml8lang_roll10_stdscaler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y0kH2f8ZRwp"
      },
      "source": [
        "roll_ls = [10]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=roll_ls, col_mod='medianiqr')\n",
        "test_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ks0Xq3KXpFB"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# Test reversing order of operations: first smooth (elim outliers) then Standardize\n",
        "\n",
        "# Create the plain roll10 series\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=models_transformer_ls, col_mod='roll10')\n",
        "print(f'test_df/roll10 shape: {test_df.shape}')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "temp_ls = [f'{x}_roll10' for x in models_transformer_ls]\n",
        "print(f'ROLL_LS: {temp_ls}')\n",
        "\n",
        "# Create the roll10_stdscaler series\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=temp_ls, col_mod='std-stdscaler')\n",
        "print(f'test_df/std-stdscaler shape: {test_df.shape}')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "\n",
        "temp_ls = [f'{x}_roll10_stdscaler' for x in models_transformer_ls]\n",
        "print(f'ROLL_STDSCALER_LS: {temp_ls}')\n",
        "\n",
        "# Plot \n",
        "# corpus_transformer_df[models_transformer_ls].plot()\n",
        "corpus_transformer_df['robertaxml8lang_roll10'].plot()\n",
        "\n",
        "# corpus_transformer_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} Transformer 8 Models\\n[SAM 10% then StdScaler]')\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NygCjCgGuFAb"
      },
      "source": [
        "# CORPUS_FULL = 'The Guermantes Way - English, Marcel Proust'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YsA6DGcS_3e"
      },
      "source": [
        "# Derive Rolling Means Time Series\n",
        "\n",
        "temp_ls = [f'{x}_roll10' for x in models_transformer_ls]\n",
        "# corpus_transformer_df[models_transformer_ls].plot()\n",
        "corpus_transformer_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} Transformer 8 Models\\n[SMA 10%]')\n",
        "\n",
        "temp_ls = [f'{x}_minmax_roll10' for x in models_transformer_ls]\n",
        "# corpus_transformer_df[models_transformer_ls].plot()\n",
        "corpus_transformer_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} Transformer 8 Models\\n[MinMax -> SMA 10%]')\n",
        "\n",
        "temp_ls = [f'{x}_stdscaler_roll10' for x in models_transformer_ls]\n",
        "# corpus_transformer_df[models_transformer_ls].plot()\n",
        "corpus_transformer_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} Transformer 8 Models\\n[StdScaler -> SMA 10%]')\n",
        "\n",
        "temp_ls = [f'{x}_lnorm_stdscaler_roll10' for x in models_transformer_ls]\n",
        "# corpus_transformer_df[models_transformer_ls].plot()\n",
        "corpus_transformer_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} Transformer 8 Models\\n[lnorm -> StdScaler -> SMA 10%]')\n",
        "\n",
        "temp_ls = [f'{x}_medianiqr_roll10' for x in models_transformer_ls]\n",
        "# corpus_transformer_df[models_transformer_ls].plot()\n",
        "corpus_transformer_df[temp_ls].plot()\n",
        "plt.title(f'{CORPUS_FULL} Transformer 8 Models\\n[MedianIQR -> SMA 10%]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FiDm09nbAdF"
      },
      "source": [
        "### **Save Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZg_ve-7bAdH"
      },
      "source": [
        "# Save Corpus DataFrames\n",
        "\n",
        "save_dataframes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfyebD6HhQIU"
      },
      "source": [
        "# TODO: If missing, generate derived values for each model, lnorm/not, stdscaler/medianiqr, roll10/not"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d-AM_WvbqFu"
      },
      "source": [
        "\n",
        "# Verfiy there are no NaN or Empty strings that passed the cleaning process\n",
        "\n",
        "# Only execute if previously datafile/corpus_sents_df\n",
        "\n",
        "\n",
        "# corpus_sents_df[corpus_sents_df['sent_clean'].isnull()]\n",
        "\n",
        "# corpus_sents_df[corpus_sents_df['sent_clean'].apply(lambda x: len(str(x)) <= 0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAAQ0SwfZynO"
      },
      "source": [
        "# Verify that hyphenated words are correctly handled (e.g. 'summer-mroning' -> 'summer morning')\n",
        "\n",
        "# corpus_sents_df[corpus_sents_df['sent_clean'].str.contains('summer', na=False)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsEbvCoCX7HY"
      },
      "source": [
        "## **(b) Compute Baseline Sentiments (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZK1gA47xzkS"
      },
      "source": [
        "### **Select Sentiment Models (Manual)**\n",
        "\n",
        "NOTE:\n",
        "\n",
        "* Stanza (Stanford OpenNLP) can take upto 50 minutes to run\n",
        "\n",
        "* Listed in increasing order of (approx) run time\n",
        "\n",
        "* MPQA/SentiStrength not yet implemented (placeholders only for now)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0pStg1gJTZA"
      },
      "source": [
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "NRC_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "# MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "# SentiStrength_Arc = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0IACAN5JTZC"
      },
      "source": [
        "# Create and Verify custom list of Models to include\n",
        "\n",
        "MODELS_CUSTOM_LS = []\n",
        "\n",
        "if VADER_Arc:\n",
        "  MODELS_CUSTOM_LS.append('vader')\n",
        "if TextBlob_Arc:\n",
        "  MODELS_CUSTOM_LS.append('textblob')\n",
        "if Flair_Arc:\n",
        "  MODELS_CUSTOM_LS.append('flair')\n",
        "if Stanza_Arc:\n",
        "  MODELS_CUSTOM_LS.append('stanza')\n",
        "if SentimentR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentimentr')\n",
        "if Syuzhet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('syuzhet')\n",
        "if AFINN_Arc:\n",
        "  MODELS_CUSTOM_LS.append('afinn')\n",
        "if Bing_Arc:\n",
        "  MODELS_CUSTOM_LS.append('bing')\n",
        "if Pattern_Arc:\n",
        "  MODELS_CUSTOM_LS.append('pattern')\n",
        "if SentiWord_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentiword')\n",
        "if SenticNet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('senticnet')\n",
        "if NRC_Arc:\n",
        "  MODELS_CUSTOM_LS.append('nrc')\n",
        "\n",
        "print(f'Here are the Models we are using to ensemble and save:\\n\\n   {MODELS_CUSTOM_LS}')\n",
        "\n",
        "\"\"\"\n",
        "models_incl_ls = []\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  models_incl_ls.append(amodel[:2])\n",
        "models_incl_str = ''.join(models_incl_ls)\n",
        "\n",
        "print(f'Here is a custom name abbr: {models_incl_str}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw0JPNe6T2ap"
      },
      "source": [
        "# Calculate (win_(x)1per) 1% of Corpus length for smallest (odd-valued) rolling window\n",
        "\n",
        "# Sentences\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "win_raw_s1per = int(corpus_sents_len * 0.01)\n",
        "# print(f'1% Rolling Window: {win_raw_s1per}')\n",
        "\n",
        "if win_raw_s1per % 2:\n",
        "  win_s1per = win_raw_s1per\n",
        "else:\n",
        "  win_s1per = win_raw_s1per + 1\n",
        "\n",
        "# Paragraphs\n",
        "# corpus_parags_df = corpus_all_df\n",
        "corpus_parags_len = len(corpus_sents_df['parag_no'].unique())\n",
        "\n",
        "win_raw_p1per = int(corpus_parags_len * 0.01)\n",
        "# print(f'1% Rolling Window: {win_raw_1per}')\n",
        "\n",
        "if win_raw_p1per % 2:\n",
        "  win_p1per = win_raw_p1per\n",
        "else:\n",
        "  win_p1per = win_raw_p1per + 1\n",
        "\n",
        "\n",
        "# Sections\n",
        "\n",
        "# NO NEED FOR SLIDING WINDOW ON SECTIONS\n",
        "\n",
        "\n",
        "print(f'Sentence 1 Percent window: {win_s1per}')\n",
        "print(f'Paragraph 1 Percent window: {win_p1per}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGjfPK9u21HH"
      },
      "source": [
        "# Verify Sentiment Lexicon hash files are accessable\n",
        "\n",
        "lexicons_path = f'/gdrive/MyDrive/{LEXICONS_SUBDIR[1:]}/hash*.csv'\n",
        "!pwd\n",
        "!ls $lexicons_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tAxuAxU7ueg"
      },
      "source": [
        "### **Calculate SentimentR (Jockers-Rinker) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foQeDPQpvyB3"
      },
      "source": [
        "if SentimentR_Arc == True:\n",
        "  model_base = 'sentimentr'\n",
        "\n",
        "  \"\"\"\n",
        "  model_name = 'sentimentr_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n",
        "  \"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEnJj5rIMcSj"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_sentimentr.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq7dImOJozm5"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "\n",
        "  lexicon_sentimentr_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_sentimentr.csv')\n",
        "  lexicon_sentimentr_df['x'] = lexicon_sentimentr_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_sentimentr_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_sentimentr_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_sentimentr_df.head()\n",
        "    lexicon_sentimentr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25_Zjja0o_Hx"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "\n",
        "  id = lexicon_sentimentr_df.word.values\n",
        "  values = lexicon_sentimentr_df.polarity.values\n",
        "\n",
        "  lexicon_sentimentr_dt = dict(zip(id, values))\n",
        "  # lexicon_sentimentr_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_sentimentr_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQYYP2hiO53j"
      },
      "source": [
        "# Verify\n",
        "\n",
        "# corpus_sents_df['sent_clean'].isna().any()\n",
        "# corpus_chaps_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVSImJ3ETSYt"
      },
      "source": [
        "corpus_parags_df.columns\n",
        "print('\\n')\n",
        "corpus_sects_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxe15XjNwByS"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_sentimentr(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_sentimentr_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SentimentR_Arc == True:\n",
        "  model_base = 'sentimentr'\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentimentr, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVX6rg5KwBvF"
      },
      "source": [
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pkrp6TwY7OI"
      },
      "source": [
        "# Verify there are no empty Sentences\n",
        "\n",
        "corpus_sents_df[corpus_sents_df['token_len'] == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYeO-NotzY0-"
      },
      "source": [
        "# TODO: Put above at the start of processing each new Corpus\n",
        "\n",
        "corpus_lexicons_stats_dt = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbGz2Yi5wByY"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4aDdJcrzlE1"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOSiIyhbN2-8"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "  # corpus_sents_df['sentimentr'].plot(alpha=0.3)\n",
        "  corpus_sents_df['sentimentr_stdscaler'].plot(alpha=0.1)\n",
        "  corpus_sents_df['sentimentr_medianiqr'].plot(alpha=0.3)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8POYGujomVt"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lvswx9yovMV"
      },
      "source": [
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZjqwTvU76AR"
      },
      "source": [
        "### **Calculate Syuzhet (Jockers) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjOXGQX54lbX"
      },
      "source": [
        "# Define Model names\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "  model_base = 'syuzhet'\n",
        "  model_name = 'syuzhet_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB5fBNQ-QNwk"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_syuzhet.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkhaGcuy4lbg"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "\n",
        "  lexicon_syuzhet_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_syuzhet.csv')\n",
        "  lexicon_syuzhet_df['word'] = lexicon_syuzhet_df['word'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_syuzhet_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_syuzhet_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_syuzhet_df.head()\n",
        "    lexicon_syuzhet_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9IuINuR4lbi"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "\n",
        "  id = lexicon_syuzhet_df.word.values\n",
        "  values = lexicon_syuzhet_df.value.values\n",
        "\n",
        "  lexicon_syuzhet_dt = dict(zip(id, values))\n",
        "  # lexicon_sentimentr_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_syuzhet_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO-txaFL4lbo"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_syuzhet(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_syuzhet_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_syuzhet, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfsLZSo04lbp"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9VFBSXD0vJo"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-BwrbyXR5Hd"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "  # corpus_sents_df['syuzhet'].plot(alpha=0.3)\n",
        "  corpus_sents_df['syuzhet_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['syuzhet_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA3dWsnF78mi"
      },
      "source": [
        "### **Calculate Bing (HuLiu) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wvq1prK7n1k"
      },
      "source": [
        "if Bing_Arc == True:\n",
        "  model_base = 'bing'\n",
        "  model_name = 'bing_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDfB2f0olded"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_bing.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZELGxS8y9PiS"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if Bing_Arc == True:\n",
        "  \n",
        "  lexicon_bing_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_bing.csv')\n",
        "  lexicon_bing_df['x'] = lexicon_bing_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_bing_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_bing_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_bing_df.head()\n",
        "    lexicon_bing_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJQTaeue9VjI"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if Bing_Arc == True:\n",
        "\n",
        "  id = lexicon_bing_df.word.values\n",
        "  values = lexicon_bing_df.polarity.values\n",
        "\n",
        "  lexicon_bing_dt = dict(zip(id, values))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Ffzu1m7n10"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_bing(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_bing_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Bing_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_bing, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWITnL2a9rrx"
      },
      "source": [
        "# Calculate Bing Sentiment [0,1,2]\n",
        "\n",
        "def bing_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += lex_discrete2continous_sentiment(str(aword), lexicon_bing_dt)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+1)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xpzm3hi7n1x"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Bing_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(bing_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UFEItnO-i7h"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Bing_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=bing_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4thQWz3-i7k"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if Bing_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKlKnqmWmBEP"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Bing_Arc == True:\n",
        "  # corpus_sents_df['bing'].plot(alpha=0.3)\n",
        "  corpus_sents_df['bing_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['bing_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkNZVk128jV9"
      },
      "source": [
        "### **Calculate SentiWord Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnL4ORNp8MgB"
      },
      "source": [
        "if SentiWord_Arc == True:\n",
        "  model_base = 'sentiword'\n",
        "  model_name = 'sentiword_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ootf8xdbnHPx"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_sentiword.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpVmt0fK8xi_"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "\n",
        "  lexicon_sentiword_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_sentiword.csv')\n",
        "  lexicon_sentiword_df['x'] = lexicon_sentiword_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_sentiword_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_sentiword_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_sentiword_df.head()\n",
        "    lexicon_sentiword_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwtK2M9h879M"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "\n",
        "  id = lexicon_sentiword_df.word.values\n",
        "  values = lexicon_sentiword_df.polarity.values\n",
        "\n",
        "  lexicon_sentiword_dt = dict(zip(id, values))\n",
        "  # lexicon_sentiword_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_sentiword_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I-QwB478MgP"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_sentiword(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_sentiword_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SentiWord_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentiword, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbEF88568MgS"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9GfZ5RQnn4G"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "  # corpus_sents_df['sentiword'].plot(alpha=0.3)\n",
        "  corpus_sents_df['sentiword_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['sentiword_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUcANLM_8mtT"
      },
      "source": [
        "### **Calculate SenticNet Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://sentic.net/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OqhSberA1hZ"
      },
      "source": [
        "if SenticNet_Arc == True:\n",
        "  model_base = 'senticnet'\n",
        "  model_name = 'senticnet_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsnCzsfFnx7B"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_senticnet.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmMfvQvYBBoM"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "\n",
        "  lexicon_senticnet_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_senticnet.csv')\n",
        "  lexicon_senticnet_df['x'] = lexicon_senticnet_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_senticnet_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_senticnet_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_senticnet_df.head()\n",
        "    lexicon_senticnet_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2oS71STBIUS"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "\n",
        "  id = lexicon_senticnet_df.word.values\n",
        "  values = lexicon_senticnet_df.polarity.values\n",
        "\n",
        "  lexicon_senticnet_dt =dict(zip(id, values))\n",
        "  # lexicon_jockersrinker_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  text2sentiment(sent_test, lexicon_senticnet_dt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHQwXBl5BlRM"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_senticnet(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on senticnet lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_senticnet_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SenticNet_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_senticnet, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raHUj3a4A1hs"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9lSo0Kmn_T4"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "  # corpus_sents_df['senticnet'].plot(alpha=0.3)\n",
        "  corpus_sents_df['senticnet_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['senticnet_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn4KQYpH3glK"
      },
      "source": [
        "### **Calculate NRC Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USx6LRmoCFV8"
      },
      "source": [
        "if NRC_Arc == True:\n",
        "  model_base = 'nrc'\n",
        "  model_name = 'nrc_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EdOTv3KoFEV"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_nrc.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFEszSc13glL"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if NRC_Arc == True:\n",
        "\n",
        "  lexicon_nrc_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_nrc.csv')\n",
        "  lexicon_nrc_df['x'] = lexicon_nrc_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_nrc_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_nrc_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_nrc_df.head()\n",
        "    lexicon_nrc_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nAkx4Mv3glL"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if NRC_Arc == True:\n",
        "\n",
        "  id = lexicon_nrc_df.word.values\n",
        "  values = lexicon_nrc_df.polarity.values\n",
        "\n",
        "  lexicon_nrc_dt =dict(zip(id, values))\n",
        "  # lexicon_jockersrinker_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv1nVSATb7z4"
      },
      "source": [
        "# Calculate NRC Sentiment [0,1,2]\n",
        "\n",
        "def nrc_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += lex_discrete2continous_sentiment(str(aword), lexicon_nrc_dt)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+10)\n",
        "\n",
        "  return text_sentiment_norm\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss3Xixo_CX2q"
      },
      "source": [
        "# Test\n",
        "\n",
        "if NRC_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(nrc_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhqnljMSCX2w"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if NRC_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=nrc_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2q97Dm-CX2z"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if NRC_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBK7LQx4oXI-"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if NRC_Arc == True:\n",
        "  # corpus_sents_df['nrc'].plot(alpha=0.3)\n",
        "  corpus_sents_df['nrc_stdscaler'].plot(alpha=0.1)\n",
        "  corpus_sents_df['nrc_medianiqr'].plot(alpha=0.3)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRTjCPLb8cbB"
      },
      "source": [
        "### **Calculate Afinn Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://github.com/fnielsen/afinn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcnxqnyzDCde"
      },
      "source": [
        "if AFINN_Arc == True:\n",
        "  model_base = 'afinn'\n",
        "  model_name = 'afinn_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDG2KxvNBdj6"
      },
      "source": [
        "if AFINN_Arc == True:\n",
        "  !pip install afinn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evnkXWL58CcX"
      },
      "source": [
        "# Install and configure for English\n",
        "\n",
        "if AFINN_Arc == True:\n",
        "  from afinn import Afinn\n",
        "  afinn = Afinn(language='en')\n",
        "\n",
        "  # Test\n",
        "\n",
        "  # afinn.score('I had the worst day.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKbeW_cMXhmI"
      },
      "source": [
        "# Calculate AFINN Sentiment [0,1,2]\n",
        "\n",
        "def afinn_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += afinn.score(aword)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+0.1)\n",
        "\n",
        "  return float(text_sentiment_norm)  # return float vs np.float64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRCd4VpwDO0Q"
      },
      "source": [
        "# Test\n",
        "\n",
        "if AFINN_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(afinn_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrP_wxB3DO0S"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if AFINN_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=afinn_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgZaMPYKDO0T"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if AFINN_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clUzylGOog5h"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if AFINN_Arc == True:\n",
        "  # corpus_sents_df['afinn'].plot(alpha=0.3)\n",
        "  corpus_sents_df['afinn_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['afinn_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAEiglIPDfFI"
      },
      "source": [
        "### **Calculate VADER Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgkvefzpDgx-"
      },
      "source": [
        "if VADER_Arc == True:\n",
        "  model_base = 'vader'\n",
        "  model_name = 'vader_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wodGtjXhDmZN"
      },
      "source": [
        "if VADER_Arc == True:\n",
        "  # Sentiment evaluation function\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "  # Test\n",
        "  sid.polarity_scores('hello world')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS8e25MkDmZP"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "if VADER_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sid.polarity_scores, sentiment_type='compound')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Azyv2lDmZP"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if VADER_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq8KNwpjom4X"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if VADER_Arc == True:\n",
        "  # corpus_sents_df['vader'].plot(alpha=0.3)\n",
        "  corpus_sents_df['vader_stdscaler'].plot(alpha=0.1)\n",
        "  corpus_sents_df['vader_medianiqr'].plot(alpha=0.3)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCN4c-G48e7-"
      },
      "source": [
        "### **Calculate TextBlob Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MfVWZ34Vg8U"
      },
      "source": [
        "if TextBlob_Arc == True:\n",
        "  model_base = 'textblob'\n",
        "  model_name = 'textblob_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "118Blghk7fjp"
      },
      "source": [
        "if TextBlob_Arc == True:\n",
        "  from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhJsYxPoVhY4"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "def textblob_sentiment(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return a sentiment value between -1.0 to +1.0 using TextBlob\n",
        "  '''\n",
        "  return TextBlob(text_str).sentiment.polarity\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if TextBlob_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=textblob_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlHRf2aFVhY7"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if TextBlob_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_name, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_name, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_name, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_name, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMWewkTgord1"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if TextBlob_Arc == True:\n",
        "  # corpus_sents_df['textblob'].plot(alpha=0.3)\n",
        "  corpus_sents_df['textblob_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['textblob_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2blGfVlKb_s"
      },
      "source": [
        "### **Calculate Pattern Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU60-nqpCsl7"
      },
      "source": [
        "if Pattern_Arc == True:\n",
        "  model_base = 'pattern'\n",
        "  model_name = 'pattern_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KxnLfHoL3Fy"
      },
      "source": [
        "if Pattern_Arc == True:\n",
        "  !pip install pattern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtwmIrSOKZRm"
      },
      "source": [
        "if Pattern_Arc == True:\n",
        "  from pattern.en import sentiment as pattern_sa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vwtm_jBKZM2"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  pattern_sa(sent_test)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXq-jDPxasVY"
      },
      "source": [
        "# Calculate Pattern Sentiment [0,1,2]\n",
        "\n",
        "def pattern_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += pattern_sa(str(aword))[0]\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+0.01)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-sXRBNWC08o"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(pattern_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db0hezLKC08p"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Pattern_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=pattern_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGHixqpOC08q"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clydVLby2KhG"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  # corpus_sents_df['pattern'].plot(alpha=0.3)\n",
        "  corpus_sents_df['pattern_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['pattern_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-X3xNWkoyLS"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  corpus_sents_df['pattern'].rolling(10*win_s1per, center=True).mean().plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQwX3APKnhjP"
      },
      "source": [
        "corpus_sents_df['pattern'].plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM1OmuaiVQIH"
      },
      "source": [
        "# Check Pattern Series for Outliers\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  print('Furthest Positive Outlier in Pattern Time Series:')\n",
        "  corpus_sents_df[corpus_sents_df['pattern'] == corpus_sents_df['pattern'].max()][['sent_no', 'pattern', 'sent_raw']]\n",
        "\n",
        "  pattern_max_sentno = int(corpus_sents_df[corpus_sents_df['pattern'] == corpus_sents_df['pattern'].max()][['sent_no']].max())\n",
        "  print(f'Max Outlier for Pattern Sentiment Model is at Sentence #{pattern_max_sentno}')\n",
        "\n",
        "  print('Furthest Negative Outlier in Pattern Time Series:')\n",
        "  corpus_sents_df[corpus_sents_df['pattern'] == corpus_sents_df['pattern'].min()][['sent_no', 'pattern', 'sent_raw']]\n",
        "\n",
        "  pattern_min_sentno = int(corpus_sents_df[corpus_sents_df['pattern'] == corpus_sents_df['pattern'].min()][['sent_no']].min())\n",
        "  print(f'Max Outlier for Pattern Sentiment Model is at Sentence #{pattern_min_sentno}')\n",
        "\n",
        "  print('\\n')\n",
        "  print('Median Absolute Deviation (MAD) for Pattern Time Series:')\n",
        "  robust.mad(corpus_sents_df['pattern'])\n",
        "\n",
        "  temp_df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny19347PZ85l"
      },
      "source": [
        "# Verify what is happening in the neighborhood of the Maximum Sentiment Outlier for Pattern\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  outlier_halfwin = 10\n",
        "  outlier_winstart = pattern_max_sentno - outlier_halfwin\n",
        "  outlier_winend = pattern_max_sentno + outlier_halfwin\n",
        "\n",
        "  corpus_sents_df.iloc[outlier_winstart:outlier_winend]['pattern'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRDctGTdUfKk"
      },
      "source": [
        "# Pattern library has a bug wherein a/several Sentences have far outlying Sentiments\n",
        "#   we need to clip these to within n * MAD\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  MAD_Clip_Boundary = 5.2 #@param {type:\"slider\", min:1.0, max:10, step:0.1}\n",
        "\n",
        "  # find the limits of 2.5 Median Absolute Deviation of Pattern Time Series\n",
        "\n",
        "  clip_25mad = MAD_Clip_Boundary * robust.mad(corpus_sents_df['pattern'])\n",
        "  print(f'Clip Pattern Series at 2.5 x Median Absolute Deviation (MAD) = {clip_25mad}')\n",
        "\n",
        "  # Create a Temporary DataFrame to test/find best MAD Clipping multiplier for Pattern\n",
        "  temp_df['pattern'] = pd.Series(corpus_sents_df['pattern'].clip(upper=clip_25mad))\n",
        "  temp_df['pattern'].clip(lower=-clip_25mad, inplace=True)\n",
        "\n",
        "  # Verify \n",
        "  temp_df['pattern'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co3jaquyZom7"
      },
      "source": [
        "# Once a good Clip_MAD_multiplier if found, update Pattern Time Series with it\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  corpus_sents_df['pattern'] = temp_df['pattern']\n",
        "  corpus_sents_df['pattern'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsaziON_Z263"
      },
      "source": [
        "### **Calculate Stanza/OpenNLP Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://github.com/piyushpathak03/NLP-using-STANZA/blob/main/Stanza.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZgGfcCuFnmI"
      },
      "source": [
        "if Stanza_Arc == True:\n",
        "  model_base = 'stanza'\n",
        "  model_name = 'stanza_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoZUi2AwZ_7L"
      },
      "source": [
        "if Stanza_Arc == True:\n",
        "  !pip install stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5txTb6aIZ2tN"
      },
      "source": [
        "%time\n",
        "\n",
        "import stanza\n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  stanza.download('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NORYbxsxZ2qg"
      },
      "source": [
        "if Stanza_Arc == True:\n",
        "  nlp = stanza.Pipeline('en', processors='tokenize,sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtOtBfYwZ2na"
      },
      "source": [
        "# Test stanza directly\n",
        "\n",
        "# doc = nlp('Ram is a bad boy')\n",
        "# for i, sentence in enumerate(doc.sentences):\n",
        "#     print(i, sentence.sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKnox67kayod"
      },
      "source": [
        "# Calculate Stanza Sentiment [0,1,2]\n",
        "\n",
        "def stanza_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_tot = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    adoc = nlp(aword)\n",
        "    for i, sentence in enumerate(adoc.sentences):\n",
        "      text_sentiment_tot += float(sentence.sentiment)\n",
        "  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.1)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNngkBBAF26C"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(stanza_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrY2mrhVF26D"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# NOTE: requires about 30-50mins (20210708 at 0730) Colab Pro: GPU+RAM \n",
        "#                      2hrs30mins (20210802 at 1330) Colab Pro: CPU only\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Stanza_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=stanza_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSiYC73nF26D"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5tUYZA1DExu"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  # corpus_sents_df['stanza'].plot(alpha=0.3)\n",
        "  corpus_sents_df['stanza_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['stanza_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIGQgWvyOtg6"
      },
      "source": [
        "### **Calculate Flair Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWbcaaDDO1J1"
      },
      "source": [
        "if Flair_Arc == True:\n",
        "  model_base = 'flair'\n",
        "  model_name = 'flair_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooLdktLHO1J3"
      },
      "source": [
        "if Flair_Arc == True:\n",
        "  !pip install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77-3mVyRPJbb"
      },
      "source": [
        "if Flair_Arc == True:\n",
        "  from flair.models import TextClassifier\n",
        "  from flair.data import Sentence\n",
        "\n",
        "  classifier = TextClassifier.load('en-sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJOopGZhO1J5"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Flair_Arc == True:\n",
        "  sentence = Sentence('The food was great!')\n",
        "  classifier.predict(sentence)\n",
        "\n",
        "  # print sentence with predicted labels\n",
        "  print('Sentence above is: ', sentence.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7fZT10HQDqd"
      },
      "source": [
        "def get_flairsentiment(text_str):\n",
        "  # TODO: For efficiency, combine sentences in batches as arrays (if possible)\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return a floating point -1.0 to 1.0 value for Sentiment\n",
        "  '''\n",
        "\n",
        "  text_tokenized_obj = Sentence(text_str)\n",
        "  classifier.predict(text_tokenized_obj)\n",
        "\n",
        "  # print(f'Processing text_str: {text_str}')\n",
        "  sentiment_str = str(text_tokenized_obj.labels[0])\n",
        "\n",
        "  sentiment_ls = sentiment_str.split(' ')\n",
        "  \n",
        "  sentiment_sign = sentiment_ls[0]\n",
        "\n",
        "  if sentiment_sign.lower() == 'positive':\n",
        "    sign_multiplier = 1.0\n",
        "  else:\n",
        "    sign_multiplier = -1.0\n",
        "\n",
        "  sentiment_abs = float(sentiment_ls[1][1:-1])\n",
        "\n",
        "  sentiment_fl = sign_multiplier * sentiment_abs \n",
        "\n",
        "  return sentiment_fl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbgQVQlNSdcE"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Flair_Arc == True:\n",
        "  get_flairsentiment('it is.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX0PDn5AO1J9"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Flair_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=get_flairsentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v5Q2ota94_r"
      },
      "source": [
        "corpus_sents_df[corpus_sents_df['sent_clean'].str.find('smokeing') != -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfJEaIw1-QLs"
      },
      "source": [
        "corpus_sents_df.iloc[2844]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vlK-dyHO1J-"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if Flair_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VycWC2gDO1J_"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Flair_Arc == True:\n",
        "  corpus_sents_df['flair'].plot(alpha=0.3)\n",
        "  corpus_sents_df['flair_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['flair_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfMwbhVMwgXw"
      },
      "source": [
        "### **(Optional) Calculate SentimentR and SyuzhetR Sentiments in RStudio**\n",
        "\n",
        "**NOTE** Process in RStudio with the following R Script\n",
        "```\n",
        "# Setup\n",
        "# getwd()\n",
        "# list.files(pattern='*.csv')\n",
        "# setwd('./sentimenttime')\n",
        "\n",
        "###\n",
        "# Begin SyuzhetR Preprocessing\n",
        "library('syuzhet')\n",
        "\n",
        "# import the RAW Sentences.csv, not CLEAN\n",
        "# because SentimentR assigns greater polarity to HATE > hate\n",
        "# TODO: Compare CLEAN vs RAW for SentimentR and VADER for impact of such Heuristics\n",
        "\n",
        "# >>> CUSTOMIZE THIS FOR EACH CORPUS <<<\n",
        "corpus_name = 'ddefoe_robinsoncrusoe' \n",
        "# >>> CUSTOMIZE THIS FOR EACH CORPUS <<<\n",
        "\n",
        "# SentimentTime Preprocessing (e.g. corpus_text_sents_raw_ddefoe_robinsoncrusoe.csv)\n",
        "input_filename_prefix = 'corpus_text_sents_raw_'\n",
        "input_filename_suffix = '.csv'\n",
        "\n",
        "corpus_input_filename = trimws(paste0(input_filename_prefix, corpus_name, input_filename_suffix, sep=' '))\n",
        "\n",
        "###\n",
        "syuzhet_output_prefix = 'sum_sentiments_syuzhetR_4models_sentimenttimeraw_'\n",
        "syuzhet_output_suffix = '.csv'\n",
        "syuzhet_output = trimws(paste0(syuzhet_output_prefix, corpus_name, syuzhet_output_suffix, sep=' '))\n",
        "\n",
        "\n",
        "# Use 4 Models in Syuzhet to parse Corpus and generate 4 Sentiment Time Series\n",
        "\n",
        "# (OPTION A) Read preprocessed Corpus *.csv (Sentence Tokenized and text cleaned by sentimenttime)\n",
        "corpus_str <- read.csv(corpus_input_filename,header=T)$sent_raw\n",
        "\n",
        "# corpus_sents_v <- syuzhet::get_sentences(corpus_str) # SyuzhetR often splits a line with one Sentence into several lines\n",
        "corpus_sents_v <- read.csv(corpus_input_filename, header = TRUE, row.names = 1)\n",
        "\n",
        "# (OPTION B) Read raw textfile using Syuzhet Sentence Tokenizer and text preprocessing\n",
        "# corpus_str <- syuzhet::get_text_as_string(corpus_input)\n",
        "# corpus_sents_v <- syuzhet::get_sentences(corpus_str)\n",
        "\n",
        "# Read in RAW Sentences\n",
        "syuzhet_all_df <- data.frame(sent_raw = corpus_sents_v)\n",
        "\n",
        "# Compute Sentiment values for each Model \n",
        "syuzhet_all_df$syuzhet <- syuzhet::get_sentiment(corpus_sents_v, method='syuzhet')\n",
        "syuzhet_all_df$bing <- syuzhet::get_sentiment(corpus_sents_v, method='bing')\n",
        "syuzhet_all_df$afinn <- syuzhet::get_sentiment(corpus_sents_v, method='afinn')\n",
        "syuzhet_all_df$nrc <- syuzhet::get_sentiment(corpus_sents_v, method='nrc')\n",
        "\n",
        "# Save Syuzhet Results    \n",
        "write.csv(syuzhet_all_df, syuzhet_output)\n",
        "\n",
        "\n",
        "\n",
        "# Begin SentimentR Processing\n",
        "library('sentimentr')\n",
        "\n",
        "# Set Output Sentiments Datafile names\n",
        "sentimentr_output_prefix = 'sum_sentiments_sentimentR_7models_sentimenttimeraw_'\n",
        "sentimentr_output_suffix = '.csv'\n",
        "sentimentr_output = trimws(paste0(sentimentr_output_prefix, corpus_name, sentimentr_output_suffix, sep=' '))\n",
        "\n",
        "# Use vector of RAW sentences already read (originally parsed by SentimentTime.py) \n",
        "# sentimentr_sents_v <- sentimentr::get_sentences(corpus_sents_v)\n",
        "# sentimentr_sents_v <- corpus_sents_v\n",
        "  \n",
        "  \n",
        "# Create data.frame with jockers_rinker sentiments\n",
        "sentimentr_all_df <- data.frame(sent_raw = corpus_sents_v)\n",
        "\n",
        "# sentimentr_sents_v <- read.csv(corpus_input_filename, header = TRUE, row.names = 1)\n",
        "# Create data.frame with jockers_rinker sentiments\n",
        "# sentimentr_all_df <- data.frame(sent_raw = sentimentr_sents_v)\n",
        "\n",
        "# Code from sentimentr.R\n",
        "# \n",
        "# SentimentR function sentiment will not work on native R data types, only\n",
        "#   types of 'get_sentences'/'get_sentences_char' created by passing text\n",
        "#   through the sentence tokenizer textshape::split_sentence\n",
        "# We fool SentimentR by copying and calling it's make_class function in utils.R\n",
        "#   and passing our preprocessed text (sentence tokenization done in Python) to\n",
        "#   ensure that SentimentR has the same number/alignment of Sentences in our Corpus\n",
        "#   as all other Sentiment analysis methods\n",
        "# \n",
        "# get_sentences.character <- function(x, ...) {\n",
        "#   out <- textshape::split_sentence(x, ...)\n",
        "#   make_class(out, \"get_sentences\", \"get_sentences_character\")\n",
        "# }\n",
        "\n",
        "# Code from SentimentR.r (in utils.R)\n",
        "make_class <- function(x, ...) {\n",
        "  class(x) <- unique(c(..., class(x)))    \n",
        "  x\n",
        "}\n",
        "\n",
        "# Add other lexicon sentiments\n",
        "sentimentr_all_df$jockers_rinker <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_jockers_rinker, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$jockers <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_jockers, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$huliu <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_huliu, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$lmcd <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_loughran_mcdonald, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$nrc <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_nrc, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$senticnet <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_senticnet, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$sentiword <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_sentiword, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "\n",
        "write.csv(sentimentr_all_df, sentimentr_output)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNCGf1KZEpld"
      },
      "source": [
        "## **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuNJ9x_2lGYl"
      },
      "source": [
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DJvqmOPOcfE"
      },
      "source": [
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjOmN2yobL9d"
      },
      "source": [
        "# Save Corpus DataFrames\n",
        " \n",
        "# save_dataframes(df_ls=['baseline','sentimentr','syuzhetr','transformer','combined','subset','everything']):\n",
        "save_dataframes(df_ls=['baseline'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDhq0IizMdp7"
      },
      "source": [
        "# corpus_sents_df.iloc[2843]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0s6LIujwDD2"
      },
      "source": [
        "# **Explore Sentiment Models and Arcs**\n",
        "\n",
        "Baseline Models\n",
        "\n",
        "* VADER [-1.0 to 1.0] zero peak\n",
        "* TextBlob [-1.0 to 1.0] zero peak\n",
        "* Stanza outliers [-1.0 to 199.0] pos, outliers(+peak)\n",
        "* AFINN [-14 (-8 to 8) 20] discrete\n",
        "* SentimentR 11,710 [-5.4 to 8.8] norm\n",
        "* Syuzhet [-5.4 to 8.8] norm\n",
        "* Bing [-100.0 (-20.0 to 20.0) 100] discrete, outliers\n",
        "* Pattern [-1.0 to 1.0] norm\n",
        "* SentiWord [-3.8 to 4.4] norm\n",
        "* SenticNet [-3.8 to 10] norm\n",
        "* NRC [-100.0 (-5.0 to 5.0) 100] zero, outliers\n",
        "\n",
        "SentimentR Models\n",
        "\n",
        "* Jockers_Rinker\n",
        "* Jockers\n",
        "* HuLiu\n",
        "* NRC\n",
        "* Loughran-McDonald\n",
        "* SenticNet\n",
        "* SentiWord\n",
        "\n",
        "SyuzhetR Models\n",
        "\n",
        "* Syuzhet\n",
        "* Bing\n",
        "* AFINN\n",
        "* NRC\n",
        "\n",
        "Tranformer Models\n",
        "\n",
        "* NLPTown\n",
        "* RoBERTa Large 15 Datasets\n",
        "* BERT Yelp Dataset\n",
        "* BERT Code Switching Hinglish\n",
        "* IMDB 2-way \n",
        "* Huggingface Default (Distilled BERT)\n",
        "* T5 IMDB 50k Dataset\n",
        "* RoBERTa XML 8 Languages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGqpjXvRwS20"
      },
      "source": [
        "## **EDA Baseline Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91FoxP6w28t"
      },
      "source": [
        "# Create 4 Standardized versions of each Model: stdscaler, medianiqr both lnormed and not\n",
        "\n",
        "print('\\nBefore Standardization ----------')\n",
        "corpus_sents_df.columns\n",
        "\n",
        "standardize_ts_ls(corpus_sents_df, models_baseline_ls)\n",
        "\n",
        "print('\\nAfter Standardization ----------')\n",
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4Ddk8J8wynL"
      },
      "source": [
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfqKmcKqDz2y"
      },
      "source": [
        "##### **Interactive LOWESS Smoothed Plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ktO5sRIDzUZ"
      },
      "source": [
        "# Sentence Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "LOWESS_frac = 0.2 #@param {type:\"slider\", min:0.01, max:0.20, step:0.01}\n",
        "\n",
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = False #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = False #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "NRC_Arc = False #@param {type:\"boolean\"}\n",
        "AFINN_Arc = False #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = False #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = False #@param {type:\"boolean\"}\n",
        "Stanza_Arc = False #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = True #@param {type:\"boolean\"}\n",
        "# Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "# MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "# SentiStrength_Arc = False #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  models_subset_ls.append('sentimentr_stdscaler')\n",
        "if Syuzhet_Arc == True:\n",
        "  models_subset_ls.append('syuzhet_stdscaler')\n",
        "if Bing_Arc == True:\n",
        "  models_subset_ls.append('bing_stdscaler')\n",
        "if SenticNet_Arc == True:\n",
        "  models_subset_ls.append('senticnet_stdscaler')\n",
        "if SentiWord_Arc == True:\n",
        "  models_subset_ls.append('sentiword_stdscaler')\n",
        "if NRC_Arc == True:\n",
        "  models_subset_ls.append('nrc_stdscaler')\n",
        "if AFINN_Arc == True:\n",
        "  models_subset_ls.append('afinn_stdscaler')\n",
        "if VADER_Arc == True:\n",
        "  models_subset_ls.append('vader_stdscaler')\n",
        "if TextBlob_Arc == True:\n",
        "  models_subset_ls.append('textblob_stdscaler')\n",
        "if Flair_Arc == True:\n",
        "  models_subset_ls.append('flair_stdscaler')\n",
        "if Pattern_Arc == True:\n",
        "  models_subset_ls.append('pattern_stdscaler')\n",
        "if Stanza_Arc == True:\n",
        "  models_subset_ls.append('stanza_stdscaler')\n",
        "# if Mean_All_Arc == True:\n",
        "#   models_subset_ls.append('mean_all')\n",
        "\n",
        "\"\"\"\n",
        "if len(str(SMA_Window_Percent)) == 1:\n",
        "  roll_str = 'roll0' + str(SMA_Window_Percent)\n",
        "else:\n",
        "  roll_str = 'roll' + str(SMA_Window_Percent%100)\n",
        "\n",
        "print(f'Rolling Window: {roll_str}')\n",
        "\"\"\";\n",
        "\n",
        "# 1192.73 - All\n",
        "\n",
        "temp_df = get_lowess(corpus_sents_df, models_ls=models_subset_ls, text_unit='sentence', afrac=LOWESS_frac);\n",
        "\n",
        "# plot_models(models_subset_ls, models_type='baseline', text_unit='sent_no', win_per=SMA_Window_Percent)\n",
        "temp_df['minmax_diff'] = temp_df.max(axis=1) - temp_df.min(axis=1)\n",
        "print(f\"Sum of minmax_diff: {temp_df['minmax_diff'].sum()}\");\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC6FQTWiSt3Z"
      },
      "source": [
        "##### **Grid Search LOWESS frac**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI_FMNKIcqXs"
      },
      "source": [
        "temp_df['avg_stdscaler'] = corpus_sents_df[models_subset_ls].mean()\n",
        "temp_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkw3hBeMR9h3"
      },
      "source": [
        "Frac_Start = 0.08 #@param {type:\"slider\", min:0.01, max:0.3, step:0.01}\n",
        "Frac_End = 0.2 #@param {type:\"slider\", min:0.01, max:0.2, step:0.01}\n",
        "Frac_Step = 0.02 #@param {type:\"slider\", min:0.01, max:0.05, step:0.01}\n",
        "\n",
        "frac_start_int = int(100*Frac_Start)\n",
        "frac_end_int = int(100*Frac_End) + 1\n",
        "frac_step_int = int(100*Frac_Step)\n",
        "\n",
        "print('GRID SEARCH --------------------\\n')\n",
        "\n",
        "lowess_grid_dt = {}\n",
        "crux_ct_ls = []\n",
        "# temp_df['sent_no'] = pd.Series([x for x in corpus_sents_df['sent_no']])\n",
        "temp_df['avg_stdscaler'] = corpus_sents_df[models_subset_ls].mean()\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "\n",
        "\n",
        "for afrac in range(frac_start_int, frac_end_int, frac_step_int):\n",
        "  print(f'Processing afrac = {afrac}')\n",
        "  # Compute error between subset of models\n",
        "  afrac_fl = afrac/100\n",
        "  temp_df = get_lowess(corpus_sents_df, models_ls=models_subset_ls, text_unit='sentence', afrac=afrac_fl, do_plot=False);\n",
        "  temp_df['minmax_diff'] = temp_df.max(axis=1) - temp_df.min(axis=1)\n",
        "  diff_sum = temp_df['minmax_diff'].sum()\n",
        "  print(f\"  Sum(minmax_diff): {diff_sum}\");\n",
        "  lowess_grid_dt[afrac] = diff_sum\n",
        "  # Compute Crux Points\n",
        "  temp_df['sent_no'] = pd.Series(list(range(temp_df.shape[0])))\n",
        "  crux_ls = get_crux_points(temp_df,\n",
        "                            'median',\n",
        "                            text_type='sentence', \n",
        "                            win_per=5, \n",
        "                            sec_y_labels=False, \n",
        "                            sec_y_height=0, \n",
        "                            subtitle_str=' ', \n",
        "                            do_plot=False,\n",
        "                            save2file=False)\n",
        "  ax.plot(temp_df['sent_no'], temp_df['median'], label=f'frac={afrac}')\n",
        "  # plt.plot(data=temp_df, x='sent_no', y='median', label=f'frac={afrac}')\n",
        "  crux_ct_ls.append(len(crux_ls))\n",
        "  print(f'  {len(crux_ls)} Crux Points')\n",
        "\n",
        "plt.title(f\"{CORPUS_FULL} \\n LOWESS Smoothing Grid Search (frac={Frac_Start} to {Frac_End}\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm1ywpfuQJWA"
      },
      "source": [
        "# Plot Declining Error as a function of LOWESS frac\n",
        "\n",
        "# lowess_grid_dt\n",
        "\n",
        "lists = sorted(lowess_grid_dt.items()) # sorted by key, return a list of tuples\n",
        "\n",
        "x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
        "# plt.plot(x, y, label='Interplot Error')\n",
        "\n",
        "adj_factor = 40\n",
        "crux_ct_adj_ls = [adj_factor * x for x in crux_ct_ls]\n",
        "\n",
        "# create figure and axis objects with subplots()\n",
        "fig,ax = plt.subplots()\n",
        "# make first plot: Error\n",
        "ax.plot(x, y, color=\"red\", label='Coherence Error', marker=\"o\")\n",
        "# set x-axis label\n",
        "ax.set_xlabel(\"LOWESS frac Hyperparemeter\",fontsize=14)\n",
        "# set y-axis label\n",
        "ax.set_ylabel(\"Coherence Error\",color=\"red\",fontsize=14)\n",
        "\n",
        "# twin object for two different y-axis on the sample plot\n",
        "ax2=ax.twinx()\n",
        "\n",
        "# make second plot: Crux Count, with different y-axis using second axis object\n",
        "ax2.plot(x, crux_ct_ls,color=\"blue\",label='Crux Count', marker=\"o\")\n",
        "ax2.set_ylabel(\"Crux Count\",color=\"blue\",fontsize=14)\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment \\n Grid Search for LOWESS [frac] Hyperparemeter')\n",
        "plt.legend(loc='best')\n",
        "plt.show();\n",
        "\"\"\"\n",
        "# save the plot as a file\n",
        "fig.savefig('two_different_y_axis_for_single_python_plot_with_twinx.jpg',\n",
        "            format='jpeg',\n",
        "            dpi=100,\n",
        "            bbox_inches='tight')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9stl4BAQUn5Q"
      },
      "source": [
        "##### **Autmatic Sentence SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjUvHqm1KTa7"
      },
      "source": [
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uMTLx9Qej22"
      },
      "source": [
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8gYxSS0GRWk"
      },
      "source": [
        "# Create SMA roll=10% for all models_stdscaler as baseline\n",
        "\n",
        "win_s1per = int(corpus_sents_df.shape[0] * 1/100)\n",
        "\n",
        "col_stdscaler_roll_ls = []\n",
        "for amodel in models_baseline_ls:\n",
        "  col_stdscaler = f'{amodel}_stdscaler'\n",
        "  col_stdscaler_roll = f'{amodel}_stdscaler_{roll_str}'\n",
        "  corpus_sents_df[col_stdscaler_roll] = corpus_sents_df[col_stdscaler].rolling(10*win_s1per, center=True).mean()\n",
        "  col_stdscaler_roll_ls.append(col_stdscaler_roll)\n",
        "\n",
        "col_stdscaler_roll_mean = col_stdscaler_roll + '_mean'\n",
        "corpus_sents_df[col_stdscaler_roll_mean] = corpus_sents_df[col_stdscaler_roll_ls].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFLneZYefJ6W"
      },
      "source": [
        "col_stdscaler_roll_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZYRLVQYH6__"
      },
      "source": [
        "corpus_sents_df[col_stdscaler_roll_ls].plot()\n",
        "# corpus_sents_df['sentimentr_stdscaler_roll10'].plot()\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment \\n Baseline 12 Models StdScaler SMA 10%');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJJNNaxR0iJ0"
      },
      "source": [
        "##### **Interactive Sentence SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqmU4QRLOClI"
      },
      "source": [
        "# Sentence Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "NRC_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = False #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = True #@param {type:\"boolean\"}\n",
        "# Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "# MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "# SentiStrength_Arc = False #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  models_subset_ls.append('sentimentr')\n",
        "if Syuzhet_Arc == True:\n",
        "  models_subset_ls.append('syuzhet')\n",
        "if Bing_Arc == True:\n",
        "  models_subset_ls.append('bing')\n",
        "if SenticNet_Arc == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentiWord_Arc == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if NRC_Arc == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if AFINN_Arc == True:\n",
        "  models_subset_ls.append('afinn')\n",
        "if VADER_Arc == True:\n",
        "  models_subset_ls.append('vader')\n",
        "if TextBlob_Arc == True:\n",
        "  models_subset_ls.append('textblob')\n",
        "if Flair_Arc == True:\n",
        "  models_subset_ls.append('flair')\n",
        "if Pattern_Arc == True:\n",
        "  models_subset_ls.append('pattern')\n",
        "if Stanza_Arc == True:\n",
        "  models_subset_ls.append('stanza')\n",
        "if Mean_All_Arc == True:\n",
        "  models_subset_ls.append('mean_all')\n",
        "\n",
        "if len(str(SMA_Window_Percent)) == 1:\n",
        "  roll_str = 'roll0' + str(SMA_Window_Percent)\n",
        "else:\n",
        "  roll_str = 'roll' + str(SMA_Window_Percent%100)\n",
        "\n",
        "print(f'Rolling Window: {roll_str}')\n",
        "\n",
        "plot_models(models_subset_ls, models_type='baseline', text_unit='sent_no', win_per=SMA_Window_Percent)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bafsX4zKd-wT"
      },
      "source": [
        "##### **(ABOVE) Plotly SMA Sentence, (BELOW) Correlation Heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd8EjiQYUtEo"
      },
      "source": [
        "corr_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxRGGdcgHj2S"
      },
      "source": [
        "# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n",
        "# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n",
        "\n",
        "Correlation_Algo = \"pearson\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n",
        "# corr_methods_ls = ['pearson', 'spearman', 'kendall']\n",
        "\n",
        "col_stdscaler_rollwin_ls = []\n",
        "for amodel in models_baseline_ls:\n",
        "  col_amodel_stdscaler_rollwin = f'{amodel}_stdscaler_{roll_str}'\n",
        "  col_stdscaler_rollwin_ls.append(col_amodel_stdscaler_rollwin)\n",
        "print(f'DEFAULT Models:\\n\\n    col_stdscaler_rollwin_ls: {col_stdscaler_rollwin_ls}')\n",
        "\n",
        "# OPTIONAL EDIT: Manually select problematic model to remove from analysis \n",
        "#                (e.g. Pattern can misbehave at times)\n",
        "model_root_bad = 'pattern'\n",
        "col_stdscaler_rollwin_ls = [x for x in col_stdscaler_rollwin_ls if model_root_bad not in x]\n",
        "\n",
        "print(f'MODIFED Models:\\n\\n    col_stdscaler_rollwin_ls: {col_stdscaler_rollwin_ls}')\n",
        "\n",
        "corr_df = corpus_sents_df[col_stdscaler_rollwin_ls].dropna(axis=0, how='any').corr(method=Correlation_Algo)\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df, # corpus_sents_df[col_stdscaler_rollwin_ls].dropna(axis=0, how='any').corr(method=corr_method),\n",
        "                    row_cluster=True,\n",
        "                    col_cluster=True,\n",
        "                    figsize=(10, 10))\n",
        "\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment for Baseline Model Sentiments\\n {Correlation_Algo.capitalize()} Correlation - StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxfultUPZr-J"
      },
      "source": [
        "##### **Sentence Sentiment DTW Hierarichal Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1rU88PlFkss"
      },
      "source": [
        "# Dynamic Time Series Clustering\n",
        "\n",
        "# !pip install dtaidistance[all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Hys86nFkox"
      },
      "source": [
        "# from dtaidistance import dtw\n",
        "# from dtaidistance import dtw_visualisation as dtwvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMq5XOXTFkjL"
      },
      "source": [
        "\"\"\"\n",
        "s1 = np.array([0., 0, 1, 2, 1, 0, 1, 0, 0, 2, 1, 0, 0])\n",
        "s2 = np.array([0., 1, 2, 3, 1, 0, 0, 0, 2, 1, 0, 0, 0])\n",
        "path = dtw.warping_path(s1, s2)\n",
        "dtwvis.plot_warping(s1, s2, path, filename=\"warp.png\")\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mra_zDRSF0LN"
      },
      "source": [
        "\"\"\"\n",
        "from dtaidistance import dtw\n",
        "import numpy as np\n",
        "\n",
        "series = np.matrix([\n",
        "    [0.0, 0, 1, 2, 1, 0, 1, 0, 0],\n",
        "    [0.0, 1, 2, 0, 0, 0, 0, 0, 0],\n",
        "    [0.0, 0, 1, 2, 1, 0, 0, 0, 0]])\n",
        "ds = dtw.distance_matrix_fast(series)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wycvgBTtvJYY"
      },
      "source": [
        "##### **Top-n Crux Peaks and Valleys**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8QX8tSKM376"
      },
      "source": [
        "**Search Corpus for Substring**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* In [Search_for_Substring] enter a Substring to search for in the Corpus\n",
        "\n",
        "* Enter a Substring long enough/unique enough so only a reasonable number of Sentences will be returned\n",
        "\n",
        "* Substring can contain spaces/punctuation, for example: 'in the garden'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJYjOu9Ks_pL"
      },
      "source": [
        "# Search Corpus Sentences for Substring\n",
        "\n",
        "Search_for_Substring = \"abuse\" #@param {type:\"string\"}\n",
        "\n",
        "sentno_matching_ls = corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Search_for_Substring, regex=False)]['sent_no']\n",
        "\n",
        "for i, asentno in enumerate(sentno_matching_ls):\n",
        "  # sentno, sentraw = asent\n",
        "  print(f\"\\n\\nMatch #{i}: Sentence #{asentno}\\n\\n\")\n",
        "  sent_highlight = re.sub(Search_for_Substring, Search_for_Substring.upper(), corpus_sents_df.iloc[asentno]['sent_raw'])\n",
        "  print(f'    {sent_highlight}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap_K_gpH0FTm"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHPx5a0xvJYb"
      },
      "source": [
        "Crux_Window_Percent = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Baseline_SMA_Model = \"Stanza\" #@param [\"SentimentR\", \"SyuzhetR\", \"Bing\", \"SenticNet\", \"SentiWord\", \"NRC\", \"AFINN\", \"VADER\", \"TextBlob\", \"Flair\", \"Pattern\", \"Stanza\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Vertical_Labels = True #@param {type:\"boolean\"}\n",
        "Vertical_Labels_Height = -0.1 #@param {type:\"slider\", min:-50, max:50, step:0.1}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Baseline_SMA_Model == 'SentimentR':\n",
        "  model_selected = f'sentimentr'\n",
        "if Baseline_SMA_Model == 'SyuzhetR':\n",
        "  model_selected = f'syuzhet'\n",
        "if Baseline_SMA_Model == 'Bing':\n",
        "  model_selected = f'bing'\n",
        "if Baseline_SMA_Model == 'SenticNet':\n",
        "  model_selected = f'senticnet'\n",
        "if Baseline_SMA_Model == 'SentiWord':\n",
        "  model_selected = f'sentiword'\n",
        "if Baseline_SMA_Model == 'NRC':\n",
        "  model_selected = f'nrc'\n",
        "if Baseline_SMA_Model == 'AFINN':\n",
        "  model_selected = f'afinn'\n",
        "if Baseline_SMA_Model == 'VADER':\n",
        "  model_selected = f'vader'\n",
        "if Baseline_SMA_Model == 'TextBlob':\n",
        "  model_selected = f'textblob'\n",
        "if Baseline_SMA_Model == 'Flair':\n",
        "  model_selected = f'flair'\n",
        "if Baseline_SMA_Model == 'Pattern':\n",
        "  model_selected = f'pattern'\n",
        "if Baseline_SMA_Model == 'Stanza':\n",
        "  model_selected = f'stanza'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_stdscaler_{roll_str}'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_selected_ls = [model_selected_fullname]\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "for amodel in corpus_models_selected_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_df, \n",
        "                                         col_series=corpus_models_selected_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_labels=Vertical_Labels,\n",
        "                                         sec_y_height=Vertical_Labels_Height, \n",
        "                                         subtitle_str= '5% Crux ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False);\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6PU1zR8vJYf"
      },
      "source": [
        "### **Context around Top-n Crux Peaks/Valleys**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUoJz_nyvJYh"
      },
      "source": [
        "# Crux Point Details\n",
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "\n",
        "\n",
        "print(f'Crux Report --------------------\\n')\n",
        "print(f'            Corpus: {CORPUS_FULL}')\n",
        "print(f'            Model: {Baseline_SMA_Model}')\n",
        "print(f'            Crux Win%: {Crux_Window_Percent}')\n",
        "print(f'            SMA Win%: {roll_str}')\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        ts_df = corpus_sents_df,\n",
        "                        library_type='baseline', \n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes,\n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "else:\n",
        "  # import sys\n",
        "  # with open('filename.txt', 'w') as f:\n",
        "  #   print('This message will be written to a file.', file=f)\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJlFM5kFJDdA"
      },
      "source": [
        "asent_no = 124\n",
        "corpus_df = corpus_sents_df\n",
        "asent_raw = str(corpus_df[corpus_df['sent_no'] == int(asent_no)]['sent_raw'].values[0])\n",
        "asent_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-XtE7xovJYj"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1By1LTGvJYk"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  200#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 4 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "\n",
        "# get_sentnocontext_report(ts_df = corpus_sents_df, the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n",
        "get_sentnocontext_report(ts_df=corpus_sents_df, the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmXIEIpY3jrX"
      },
      "source": [
        "### **Select Interactive Sentence Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTD3VvWj3jrY"
      },
      "source": [
        "# Multiple Sentence Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = False #@param {type:\"boolean\"}\n",
        "Bing_Arc = False #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = False #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "NRC_Arc = False #@param {type:\"boolean\"}\n",
        "AFINN_Arc = False #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "# MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "# SentiStrength_Arc = False #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  models_subset_ls.append('sentimentr')\n",
        "if Syuzhet_Arc == True:\n",
        "  models_subset_ls.append('syuzhet')\n",
        "if Bing_Arc == True:\n",
        "  models_subset_ls.append('bing')\n",
        "if SenticNet_Arc == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentiWord_Arc == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if NRC_Arc == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if AFINN_Arc == True:\n",
        "  models_subset_ls.append('afinn')\n",
        "if VADER_Arc == True:\n",
        "  models_subset_ls.append('vader')\n",
        "if TextBlob_Arc == True:\n",
        "  models_subset_ls.append('textblob')\n",
        "if Flair_Arc == True:\n",
        "  models_subset_ls.append('flair')\n",
        "if Pattern_Arc == True:\n",
        "  models_subset_ls.append('pattern')\n",
        "if Stanza_Arc == True:\n",
        "  models_subset_ls.append('stanza')\n",
        "if Mean_All_Arc == True:\n",
        "  models_subset_ls.append('mean_all')\n",
        "\n",
        "plot_models(models_subset_ls, models_type='baseline', text_unit='sent_no', win_per=SMA_Window_Percent)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84HximpOla3K"
      },
      "source": [
        "### **END WORKING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuvAAR2x0x1B"
      },
      "source": [
        "##### **Selected Paragraph Interactive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhh0wT5yGaa0"
      },
      "source": [
        "# Paragraph Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = False #@param {type:\"boolean\"}\n",
        "Bing_Arc = False #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = False #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "NRC_Arc = False #@param {type:\"boolean\"}\n",
        "AFINN_Arc = False #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  models_subset_ls.append('sentimentr')\n",
        "if Syuzhet_Arc == True:\n",
        "  models_subset_ls.append('syuzhet')\n",
        "if Bing_Arc == True:\n",
        "  models_subset_ls.append('bing')\n",
        "if SenticNet_Arc == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentiWord_Arc == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if NRC_Arc == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if AFINN_Arc == True:\n",
        "  models_subset_ls.append('afinn')\n",
        "if VADER_Arc == True:\n",
        "  models_subset_ls.append('vader')\n",
        "if TextBlob_Arc == True:\n",
        "  models_subset_ls.append('textblob')\n",
        "if Flair_Arc == True:\n",
        "  models_subset_ls.append('flair')\n",
        "if Pattern_Arc == True:\n",
        "  models_subset_ls.append('pattern')\n",
        "if Stanza_Arc == True:\n",
        "  models_subset_ls.append('stanza')\n",
        "if Mean_All_Arc == True:\n",
        "  models_subset_ls.append('mean_all')\n",
        "\n",
        "plot_models(models_subset_ls, models_type='baseline', text_unit='parag_no', win_per=SMA_Window_Percent)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYt6fgEsapFj"
      },
      "source": [
        "##### **Selected Section Interactive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "culyTw0tapFk"
      },
      "source": [
        "# Paragraph Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_Arc = False #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = False #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = False #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = False #@param {type:\"boolean\"}\n",
        "NRC_Arc = False #@param {type:\"boolean\"}\n",
        "AFINN_Arc = False #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  models_subset_ls.append('sentimentr')\n",
        "if Syuzhet_Arc == True:\n",
        "  models_subset_ls.append('syuzhet')\n",
        "if Bing_Arc == True:\n",
        "  models_subset_ls.append('bing')\n",
        "if SenticNet_Arc == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentiWord_Arc == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if NRC_Arc == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if AFINN_Arc == True:\n",
        "  models_subset_ls.append('afinn')\n",
        "if VADER_Arc == True:\n",
        "  models_subset_ls.append('vader')\n",
        "if TextBlob_Arc == True:\n",
        "  models_subset_ls.append('textblob')\n",
        "if Flair_Arc == True:\n",
        "  models_subset_ls.append('flair')\n",
        "if Pattern_Arc == True:\n",
        "  models_subset_ls.append('pattern')\n",
        "if Stanza_Arc == True:\n",
        "  models_subset_ls.append('stanza')\n",
        "if Mean_All_Arc == True:\n",
        "  models_subset_ls.append('mean_all')\n",
        "\n",
        "plot_models(models_subset_ls, models_type='baseline', text_unit='sect_no', win_per=SMA_Window_Percent)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRt69D1Zbem0"
      },
      "source": [
        "##### **Chapter SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjlr8LEybem0"
      },
      "source": [
        "# Paragraph Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_Arc = False #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = False #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = False #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = False #@param {type:\"boolean\"}\n",
        "NRC_Arc = False #@param {type:\"boolean\"}\n",
        "AFINN_Arc = False #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  models_subset_ls.append('sentimentr')\n",
        "if Syuzhet_Arc == True:\n",
        "  models_subset_ls.append('syuzhet')\n",
        "if Bing_Arc == True:\n",
        "  models_subset_ls.append('bing')\n",
        "if SenticNet_Arc == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentiWord_Arc == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if NRC_Arc == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if AFINN_Arc == True:\n",
        "  models_subset_ls.append('afinn')\n",
        "if VADER_Arc == True:\n",
        "  models_subset_ls.append('vader')\n",
        "if TextBlob_Arc == True:\n",
        "  models_subset_ls.append('textblob')\n",
        "if Flair_Arc == True:\n",
        "  models_subset_ls.append('flair')\n",
        "if Pattern_Arc == True:\n",
        "  models_subset_ls.append('pattern')\n",
        "if Stanza_Arc == True:\n",
        "  models_subset_ls.append('stanza')\n",
        "if Mean_All_Arc == True:\n",
        "  models_subset_ls.append('mean_all')\n",
        "\n",
        "plot_models(models_subset_ls, models_type='baseline', text_unit='chap_no', win_per=SMA_Window_Percent)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TszUw6HFQ6lU"
      },
      "source": [
        "##### **Comparison of Sentence Baseline Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSNxgw5TjU8L"
      },
      "source": [
        "# Compare Sentence Baseline Length-Normed Standardized Sentiment Values\n",
        "\n",
        "\"\"\"\n",
        "model_baselines_ls = ['sentimentr', 'syuzhet', 'bing',\n",
        "                  'sentiword', 'senticnet', 'nrc',\n",
        "                  'afinn', 'vader', 'textblob',\n",
        "                  'flair', 'pattern', 'stanza']\n",
        "\"\"\";\n",
        "\n",
        "\"\"\"\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# col_roll_ls = []\n",
        "model_base_standardized_roll_ls = []\n",
        "for amodel in model_baselines_ls:\n",
        "  # Create the simple model_rollxxx rolling mean\n",
        "  col_roll = f'{amodel}_{roll_str}'\n",
        "  corpus_sents_df['col_roll'] = corpus_sents_df[amodel].rolling(10*win_s1per, center=True).mean()\n",
        "\n",
        "  # Create list of column names for model_lnorm_medianiqr_rollxxx\n",
        "  # col_name = f'{amodel}_lnorm_medianiqr_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here                                                   # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  col_name = f'{amodel}_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here                                                   # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_base_standardized_roll_ls.append(col_name)\n",
        "\n",
        "  # for i,amodel in enumerate(model_base_standardized_roll_ls):\n",
        "\n",
        "  col_name_roll_stand = f'{col_roll}_stdscale'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_sents_df[col_roll])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_sents_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_sents_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentences for Baseline Model Sentiments\\nMean StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YsG5H1xway0"
      },
      "source": [
        "## **EDA SentimentR Plots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id_VKOPiR7Mg"
      },
      "source": [
        "#### **(If not already exists) Import SentimentR Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTPbrXDM0Gm1"
      },
      "source": [
        "# Verify SentimentR Sentiment Files exported from RStudio\n",
        "!pwd\n",
        "!ls -altr *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlONG-ZwuuVb"
      },
      "source": [
        "# Get SentimentR Sentiment Datafile (with data on 7 Models)\n",
        "\n",
        "SentimentR_sentiment_datafile = 'sum_sentiments_sentimentR_7models_sentimenttimeraw_hpotter1_sorcerersstone.csv' #@param {type:\"string\"}\n",
        "\n",
        "sum_sentiments_sentimentr_filename = SentimentR_sentiment_datafile\n",
        "\n",
        "!head -n 3 $sum_sentiments_sentimentr_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7jOvdLrIkSm"
      },
      "source": [
        "corpus_sentimentr_df = pd.read_csv(sum_sentiments_sentimentr_filename)\n",
        "corpus_sentimentr_df.head(2)\n",
        "corpus_sentimentr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_fV4UKUJTdS"
      },
      "source": [
        "# corpus_sents_df = corpus_sents_df.loc[:, ~corpus_sents_df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "corpus_sentimentr_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n",
        "corpus_sentimentr_df['sent_raw'] = corpus_sentimentr_df['sent_raw'].astype('string')\n",
        "corpus_sentimentr_df.head(2)\n",
        "corpus_sentimentr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJp4DgcIwa7A"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# (Optional) Read Sentiment Series generated in RStudio by SentimentR into DataFrame: corpus_sents_sentimentr_df\n",
        "#            SKIP if no SyuzhetR sentiment datafile to read in\n",
        "\n",
        "# MANUALLY: copy and paste the filename above into the quotes below for sum_sentiment_sentimentR_filename\n",
        "\n",
        "corpus_sentimentr_df = pd.read_csv(sum_sentiments_sentimentr_filename, encoding = 'unicode_escape', engine ='python')\n",
        "\n",
        "# Rename columns if necessary\n",
        "corpus_sentimentr_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n",
        "corpus_sentimentr_df['sent_raw'] = corpus_sentimentr_df['sent_raw'].astype('string')\n",
        "corpus_sentimentr_df['sent_raw'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : re.sub(f'[^{re.escape(string.printable)}]', '', x))\n",
        "corpus_sentimentr_df['sent_raw'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : filter_nonprintable(x))\n",
        "\n",
        "\n",
        "# create a clean version of Sentence in sent_clean column\n",
        "corpus_sentimentr_df['sent_clean'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : clean_text(x))\n",
        "corpus_sentimentr_df['sent_clean'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : re.sub(f'[^{re.escape(string.printable)}]', '', x))\n",
        "corpus_sentimentr_df['sent_clean'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : filter_nonprintable(x))\n",
        "\n",
        "corpus_sentimentr_df['sent_clean'] = corpus_sentimentr_df['sent_clean'].astype('string')\n",
        "\n",
        "corpus_sentimentr_df.head(2)\n",
        "corpus_sentimentr_df.info()\n",
        "corpus_sentimentr_df.columns\n",
        "\n",
        "corpus_sents_sentimentr_len = corpus_sentimentr_df.shape[0]\n",
        "\n",
        "# BUG FIX: SentimentR can create many additional rows that must be deleted \n",
        "#          to enable it to be merged with other Sentiment Models on the same Corpus\n",
        "#          OR just leave SentimentR unmerged and analyze separately (preferred)\n",
        "#\n",
        "# POSSIBLE SOLUTIONS (from worst/easiest to better)\n",
        "#     \n",
        "#   1) Trim extra n rows from head/tail\n",
        "#   2) Naive Downsampling of Series\n",
        "#   3) Clustering and distribute deletes of near-medians from longest runs, avoiding outliers, start/end\n",
        "#   4) DTW character-preserving Compression\n",
        "# \n",
        "#          Simplification, 1D cluster jockers_rinker column as proxy for full interrow distance features\n",
        "#                          and delete rows near the median from the largest cluster (vs taking into account\n",
        "#                          all features in Euclidian or other distance metric)\n",
        "\n",
        "# import kmeans1d\n",
        "\n",
        "# Approximate k cluster number as 1 cluster for every 500 sentences in Corpus\n",
        "# k = corpus_sentimentr_df.shape[0]//500  \n",
        "# clusters, centroids = kmeans1d.cluster(np.array(corpus_sentimentr_df['jockers_rinker']), k)\n",
        "\n",
        "def del_oneincluster(df, cluster_per=1):\n",
        "  '''\n",
        "  TODO: Skip for now and use kmeans1d instead\n",
        "  Given a DataFrame and a Cluster Percent to calculate a sliding window\n",
        "  Return DataFrame with one row removed within a sliding window cluster with most self-similiar rows\n",
        "  '''\n",
        "\n",
        "  # Compute sliding window for cluster size\n",
        "  win_cluster_len = int(cluster_per/100 * df.shape[0])\n",
        "  win_start = 0\n",
        "  win_stop = df.shape[0] - win_cluster_len\n",
        "\n",
        "  # Get numeric columns\n",
        "  numeric_df = df.select_dtypes(include=numerics)\n",
        "\n",
        "  most_selfsimilar_value = 0\n",
        "  most_selfsimilar_index = 0\n",
        "  for i in range(win_start, win_stop, 1):\n",
        "    selfsim_score = selfsim_metric(numeric_df.iloc[i:win_cluster_len+1])\n",
        "    if selfsim_score > most_selfsimilar_value:\n",
        "      most_selfsimilar_index = i\n",
        "\n",
        "  oneless_df = del_onerow(most_selfsimilar_index)\n",
        "\n",
        "  return oneless_df\n",
        "\n",
        "# BAD SOLUTION, just trim the last n rows of corpus_sentimentr_df to make lengths match for merging\n",
        "# corpus_sentimentr_df = corpus_sentimentr_df.iloc[:-n,:]\n",
        "\n",
        "corpus_sentimentr_len = corpus_sentimentr_df.shape[0]\n",
        "if corpus_sentimentr_len != corpus_sents_df.shape[0]:\n",
        "  print('\\n\\n\\n======================================================================\\n')\n",
        "  print(f'ERROR: sentence sentiment values read into corpus_syuzhetr (len={corpus_sents_sentimentr_len})')\n",
        "  print(f'       is not the same length as corpus_sents_df (len={corpus_sents_df.shape[0]}) ')\n",
        "  print(f'\\nRECOMMENDATION: Use the preprocessed corpus output created by this notebook ')\n",
        "  print(f'                as input to SentimentR in RStudio to generate sentiment series')\n",
        "  print(f'                and then retry importing')\n",
        "  print('\\n======================================================================\\n');\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWlzPu0ygMRD"
      },
      "source": [
        "corpus_sentimentr_df.info('sent_raw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgtj_-KthMdS"
      },
      "source": [
        "# Insert Sentence Numbers (sent_no)\n",
        "\"\"\"\n",
        "sent_ct = corpus_sentimentr_df.shape[0]\n",
        "sent_no_ls = list(range(sent_ct))\n",
        "corpus_sentimentr_df.insert(0, 'sent_no', sent_no_ls)\n",
        "corpus_sentimentr_df.head(2)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAiKRKTehzmX"
      },
      "source": [
        "corpus_sentimentr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGxoCc5WgUNt"
      },
      "source": [
        "# Rename columns if necessary\n",
        "corpus_sentimentr_df['sent_raw'] = corpus_sentimentr_df['sent_raw'].astype('string')\n",
        "corpus_sentimentr_df['sent_raw'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : re.sub(f'[^{re.escape(string.printable)}]', '', x))\n",
        "corpus_sentimentr_df['sent_raw'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : filter_nonprintable(x))\n",
        "\n",
        "\n",
        "# create a clean version of Sentence in sent_clean column\n",
        "corpus_sentimentr_df['sent_clean'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : clean_text(x))\n",
        "corpus_sentimentr_df['sent_clean'] = corpus_sentimentr_df['sent_clean'].astype('string')\n",
        "\n",
        "corpus_sentimentr_df.head(2)\n",
        "corpus_sentimentr_df.info()\n",
        "corpus_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bpDK9m_5Ea6"
      },
      "source": [
        "# Add summary statistics\n",
        "\n",
        "corpus_sentimentr_df['char_len'] = corpus_sentimentr_df['sent_clean'].apply(lambda x: len(x))\n",
        "corpus_sentimentr_df['token_len'] = corpus_sentimentr_df['sent_clean'].apply(lambda x: len(x.split())) \n",
        "\n",
        "# Verify\n",
        "\n",
        "corpus_sentimentr_df.head(2)\n",
        "corpus_sentimentr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnLz5_Gzh-DO"
      },
      "source": [
        "# Create 4 Standardized versions of each Model: stdscaler, medianiqr both lnormed and not\n",
        "\n",
        "print('\\nBefore Standardization ----------')\n",
        "corpus_sentimentr_df.columns\n",
        "\n",
        "standardize_ts_ls(corpus_sentimentr_df, models_sentimentr_ls)\n",
        "\n",
        "print('\\nAfter Standardization ----------')\n",
        "corpus_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ0IX-wJ-1fv"
      },
      "source": [
        "# Create SMA roll=10% for all models_stdscaler as baseline\n",
        "\n",
        "win_s1per = int(corpus_sentimentr_df.shape[0] * 1/100)\n",
        "\n",
        "col_stdscaler_roll_ls = []\n",
        "for amodel in models_sentimentr_ls:\n",
        "  col_stdscaler = f'{amodel}_stdscaler'\n",
        "  col_stdscaler_roll = f'{amodel}_stdscaler_{roll_str}'\n",
        "  corpus_sentimentr_df[col_stdscaler_roll] = corpus_sentimentr_df[col_stdscaler].rolling(10*win_s1per, center=True).mean()\n",
        "  col_stdscaler_roll_ls.append(col_stdscaler_roll)\n",
        "\n",
        "col_stdscaler_roll_mean = col_stdscaler_roll + '_mean'\n",
        "corpus_sentimentr_df[col_stdscaler_roll_mean] = corpus_sentimentr_df[col_stdscaler_roll_ls].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpoUy0Dm5Ea8"
      },
      "source": [
        "# Standardize all values with MedianIQR\n",
        "\"\"\"\n",
        "model_sentimentr_ls = ['jockers_rinker', 'jockers', 'huliu', 'lmcd', 'nrc', 'senticnet', 'sentiword']\n",
        "\n",
        "for model_sentimentr in models_sentimentr_ls:\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing Sentiment by Sentence Length\n",
        "  sents_len_ls = list(corpus_sentimentr_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_sentimentr_df[model_sentimentr])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/(sents_len_ls[i]+0.01) for i in range(len(sents_len_ls))]\n",
        "\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_sentimentr_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  col_medianiqr = f'{model_sentimentr}_medianiqr'\n",
        "  corpus_sentimentr_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sentimentr_df[model_sentimentr]).reshape(-1, 1))\n",
        "  col_lnorm_medianiqr = f'{model_sentimentr}_lnorm_medianiqr'\n",
        "  corpus_sentimentr_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\"\"\";\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqrVGplu3txO"
      },
      "source": [
        "#### **Interactive Sentence SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFV8VTo1dtf4"
      },
      "source": [
        "# Sentence Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_JockersRinker = True #@param {type:\"boolean\"}\n",
        "SentimentR_Jockers = True #@param {type:\"boolean\"}\n",
        "SentimentR_HuLiu = True #@param {type:\"boolean\"}\n",
        "SentimentR_SenticNet = True #@param {type:\"boolean\"}\n",
        "SentimentR_SentiWord = True #@param {type:\"boolean\"}\n",
        "SentimentR_NRC = True #@param {type:\"boolean\"}\n",
        "SentimentR_LoughranMcDonald = True #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "\n",
        "if SentimentR_JockersRinker == True:\n",
        "  models_subset_ls.append('jockers_rinker')\n",
        "if SentimentR_Jockers == True:\n",
        "  models_subset_ls.append('jockers')\n",
        "if SentimentR_HuLiu == True:\n",
        "  models_subset_ls.append('huliu')\n",
        "if SentimentR_SenticNet == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentimentR_SentiWord == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if SentimentR_NRC == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if SentimentR_LoughranMcDonald == True:\n",
        "  models_subset_ls.append('lmcd')\n",
        "\n",
        "print(f'models_subset_ls:\\n\\n    {models_subset_ls}')\n",
        "plot_models(models_subset_ls, models_type='sentimentr', text_unit='sent_no', win_per=SMA_Window_Percent);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP-Moy3DdrRr"
      },
      "source": [
        "#### **(ABOVE) Plotly SMA Sentence, (BELOW) Correlation Heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaY7ZYNSm0MM"
      },
      "source": [
        "# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n",
        "# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n",
        "\n",
        "Correlation_Algo = \"kendall\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n",
        "# corr_methods_ls = ['pearson', 'spearman', 'kendall']\n",
        "\n",
        "col_stdscaler_rollwin_ls = []\n",
        "for amodel in models_sentimentr_ls:\n",
        "  col_amodel_stdscaler_rollwin = f'{amodel}_stdscaler_{roll_str}'\n",
        "  col_stdscaler_rollwin_ls.append(col_amodel_stdscaler_rollwin)\n",
        "print(f'col_stdscaler_rollwin_ls: {col_stdscaler_rollwin_ls}')\n",
        "\n",
        "corr_df = corpus_sentimentr_df[col_stdscaler_rollwin_ls].dropna(axis=0, how='any').corr(method=Correlation_Algo)\n",
        "corr_df\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df, # corpus_sents_df[col_stdscaler_rollwin_ls].dropna(axis=0, how='any').corr(method=corr_method),\n",
        "                    row_cluster=True,\n",
        "                    col_cluster=True,\n",
        "                    figsize=(10, 10))\n",
        "\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment for SentimentR Model Sentiments\\n {Correlation_Algo.capitalize()} Correlation - StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yaxyt-MqOxdV"
      },
      "source": [
        "#### **Sentence Sentiment DTW Hierarichal Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S83JA_esuSxc"
      },
      "source": [
        "#### **Top-n Crux Peaks and Valleys**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgETYM8xMGGW"
      },
      "source": [
        "**Search Corpus for Substring**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* In [Search_for_Substring] enter a Substring to search for in the Corpus\n",
        "\n",
        "* Enter a Substring long enough/unique enough so only a reasonable number of Sentences will be returned\n",
        "\n",
        "* Substring can contain spaces/punctuation, for example: 'in the garden'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nyUrE2_L_yr"
      },
      "source": [
        "# Search Corpus Sentences for Substring\n",
        "\n",
        "Search_for_Substring = \"love\" #@param {type:\"string\"}\n",
        "\n",
        "sentno_matching_ls = corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Search_for_Substring, regex=False)]['sent_no']\n",
        "\n",
        "for i, asentno in enumerate(sentno_matching_ls):\n",
        "  # sentno, sentraw = asent\n",
        "  print(f\"\\n\\nMatch #{i}: Sentence #{asentno}\\n\\n\")\n",
        "  sent_highlight = re.sub(Search_for_Substring, Search_for_Substring.upper(), corpus_sents_df.iloc[asentno]['sent_raw'])\n",
        "  print(f'    {sent_highlight}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7TRTSm34mnl"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8SRc-bSKZiO"
      },
      "source": [
        "Crux_Window_Percent = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SentimentR_SMA_Model = \"SentiWord\" #@param [\"Jockers-Rinker\", \"Jockers\", \"Hu-Liu\", \"SenticNet\", \"SentiWord\", \"NRC\", \"Loughan-McDonald\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Vertical_Labels = True #@param {type:\"boolean\"}\n",
        "Vertical_Labels_Height = -0.1 #@param {type:\"slider\", min:-5, max:5, step:0.05}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SentimentR_SMA_Model == 'Jockers-Rinker':\n",
        "  model_selected = f'jockers_rinker'\n",
        "if SentimentR_SMA_Model == 'Jockers':\n",
        "  model_selected = f'jockers'\n",
        "if SentimentR_SMA_Model == 'Hu-Liu':\n",
        "  model_selected = f'huliu'\n",
        "if SentimentR_SMA_Model == 'SenticNet':\n",
        "  model_selected = f'senticnet'\n",
        "if SentimentR_SMA_Model == 'SentiWord':\n",
        "  model_selected = f'sentiword'\n",
        "if SentimentR_SMA_Model == 'NRC':\n",
        "  model_selected = f'nrc'\n",
        "if SentimentR_SMA_Model == 'Loughran-McDonald':\n",
        "  model_selected = f'lmcd'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_stdscaler_{roll_str}'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_selected_ls = [model_selected_fullname]\n",
        "print(f'corpus_models_selected_ls: {corpus_models_selected_ls}')\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "\n",
        "for amodel in corpus_models_selected_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sentimentr_df, \n",
        "                                         col_series=corpus_models_selected_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_labels=Vertical_Labels,\n",
        "                                         sec_y_height=Vertical_Labels_Height, \n",
        "                                         subtitle_str='5% Crux - ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_FRnxmUtRIY"
      },
      "source": [
        "### **Context around Top-n Crux Peaks/Valleys**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FeWNQLqscUy"
      },
      "source": [
        "corpus_sentimentr_df['sent_raw'].isna().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyeQ7Jpfszhq"
      },
      "source": [
        "# Crux Details\n",
        "Get_Peak_Cruxes = False #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 1 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = False #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "  \n",
        "\n",
        "\n",
        "print(f'Crux Report --------------------\\n')\n",
        "print(f'            Corpus: {CORPUS_FULL}')\n",
        "print(f'            Model: {SentimentR_SMA_Model}')\n",
        "print(f'            Crux Win%: {Crux_Window_Percent}')\n",
        "print(f'            SMA Win%: {roll_str}')\n",
        "\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        ts_df = corpus_sentimentr_df,\n",
        "                        library_type='sentimentr',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fHyY7YBsbcn"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYZROB3sgL3q"
      },
      "source": [
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W7iWQErsbco"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  1839#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 0 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "\n",
        "get_sentnocontext_report(ts_df=corpus_sentimentr_df, \n",
        "                         the_sent_no=Crux_Sentence_No, \n",
        "                         the_n_sideparags=No_Paragraphs_on_Each_Side, \n",
        "                         the_sent_highlight=Highlight_Crux_Sentence)\n",
        "\n",
        "\n",
        "# get_sentnocontext_report(the_sent_no=Crux_Sentence_No, \n",
        "#                          the_n_sideparags=No_Paragraphs_on_Each_Side, \n",
        "#                          the_sent_highlight=Highlight_Crux_Sentence)\n",
        "\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lbzxj1zwdC6"
      },
      "source": [
        "## **EDA Syuzhet Plots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1d2Q3zqSES1"
      },
      "source": [
        "#### **(If not already exists) Import SyuzhetR Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npwz74KeliXE"
      },
      "source": [
        "# Verify SentimentR Sentiment Files exported from RStudio\n",
        "!pwd\n",
        "!ls -altr sum_sentiments*syuzhet*.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVZB9Iff0_Nh"
      },
      "source": [
        "# Get SyuzhetR Sentiment Datafile (with data on 4 Models)\n",
        "\n",
        "SyuzhetR_sentiment_datafile = 'sum_sentiments_syuzhetR_4models_sentimenttimeraw_hpotter1_sorcerersstone.csv' #@param {type:\"string\"}\n",
        "\n",
        "sum_sentiments_syuzhetr_filename = SyuzhetR_sentiment_datafile\n",
        "\n",
        "!head -n 3 $sum_sentiments_syuzhetr_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUdqcl13LV-q"
      },
      "source": [
        "corpus_syuzhetr_df = pd.read_csv(sum_sentiments_syuzhetr_filename)\n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuZsrB9TLV-5"
      },
      "source": [
        "# corpus_sents_df = corpus_sents_df.loc[:, ~corpus_sents_df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "corpus_syuzhetr_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n",
        "corpus_syuzhetr_df['sent_raw'] = corpus_syuzhetr_df['sent_raw'].astype('string')\n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfJkHlYp4XGZ"
      },
      "source": [
        "# Add summary statistics\n",
        "\n",
        "corpus_syuzhetr_df['char_len'] = corpus_syuzhetr_df['sent_raw'].apply(lambda x: len(x))\n",
        "corpus_syuzhetr_df['token_len'] = corpus_syuzhetr_df['sent_raw'].apply(lambda x: len(x.split())) \n",
        "\n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ismjoxwCmFvg"
      },
      "source": [
        "# Insert Sentence Numbers (sent_no)\n",
        "\"\"\"\n",
        "sent_ct = corpus_syuzhetr_df.shape[0]\n",
        "sent_no_ls = list(range(sent_ct))\n",
        "corpus_syuzhetr_df.insert(0, 'sent_no', sent_no_ls)\n",
        "corpus_syuzhetr_df.head(2)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTwYKJWZmFvi"
      },
      "source": [
        "# Rename columns if necessary\n",
        "corpus_syuzhetr_df['sent_raw'] = corpus_syuzhetr_df['sent_raw'].astype('string')\n",
        "corpus_syuzhetr_df['sent_raw'] = corpus_syuzhetr_df['sent_raw'].apply(lambda x : re.sub(f'[^{re.escape(string.printable)}]', '', x))\n",
        "corpus_syuzhetr_df['sent_raw'] = corpus_syuzhetr_df['sent_raw'].apply(lambda x : filter_nonprintable(x))\n",
        "\n",
        "\n",
        "# create a clean version of Sentence in sent_clean column\n",
        "corpus_syuzhetr_df['sent_clean'] = corpus_syuzhetr_df['sent_raw'].apply(lambda x : clean_text(x))\n",
        "corpus_syuzhetr_df['sent_clean'] = corpus_syuzhetr_df['sent_clean'].astype('string')\n",
        "\n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()\n",
        "corpus_syuzhetr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ElmJTvD1Ntb"
      },
      "source": [
        "corpus_syuzhetr_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlJ6MvuOEV5o"
      },
      "source": [
        "print(*corpus_syuzhetr_df.columns, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgqSba8FKcMd"
      },
      "source": [
        "# Create 4 Standardized versions of each Model: stdscaler, medianiqr both lnormed and not\n",
        "\n",
        "print('\\nBefore Standardization ----------')\n",
        "corpus_syuzhetr_df.columns\n",
        "\n",
        "standardize_ts_ls(corpus_syuzhetr_df, models_syuzhetr_ls)\n",
        "\n",
        "print('\\nAfter Standardization ----------')\n",
        "corpus_syuzhetr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IDpH20z3lhu"
      },
      "source": [
        "#### **Interactive Sentence SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acNruMABNvte"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# display(corpus_sentimentr_df.head())\n",
        "\n",
        "win_per = SMA_Window_Percent             \n",
        "win_roll = int(win_per/100 * corpus_sentimentr_df.shape[0])\n",
        "\n",
        "# model_syuzhetr_ls = ['syuzhet', 'bing', 'afinn', 'nrc']\n",
        "\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in models_syuzhetr_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  if len(str(win_per)) == 1:\n",
        "    roll_str = 'roll0' + str(win_per)\n",
        "  else:\n",
        "    roll_str = 'roll' + str(win_per)\n",
        "  col_name_roll = f'{amodel}_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  corpus_syuzhetr_df[col_name_roll] = corpus_syuzhetr_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_' + roll_str\n",
        "corpus_syuzhetr_df[col_mean_roll] = corpus_syuzhetr_df[col_name_roll_ls].mean(axis=1)\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Bold)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "# add traces\n",
        "\n",
        "model = 'mean_' + roll_str\n",
        "fig.add_traces(go.Line(x=corpus_syuzhetr_df['sent_no'],\n",
        "                       y = corpus_syuzhetr_df[model],\n",
        "                       line=dict(\n",
        "                            color='#000000',\n",
        "                            width=5\n",
        "                            ),\n",
        "                       text = corpus_syuzhetr_df.index.values,\n",
        "                       name = model,\n",
        "                       hovertemplate = \"Model <b>Mean: \"+str(win_per)+\"%</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                       marker_color=next(palette)))\n",
        "\n",
        "\n",
        "for amodel in models_syuzhetr_ls:\n",
        "  model_roll = f'{amodel}_' + roll_str\n",
        "  fig.add_traces(go.Line(x=corpus_syuzhetr_df['sent_no'],\n",
        "                        y = corpus_syuzhetr_df[model_roll],\n",
        "                        text = corpus_syuzhetr_df['sent_raw'],\n",
        "                        name = model_roll,\n",
        "                        hovertemplate = \"Model <b>\"+model_roll+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b><br>Index: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"SyuzhetR Sentence Sentiment Models <b><i>\" + roll_str.upper() + '</i></b>',\n",
        "    xaxis_title=\"Sentence Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMIE4UF_PlxL"
      },
      "source": [
        "#### **Comparison of Sentence SyuzhetR Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0iHrpqpmnuE"
      },
      "source": [
        "# Compare Sentence SyuzhetR Standardized Sentiment Values\n",
        "\n",
        "# model_syuzhetr_ls = ['syuzhet', 'bing', 'afinn', 'nrc']\n",
        "\n",
        "model_syuzhetr_standardized_roll_ls = []\n",
        "for amodel in models_syuzhetr_ls:\n",
        "  col_roll_name = f'{amodel}_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_syuzhetr_standardized_roll_ls.append(col_roll_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,arollmodel in enumerate(model_syuzhetr_standardized_roll_ls):\n",
        "  print(f'Processing model: {arollmodel}')\n",
        "  col_name_roll_stand = f'{arollmodel}_stdscale'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_syuzhetr_df[arollmodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  print(f'  Adding StdScaler Column: {col_name_roll_stand}')\n",
        "  corpus_syuzhetr_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_syuzhetr_df[col_name_roll_stand].plot(label=arollmodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence SyuzhetR Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aUtGZvXd406"
      },
      "source": [
        "#### **(ABOVE) Plotly SMA Sentence, (BELOW) Correlation Heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onf1ah-vPlAT"
      },
      "source": [
        "# Create a comparison DataFrame of SentimentR Sentence Models\n",
        "# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n",
        "# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n",
        "\n",
        "Correlation_Algo = \"pearson\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n",
        "\n",
        "# syuzhetr_corr_models_ls = ['syuzhet', 'bing', 'afinn', 'nrc']\n",
        "\n",
        "corr_df = corpus_syuzhetr_df[models_syuzhetr_ls].corr(method=Correlation_Algo)\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment for SyuzhetR Model Sentiments\\n {Correlation_Algo.capitalize()} Correlation - StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_C27J8moEZl"
      },
      "source": [
        "#### **Sentence Sentiment DTW Hierarichal Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OemcAFXd6JD5"
      },
      "source": [
        "#### **Top-n Crux Peaks and Valleys**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oGL8mXZNBUw"
      },
      "source": [
        "**Search Corpus for Substring**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* In [Search_for_Substring] enter a Substring to search for in the Corpus\n",
        "\n",
        "* Enter a Substring long enough/unique enough so only a reasonable number of Sentences will be returned\n",
        "\n",
        "* Substring can contain spaces/punctuation, for example: 'in the garden'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMelwTuTNBUx"
      },
      "source": [
        "# Search Corpus Sentences for Substring\n",
        "\n",
        "Search_for_Substring = \"death.\" #@param {type:\"string\"}\n",
        "\n",
        "sentno_matching_ls = corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Search_for_Substring, regex=False)]['sent_no']\n",
        "\n",
        "for i, asentno in enumerate(sentno_matching_ls):\n",
        "  # sentno, sentraw = asent\n",
        "  print(f\"\\n\\nMatch #{i}: Sentence #{asentno}\\n\\n\")\n",
        "  sent_highlight = re.sub(Search_for_Substring, Search_for_Substring.upper(), corpus_sents_df.iloc[asentno]['sent_raw'])\n",
        "  print(f'    {sent_highlight}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "262TERB06JD8"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhmPgogw6JEA"
      },
      "source": [
        "Crux_Window_Percent = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SyuzhetR_SMA_Model = \"Syuzhet\" #@param [\"Syuzhet\", \"Bing\", \"AFINN\", \"NRC\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Vertical_Labels = True #@param {type:\"boolean\"}\n",
        "Vertical_Labels_Height = 0.2 #@param {type:\"slider\", min:-5.0, max:5.0, step:0.05}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SyuzhetR_SMA_Model == 'Syuzhet':\n",
        "  model_selected = f'syuzhet'\n",
        "if SyuzhetR_SMA_Model == 'Bing':\n",
        "  model_selected = f'bing'\n",
        "if SyuzhetR_SMA_Model == 'AFINN':\n",
        "  model_selected = f'afinn'\n",
        "if SyuzhetR_SMA_Model == 'NRC':\n",
        "  model_selected = f'nrc'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_{roll_str}'\n",
        "  print(f'model_selected_fullname: {model_selected_fullname}')\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_selected_ls = [model_selected_fullname]\n",
        "print(f'corpus_models_selected_ls: {corpus_models_selected_ls}')\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "for amodel in corpus_models_selected_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_syuzhetr_df, \n",
        "                                         col_series=corpus_models_selected_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_labels=Vertical_Labels,\n",
        "                                         sec_y_height=Vertical_Labels_Height, \n",
        "                                         subtitle_str='5% Crux - ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "print(f'model_crux_ls: {model_crux_ls}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhx-E08q6JEF"
      },
      "source": [
        "### **Context around Top-n Crux Peaks/Valleys**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxq3Irr36JEK"
      },
      "source": [
        "# Crux Details\n",
        "Get_Peak_Cruxes = False #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "\n",
        "print(f'Crux Report --------------------\\n')\n",
        "print(f'            Corpus: {CORPUS_FULL}')\n",
        "print(f'            Model: {SyuzhetR_SMA_Model}')\n",
        "print(f'            Crux Win%: {Crux_Window_Percent}')\n",
        "print(f'            SMA Win%: {roll_str}')\n",
        "\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        ts_df = corpus_syuzhetr_df,\n",
        "                        library_type='syuzhetr',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FE5BtcJ6JEN"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsfMfACJ6JES"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  200#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 5 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "\n",
        "get_sentnocontext_report(ts_df=corpus_syuzhetr_df, \n",
        "                         the_sent_no=Crux_Sentence_No, \n",
        "                         the_n_sideparags=No_Paragraphs_on_Each_Side, \n",
        "                         the_sent_highlight=Highlight_Crux_Sentence)\n",
        "\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpHVFidu7whr"
      },
      "source": [
        "#### **Compare Sentence SentimentR vs Syuzhet Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdtWV6ViKnWH"
      },
      "source": [
        "roll_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzzmTp93CTiE"
      },
      "source": [
        "corpus_syuzhetr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLwVnxNdBx-W"
      },
      "source": [
        "corpus_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lb8TqPsEN4A"
      },
      "source": [
        "roll_str = 'roll10'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXY05cztTMl_"
      },
      "source": [
        "# Compare Sentence SentimentR vs SyuzhetR SMA smoothed series\n",
        "\"\"\"\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Standardize SentimentR Mean Rolling\n",
        "sentimentr_mean_roll_np = np.array(corpus_sentimentr_df[f'mean_{roll_str}'])\n",
        "sentimentr_mean_roll_np = sentimentr_mean_roll_np.reshape((len(sentimentr_mean_roll_np), 1))\n",
        "\n",
        "scaler = scaler.fit(sentimentr_mean_roll_np)\n",
        "print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "# standardization the dataset and print the first 5 rows\n",
        "sentimentr_mean_roll_norm_np = scaler.transform(sentimentr_mean_roll_np)\n",
        "\n",
        "# Standardize SyuzhetR Mean Rolling\n",
        "syuzhetr_mean_roll_np = np.array(corpus_syuzhetr_df[f'mean_{roll_str}'])\n",
        "syuzhetr_mean_roll_np = syuzhetr_mean_roll_np.reshape((len(syuzhetr_mean_roll_np), 1))\n",
        "\n",
        "scaler = scaler.fit(syuzhetr_mean_roll_np)\n",
        "print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "# standardization the dataset and print the first 5 rows\n",
        "syuzhetr_mean_roll_norm_np = scaler.transform(syuzhetr_mean_roll_np)\n",
        "\n",
        "\n",
        "# Plot normalized Series\n",
        "plt.plot(sentimentr_mean_roll_norm_np, label=\"SentimentR\")\n",
        "plt.plot(syuzhetr_mean_roll_norm_np, label=\"SyuzhetR\")\n",
        "# plt.plot(transformer_mean_roll_norm_np, label=\"SyuzhetR\")\n",
        "\"\"\";\n",
        "\n",
        "# corpus_syuzhetr_df[f'mean_{roll_str}'].apply(lambda x: Scale_SyuzhetR*x).plot(label='SyuzhetR')\n",
        "col_mean_stdscaler_roll = f'mean_stdscaler_{roll_str}'\n",
        "\n",
        "col_all_sentimentr_stdscaler_roll_ls = []\n",
        "for x in models_sentimentr_ls:\n",
        "  col_name = f'{x}_stdscaler_{roll_str}'\n",
        "  col_all_sentimentr_stdscaler_roll_ls.append(col_name)\n",
        "corpus_sentimentr_df[col_mean_stdscaler_roll] = corpus_sentimentr_df[col_all_sentimentr_stdscaler_roll_ls].mean()\n",
        "\n",
        "col_all_syuzhetr_stdscaler_roll_ls = []\n",
        "for x in models_syuzhetr_ls:\n",
        "  col_name = f'{x}_stdscaler_{roll_str}'\n",
        "  col_all_syuzhetr_stdscaler_roll_ls.append(col_name)\n",
        "corpus_syuzhetr_df[col_mean_stdscaler_roll] = corpus_syuzhetr_df[col_all_syuzhetr_stdscaler_roll_ls].mean()\n",
        "\n",
        "# Plot\n",
        "plt.plot(corpus_sentimentr_df[col_mean_stdscaler_roll], label=f\"SentimentR Mean StdScaler {roll_str}\")\n",
        "plt.plot(corpus_syuzhetr_df[col_mean_stdscaler_roll], label=f\"SyuzhetR Mean StdScaler {roll_str}\")\n",
        "\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Sentence SentimentR vs Syuzhet Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twq2IuGz7vlR"
      },
      "source": [
        "# Compare Sentence SentimentR vs SyuzhetR SMA smoothed series\n",
        "# TODO: Delete or convert to fine grained/multi-model DTW/correlation\n",
        "\n",
        "# Create a unified DataFrame of Mean Roll_{win_per} from\n",
        "#     SentimentR and SyuzhetR\n",
        "\n",
        "col_mean_roll = f'mean_{roll_str}'\n",
        "\n",
        "compare_sentimentr_syuzhetr_df = pd.concat([\n",
        "    corpus_sentimentr_df[col_mean_roll],\n",
        "    corpus_syuzhetr_df[col_mean_roll]],\n",
        "    axis=1)\n",
        "\n",
        "col_sentimentr_mean_roll = f'sentimentr_{col_mean_roll}'\n",
        "col_syuzhetr_mean_roll = f'syuzhet_{col_mean_roll}'\n",
        "\n",
        "col_mapping = {\n",
        "    compare_sentimentr_syuzhetr_df.columns[0]:'sentimentr_mean_roll', \n",
        "    compare_sentimentr_syuzhetr_df.columns[1]:'syuzhet_mean_roll'\n",
        "}\n",
        "\n",
        "compare_sentimentr_syuzhetr_df.rename(columns=col_mapping,\n",
        "                                      inplace=True)\n",
        "\n",
        "# compare_sentimentr_syuzhetr_df.iloc[1000:1005]\n",
        "\n",
        "\n",
        "# Get correlation matrix of the comparison DataFrame\n",
        "corr_df = compare_sentimentr_syuzhetr_df.corr(method='spearman')\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yF6RbP2wegA"
      },
      "source": [
        "## **EDA Transformer Plots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8wOUSyOSIVs"
      },
      "source": [
        "#### **(If not already exists) Import Transformer Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5Trl2jZ1Yi8"
      },
      "source": [
        "# Verify SentimentR Sentiment Files exported from RStudio\n",
        "!pwd\n",
        "!ls -altr sum_sentiments*trans*.csv\n",
        "# sum_sentiments_sents_trans_vwoolf_tothelighthouse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnyvNjy91YjF"
      },
      "source": [
        "# Get SyuzhetR Sentiment Datafile (with data on 4 Models)\n",
        "\n",
        "Transformer_sentiment_datafile = 'sum_sentiments_sents_trans_jrowling_thesorcerersstone.csv' #@param {type:\"string\"}\n",
        "\n",
        "sum_sentiments_transformer_filename = Transformer_sentiment_datafile\n",
        "\n",
        "!head -n 3 $sum_sentiments_transformer_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcS5aT6RNLg6"
      },
      "source": [
        "corpus_transformer_df = pd.read_csv(sum_sentiments_transformer_filename)\n",
        "corpus_transformer_df.head(2)\n",
        "corpus_transformer_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWIakgmdoZ9g"
      },
      "source": [
        "corpus_transformer_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "corpus_transformer_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYZ7xFFDQrzD"
      },
      "source": [
        "# Add summary statistics\n",
        "\n",
        "corpus_transformer_df['sent_raw'] = corpus_transformer_df['sent_raw'].astype('string')\n",
        "corpus_transformer_df['sent_clean'] = corpus_transformer_df['sent_raw'].apply(lambda x: clean_text(x))\n",
        "corpus_transformer_df['sent_clean'] = corpus_transformer_df['sent_clean'].astype('string')\n",
        "\n",
        "corpus_transformer_df['char_len'] = corpus_transformer_df['sent_raw'].apply(lambda x: len(x))\n",
        "corpus_transformer_df['token_len'] = corpus_transformer_df['sent_clean'].apply(lambda x: len(x.split())) \n",
        "\n",
        "corpus_transformer_df.head(2)\n",
        "corpus_transformer_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgzWVkAYKqS-"
      },
      "source": [
        "# Create 4 Standardized versions of each Model: stdscaler, medianiqr both lnormed and not\n",
        "\n",
        "print('\\nBefore Standardization ----------')\n",
        "corpus_transformer_df.columns\n",
        "\n",
        "standardize_ts_ls(corpus_transformer_df, models_transformer_ls)\n",
        "\n",
        "print('\\nAfter Standardization ----------')\n",
        "corpus_transformer_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP5laVK8Ee9a"
      },
      "source": [
        "# Create SMA roll=10% for all models_stdscaler as baseline\n",
        "\n",
        "\"\"\"\n",
        "win_s1per = int(corpus_transformer_df.shape[0] * 1/100)\n",
        "\n",
        "col_stdscaler_roll_ls = []\n",
        "for amodel in models_transformer_ls:\n",
        "  col_stdscaler = f'{amodel}_stdscaler'\n",
        "  col_stdscaler_roll = f'{amodel}_stdscaler_{roll_str}'\n",
        "  corpus_transformer_df[col_stdscaler_roll] = corpus_transformer_df[col_stdscaler].rolling(10*win_s1per, center=True).mean()\n",
        "  col_stdscaler_roll_ls.append(col_stdscaler_roll)\n",
        "\n",
        "col_stdscaler_roll_mean = col_stdscaler_roll + '_mean'\n",
        "corpus_transformer_df[col_stdscaler_roll_mean] = corpus_transformer_df[col_stdscaler_roll_ls].mean()\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_iGanTXZSGB"
      },
      "source": [
        "# Standardize all values with MedianIQR\n",
        "\"\"\"\n",
        "# model_transformers_ls = ['nlptown', 'roberta15lg', 'yelp', 'hinglish', 'imdb2way', 'huggingface', 't5imdb50k', 'robertaxml8lang']\n",
        "\n",
        "for model_transformer in models_transformer_ls:\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing Sentiment by Sentence Length\n",
        "  sents_len_ls = list(corpus_transformer_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_transformer_df[model_transformer])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/(sents_len_ls[i]+0.01) for i in range(len(sents_len_ls))]\n",
        "\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_transformer_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  col_medianiqr = f'{model_transformer}_medianiqr'\n",
        "  corpus_transformer_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_transformer_df[model_transformer]).reshape(-1, 1))\n",
        "  col_lnorm_medianiqr = f'{model_transformer}_lnorm_medianiqr'\n",
        "  corpus_transformer_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  # Verify\n",
        "\n",
        "corpus_transformer_df.head()\n",
        "corpus_transformer_df.info()\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNeuepF33Zyu"
      },
      "source": [
        "#### **Interactive Sentence SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfwwefDOXqq8"
      },
      "source": [
        "RoBERTaLg15_Arc = True #@param {type:\"boolean\"}\n",
        "T5IMDB50k_Arc = True #@param {type:\"boolean\"}\n",
        "Huggingface_Arc = True #@param {type:\"boolean\"}\n",
        "NLPTown_Arc = True #@param {type:\"boolean\"}\n",
        "RoBERTaXML8lang_Arc = True #@param {type:\"boolean\"}\n",
        "IMDB2way_Arc = True #@param {type:\"boolean\"}\n",
        "Hinglish_Arc = True #@param {type:\"boolean\"}\n",
        "Yelp_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = True #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viS52Ls6vrmo"
      },
      "source": [
        "models_transformer_ls\n",
        "corpus_transformer_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqYU6I5uWeGc"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "win_per = SMA_Window_Percent\n",
        "win_roll = int(win_per/100 * corpus_transformer_df.shape[0])\n",
        "\n",
        "if len(str(win_per)) == 1:\n",
        "  roll_str = 'roll0' + str(win_per)\n",
        "else:\n",
        "  roll_str = 'roll' + str(win_per)\n",
        "\n",
        "# display(corpus_transformer_df.head())\n",
        "\n",
        "\"\"\"\n",
        "model_transformers_ls = ['nlptown', 'roberta15lg',\n",
        "                         'yelp', 'hinglish',\n",
        "                         'imdb2way', 'huggingface',\n",
        "                         't5imdb50k', 'robertaxml8lang']\n",
        "\"\"\"\n",
        "\n",
        "# list of (scale, center) adjustments for each model so they can be compared on same graph\n",
        "# Scaling Dictionary for each plot in form of tuple (scale, center) \n",
        "#     so they can be compared on same graph\n",
        "model_transformers_scale_dt = {'nlptown' : (0.5, -1),\n",
        "                               'roberta15lg' : (1,0),\n",
        "                               'yelp' : (1.0, 0.5),\n",
        "                               'hinglish' : (1.0, 0.5),\n",
        "                               'imdb2way' : (1.0, 0.5),\n",
        "                               'huggingface' : (1.0, 0.5),\n",
        "                               't5imdb50k' : (1.0, 0.5),\n",
        "                               'robertxml8lang' : (1.0, 0.5)}\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in models_transformer_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  # col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "  col_name_roll = f'{amodel}_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  # print(f'creating: {col_name_roll}')\n",
        "  corpus_transformer_df[col_name_roll] = corpus_transformer_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "  col_name_roll_stdscale = f'{col_name_roll}_stdscale'\n",
        "  corpus_transformer_df[col_name_roll_stdscale] = get_standardscaler(col_name_roll, corpus_transformer_df[col_name_roll])\n",
        "\n",
        "\n",
        "col_mean_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "# model_transformers_lnorm_medianiqr_ls = []\n",
        "corpus_transformer_df[col_mean_roll] = corpus_transformer_df[col_name_roll_ls].mean(axis=1)\n",
        "\n",
        "col_mean_lnorm_median_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "model_transformers_lnorm_medianiqr_ls = []\n",
        "for acol_name in models_transformer_ls:\n",
        "  # model_transformers_lnorm_medianiqr_ls.append(acol_name+'_lnorm_medianiqr_'+roll_str)\n",
        "  model_transformers_lnorm_medianiqr_ls.append(acol_name+ '_' +roll_str + '_stdscale')\n",
        "corpus_transformer_df[col_mean_lnorm_median_roll] = corpus_transformer_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\n",
        "\n",
        "# display(corpus_transformer_df.head())\n",
        "\n",
        "\n",
        "model_transformers_subset_ls = []\n",
        "\n",
        "\n",
        "if NLPTown_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'nlptown_{roll_str}_stdscale')\n",
        "if T5IMDB50k_Arc == True:\n",
        "  model_transformers_subset_ls.append(f't5imdb50k_{roll_str}_stdscale')\n",
        "if Huggingface_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'huggingface_{roll_str}_stdscale')\n",
        "if RoBERTaLg15_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'roberta15lg_{roll_str}_stdscale')\n",
        "if RoBERTaXML8lang_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'robertaxml8lang_{roll_str}_stdscale')\n",
        "  #                                     robertaxml8lang_roll100_stdscale\n",
        "if IMDB2way_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'imdb2way_{roll_str}_stdscale')\n",
        "if Hinglish_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'hinglish_{roll_str}_stdscale')\n",
        "if Yelp_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'yelp_{roll_str}_stdscale')\n",
        "\n",
        "\"\"\"\n",
        "# Manually adjust for custom French Models\n",
        "\n",
        "model_transformers_subset_ls = ['flaubert_roll10_stdscale', \n",
        "                                'nlptown_roll10_stdscale', \n",
        "                                'robertaxml8lang_roll10_stdscale']\n",
        "\"\"\"\n",
        "\n",
        "for i,amodel in enumerate(model_transformers_subset_ls):\n",
        "  print(f'Plot model: {amodel}')\n",
        "  # corpus_transformer_df[amodel].plot()\n",
        "\n",
        "print(f'model_transformers_subset_ls: {model_transformers_subset_ls}')\n",
        "\n",
        "\"\"\"\n",
        "col_mean_subset_roll = f'mean_subset_{roll_str}'\n",
        "corpus_transformer_df[col_mean_subset_roll] = corpus_transformer_df[model_transformers_subset_ls].mean(axis=1)\n",
        "if Mean_Subset_Arc == True:\n",
        "  model_transformers_subset_ls.append(col_mean_subset_roll)\n",
        "\"\"\";\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Bold)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "# add traces\n",
        "# old: y = model_transformers_subset_ls[i][0]*corpus_transformer_df[amodel]+model_baselines_scale_ls[i][1],\n",
        "for amodel in model_transformers_subset_ls:\n",
        "  # print(f'Plot model: {amodel}')\n",
        "  # corpus_transformer_df[amodel].plot()\n",
        "  fig.add_traces(go.Line(x = corpus_transformer_df['sent_no'],\n",
        "                        y = corpus_transformer_df[amodel],\n",
        "                        text = corpus_transformer_df['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model: <b>\"+amodel+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "if Mean_Subset_Arc == True:\n",
        "  mean_subset_col = 'mean_subset_'+roll_str\n",
        "  corpus_transformer_df[mean_subset_col] = corpus_transformer_df[model_transformers_subset_ls].mean(axis=1)\n",
        "  fig.add_traces(go.Line(x=corpus_transformer_df['sent_no'],\n",
        "                        y = 0.1*corpus_transformer_df[mean_subset_col],\n",
        "                        line=dict(\n",
        "                              # color='#000000',\n",
        "                              width=5\n",
        "                              ),\n",
        "                        text = 'NA', # corpus_transformer_df['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model <b>%{mean_subset_col}</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\"\"\";\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Transformer Sentence Sentiment Models <b><i>\" + roll_str.upper() + '</i></b>',\n",
        "    xaxis_title=\"Sentence Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UWlGJ1IQG9t"
      },
      "source": [
        "#### **Comparison of Sentence Transformer Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiUBs2-go_yi"
      },
      "source": [
        "# Compare Sentence Transformer Standardized Sentiment Values\n",
        "\n",
        "# model_syuzhetr_ls = ['syuzhet', 'bing', 'afinn', 'nrc']\n",
        "\n",
        "model_transformer_standardized_roll_ls = []\n",
        "for amodel in models_transformer_ls:\n",
        "  col_roll_name = f'{amodel}_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_transformer_standardized_roll_ls.append(col_roll_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,arollmodel in enumerate(model_transformer_standardized_roll_ls):\n",
        "  print(f'Processing model: {arollmodel}')\n",
        "  col_name_roll_stand = f'{arollmodel}_stdscale'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_transformer_df[arollmodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  print(f'  Adding StdScaler Column: {col_name_roll_stand}')\n",
        "  corpus_transformer_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_transformer_df[col_name_roll_stand].plot(label=arollmodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Transformer Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16QqsYHTdYqY"
      },
      "source": [
        "#### **(ABOVE) Plotly SMA Sentence, (BELOW) Correlation Heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD3H_yQcLu8O"
      },
      "source": [
        "# Create a comparison DataFrame of Transformer Sentence Models\n",
        "# Create a comparison DataFrame of Transformer Sentence Models\n",
        "# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n",
        "# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n",
        "\n",
        "Correlation_Algo = \"kendall\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n",
        "\n",
        "Correlation_Algo\n",
        "\n",
        "corr_df = corpus_transformer_df[models_transformer_ls].corr(method=Correlation_Algo)\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment for Transformer Model Sentiments\\n {Correlation_Algo.capitalize()} Correlation - StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkaD9lRIpsYV"
      },
      "source": [
        "#### **Sentence Sentiment DTW Hierarichal Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqnwt6ygQJws"
      },
      "source": [
        "# Compare Sentence Transformer Standardized Sentiment Values\n",
        "\"\"\"\n",
        "model_transformers_ls = ['nlptown', 'roberta15lg',\n",
        "                         'yelp', 'hinglish',\n",
        "                         'imdb2way', 'huggingface',\n",
        "                         't5imdb50k', 'robertaxml8lang']\n",
        "\n",
        "model_trans_standardized_roll_ls = []\n",
        "for amodel in model_transformers_ls:\n",
        "  col_name = f'{amodel}_{roll_str}_stdscale'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_trans_standardized_roll_ls.append(col_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,amodel in enumerate(model_trans_standardized_roll_ls):\n",
        "  col_name_roll_stand = f'{col_name}_stand'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_transformer_df[amodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_transformer_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_transformer_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Transformer Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhnAu46O7aq_"
      },
      "source": [
        "#### **Top-n Crux Peaks and Valleys**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpKbmbsCNFI7"
      },
      "source": [
        "**Search Corpus for Substring**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* In [Search_for_Substring] enter a Substring to search for in the Corpus\n",
        "\n",
        "* Enter a Substring long enough/unique enough so only a reasonable number of Sentences will be returned\n",
        "\n",
        "* Substring can contain spaces/punctuation, for example: 'in the garden'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b35u0YDONFI9"
      },
      "source": [
        "# Search Corpus Sentences for Substring\n",
        "\n",
        "Search_for_Substring = \"love\" #@param {type:\"string\"}\n",
        "\n",
        "sentno_matching_ls = corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Search_for_Substring, regex=False)]['sent_no']\n",
        "\n",
        "for i, asentno in enumerate(sentno_matching_ls):\n",
        "  # sentno, sentraw = asent\n",
        "  print(f\"\\n\\nMatch #{i}: Sentence #{asentno}\\n\\n\")\n",
        "  sent_highlight = re.sub(Search_for_Substring, Search_for_Substring.upper(), corpus_sents_df.iloc[asentno]['sent_raw'])\n",
        "  print(f'    {sent_highlight}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTFCZI667arB"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSmeqWhSx8Wt"
      },
      "source": [
        "models_transformer_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xeZ5Qhb7arB"
      },
      "source": [
        "Crux_Window_Percent = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Transformer_SMA_Model = \"RoBERTa XML 8 Languages\" #@param [\"RoBERTa Large 15 Databases\", \"NLPTown\", \"Yelp\", \"Hinglish\", \"IMDB 2 Sentiment\", \"Huggingface\", \"T5 IMDB 50k\", \"RoBERTa XML 8 Languages\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Vertical_Labels = True #@param {type:\"boolean\"}\n",
        "Vertical_Labels_Height = -1.7 #@param {type:\"slider\", min:-50, max:50, step:0.1}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Transformer_SMA_Model == 'RoBERTa Large 15 Databases':\n",
        "  model_selected = f'roberta15lg'\n",
        "if Transformer_SMA_Model == 'NLPTown':\n",
        "  model_selected = f'nlptown'\n",
        "if Transformer_SMA_Model == 'Yelp':\n",
        "  model_selected = f'yelp'\n",
        "if Transformer_SMA_Model == 'Hinglish':\n",
        "  model_selected = f'hinglish'\n",
        "if Transformer_SMA_Model == 'IMDB 2 Sentiment':\n",
        "  model_selected = f'imdb2way'\n",
        "if Transformer_SMA_Model == 'Huggingface':\n",
        "  model_selected = f'huggingface'\n",
        "if Transformer_SMA_Model == 'T5 IMDB 50k':\n",
        "  model_selected = f't5imdb50k'\n",
        "if Transformer_SMA_Model == 'RoBERTa XML 8 Languages':\n",
        "  model_selected = f'robertaxml8lang'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_{roll_str}_stdscale'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_selected_ls = [model_selected_fullname]\n",
        "print(f'corpus_models_selected_ls: {corpus_models_selected_ls}')\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "\n",
        "for amodel in corpus_models_selected_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_transformer_df, \n",
        "                                         col_series=corpus_models_selected_ls, \n",
        "                                         # Manually adjust for special French Models\n",
        "                                         # col_series=['robertaxml8lang_roll10_stdscale'], \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_labels=Vertical_Labels,\n",
        "                                         sec_y_height=Vertical_Labels_Height, \n",
        "                                         subtitle_str='5% Crux - ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdGGia83NqcF"
      },
      "source": [
        "corpus_sentimentr_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utHumiBkNgeT"
      },
      "source": [
        "corpus_transformer_df.shape\n",
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P34IC3Kh7arD"
      },
      "source": [
        "### **Context around Top-n Crux Peaks/Valleys**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99RhVdHc7arE"
      },
      "source": [
        "# Crux Point Details\n",
        "Get_Peak_Cruxes = False #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "\n",
        "print(f'Crux Report --------------------\\n')\n",
        "print(f'            Corpus: {CORPUS_FULL}')\n",
        "print(f'            Model: {Transformer_SMA_Model}')\n",
        "print(f'            Crux Win%: {Crux_Window_Percent}')\n",
        "print(f'            SMA Win%: {roll_str}')\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        ts_df = corpus_transformer_df,\n",
        "                        library_type='transformer',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "\n",
        "\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y90bCSB7arF"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFi5kyx-7arG"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  1047#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "\n",
        "# get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "\n",
        "get_sentnocontext_report(ts_df=corpus_transformer_df, \n",
        "                         the_sent_no=Crux_Sentence_No, \n",
        "                         the_n_sideparags=No_Paragraphs_on_Each_Side, \n",
        "                         the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xv7Ah7zVpux"
      },
      "source": [
        "### **Stop Here (Paragraph Under Construction)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRjHsOIpXovL"
      },
      "source": [
        "##### **Paragraph Transformers SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBrSjSA4Qytu"
      },
      "source": [
        "# (Optional) Read Paragraph Sentiment Data generated by Transformers into DataFrame: corpus_parags_trans_df\n",
        "#            SKIP if no Transformer Paragraph Sentiment datafile to read in\n",
        "\n",
        "sum_sentiment_parags_transformers_filename = 'sum_sentiments_sents_transformers_ianmcewan_machineslikeme.csv'\n",
        "corpus_parags_trans_df = pd.read_csv(sum_sentiment_parags_transformers_filename)\n",
        "\n",
        "# Optional columns to drop\n",
        "corpus_parags_trans_df.drop(columns=['bertsst_pol', 'bertsst_prob'], axis=1, inplace=True)\n",
        "\n",
        "corpus_parags_trans_df.head(2)\n",
        "corpus_parags_trans_df.info()\n",
        "corpus_parags_trans_df.columns\n",
        "\n",
        "\"\"\"\n",
        "corpus_parags_trans_len = corpus_parags_trans_df.shape[0]\n",
        "\n",
        "if corpus_parags_trans_len != corpus_sents_df.shape[0]:\n",
        "  print('\\n\\n\\n======================================================================\\n')\n",
        "  print(f'ERROR: sentence sentiment values read into corpus_syuzhetr (len={corpus_transformers_len})')\n",
        "  print(f'       is not the same length as corpus_parags_trans_df (len={corpus_parags_trans_df.shape[0]}) ')\n",
        "  print(f'\\nRECOMMENDATION: Use the preprocessed corpus output created by this notebook ')\n",
        "  print(f'                as input to SyuzhetR in RStudio to generate sentiment series')\n",
        "  print(f'                and then retry importing')\n",
        "  print('\\n======================================================================\\n')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h1hIcSobYfQ"
      },
      "source": [
        "# Standardize all values with MedianIQR\n",
        "\n",
        "model_transformers_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "\n",
        "for model_transformer in model_transformers_ls:\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "  parags_len_ls = list(corpus_parags_trans_df['token_len'])\n",
        "  parags_sentiment_ls = list(corpus_parags_trans_df[model_transformer])\n",
        "  parags_sentiment_norm_ls = [parags_sentiment_ls[i]/parags_len_ls[i] for i in range(len(parags_len_ls))]\n",
        "\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_parags_trans_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  col_medianiqr = f'{model_transformer}_medianiqr'\n",
        "  corpus_parags_trans_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_parags_trans_df[model_transformer]).reshape(-1, 1))\n",
        "  col_lnorm_medianiqr = f'{model_transformer}_lnorm_medianiqr'\n",
        "  corpus_parags_trans_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2ee-U3HUbre"
      },
      "source": [
        "# Calculate Transformer Rolling Windows = win_per of Corpus (default 5%)\n",
        "\n",
        "\n",
        "\n",
        "win_per = 10           \n",
        "win_roll = int(win_per/100 * corpus_parags_trans_df.shape[0])\n",
        "\n",
        "# model_transformers_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "model_transformers_ls = ['nlptown_lnorm_medianiqr', 'robertalg15_lnorm_medianiqr', 'distillbertsst_lnorm_medianiqr', 'bertsst_lnorm_medianiqr']\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_transformers_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  col_name_roll = f'{amodel}_roll050'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  corpus_parags_trans_df[col_name_roll] = corpus_parags_trans_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "corpus_parags_trans_df['mean_all_roll050'] = corpus_parags_trans_df[col_name_roll_ls].mean(axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Kh--NklUbmp"
      },
      "source": [
        "NLPTown_Arc = True #@param {type:\"boolean\"}\n",
        "RoBERTaLg15_Arc = True #@param {type:\"boolean\"}\n",
        "DistillBERTSST_Arc = True #@param {type:\"boolean\"}\n",
        "BERTSST_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = True #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr4Wqoh36V29"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_transformers_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  if len(str(win_per)) == 1:\n",
        "    roll_str = 'roll0' + str(win_per) + '0'\n",
        "  else:\n",
        "    roll_str = 'roll' + str(win_per) + '0'\n",
        "  col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  corpus_sents_trans_df[col_name_roll] = corpus_sents_trans_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_' + roll_str\n",
        "model_transformers_lnorm_medianiqr_ls = []\n",
        "corpus_sents_trans_df[col_mean_roll] = corpus_sents_trans_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\n",
        "col_mean_lnorm_median_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "model_transformers_lnorm_medianiqr_ls = []\n",
        "for acol_name in model_transformers_ls:\n",
        "  model_transformers_lnorm_medianiqr_ls.append(acol_name+'_lnorm_medianiqr_'+roll_str)\n",
        "corpus_sents_trans_df[col_mean_lnorm_median_roll] = corpus_sents_trans_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IVvASZFUbdJ"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "win_per = SMA_Window_Percent\n",
        "win_roll = int(win_per/100 * corpus_parags_trans_df.shape[0])\n",
        "\n",
        "if len(str(win_per)) == 1:\n",
        "  roll_str = 'roll0' + str(win_per)\n",
        "else:\n",
        "  roll_str = 'roll' + str(win_per)\n",
        "\n",
        "# display(corpus_sents_df.head())\n",
        "\n",
        "model_transformers_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "\n",
        "# list of (scale, center) adjustments for each model so they can be compared on same graph\n",
        "model_transformers_scale_ls = [(0.3, -0.6), (1,0.1), (1,0.1), (1,0.1)]\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_transformers_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  print(f'creating: {col_name_roll}')\n",
        "  corpus_parags_trans_df[col_name_roll] = corpus_parags_trans_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "# model_transformers_lnorm_medianiqr_ls = []\n",
        "corpus_parags_trans_df[col_mean_roll] = corpus_parags_trans_df[col_name_roll_ls].mean(axis=1)\n",
        "\n",
        "\"\"\"\n",
        "col_mean_lnorm_median_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "# model_transformers_lnorm_medianiqr_ls = []\n",
        "for acol_name in model_transformers_ls:\n",
        "  model_transformers_lnorm_medianiqr_ls.append(acol_name)\n",
        "corpus_parags_trans_df[col_mean_lnorm_median_roll] = corpus_parags_trans_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\"\"\";\n",
        "\n",
        "\n",
        "model_transformers_subset_ls = []\n",
        "if NLPTown_Arc == True:\n",
        "  # model_transformers_subset_ls.append('nlptown_lnorm_medianiqr_roll050')\n",
        "  model_transformers_subset_ls.append('nlptown_lnorm_medianiqr' + '_' + roll_str)\n",
        "if RoBERTaLg15_Arc == True:\n",
        "  model_transformers_subset_ls.append('robertalg15_lnorm_medianiqr' +'_' + roll_str)\n",
        "if DistillBERTSST_Arc == True:\n",
        "  model_transformers_subset_ls.append('distillbertsst_lnorm_medianiqr' + '_' + roll_str)\n",
        "if BERTSST_Arc == True:\n",
        "  model_transformers_subset_ls.append('bertsst_lnorm_medianiqr' + '_' + roll_str)\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Safe)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "# add traces\n",
        "for i,amodel in enumerate(model_transformers_subset_ls):\n",
        "  fig.add_traces(go.Line(x = corpus_parags_trans_df['sent_no'],\n",
        "                        y = model_transformers_scale_ls[i][0]*corpus_parags_trans_df[amodel]+model_transformers_scale_ls[i][1],\n",
        "                        text = corpus_parags_trans_df['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model: <b>\"+amodel+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "if Mean_Subset_Arc == True:\n",
        "  mean_subset_col = 'mean_subset_roll050'\n",
        "  corpus_parags_trans_df[mean_subset_col] = corpus_parags_trans_df[model_transformers_subset_ls].mean(axis=1)\n",
        "  fig.add_traces(go.Line(x=corpus_parags_trans_df['sent_no'],\n",
        "                        y = corpus_parags_trans_df[mean_subset_col],\n",
        "                        line=dict(\n",
        "                              # color='#000000',\n",
        "                              width=5\n",
        "                              ),\n",
        "                        text = 'NA', # corpus_parags_trans_df['sent_raw'],\n",
        "                        name = mean_subset_col,\n",
        "                        hovertemplate = \"Model <b>%{mean_subset_col}</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Paragraph Transformer Sentiment Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "    xaxis_title=\"Paragraph Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89X8Sz2QdMn-"
      },
      "source": [
        "##### **(ABOVE) Plotly SMA Paragraph Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmbxdtyvQo3K"
      },
      "source": [
        "##### **Comparison of Paragraph Transformer Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIaxX1dEbvkE"
      },
      "source": [
        "# Compare Paragraph Transformer Standardized Sentiment Values\n",
        "\n",
        "model_trans_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "\n",
        "model_trans_standardized_roll_ls = []\n",
        "for amodel in model_trans_ls:\n",
        "  col_name = f'{amodel}_lnorm_medianiqr_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_trans_standardized_roll_ls.append(col_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,amodel in enumerate(model_trans_standardized_roll_ls):\n",
        "  col_name_roll_stand = f'{col_name}_stand'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')  # sent\n",
        "  model_roll_stand_np = np.array(corpus_parags_trans_df[amodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_parags_trans_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_parags_trans_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Paragraph Transformer Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtxzLN7CQo3R"
      },
      "source": [
        "# Create a comparison DataFrame of SentimentR Paragraph Models\n",
        "\n",
        "corr_transformers_models_ls = ['nlptown','robertalg15', 'distillbertsst','bertsst']\n",
        "\n",
        "corr_transformers_df = corpus_parags_trans_df[corr_transformers_models_ls].corr(method='spearman')\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dowF8jrw67W7"
      },
      "source": [
        "# Quick Sentence vs Paragraph Transformer Sentiment SMA Comparison\n",
        "\n",
        "plt.close()\n",
        "\n",
        "# Colab Jupyter wouldn't plot pd.Series from 2 different DataFrames on the same graph\n",
        "#   so combine into temporary DataFrame as a workaround\n",
        "\n",
        "temp_df = pd.DataFrame(columns = ['Sentences', 'Paragraphs'])\n",
        "\n",
        "y_col = f'mean_lnorm_medianiqr_{roll_str}'\n",
        "temp_df['Sentences'] = corpus_sents_trans_df[y_col]\n",
        "temp_df['Paragraphs'] = corpus_parags_trans_df[y_col]\n",
        "\n",
        "temp_df['Sentences'].plot(linewidth=10)\n",
        "temp_df['Paragraphs'].plot()\n",
        "\n",
        "\n",
        "# plt.plot(corpus_sents_trans_df[y_col], label='Sentences')\n",
        "\n",
        "# y_col = f'mean_lnorm_medianiqr_{roll_str}'\n",
        "# plt.plot(corpus_parags_trans_df[y_col], label='Paragraphs')\n",
        "\n",
        "# plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Paragraph Transformer Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJIK_j3wbx97"
      },
      "source": [
        "corpus_sents_trans_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZorifAHe7ye"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5CazR5rY_u2"
      },
      "source": [
        "##### **Explore Paragraph Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHihplfSY_u4"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvndYD2CY_u6"
      },
      "source": [
        "Crux_Window_Percent = 2 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SentimentR_SMA_Model = \"RoBERTa Large 15 Databases\" #@param [\"NLPTown\", \"RoBERTa Large 15 Databases\", \"Distilled BERT SST\", \"BERT SST\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SentimentR_SMA_Model == 'NLPTown':\n",
        "  model_selected = f'nlptown'\n",
        "if SentimentR_SMA_Model == 'RoBERTa Large 15 Databases':\n",
        "  model_selected = f'robertalg15'\n",
        "if SentimentR_SMA_Model == 'Distilled BERT SST':\n",
        "  model_selected = f'distillbertsst'\n",
        "if SentimentR_SMA_Model == 'BERT SST':\n",
        "  model_selected = f'bertsst'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_lnorm_medianiqr_{roll_str}'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_stand_ls = [model_selected_fullname]\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "for amodel in corpus_models_stand_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_trans_df, \n",
        "                                         col_series=corpus_models_stand_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_height=0, \n",
        "                                         subtitle_str=' ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR0h5f7bY_u8"
      },
      "source": [
        "**Get Top-n Crux Peaks/Valleys with surrounding Context**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beSU4keNY_u-"
      },
      "source": [
        "# Crux Details\n",
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 6 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        library_type='transformers',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG8t02zqY_vA"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Sm942CY_vB"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  4494#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISIf0aUPxe6y"
      },
      "source": [
        "# **Compare All Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAx6TUxysYoG"
      },
      "source": [
        "## **Review 4 Sentiment Model Groups**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zURjifUsh3t"
      },
      "source": [
        "**Process**\n",
        "\n",
        "NOTE: Assume only base_model Raw Sentiment Series exist in each of the 4 Library DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml635x83yFUp"
      },
      "source": [
        "### **12 Baseline Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk86ubNB_Hzn"
      },
      "source": [
        "# StandardScaler SMA for Baseline Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "\n",
        "corpus_plots_std_df = pd.DataFrame()\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_sents_df.shape[0]*win_per/100)\n",
        "\n",
        "baseline_plots_std_ls = []\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_baseline_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_sents_df[col_sma_winper] = corpus_sents_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{col_sma_winper}_stdscale'\n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_sents_df[col_sma_winper])\n",
        "  corpus_sents_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "  corpus_plots_std_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "\n",
        "  # Plot\n",
        "  # corpus_sents_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  print(f'       Plotting: {col_sma_winper_stdscale}\\n')\n",
        "  corpus_sents_df[col_sma_winper_stdscale].plot()\n",
        "  baseline_plots_std_ls.append(col_sma_winper_stdscale)\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Baseline 12 Models\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKyNxuUmyH4z"
      },
      "source": [
        "### **7 SentimentR Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYh8EvZKqGPY"
      },
      "source": [
        "# StandardScaler SMA for SentimentR Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_sentimentr_df.shape[0]*win_per/100)\n",
        "\n",
        "sentimentr_plots_std_ls = []\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_sentimentr_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_sentimentr_df[col_sma_winper] = corpus_sentimentr_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{amodel}_stdscaler_{roll_str}'\n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_sentimentr_df[col_sma_winper])\n",
        "  corpus_sentimentr_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "  corpus_plots_std_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "\n",
        "  # Plot\n",
        "  # corpus_sentimentr_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  print(f'       Plotting: {col_sma_winper_stdscale}\\n')\n",
        "  corpus_sentimentr_df[col_sma_winper_stdscale].plot()\n",
        "  sentimentr_plots_std_ls.append(col_sma_winper_stdscale)\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence SentimentR 7 Models\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iGbNzPGyK8-"
      },
      "source": [
        "### **4 SyuzhetR Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnUuECApALc9"
      },
      "source": [
        "# StandardScaler SMA for SyuzhetR Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_syuzhetr_df.shape[0]*win_per/100)\n",
        "\n",
        "syuzhetr_plots_std_ls = []\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_syuzhetr_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_syuzhetr_df[col_sma_winper] = corpus_syuzhetr_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{amodel}_stdscaler_{roll_str}'\n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_syuzhetr_df[col_sma_winper])\n",
        "  corpus_syuzhetr_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "  corpus_plots_std_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "  \n",
        "  # Plot\n",
        "  # corpus_syuzhetr_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  print(f'       Plotting: {col_sma_winper_stdscale}\\n')\n",
        "  corpus_syuzhetr_df[col_sma_winper_stdscale].plot()\n",
        "  syuzhetr_plots_std_ls.append(col_sma_winper_stdscale)\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Syuzhet 4 Models\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3iWkOCuyNB3"
      },
      "source": [
        "### **8 Transformer Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SN6gRLJALZP"
      },
      "source": [
        "# StandardScaler SMA for Transformer Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "\"\"\"\n",
        "models_transformer_ls = ['roberta15lg',\n",
        "                         'nlptown',\n",
        "                         'yelp',\n",
        "                         'hinglish',\n",
        "                         'imdb2way',\n",
        "                         'huggingface',\n",
        "                         't5imdb50k',\n",
        "                         'robertaxml8lang']\n",
        "\"\"\"\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_transformer_df.shape[0]*win_per/100)\n",
        "\n",
        "transformer_plots_std_ls = []\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_transformer_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_transformer_df[col_sma_winper] = corpus_transformer_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{amodel}_stdscaler_{roll_str}'\n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_transformer_df[col_sma_winper])\n",
        "  corpus_transformer_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "  corpus_plots_std_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "\n",
        "  # Plot\n",
        "  # corpus_transformer_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  print(f'       Plotting: {col_sma_winper_stdscale}\\n')\n",
        "  corpus_transformer_df[col_sma_winper_stdscale].plot()\n",
        "  transformer_plots_std_ls.append(col_sma_winper_stdscale)\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Transformer 8 Models\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\"\"\"\n",
        "for agroup in groups_ls:\n",
        "  for amodel in globals()[agroup]:\n",
        "    print(f'Processing Model: {amodel:>15} in Group: {agroup}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09fTREaV0Afb"
      },
      "source": [
        "## **Combine ALL 31 Sentence Models into Unified DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v4Tm6pXe-Zt"
      },
      "source": [
        "# Verfiy Consistent DataFrame across all 4 Libraries\n",
        "\n",
        "print(f'   Baseline Sentences shape: {corpus_sents_df.shape}')\n",
        "print(f' SentimentR Sentences shape: {corpus_sentimentr_df.shape}')\n",
        "print(f'   SyuzhetR Sentences shape: {corpus_syuzhetr_df.shape}')\n",
        "print(f'Transformer Sentences shape: {corpus_transformer_df.shape}')\n",
        "\n",
        "print('\\n----------[ Consistency Check on Sentence length of all Models ]----------\\n')\n",
        "\n",
        "if corpus_sents_df.shape[0] == corpus_sentimentr_df.shape[0] == corpus_syuzhetr_df.shape[0] == corpus_transformer_df.shape[0]:\n",
        "  print(f'Great! All models have the same Sentence lengths: {corpus_sents_df.shape[0]}')\n",
        "else:\n",
        "  print(f'ERROR: Sentence DataFrames have unequal lengths, check statistics above and fix')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHV1LEiPg7XL"
      },
      "source": [
        "corpus_sents_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bcPKU2ZhASh"
      },
      "source": [
        "corpus_sentimentr_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXRvty-ffZOZ"
      },
      "source": [
        "smaller_ls = list(corpus_sents_df['sent_raw'])\n",
        "bigger_ls = list(corpus_sentimentr_df['sent_raw'])\n",
        "\n",
        "# orig_ser.compare(new_ser)\n",
        "# s2[s2.isin(s1)]\n",
        "\n",
        "diff_ls = [x for x in bigger_ls if x in smaller_ls]\n",
        "print(f'diff_ls: {len(diff_ls)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCmKM0w2umQt"
      },
      "source": [
        "\"\"\"\n",
        "# Create 4 Standardized versions of each Model: stdscaler, medianiqr both lnormed and not\n",
        "\n",
        "corpus_unified_df = pd.DataFrame()\n",
        "\n",
        "print('\\nBefore Standardization ----------')\n",
        "corpus_unified_df.columns\n",
        "\n",
        "models_unified_ls = models_baseline_ls + models_sentimentr_ls + models_syuzhetr_ls + models_transformer_ls\n",
        "\n",
        "standardize_ts_ls(corpus_unified_df, models_unified_ls)\n",
        "\n",
        "print('\\nAfter Standardization ----------')\n",
        "corpus_unified_df.columns\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06NwK2PrmIV9"
      },
      "source": [
        "# Vertically Concatenate ALL 4 Sentiment Groups StandardizedScaled SMA Sentence Sentiment Series into 1 Big DataFrame\n",
        "\n",
        "corpus_unified_df = pd.DataFrame()\n",
        "\n",
        "# Get Baseline Model StandardScaled SMA column names\n",
        "cols_baseline_stdscaler_ls = []\n",
        "for amodel in models_baseline_ls:\n",
        "  col_roll_stdscale = f'{amodel}_stdscaler_{roll_str}'\n",
        "  cols_baseline_stdscaler_ls.append(col_roll_stdscale)\n",
        "# print(f'\\nBaseline StdScaled SMA Columns:\\n    {cols_baseline_stdscaler_ls}')\n",
        "\n",
        "temp_baseline_df = corpus_sents_df[cols_baseline_stdscaler_ls].copy()\n",
        "temp_baseline_df = temp_baseline_df.add_prefix('baseline_')\n",
        "temp_baseline_df.columns\n",
        "\n",
        "# Get SentimentR Model StandardScaled SMA column names\n",
        "cols_sentimentr_stdscaler_ls = []\n",
        "for amodel in models_sentimentr_ls:\n",
        "  col_roll_stdscale = f'{amodel}_stdscaler_{roll_str}'\n",
        "  cols_sentimentr_stdscaler_ls.append(col_roll_stdscale)\n",
        "# print(f'\\nSentimentR StdScaled SMA Columns:\\n    {cols_sentimentr_stdscaler_ls}')\n",
        "\n",
        "temp_sentimentr_df = corpus_sentimentr_df[cols_sentimentr_stdscaler_ls].copy()\n",
        "temp_sentimentr_df = temp_sentimentr_df.add_prefix('sentimentr_')\n",
        "temp_sentimentr_df.columns\n",
        "\n",
        "# Get SyuzhetR Model StandardScaled SMA column names\n",
        "cols_syuzhetr_stdscaler_ls = []\n",
        "for amodel in models_syuzhetr_ls:\n",
        "  col_roll_stdscaler = f'{amodel}_stdscaler_{roll_str}'\n",
        "  cols_syuzhetr_stdscaler_ls.append(col_roll_stdscaler)\n",
        "# print(f'\\nSyuzhetR StdScaled SMA Columns:\\n    {cols_syuzhetr_stdscaler_ls}')\n",
        "\n",
        "temp_syuzhetr_df = corpus_syuzhetr_df[cols_syuzhetr_stdscaler_ls].copy()\n",
        "temp_syuzhetr_df = temp_syuzhetr_df.add_prefix('syuzhetr_')\n",
        "temp_syuzhetr_df.columns\n",
        "\n",
        "# Get Transformer Model StandardScaled SMA column names\n",
        "cols_transformer_stdscaler_ls = []\n",
        "for amodel in models_transformer_ls:\n",
        "  col_roll_stdscale = f'{amodel}_stdscaler_{roll_str}'\n",
        "  cols_transformer_stdscaler_ls.append(col_roll_stdscale)\n",
        "# print(f'\\nTransformer StdScaled SMA Columns:\\n    {cols_transformer_stdscalerls}')\n",
        "\n",
        "# If Transformers Sentiment DataFrame exists, add it to the Unified DataFrame\n",
        "var_name = 'corpus_transformer_df'\n",
        "if var_name in globals():\n",
        "  # print(f'{var_name} is declared globally')\n",
        "  # print(eval(f'{var_name}.shape[0]'))\n",
        "  corpus_transformer_df_len = eval(f'{var_name}.shape[0]')\n",
        "  print(f'{var_name} has {corpus_transformer_df_len} Sentences')\n",
        "\n",
        "  temp_transformer_df = corpus_transformer_df[cols_transformer_stdscaler_ls].copy()\n",
        "  temp_transformer_df = temp_transformer_df.add_prefix('transformer_')\n",
        "  temp_transformer_df.columns\n",
        "\n",
        "  print(f'\\n\\n{var_name} IS declared\\n    so adding to Unified DataFrame')\n",
        "  corpus_unified_df = pd.concat([temp_baseline_df,\n",
        "                                  temp_sentimentr_df,\n",
        "                                  temp_syuzhetr_df,\n",
        "                                  temp_transformer_df],\n",
        "                                  axis=1)\n",
        "\n",
        "  temp_transformer_df = pd.DataFrame()\n",
        "\n",
        "else:\n",
        "  print(f'\\n\\n{var_name} IS NOT declared\\n    so NOT adding to Unified DataFrame')\n",
        "\n",
        "  corpus_unified_df = pd.concat([temp_baseline_df,\n",
        "                                  temp_sentimentr_df,\n",
        "                                  temp_syuzhetr_df],\n",
        "                                  axis=1)\n",
        "\n",
        "# Insert Sentence Number (sent_no) column\n",
        "sent_no_ls = list(range(corpus_unified_df.shape[0]))\n",
        "corpus_unified_df.insert(0, 'sent_no', sent_no_ls)\n",
        "sent_raw_ls = [str(x) for x in corpus_sents_df['sent_raw']]\n",
        "corpus_unified_df.insert(1, 'sent_raw', sent_raw_ls)\n",
        "# corpus_unified_df.insert(2, 'sent_clean', sent_no_ls)\n",
        "\n",
        "temp_baseline_df = pd.DataFrame()\n",
        "temp_sentimentr_df = pd.DataFrame()\n",
        "temp_syuzhetr_df = pd.DataFrame()\n",
        "\n",
        "print(f'\\corpus_unified_df.shape: {corpus_unified_df.shape}')\n",
        "corpus_unified_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWw-EKcETQWO"
      },
      "source": [
        "  corpus_plots_std_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USyCByrkibLp"
      },
      "source": [
        "# Optionally specificy and malformed models to exclude from Unified DataFrame\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "model_base_bad = 'pattern'\n",
        "\n",
        "# drop malformed models manually specified in prior code cell\n",
        "print(\"\\n=====================================================================\\n\")\n",
        "print(f\"    WARNING: Dropping Model >>>{model_base_bad}<<< and all it's derivatives\")\n",
        "print(\"\\n=====================================================================\\n\\n\")\n",
        "model_bad_ls = [x for x in corpus_unified_df.columns if model_base_bad in x]\n",
        "\n",
        "print(f'    DROPPED COLUMNS: {model_bad_ls}\\n')\n",
        "corpus_unified_df.drop(columns=model_bad_ls, inplace=True)\n",
        "print(f'    REMAINING COLUMNS: {corpus_unified_df.columns}')\n",
        "\n",
        "# Mirror changes in this plotting DataFrame too\n",
        "model_bad_ls = [x for x in corpus_plots_std_df.columns if model_base_bad in x]\n",
        "corpus_plots_std_df.drop(columns=model_bad_ls, inplace=True)\n",
        "\n",
        "# Also individually delete from all 4 composite DataFrames\n",
        "baseline_plots_std_ls = [x for x in baseline_plots_std_ls if not model_base_bad in x]\n",
        "sentimentr_plots_std_ls = [x for x in sentimentr_plots_std_ls if not model_base_bad in x]\n",
        "syuzhetr_plots_std_ls = [x for x in syuzhetr_plots_std_ls if not model_base_bad in x]\n",
        "transformer_plots_std_ls = [x for x in transformer_plots_std_ls if not model_base_bad in x]\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yzwajN3h9AK"
      },
      "source": [
        "baseline_plots_std_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OMIp6-AhYJp"
      },
      "source": [
        "corpus_plots_std_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_14bxPnjzzM"
      },
      "source": [
        "models_sentimentr_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQEPm1d2i3rF"
      },
      "source": [
        "# Create <model>_stdscaler_roll<dd> series\n",
        "\n",
        "# Step 1: Create the <model>_stdscaler\n",
        "# Baseline 12 Models\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=models_baseline_ls, col_mod='std-stdscaler')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "corpus_plots_std_df = pd.concat([corpus_plots_std_df, test_df], axis=1)\n",
        "corpus_plots_std_df = corpus_plots_std_df.loc[:,~corpus_plots_std_df.columns.duplicated()]\n",
        "\n",
        "# SentimentR 7 Models\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=models_sentimentr_ls, col_mod='std-stdscaler')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "corpus_plots_std_df = pd.concat([corpus_plots_std_df, test_df], axis=1)\n",
        "corpus_plots_std_df = corpus_plots_std_df.loc[:,~corpus_plots_std_df.columns.duplicated()]\n",
        "\n",
        "# SyuzhetR 4 Models\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=models_syuzhetr_ls, col_mod='std-stdscaler')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "corpus_plots_std_df = pd.concat([corpus_plots_std_df, test_df], axis=1)\n",
        "corpus_plots_std_df = corpus_plots_std_df.loc[:,~corpus_plots_std_df.columns.duplicated()]\n",
        "\n",
        "# Transformer 8 Models\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=models_transformer_ls, col_mod='std-stdscaler')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "corpus_plots_std_df = pd.concat([corpus_plots_std_df, test_df], axis=1)\n",
        "corpus_plots_std_df = corpus_plots_std_df.loc[:,~corpus_plots_std_df.columns.duplicated()]\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Create the <model>_stdscaler_rollxx\n",
        "# Baseline 12 Models\n",
        "stdstdscaler_ls = [f'{x}_stdscaler' for x in models_baseline_ls]\n",
        "test_df = process_timeseries(ts_df=corpus_sents_df, col_models_ls=stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_sents_df = pd.concat([corpus_sents_df, test_df], axis=1)\n",
        "corpus_sents_df = corpus_sents_df.loc[:,~corpus_sents_df.columns.duplicated()]\n",
        "corpus_plots_std_df = pd.concat([corpus_plots_std_df, test_df], axis=1)\n",
        "corpus_plots_std_df = corpus_plots_std_df.loc[:,~corpus_plots_std_df.columns.duplicated()]\n",
        "\n",
        "# SentimentR 7 Models\n",
        "stdstdscaler_ls = [f'{x}_stdscaler' for x in models_sentimentr_ls]\n",
        "test_df = process_timeseries(ts_df=corpus_sentimentr_df, col_models_ls=stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_sentimentr_df = pd.concat([corpus_sentimentr_df, test_df], axis=1)\n",
        "corpus_sentimentr_df = corpus_sentimentr_df.loc[:,~corpus_sentimentr_df.columns.duplicated()]\n",
        "corpus_plots_std_df = pd.concat([corpus_plots_std_df, test_df], axis=1)\n",
        "corpus_plots_std_df = corpus_plots_std_df.loc[:,~corpus_plots_std_df.columns.duplicated()]\n",
        "\n",
        "# SyuzhetR 4 Models\n",
        "stdstdscaler_ls = [f'{x}_stdscaler' for x in models_syuzhetr_ls]\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "corpus_plots_std_df = pd.concat([corpus_plots_std_df, test_df], axis=1)\n",
        "corpus_plots_std_df = corpus_plots_std_df.loc[:,~corpus_plots_std_df.columns.duplicated()]\n",
        "\n",
        "# Transformer 8 Models\n",
        "stdstdscaler_ls = [f'{x}_stdscaler' for x in models_transformer_ls]\n",
        "test_df = process_timeseries(ts_df=corpus_transformer_df, col_models_ls=stdstdscaler_ls, col_mod='roll10')\n",
        "corpus_transformer_df = pd.concat([corpus_transformer_df, test_df], axis=1)\n",
        "corpus_transformer_df = corpus_transformer_df.loc[:,~corpus_transformer_df.columns.duplicated()]\n",
        "corpus_plots_std_df = pd.concat([corpus_plots_std_df, test_df], axis=1)\n",
        "corpus_plots_std_df = corpus_plots_std_df.loc[:,~corpus_plots_std_df.columns.duplicated()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI_glR0-QuZL"
      },
      "source": [
        "# Combine Standardned Plots\n",
        "\n",
        "# NOTE: Baselinse has 'roll100_stdscale' vs 'stdscale_roll10' for the other 3 libraries\n",
        "unified_plots_std_ls = baseline_plots_std_ls + sentimentr_plots_std_ls + syuzhetr_plots_std_ls + transformer_plots_std_ls\n",
        "\n",
        "# Baseline plots need to be reduced/normalized on the y-axis to match other 3 libraries\n",
        "# unified_plots_std_ls = baseline_plots_std_ls + transformer_plots_std_ls\n",
        "# unified_plots_std_ls =  syuzhetr_plots_std_ls + sentimentr_plots_std_ls\n",
        "# unified_plots_std_ls = baseline_plots_std_ls + syuzhetr_plots_std_ls\n",
        "# unified_plots_std_ls = transformer_plots_std_ls + sentimentr_plots_std_ls + syuzhetr_plots_std_ls\n",
        "\n",
        "print(f'Plotting Standardized Plots for ALL \\n   {len(unified_plots_std_ls)} Models across the 4 libraries\\n')\n",
        "\n",
        "corpus_plots_std_df[unified_plots_std_ls].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvyVLE1Yh-n2"
      },
      "source": [
        "corpus_unified_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrsydSPi43Yd"
      },
      "source": [
        "# Standardize all 31 Models (stdscaler_roll10) in Unified DataFrame for easy comparison\n",
        "\n",
        "# For Visual Check and Algo Clustering: https://www.listendata.com/2017/04/how-to-standardize-variable-in-regression.html\n",
        "\n",
        "unified_stdroll_ls = [x for x in corpus_unified_df.columns if x.endswith('roll10')]\n",
        "\n",
        "temp_df = corpus_unified_df[unified_stdroll_ls].copy(deep=True)\n",
        "temp_df.fillna(value=0, inplace=True)\n",
        "\n",
        "for amodel in unified_stdroll_ls:\n",
        "  print(amodel)\n",
        "\n",
        "  # temp_df = process_timeseries(ts_df=corpus_unified_df, col_models_ls=unified_stdroll_ls, col_mod='std-minmax')\n",
        "\n",
        "  col_mod_std = f'{amodel}_minmax'\n",
        "  # temp_std_np = minmax_scaler.fit_transform(np.array(temp_df[amodel].reshape(-1, 1)))\n",
        "  # temp_df[col_mod_std] = pd.Series(temp_std_np.ravel())\n",
        "\n",
        "\n",
        "  # corpus_unified_df = pd.concat([corpus_unified_df, temp_df], axis=1)\n",
        "  # corpus_unified_df = corpus_unified_df.loc[:,~corpus_unified_df.columns.duplicated()]\n",
        "  # temp_df = pd.concat([corpus_unified_df, temp_df], axis=1)\n",
        "  # temp_df = test.loc[:,~temp_df.columns.duplicated()]\n",
        "\n",
        "temp_df.head(2)\n",
        "temp_df.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoHelh9N0e8u"
      },
      "source": [
        "## **Correlation Heatmap for ALL StdScaler/Roll Sentence Sentiment Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yoglROBkH4_"
      },
      "source": [
        "corpus_plots_std_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qnmuzh9Xi4p"
      },
      "source": [
        "# Create a Correlation Heatmap for All Sentence Models\n",
        "\n",
        "# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n",
        "# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n",
        "\n",
        "Correlation_Algo = \"pearson\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n",
        "# corr_methods_ls = ['pearson', 'spearman', 'kendall']\n",
        "\n",
        "# corr_df = corpus_sents_syuzhetr_df[syuzhetr_corr_models_ls].corr(method='spearman')\n",
        "corr_df = corpus_plots_std_df.filter(like='roll').corr(method=Correlation_Algo)\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "# plt.title(f'{CORPUS_FULL} Sentence Sentiment for All Model Sentiments\\n StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment for Transformer Model Sentiments\\n {Correlation_Algo.capitalize()} Correlation Heatmap [StdScale -> SMA 10%] ({roll_str.capitalize()})')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-E48pxvt6Uw"
      },
      "source": [
        "corr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r34lNnz0oeQN"
      },
      "source": [
        "# Create a Correlation Heatmap for All Sentence Models\n",
        "\n",
        "# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n",
        "# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n",
        "\n",
        "Correlation_Algo = \"kendall\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n",
        "# corr_methods_ls = ['pearson', 'spearman', 'kendall']\n",
        "\n",
        "# corr_df = corpus_sents_syuzhetr_df[syuzhetr_corr_models_ls].corr(method='spearman')\n",
        "corr_df = corpus_unified_df.filter(like='stdscale').corr(method=Correlation_Algo)\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "# plt.title(f'{CORPUS_FULL} Sentence Sentiment for All Model Sentiments\\n StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment for Transformer Model Sentiments\\n {Correlation_Algo.capitalize()} Correlation Heatmap [StdScale -> SMA 10%] ({roll_str.capitalize()})')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBbzrg1Tt95S"
      },
      "source": [
        "corr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nznsrhwwioqq"
      },
      "source": [
        "# **Compare Subset of Unified Models** \n",
        "\n",
        "**Sentences (StdScaler/Roll)**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select any combination of Sentiment Models in order of the following four Groups: Baseline, SentimentR, SyuzhetR and Transformers\n",
        "\n",
        "* All Sentiment Time Series are StandardizedScaled version of SMA created in the notebook above this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4h3V4Pa0voZ"
      },
      "source": [
        "#### **Interactive Sentence SMA Plots**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEaMXFw6hayU"
      },
      "source": [
        "Baseline_SentimentR = True #@param {type:\"boolean\"}\n",
        "Baseline_Syuzhet = False #@param {type:\"boolean\"}\n",
        "Baseline_Bing = False #@param {type:\"boolean\"}\n",
        "Baseline_SenticNet = False #@param {type:\"boolean\"}\n",
        "Baseline_SentiWord = True #@param {type:\"boolean\"}\n",
        "Baseline_NRC = False #@param {type:\"boolean\"}\n",
        "Baseline_AFINN = False #@param {type:\"boolean\"}\n",
        "Baseline_VADER = True #@param {type:\"boolean\"}\n",
        "Baseline_TextBlob = True #@param {type:\"boolean\"}\n",
        "Baseline_Flair = True #@param {type:\"boolean\"}\n",
        "Baseline_Pattern = False #@param {type:\"boolean\"}\n",
        "Baseline_Stanza = True #@param {type:\"boolean\"}\n",
        "# Baseline-Mean_All = False #@param {type:\"boolean\"}\n",
        "# Baseline-Mean_Subset = False #@param {type:\"boolean\"}\n",
        "# Baseline-MPQA = False #@param {type:\"boolean\"}\n",
        "# Baseline-SentiStrength = False #@param {type:\"boolean\"}\n",
        "\n",
        "SentimentR_JockersRinker = True #@param {type:\"boolean\"}\n",
        "SentimentR_Jockers = True #@param {type:\"boolean\"}\n",
        "SentimentR_HuLiu = False #@param {type:\"boolean\"}\n",
        "SentimentR_SenticNet = False #@param {type:\"boolean\"}\n",
        "SentimentR_SentiWord = False #@param {type:\"boolean\"}\n",
        "SentimentR_NRC = False #@param {type:\"boolean\"}\n",
        "SentimentR_LoughranMcDonald = False #@param {type:\"boolean\"}\n",
        "\n",
        "SyuzhetR_Syuzhet = True #@param {type:\"boolean\"}\n",
        "SyuzhetR_Bing = False #@param {type:\"boolean\"}\n",
        "SyuzhetR_AFINN = False #@param {type:\"boolean\"}\n",
        "SyuzhetR_NRC = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "Transformer_RoBERTaLg15 = True #@param {type:\"boolean\"}\n",
        "Transformer_T5IMDB50k = False #@param {type:\"boolean\"}\n",
        "Transformer_Huggingface = True #@param {type:\"boolean\"}\n",
        "Transformer_NLPTown = False #@param {type:\"boolean\"}\n",
        "Transformer_RoBERTaXML8lang = False #@param {type:\"boolean\"}\n",
        "Transformer_IMDB2way = False #@param {type:\"boolean\"}\n",
        "Transformer_Hinglish = False #@param {type:\"boolean\"}\n",
        "Transformer_Yelp = False #@param {type:\"boolean\"}\n",
        "# Mean_Subset_Arc = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkR7gXEehbbf"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "# SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# win_per = SMA_Window_Percent\n",
        "# win_roll = int(win_per/100 * corpus_unified_df.shape[0])\n",
        "\n",
        "# NOTE: all 4 Groups need to be run with the same SMA roll_per (usually 5 or 10%)\n",
        "#       to compare Models from the 4 different Groups\n",
        "\n",
        "\n",
        "model_all_subset_ls = []\n",
        "\n",
        "if Baseline_SentimentR == True:\n",
        "  model_all_subset_ls.append('baseline_sentimentr_stdscaler_roll10')\n",
        "if Baseline_Syuzhet == True:\n",
        "  model_all_subset_ls.append('baseline_syuzhet_stdscaler_roll10')\n",
        "if Baseline_Bing == True:\n",
        "  model_all_subset_ls.append('baseline_bing_stdscaler_roll10')\n",
        "if Baseline_SenticNet == True:\n",
        "  model_all_subset_ls.append('baseline_senticnet_stdscaler_roll10')\n",
        "if Baseline_SentiWord == True:\n",
        "  model_all_subset_ls.append('baseline_sentiword_stdscaler_roll10')\n",
        "if Baseline_NRC == True:\n",
        "  model_all_subset_ls.append('baseline_nrc_stdscaler_roll10')\n",
        "if Baseline_AFINN == True:\n",
        "  model_all_subset_ls.append('baseline_afinn_stdscaler_roll10')\n",
        "if Baseline_VADER == True:\n",
        "  model_all_subset_ls.append('baseline_vader_stdscaler_roll10')\n",
        "if Baseline_TextBlob == True:\n",
        "  model_all_subset_ls.append('baseline_textblob_stdscaler_roll10')\n",
        "if Baseline_Flair == True:\n",
        "  model_all_subset_ls.append('baseline_flair_stdscaler_roll10')\n",
        "if Baseline_Pattern == True:\n",
        "  model_all_subset_ls.append('baseline_pattern_stdscaler_roll10')\n",
        "if Baseline_Stanza == True:\n",
        "  model_all_subset_ls.append('baseline_stanza_stdscaler_roll10')\n",
        "\n",
        "if SentimentR_JockersRinker == True:\n",
        "  model_all_subset_ls.append('sentimentr_jockers_rinker_stdscaler_roll10')\n",
        "if SentimentR_Jockers == True:\n",
        "  model_all_subset_ls.append('sentimentr_jockers_stdscaler_roll10')\n",
        "if SentimentR_HuLiu == True:\n",
        "  model_all_subset_ls.append('sentimentr_huliu_stdscaler_roll10')\n",
        "if SentimentR_SenticNet == True:\n",
        "  model_all_subset_ls.append('sentimentr_senticnet_stdscaler_roll10')\n",
        "if SentimentR_SentiWord == True:\n",
        "  model_all_subset_ls.append('sentimentr_sentiword_stdscaler_roll10')\n",
        "if SentimentR_NRC == True:\n",
        "  model_all_subset_ls.append('sentimentr_nrc_stdscaler_roll10')\n",
        "if SentimentR_LoughranMcDonald == True:\n",
        "  model_all_subset_ls.append('sentimentr_lmcd_stdscaler_roll10')\n",
        "\n",
        "\n",
        "if SyuzhetR_Syuzhet == True:\n",
        "  model_all_subset_ls.append('syuzhetr_syuzhet_stdscaler_roll10')\n",
        "if SyuzhetR_Bing == True:\n",
        "  model_all_subset_ls.append('syuzhetr_bing_stdscaler_roll10')\n",
        "if SyuzhetR_AFINN == True:\n",
        "  model_all_subset_ls.append('syuzhetr_afinn_stdscaler_roll10')\n",
        "if SyuzhetR_NRC == True:\n",
        "  model_all_subset_ls.append('syuzhetr_nrc_stdscaler_roll10')\n",
        "\n",
        "# Exclude Transformer Models if not loaded/defined\n",
        "var_name = 'corpus_transformer_df'\n",
        "if var_name in globals():\n",
        "  # print(f'{var_name} is declared globally')\n",
        "  # print(eval(f'{var_name}.shape[0]'))\n",
        "  corpus_transformer_df_len = eval(f'{var_name}.shape[0]')\n",
        "  print(f'{var_name} has {corpus_transformer_df_len} Sentences')\n",
        "\n",
        "  if Transformer_RoBERTaLg15 == True:\n",
        "    model_all_subset_ls.append('transformer_roberta15lg_stdscaler_roll10')\n",
        "  if Transformer_T5IMDB50k == True:\n",
        "    model_all_subset_ls.append('transformer_t5imdb50k_stdscaler_roll10')\n",
        "  if Transformer_Huggingface == True:\n",
        "    model_all_subset_ls.append('transformer_huggingface_stdscaler_roll10')\n",
        "  if Transformer_NLPTown == True:\n",
        "    model_all_subset_ls.append('transformer_nlptown_stdscaler_roll10')\n",
        "  if Transformer_RoBERTaXML8lang == True:\n",
        "    model_all_subset_ls.append('transformer_robertaxml8lang_stdscaler_roll10')\n",
        "  if Transformer_IMDB2way == True:\n",
        "    model_all_subset_ls.append('transformer_imdb2way_stdscaler_roll10')\n",
        "  if Transformer_Hinglish == True:\n",
        "    model_all_subset_ls.append('transformer_hinglish_stdscaler_roll10')\n",
        "  if Transformer_Yelp == True:\n",
        "    model_all_subset_ls.append('transformer_yelp_stdscaler_roll10')\n",
        "\n",
        "\n",
        "else:\n",
        "  print(f'ERROR: {var_name} IS NOT declared\\n    Go back and load Transformer Sentiment Datafile\\n    and re-run this code cell')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Safe)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "\n",
        "# Add Sentiment Arc Plot Traces\n",
        "for i,amodel in enumerate(model_all_subset_ls):\n",
        "  print(f'Processing Model: {amodel}')\n",
        "  # TODO: Fix this discrepency at source\n",
        "  if amodel.startswith('baseline'):\n",
        "    scale_factor = 10\n",
        "  else:\n",
        "    scale_factor = 1\n",
        "  fig.add_traces(go.Line(x = corpus_unified_df.index.values,\n",
        "                        y = scale_factor * corpus_unified_df[amodel],\n",
        "                        text = corpus_sents_df['sent_raw'], # corpus_sents_df.iloc[x]['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model: <b>\"+amodel+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=CORPUS_FULL + \"<br>Compare Sentence StandardizedScaled SMA Sentiment Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "    xaxis_title=\"Sentence Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQvLaYbcsNWr"
      },
      "source": [
        "#### **Hierarichal Clustering**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH7beIkM1Xzg"
      },
      "source": [
        "#### **Top-n Crux Peaks and Valleys**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBRQB7Gi1Oyb"
      },
      "source": [
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "No_Paragraphs_on_Each_Side = 0 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "  \n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        ts_df = corpus_unified_df,\n",
        "                        library_type='unified',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "\n",
        "\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yz3pVMy4y0v"
      },
      "source": [
        "corpus_unified_df[model_all_subset_ls].head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64ytlcCd9eQS"
      },
      "source": [
        "# Verify selected models and DataFrame shapes\n",
        "\n",
        "model_all_subset_ls\n",
        "print('\\n')\n",
        "corpus_unified_df.shape\n",
        "print('\\n')\n",
        "corpus_unified_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDP1T4Vo9535"
      },
      "source": [
        "len(corpus_unified_df['transformer_robertaxml8lang_stdscaler_roll10'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnPA8Eci-3hl"
      },
      "source": [
        "# def get_crux_points(ts_df, col_series, text_type='sentence', win_per=5, sec_y_labels=True, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "get_crux_points(corpus_unified_df, 'baseline_flair_stdscaler_roll10', text_type='sentence', win_per=5, sec_y_labels=False, do_plot=False )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaZxD8kbV271"
      },
      "source": [
        "cruxes_unified_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZtx8NfhowLJ"
      },
      "source": [
        "# minIndex = myList.index(min(myList))\n",
        "# min(sentiment_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAfMs8Bmz2vE"
      },
      "source": [
        "#### **Crux Points: Table**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQE4lt_ZvTw2"
      },
      "source": [
        "# Compute Crux Values for selected Models\n",
        "\n",
        "cruxes_unified_dt = {}\n",
        "Crux_Window_Percent = 10\n",
        "\n",
        "for i,amodel in enumerate(model_all_subset_ls):\n",
        "  print(f'Processing Model: {amodel}')\n",
        "  # cruxes_unified_dt[amodel]\n",
        "  amodel_cruxes_ls = get_crux_points(ts_df=corpus_unified_df, \n",
        "                                         col_series=amodel, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=5, \n",
        "                                         sec_y_height=0, \n",
        "                                         subtitle_str=' Selected Models ', \n",
        "                                         do_plot=False, \n",
        "                                         save2file=False)\n",
        "  \n",
        "  print(f'    {len(amodel_cruxes_ls)} Cruxes found for {amodel}\\n')\n",
        "\n",
        "  cruxes_unified_dt[amodel] = amodel_cruxes_ls\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb3-IZu5KjWR"
      },
      "source": [
        "# Create Unified Crux Points DataFrame for selected models\n",
        "\n",
        "model_ls = []\n",
        "type_ls = []\n",
        "sent_no_ls = []\n",
        "sentiment_ls = []\n",
        "sent_raw_ls = []\n",
        "for model_name, model_crux_ls in cruxes_unified_dt.items():\n",
        "\n",
        "  print(f'Model: {model_name} with {len(model_crux_ls)} Cruxes')\n",
        "\n",
        "  for i, acrux_tup in enumerate(model_crux_ls):\n",
        "    model_ls.append(model_name)\n",
        "\n",
        "    atype, asent_no, asentiment = acrux_tup \n",
        "    type_ls.append(atype)\n",
        "    sent_no_ls.append(asent_no)\n",
        "    sentiment_ls.append(asentiment)\n",
        "\n",
        "    sent_raw = corpus_sents_df.iloc[asent_no]['sent_raw']\n",
        "    sent_raw_ls.append(sent_raw)\n",
        "\n",
        "\n",
        "# Hack: scale sentiment from approx -1.0-+1.0 to -100-+100\n",
        "sentiment_min = min(sentiment_ls)\n",
        "if sentiment_min < 0:\n",
        "  add_adj = np.abs(sentiment_min)\n",
        "sentiment100_ls = [x+add_adj for x in sentiment_ls]\n",
        "print(f'BEFORE: min of sentiment100_ls is {min(sentiment100_ls)}')\n",
        "print(f'BEFORE: max of sentiment100_ls is {max(sentiment100_ls)}')\n",
        "sentiment100_ls = [int(x*10) for x in sentiment100_ls]\n",
        "print(f'AFTER: min of sentiment100_ls is {min(sentiment100_ls)}')\n",
        "print(f'AFTER: max of sentiment100_ls is {max(sentiment100_ls)}')\n",
        "\n",
        "unified_crux_df = pd.DataFrame({'model':pd.Series(model_ls,dtype='string'),\n",
        "                                'type':pd.Series(type_ls,dtype='string'),\n",
        "                                'sent_no':pd.Series(sent_no_ls,dtype='int'),\n",
        "                                'sent_raw':pd.Series(sent_raw_ls,dtype='str'),\n",
        "                                'sentiment':pd.Series(sentiment_ls,dtype='float'),\n",
        "                                'sentiment100':pd.Series(sentiment100_ls,dtype='int')})\n",
        "\n",
        "# Add token_len column\n",
        "unified_crux_df['token_len'] = unified_crux_df['sent_raw'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Add Abbreviated raw_sent column\n",
        "unified_crux_df['sent_raw_abbr'] = unified_crux_df['sent_raw'].apply(lambda x: x[:100])\n",
        "\n",
        "# Trim suffix '_stdscaler_roll10' off Model name\n",
        "unified_crux_df['model'] = unified_crux_df['model'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
        "\n",
        "# Sort by sent_no\n",
        "unified_crux_df = unified_crux_df.sort_values(by=['sent_no','model'], ascending=True)\n",
        "\n",
        "\n",
        "# Verify\n",
        "print('\\n')\n",
        "unified_crux_df.head(10)\n",
        "print('\\n')\n",
        "print(f'{unified_crux_df.shape[0]} Cruxes were found in the Subset of the Unified Models.')\n",
        "print('\\n\\n')\n",
        "\n",
        "# Save table of Cruxes (extracted from a subset of models) to file\n",
        "title_str = ''.join(title_str.split('.')).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "unifed_crux_filename = f'cruxes_table_unified_subset_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "unified_crux_df.to_csv(unifed_crux_filename)\n",
        "\n",
        "print(f'Saving Cruxs found in these Models:\\n\\n')\n",
        "print(*model_all_subset_ls, sep=os.linesep)\n",
        "\n",
        "print(f\"\\n\\nSaving to file:\\n\\n{unifed_crux_filename}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_HCP_gKJzQ3"
      },
      "source": [
        "# Download *csv table of Unified Crux Points from Selected Models\n",
        "\n",
        "files.download(unifed_crux_filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DzDgKJp0Cl0"
      },
      "source": [
        "#### **Crux Points: 2D Interactive Clusters**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dCf8C3iYzGd"
      },
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Emphasize token_len\n",
        "# fig = px.scatter(unified_crux_df, x=\"sent_no\", y=\"sentiment\", symbol='type', symbol_map={'peak':5, 'valley':6}, color=\"model\", opacity=0.5, size='token_len', hover_data=['sent_raw_abbr'])\n",
        "\n",
        "# Emphasize Peak/Valley\n",
        "fig = px.scatter(unified_crux_df, x=\"sent_no\", y=\"sentiment\", symbol='type', symbol_map={'peak':5, 'valley':6}, color=\"model\", opacity=0.5, size='sentiment100', hover_data=['sent_raw_abbr'])\n",
        "\n",
        "fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8lLn8jZmyxh"
      },
      "source": [
        "# Create Mean of selected Models\n",
        "\n",
        "if 'mean_roll10' in model_all_subset_ls:\n",
        "  model_all_subset_ls.remove('mean_roll10')\n",
        "models_all_subset_mean_ser = corpus_unified_df[model_all_subset_ls].mean(axis=1)\n",
        "# unified_crux_df.insert(5, 'mean_roll', list(models_all_subset_mean_ser))\n",
        "\n",
        "# Create Median of selected Models\n",
        "# unified_crux_df['model'] = unified_crux_df['model'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
        "\n",
        "models_all_subset_mean_ser.plot()\n",
        "plt.title(f'{CORPUS_FULL}\\nSentence Mean of SMA 10% for {len(model_all_subset_ls)} Selected Models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfBR_bpzq7qh"
      },
      "source": [
        "# Create Mean of selected Models\n",
        "\n",
        "if 'mean_roll10' in model_all_subset_ls:\n",
        "  model_all_subset_ls.remove('mean_roll10')\n",
        "models_all_subset_mean_ser = corpus_unified_df[model_all_subset_ls].mean(axis=1)\n",
        "# unified_crux_df.insert(5, 'mean_roll', list(models_all_subset_mean_ser))\n",
        "\n",
        "# Create Median of selected Models\n",
        "# unified_crux_df['model'] = unified_crux_df['model'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
        "\n",
        "# models_all_subset_mean_ser.plot()\n",
        "\n",
        "sns.scatterplot(\n",
        "    data=unified_crux_df, x=\"sent_no\", y=\"sentiment\", hue=\"sentiment\", size=\"sentiment100\", sizes=(20, 200), style='model', hue_norm=(0, 7), legend=\"auto\"\n",
        ")\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohUZSIpduuDs"
      },
      "source": [
        "# dbscan clustering\n",
        "\n",
        "\n",
        "EPS_Max_Distance = 20\n",
        "\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.cluster import DBSCAN\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# define dataset\n",
        "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
        "\n",
        "# define the model\n",
        "model = DBSCAN(eps=EPS_Max_Distance, min_samples=4)\n",
        "\n",
        "# create numpy array of crux subset values\n",
        "crux_subset_np = unified_crux_df[['sent_no','sentiment']].to_numpy()\n",
        "crux_subset_np.shape\n",
        "\n",
        "# fit model and predict clusters\n",
        "yhat = model.fit_predict(crux_subset_np)\n",
        "\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "\n",
        "# create scatter plot for samples from each cluster\n",
        "for cluster in clusters:\n",
        "\t# get row indexes for samples with this cluster\n",
        "\trow_ix = where(yhat == cluster)\n",
        "\t# create scatter of these samples\n",
        "\tpyplot.scatter(crux_subset_np[row_ix, 0], crux_subset_np[row_ix, 1])\n",
        " \n",
        "# show the plot\n",
        "plt.title(f'{CORPUS_FULL} DBSCAN (eps={EPS_Max_Distance}) Clusters of Cruxes\\nSentence Mean of SMA 10% for {len(model_all_subset_ls)} Selected Models')\n",
        "pyplot.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SY5jcKfx82Z"
      },
      "source": [
        "# AutoML find opt eps based upon dataset neighborhood distances\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "neigh = NearestNeighbors(n_neighbors=2)\n",
        "nbrs = neigh.fit(crux_subset_np)\n",
        "distances, indices = nbrs.kneighbors(X)\n",
        "\n",
        "distances = np.sort(distances, axis=0)\n",
        "distances = distances[:,1]\n",
        "plt.plot(distances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "565hDwuex8ud"
      },
      "source": [
        "m = DBSCAN(eps=50, min_samples=5)\n",
        "\n",
        "m.fit(crux_subset_np)\n",
        "\n",
        "clusters = m.labels_\n",
        "\n",
        "colors = ['royalblue', 'maroon', 'forestgreen', 'mediumorchid', 'tan', 'deeppink', 'olive', 'goldenrod', 'lightcyan', 'navy']\n",
        "\n",
        "vectorizer = np.vectorize(lambda x: colors[x % len(colors)])\n",
        "\n",
        "plt.scatter(crux_subset_np[:,0], crux_subset_np[:,1], c=vectorizer(clusters))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6993R3tMut_W"
      },
      "source": [
        "# k-means clustering\n",
        "\n",
        "How_Many_Clusters = 15 #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "\n",
        "\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# define dataset\n",
        "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
        "\n",
        "# define the model\n",
        "model = KMeans(n_clusters=How_Many_Clusters)\n",
        "\n",
        "# fit the model\n",
        "model.fit(crux_subset_np)\n",
        "\n",
        "# assign a cluster to each example\n",
        "yhat = model.predict(crux_subset_np)\n",
        "\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "\n",
        "# create scatter plot for samples from each cluster\n",
        "for cluster in clusters:\n",
        "\t# get row indexes for samples with this cluster\n",
        "\trow_ix = where(yhat == cluster)\n",
        "\t# create scatter of these samples\n",
        "\tpyplot.scatter(crux_subset_np[row_ix, 0], crux_subset_np[row_ix, 1])\n",
        "# show the plot\n",
        "plt.title(f'{CORPUS_FULL} kMeans (k={How_Many_Clusters}) Clusters of Cruxes\\nSentence Mean of SMA 10% for {len(model_all_subset_ls)} Selected Models')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qda1yIqM0Qvw"
      },
      "source": [
        "#### **Crux Points: 1D Interactive Clusters**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB8r3yrPcHwi"
      },
      "source": [
        "y0_ls = [0] * unified_crux_df.shape[0]\n",
        "\n",
        "df = px.data.iris()\n",
        "fig = px.scatter(unified_crux_df, x=\"sent_no\", y=y0_ls, symbol='type', symbol_map={'peak':5, 'valley':6}, color=\"model\", opacity=0.5, size='token_len', hover_data=['sent_raw_abbr'])\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7WUuj1j4XW7"
      },
      "source": [
        "## **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18kf77WgvsK-"
      },
      "source": [
        "# Save Corpus DataFrames\n",
        "\n",
        "save_dataframes(df_ls=['everything'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_j92DSgyhGc"
      },
      "source": [
        "# **END OF WORKING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15gmvgjXut6N"
      },
      "source": [
        "win_s1per"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwWrlKMU9rgC"
      },
      "source": [
        "temp_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "for f_val in [0.04, 0.06, 0.08, 1.0]:\n",
        "  temp_df['model_lowess'] = Lowess(corpus_sents_df['vader'], f=f_val)\n",
        "  temp_df['model_lowess'].plot(alpha=0.3)\n",
        "temp_df['model_roll10'] = corpus_sents_df['vader'].rolling(5*win_s1per, center=True).mean()\n",
        "temp_df['model_roll10'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozvJTh5o9h3Y"
      },
      "source": [
        "temp_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "for f_val in [0.04, 0.06, 0.08, 1.0]:\n",
        "  temp_df['model_lowess'] = Lowess(corpus_sents_df['vader'], f=f_val)\n",
        "  temp_df['model_lowess'].plot(alpha=0.3)\n",
        "temp_df['model_roll10'] = corpus_sents_df['vader'].rolling(5*win_s1per, center=True).mean()\n",
        "temp_df['model_roll10'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fPuEQGocvyy"
      },
      "source": [
        "temp_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "for f_val in [0.04, 0.06, 0.08, 1.0]:\n",
        "  temp_df['bing_lowess'] = Lowess(corpus_sents_df['bing'], f=f_val)\n",
        "  temp_df['bing_lowess'].plot(alpha=0.3)\n",
        "temp_df['bing_roll10'] = corpus_sents_df['bing'].rolling(5*win_s1per, center=True).mean()\n",
        "temp_df['bing_roll10'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_3wznhF88j3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDD6tesE8ElK"
      },
      "source": [
        "# Crux Details\n",
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "unified_crux_ls = cruxes_unified_dt[amodel]\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(unified_crux_ls, \n",
        "                        ts_df = corpus_unified_df,\n",
        "                        library_type='unified',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2ObHaYW7vLa"
      },
      "source": [
        "def get_crux_points(ts_df, col_series, text_type='sentence', win_per=5, sec_y_labels=True, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drxh6nPX7KCB"
      },
      "source": [
        "corpus_unified_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD_2Wqup61W8"
      },
      "source": [
        "corpus_cruxes_unified_dt.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ls0-kk65TI"
      },
      "source": [
        "corpus_cruxes_unified_dt['baseline_flair_stdscaler_roll10']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndY1tuRa6U7x"
      },
      "source": [
        "corpus_unified_df['baseline_flair_stdscaler_roll10'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2eE7o7H-y4f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEljD21qyKg-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaNSxZi8yAOO"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "for amodel in corpus_models_stand_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_trans_df, \n",
        "                                         col_series=corpus_models_stand_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_height=0, \n",
        "                                         subtitle_str=' ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k80YU4ZoHCs2"
      },
      "source": [
        "## **Calculate Lexical Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CokfceWPtfD"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9cZmIkx7Too"
      },
      "source": [
        "# **EDA (Repeat for each Sentiment Model)** (Auto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJRV2n0M_xN6"
      },
      "source": [
        "#### **Histograms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJTcj9faMh61"
      },
      "source": [
        "**Sentiment Histogram Plots**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Histograms are provided for the (a) Length-Normed and (b) Scaled (Median Interquartile Range) Sentiment values for Sentences, Paragraphs and Sections. \n",
        "\n",
        "* There we used extensively early on to compare which Sentiment Time series preprocessing techniques worked best with our various Novel corpora according to two criteria: \n",
        "\n",
        "* (a) Vertical Scaling Method with the ability to transform histograms of sentiment values to well-behaved near-gaussian distributions and clipping outliers. After experimenting with various techniques including: mean/STD, median/MAD, and various two stage outlier/normalization methods median/IQR proved best (define).\n",
        "\n",
        "* (b) Horizontal Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPEv0DsBRhhQ"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU46yaCPPdSm"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYs2V2ILENaZ"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8b4CjTYwlgP"
      },
      "source": [
        "# Plot Histogram of Sentence lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuwVO3Q4Rmu4"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MhJ4T18PjTM"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SajUdBthwlgR"
      },
      "source": [
        "# Plot Histogram of Paragraph lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nuOQ7cZRrue"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfhjLMdjRwZX"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBFb-SLywlgS"
      },
      "source": [
        "# Plot Histogram of Section lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43YL_Iuf0JHZ"
      },
      "source": [
        "#### **Raw Sentiment Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AHHijpONvpt"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GorGKbFbR28W"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN27M4WlwlgT"
      },
      "source": [
        "# Plot Raw Sentence Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0QCx2OMN_jm"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxL5FryPR-ln"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d8uXvYJwlgU"
      },
      "source": [
        "# Plot Raw Paragraph Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_raw_sentiments(model_name=model_name, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp7xjR7GvxkT"
      },
      "source": [
        "# TODO: Add Section Crux Nos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2hFkRQHSLVE"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adhoCpB5SFQv"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "# col_meanstd = f'{model_name}_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='section', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_meanstd, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjZZqY6cwlgW"
      },
      "source": [
        "# Plot Raw Standardized Section Sentiments\n",
        "\n",
        "# NOTE: Compared with Length-Normed, the Raw Standardizations lose most SATS features\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "# col_lnorm_meanstd = f'{model_name}_lnorm_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='section', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_lnorm_meanstd, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDF_-tZhr44H"
      },
      "source": [
        "# Plot Raw Standardized Chapter Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "# col_lnorm_meanstd = f'{model_name}_lnorm_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='chapter', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_lnorm_meanstd, semantic_type='chapter', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1tIJZepmvLu"
      },
      "source": [
        "#### **Crux Points and Surrounding Contexts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FBWrNEJyit6"
      },
      "source": [
        "# Veify all the model sentiment variations\n",
        "\n",
        "[x for x in corpus_sects_df.columns if x.startswith(model_base)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKazAFV_qpCn"
      },
      "source": [
        "**Compare Chapters vs Sections Crux Points**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At the highest level, the Corpus is divided into Chapters which may then futher subdivided into Sections (e.g. extra spaces, punctuation like '* * *' or special printer glyph/fleuron).  Horizonal dark blue lines indicate Chapter divisions while Section boundries lie at both dark and light blue vertical lines.\n",
        "\n",
        "* Since each Chapter may contain multiple Sections, the Section Sentiment plot is more detailed/jagged than the Chapter Sentiment plots. By plotting both together, the smoother Chapter Sentiment plot gives a more general sense of the Corpus Sentiment Arc while the next, more-detailed Section Sentiment plot enables a more detailed investigation/localization of Crux Point neighborhoods.\n",
        "\n",
        "* At this early stage both the Chapter and Section Sentiment plots are too general to provide accurate/fixed Crux localization. As such, only aggregrate Sentiment values for each Chapter/Section are assigned to the mid-point of each Chapter/Section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sme-1HVRZOGs"
      },
      "source": [
        "# col_lnorm_medianiqr = 'pattern_lnorm_medianiqr'\n",
        "col_lnorm_medianiqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08xDu5WgkGAg"
      },
      "source": [
        "# col_lnorm_medianiqr = 'vader_lnorm_medianiqr'\n",
        "\n",
        "# corpus_chaps_df.drop(columns=['textblob_lnorm_medianiqr_lnorm_medianiqr', 'textblob_lnorm_medianiqr_medianiqr', 'textblob_lnorm_medianiqr_lnorm_meanstd'], inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06m1xDRzmWbq"
      },
      "source": [
        "corpus_chaps_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tojSdIGkPDz"
      },
      "source": [
        "corpus_chaps_df.columns\n",
        "# corpus_chaps_df.iloc[:20][['chap_no','sent_no_start','sent_no_mid','char_len','token_len','vader', 'vader_lnorm_medianiqr', 'textblob', 'textblob_lnorm_medianiqr']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu6F5kWerAPZ"
      },
      "source": [
        "corpus_chaps_df.vader_lnorm_medianiqr.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLgGy3HgO9--"
      },
      "source": [
        "col_lnorm_medianiqr = 'syuzhet_lnorm_medianiqr'\n",
        "model_name = 'syuzhet_lnorm_medianiqr'\n",
        "model_base = 'syuzhet'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M1bg8RcPH_2"
      },
      "source": [
        "# Plot Annotated Section Cruxes of Raw Sentiment Time Series\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[model_base], semantic_type='chapter', label_token_ct=3, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[model_base], semantic_type='section', label_token_ct=-1, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jku9ZQ9v9Wg"
      },
      "source": [
        "# Plot Annotated Section Cruxes of Standardized Sentiment Time Series\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='chapter', label_token_ct=3, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=-1, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxA0sDvlcdLx"
      },
      "source": [
        "**Sections Crux Points in Detail**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2tKXTD8v9RH"
      },
      "source": [
        "# Corpus Section Standardized Sentiment Time Series\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSw-wARBF2fW"
      },
      "source": [
        "**Verify Crux Point Sentence Number and Text Match**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At [Crux_Sentence_Text] enter the first few words that uniquely identify the Crux Sentence and confirm the Sentence No matches the information in the plot above. (NOTE: Search is for an exact match including case and puncutation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU6l4UaPdmAF"
      },
      "source": [
        "Crux_Sentence_Text = \"haiku\" #@param {type:\"string\"}\n",
        "\n",
        "# Verify individual Crux Sentence Numbers matches Content\n",
        "\n",
        "crux_sent_no = int(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Crux_Sentence_Text)]['sent_no'])\n",
        "\n",
        "print(f'The Sentence:\\n\\n    {Crux_Sentence_Text}\\n\\nMatches Sentence #{crux_sent_no}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Q_weR3H5Qu"
      },
      "source": [
        "**Review Context Around Any Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYAqkxi2FSw1"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  4494#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7-PoXAcq4S4"
      },
      "source": [
        "**Compare Paragraph vs Section Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzgfs-U9QZeq"
      },
      "source": [
        "# Verify the valid ranges for Sentences and Paragraphs\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "print(f'There are {corpus_sents_len} Sentences in the Corpus')\n",
        "corpus_parags_len = corpus_parags_df.shape[0]\n",
        "print(f'There are {corpus_parags_len} Paragraphs in the Corpus')\n",
        "\n",
        "# Create a new Corpus Paragraph DataFrame (corpus_parags_zoom_df) that is streteched out to have as many sample points as there are Sentences\n",
        "#   That is, go from an original corpus_parags_df of #Paragraph datapoints to an expanded corpus_parags_zoom_df of #Sentences datapoints using scipy.ndimage.interpolation.zoom\n",
        "\n",
        "corpus_parags_zoom_df = pd.DataFrame()\n",
        "\n",
        "resample_ratio = corpus_sents_df.shape[0]/corpus_parags_df.shape[0]   # ratio = no_sents/no_parags\n",
        "\n",
        "corpus_parags_df_numcols_ls = corpus_parags_df.select_dtypes(include=['number']).columns\n",
        "\n",
        "for acol in corpus_parags_df_numcols_ls:\n",
        "  parags_zoom_temp_np = zoom(np.array(corpus_parags_df[acol]), resample_ratio)\n",
        "  corpus_parags_zoom_df[acol] = pd.Series(parags_zoom_temp_np)\n",
        "\n",
        "print('\\n')\n",
        "print(f'New expanded corpus_parags_zoom_df.shape = {corpus_parags_zoom_df.shape}')\n",
        "print(f'           matches corpus_sents_df.shape = {corpus_sents_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wskHXVNSmjfQ"
      },
      "source": [
        "**Adjust the Paragraph Sentiment Plot to compare it with the Section Sentiment Plot for this Corpus**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Adjust [Scale_Vertical_Rolling_Paragraph] until the vertical min/max spans of the two plots are approximately equal\n",
        "\n",
        "* Adjust [Set_Paragraph_Rolling_Window_Percent] to set horizonal smoothness of Rolling Paragraph (typically 5,10 or 20%)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9P9k98ibh4n"
      },
      "source": [
        "col_lnorm_medianiqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgaBEAVCTMRt"
      },
      "source": [
        "Scale_Vertical_Rolling_Paragraph = 2.7 #@param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "Set_Paragraph_Rolling_Window_Percent = 3 #@param {type:\"slider\", min:0, max:30, step:1}\n",
        "\n",
        "# Compare Section Midpoints vs Sentence SMA Sentiment Values\n",
        "\n",
        "scale_sma_paragraph = Scale_Vertical_Rolling_Paragraph\n",
        "sentence_count = corpus_sents_df.shape[0]\n",
        "if Set_Paragraph_Rolling_Window_Percent == 0:\n",
        "  sma_parag_win = 1\n",
        "else:\n",
        "  sma_parag_win = int((Set_Paragraph_Rolling_Window_Percent/100)*sentence_count)\n",
        "\n",
        "\n",
        "# corpus_parags_zoom_df['vader_lnorm_medianiqr'].rolling(window=sma_parag_win, center=True).mean().apply(lambda x: x*scale_sma_paragraph).plot(label=f'{model_base} Paragraphs', alpha=0.3)\n",
        "corpus_parags_zoom_df[col_lnorm_medianiqr].rolling(window=sma_parag_win, center=True).mean().apply(lambda x: x*scale_sma_paragraph).plot(label=f'{model_base} Paragraphs', alpha=0.3)\n",
        "\n",
        "_ = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=0, title_xpos=0.5, title_ypos=1.0, sec_y_height=0, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} \\n SMA vs midpoint Paragraph MedianIQR Sentiment with Crux Points via SciPy.peaks')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUyZF6tE1rL0"
      },
      "source": [
        "#### **Zoom into a Section**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At [Select_Section_No] pick a Section of the Corpus to zoom into\n",
        "\n",
        "* Adjust [Scale_Vertical_Rolling_Sentences] until the vertical min/max spans of the two plots are approximately equal \n",
        "\n",
        "* Adjust [Set_Sentence_Rolling_Window_Percent] to set horizonal smoothness of Rolling Paragraph (typically 5,10 or 20%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu7DYRELQBoH"
      },
      "source": [
        "# Explore a Corpus Section Up-Close\n",
        "\n",
        "section_count = corpus_sects_df.shape[0]\n",
        "print(f'There are {section_count} Sections in this corpus,\\n  pick one numbered between 0 and {section_count-1}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Q2a854Zxl1"
      },
      "source": [
        "Select_Section_No =  24#@param {type:\"integer\"}\n",
        "Scale_Vertical_Rolling_Sentences = 0.2 #@param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "Set_Sentence_Rolling_Window_Percent = 3 #@param {type:\"slider\", min:0, max:30, step:1}\n",
        "\n",
        "# Make Copies instead of just using References / Only Reference, not copy()\n",
        "# section_sents_df = pd.DataFrame()\n",
        "# section_parags_df = pd.DataFrame()\n",
        "\n",
        "section_sents_df, section_parags_df = get_section_timeseries(Select_Section_No)\n",
        "\n",
        "# section_sents_df.head()\n",
        "\n",
        "print(f'section_sents_df.shape: {section_sents_df.shape}')\n",
        "print(f'section_parags_df.shape: {section_parags_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MbSPmp2dN9Z"
      },
      "source": [
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE-CDUmvFDOe"
      },
      "source": [
        "# Add expanded Paragraph sentiment to corpus_sents_df\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr_parag'] = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df')\n",
        "\n",
        "# NOTE: Define section_sents_df, MUST BE EXECUTED BEFORE ANY CRUX POINT DETECTION!!!\n",
        "\n",
        "parags_midpoint_ls = []\n",
        "col_name_parag = f'{model_name}_parag'\n",
        "section_sents_df[col_name_parag], parags_midpoint_ls = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df', model_name=model_name)\n",
        "\n",
        "# Verify Sentences and Expanded Paragraph lengths match\n",
        "print(f'\\nIn Section #{Select_Section_No}\\n')\n",
        "print(f'            Sentence Count: {section_sents_df.shape[0]}')\n",
        "print(f\"(expanded) Paragraph Count: {str(section_sents_df['parag_no'].unique().shape[0])}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOBqRxQz3gbh"
      },
      "source": [
        "##### **Section Histograms**\n",
        "\n",
        "EDA Unbalanced Paragraph and Sentence Features within selected Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MoMM3l8Px2R"
      },
      "source": [
        "# Verify the non-uniform distribution of Paragraph lengths within selected Section (thus necessity for noralizing Paragraph Sentiment by Paragraph length)\n",
        "\n",
        "section_sents_df['parag_no'].value_counts().hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Number of Sentences per Paragraph in Section #{Select_Section_No}')\n",
        "plt.xlabel('Number of Sentences per Paragraph')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5eC0i5KO9cm"
      },
      "source": [
        "section_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoVdCaReDJvl"
      },
      "source": [
        "section_sents_df[model_name].hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.xlabel('Sentence Sentiment')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSrXWnnEEm00"
      },
      "source": [
        "# Histogram of Paragraph Sentiments within selected Section\n",
        " \n",
        "# create a unified Section DataFrame with equal length Sentences and (expanded) Paragraphs Sentiment Series\n",
        "# section_sents_parags_df = section_sents_df.copy()\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr_parag_approx'] = parag_sentiment_expanded_ls\n",
        "\n",
        "\n",
        "section_sents_df[col_name_parag].hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Length-Normed Paragraph Sentiment in Section #{Select_Section_No}')\n",
        "plt.xlabel('Length-Normed Sentiment of Paragraph')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcao3RNuyCr_"
      },
      "source": [
        "**Naive Raw and LOWESS Smoothed Paragraph Sentiment plots within selected Section**\n",
        "\n",
        "NOTE: Horizonal x-axis narrative time axis not adjusted for variable paragraph lengths - simply used midpoints assuming equal length Paragraphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag47-b4EEO6k"
      },
      "source": [
        "**Raw and SMA Sentence Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkpQiVCoZTJ0"
      },
      "source": [
        "# section_crux_sents_dt\n",
        "\n",
        "# type(section_crux_sents_dt['vader_lnorm_medianiqr_roll50_frac14_win10'][0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB4fEZzJQPkg"
      },
      "source": [
        "##### **Section SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAIAX2OhW2k0"
      },
      "source": [
        "# SMA with Raw Sentiment values over entire Corpus\n",
        "\n",
        "sec_y_ht = -0.06\n",
        "\n",
        "plot_smas(section_view=False, model_name=model_base, text_unit='sentence', wins_ls=[5,10,15,20], alpha=0.5, y_height=sec_y_ht, subtitle_str=f'(Model: {model_base.capitalize()})', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8xWHqNAYV_p"
      },
      "source": [
        "# SMA with Length-Normed MedianIQR Sentiment values over entire Corpus\n",
        "\n",
        "sec_y_ht = -0.11\n",
        "\n",
        "plot_smas(section_view=False, model_name=model_name, text_unit='sentence', wins_ls=[5,10,15,20], alpha=0.5, y_height=sec_y_ht, subtitle_str=f'(Model: {model_base.capitalize()})', save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nA285mHvM4T"
      },
      "source": [
        "# Within the Selected_Section_No, plot Sentence SMA with vertical Paragraph boundries indicated\n",
        "\n",
        "sec_y_ht = 0.65\n",
        "\n",
        "# At Section boundries draw blue vertical lines \n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "  # 'BigNews1', xy=(sent_no, 0.5), xytext=(-10, 25), textcoords='offset points',                   rotation=90, va='bottom', ha='center', annotation_clip=True)\n",
        "\n",
        "  # plt.text(sent_no, -.5, 'goodbye',rotation=90, zorder=0)\n",
        "      \n",
        "for win_size in range(50,250,25):\n",
        "  section_sents_df[model_name].rolling(win_size, center=True).mean().plot(alpha=0.5, label=f'SMA win={win_size}')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base.capitalize()})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfZiEeDhWJJM"
      },
      "source": [
        "# Within the Selected_Section_No, compare Sentence SMA with vertical Paragraph boundries indicated\n",
        "#    3 Sentence SMAs: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "\n",
        "sec_y_ht = -0.5\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "awins_ls = [10]\n",
        "get_smas(section_sents_df, model_name=model_name, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=col_medianiqr, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=1.8, subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=model_base, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=8., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False);\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqlsQS-RYT7x"
      },
      "source": [
        "# Within the Selected_Section_No, compare Paragraph SMA: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "#     Plot Paragraph by Sentence Sentiment within selected Section\n",
        "\n",
        "get_smas(section_sents_df, model_name=model_name, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=col_medianiqr, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=1.8, subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=model_base, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=8., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTHlWyjzYf7C"
      },
      "source": [
        "# Within the Selected_Section_No, compare Paragraph SMA: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "#     Plot Paragraph by Paragraph Sentiment within selected Section\n",
        "\n",
        "\"\"\"\n",
        "# TODO: Fix ValueError: Image size of 1311x82731 pixels is too large. It must be less than 2^16 in each direction.\n",
        "\n",
        "\n",
        "sec_y_ht = -15\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(aparag_no, sec_y_ht, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(aparag_no, color='blue', alpha=0.1)\n",
        "\n",
        "awins_ls = [5]\n",
        "get_smas(section_parags_df, model_name=model_name, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_parags_df, model_name=col_medianiqr, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=8., subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_parags_df, model_name=model_base, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=10., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Paragraph Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhIM7zwuQUBq"
      },
      "source": [
        "##### **Raw Sentiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp0JvXFrV-B4"
      },
      "source": [
        "# Within the Selected_Section_No, compare Sentence Sentiment: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "#     Plot Raw vs MedianIQR Sentence Sentiment within selected Section\n",
        "\n",
        "sec_y_ht = -4.0\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_base, data=section_sents_df, alpha=0.3, label=f'Raw Sentence Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_sents_df, alpha=0.5, label=f'MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.plot(col_lnorm_medianiqr, data=section_sents_df, alpha=0.5, label=f'MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw vs MedianIQR Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzoXo8mVZMq_"
      },
      "source": [
        "# Within the Selected_Section_No, compare Sentence Sentiment: \n",
        "#     MedianIQR vs Length-Normed MedianIQR Sentence Sentiment within selected Section\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_name, data=section_sents_df, alpha=0.3, label=f'Length-Normed MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_sents_df, alpha=0.3, label=f'MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLength-Normed vs non-Normed Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tag3LC8Y9Is"
      },
      "source": [
        "# Within the Selected_Section_No, compare Paragraph Sentiment\n",
        "#    Standardized MedianIQR vs Length-Normed Standardized MedianIQR\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(aparag_no, sec_y_ht, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(aparag_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_name, data=section_parags_df, alpha=0.3, label=f'Length-Normed MedianIQR Paragraph Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_parags_df, alpha=0.3, label=f'MedianIQR Paragraph Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLength-Normed vs non-Normed Paragraph Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJmDFQeH6gI"
      },
      "source": [
        "##### **LOWESS Smoothed**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhYnx-MgH_G0"
      },
      "source": [
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0bylmqBf7Js"
      },
      "source": [
        "# Within the Selected_Section_No, use Lowess Smoothing and SciPy find_peaks()\n",
        "#    to get Sentence Crux Points \n",
        "\n",
        "win_lowess_start = 20\n",
        "win_lowess_end = 50\n",
        "win_lowess_step = 10\n",
        "\n",
        "win_lowess_no = 10\n",
        "sec_y_ht = 0\n",
        "\n",
        "for win_lowess_no in range(win_lowess_start, win_lowess_end, win_lowess_step):\n",
        "  section_crux_ls = get_lowess_cruxes(ts_df=section_sents_df, col_series=model_name, text_type='sentence', win_lowess=win_lowess_no, sec_y_height=sec_y_ht, subtitle_str=f'win={win_lowess_no}', do_plot=True, save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DQ4zGtRizlM"
      },
      "source": [
        "# TODO: Only printing sentiment for first crux point\n",
        "\n",
        "section_sents_df.iloc[62]['sent_raw']\n",
        "print('\\n')\n",
        "section_sents_df.iloc[62]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B0vm5RjgCLS"
      },
      "source": [
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "\n",
        "\n",
        "crux_sortsents_report(section_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhDPBG0g6MOr"
      },
      "source": [
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEmBJc6S6R_S"
      },
      "source": [
        "# LOWESS Smoothed Sentences within chosen Selection No\n",
        "\n",
        "modelname='vader'\n",
        "\n",
        "my_afrac = 1./12   # 1./12 ~ 0.08\n",
        "\n",
        "temp_df = get_lowess(corpus_sents_df, [model_name], \n",
        "                     plot_subtitle='LOWESS Smoothed MedianIRQ Sentence Sentiment', \n",
        "                     alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                     afrac=my_afrac, \n",
        "                     ait=7, \n",
        "                     alpha=0.8, \n",
        "                     do_plot=True, \n",
        "                     save2file=False)\n",
        "\n",
        "temp_df.columns\n",
        "col_lowess = f'{model_name}_{my_afrac:2.3f}lowess'\n",
        "col_lowess_clean = col_lowess.replace('_0.','_')\n",
        "# section_sents_df[col_lowess_clean] = temp_df['median']\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Sentence Sentiments with selected Section #{Select_Section_No} (LOWESS frac={my_afrac:.2f})')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend('',frameon=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa124qbuKzzH"
      },
      "source": [
        "# LOWESS Smoothed Sentences within chosen Selection No\n",
        "\n",
        "my_afrac = 1./12   # 1./12 ~ 0.08\n",
        "\n",
        "temp_df = get_lowess(section_sents_df, [model_name], plot_subtitle='LOWESS Smoothed MedianIRQ Sentence Sentiment', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                afrac=my_afrac, ait=7, alpha=0.8, do_plot=True, save2file=False)\n",
        "temp_df.columns\n",
        "col_lowess = f'{model_name}_{my_afrac:2.3f}lowess'\n",
        "col_lowess_clean = col_lowess.replace('_0.','_')\n",
        "section_sents_df[col_lowess_clean] = temp_df['median']\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Sentence Sentiments with selected Section #{Select_Section_No} (LOWESS frac={my_afrac:.2f})')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend('',frameon=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLeyWMtMaEqZ"
      },
      "source": [
        "# LOWESS Smoothed Paragraphs within chosen Selection No\n",
        "\n",
        "my_afrac = 1./8 # 1./8 ~ 0.125\n",
        "\n",
        "temp_df = get_lowess(section_parags_df, [model_name], plot_subtitle='LOWESS Smoothed Mean Rolling Sentence Sentiment', alabel=f'LOWESS Smoothed (afrac={my_afrac})', \n",
        "                afrac=my_afrac, ait=7, alpha=0.8, do_plot=True, save2file=False)\n",
        "\n",
        "section_parags_df[f'{model_name}_{my_afrac:.2f}_lowess'] = temp_df['median']\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Sentence Sentiments with selected Section #{Select_Section_No} (LOWESS frac={my_afrac:.2f})')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend('',frameon=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GfIuHNNmu-H"
      },
      "source": [
        "section_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEMWOxr7m2cl"
      },
      "source": [
        "section_crux_sents_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zouNSYD9Yloj"
      },
      "source": [
        "section_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vVdmcTcYb40"
      },
      "source": [
        "model_name = 'sentimentr_lnorm_medianiqr'\n",
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJa85Yx-rkkx"
      },
      "source": [
        "!pip install hdbscan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGA7kXGjrNET"
      },
      "source": [
        "from hdbscan import HDBSCAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZYo_LR8rF_J"
      },
      "source": [
        "\n",
        "\n",
        "y_ls = [1,2,4,7,9,5,4,7,9,56,57,54,60,200,297,275,243]\n",
        "y = np.reshape(y_ls, (-1, 1))\n",
        "type(y)\n",
        "y.shape\n",
        "\n",
        "clusterer = HDBSCAN(min_cluster_size=3)\n",
        "cluster_labels = clusterer.fit_predict(y)\n",
        "\n",
        "best_cluster = clusterer.exemplars_[cluster_labels[y.argmax()]].ravel()\n",
        "print(best_cluster)\n",
        "cluster_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsvTX-vtz8lq"
      },
      "source": [
        "type(y_ls)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo2zOaWVtUsm"
      },
      "source": [
        "y = np.reshape(crux_points_x_ls, (-1,1))\n",
        "\n",
        "clusterer = HDBSCAN(min_cluster_size=3)\n",
        "cluster_labels = clusterer.fit_predict(y)\n",
        "\n",
        "best_cluster = clusterer.exemplars_[cluster_labels[y.argmax()]].ravel()\n",
        "print(best_cluster)\n",
        "cluster_labels\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBXI8l6ywagn"
      },
      "source": [
        "len(color_palette)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB5VL_sV0tAG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRoIWeY1vx8M"
      },
      "source": [
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=3).fit(y)\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')\n",
        "\n",
        "color_palette = sns.color_palette('deep', 9)\n",
        "cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in cluster_labels]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, clusterer.probabilities_)]\n",
        "x = np.zeros_like(y) + 12.5\n",
        "plt.scatter(*y.T, x, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN7Rpw7B0xu-"
      },
      "source": [
        "print(crux_points_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLafViJ2zxrQ"
      },
      "source": [
        "len(crux_points_ls)\n",
        "crux_points_np.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irX4QSEbtTiy"
      },
      "source": [
        "crux_points_np = np.reshape(crux_points_ls, (-1,1))\n",
        "# np.reshape(crux_points_x_ls, (-1,1))\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=3).fit(crux_points_np)\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')\n",
        "\n",
        "color_palette = sns.color_palette('deep', 9)\n",
        "cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in cluster_labels]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, clusterer.probabilities_)]\n",
        "y_np = np.zeros_like(crux_points_np) + 12.5\n",
        "plt.scatter(*crux_points_np.T, y_np, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NRZ0Auka-uG"
      },
      "source": [
        "section_crux_sents_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGDccDTMfMT4"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10, 1./14, 1./16]\n",
        "# grid_fracs = [1./10, 1./14, 1./16]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.3]\n",
        "win_lowess=9\n",
        "\n",
        "\n",
        "# section_sents_df['vader_lnorm_medianiqr'].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot('vader_lnorm_medianiqr', data=section_sents_df)\n",
        "plt.title(f'LOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "section_crux_sents_dt = {}\n",
        "\n",
        "for afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  #   _ = get_lowess(section_sents_df, ['vader_lnorm_medianiqr'], plot_subtitle='Naive Raw + MedianIQR Midpoints', alabel=f'LOWESS Smoothed (afrac={my_afrac})', \n",
        "  #                afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # afrac = 1./7\n",
        "  # model_name = 'vader_lnorm_medianiqr' # model_name\n",
        "  sm_x, sm_y = sm_lowess(endog=section_sents_df[model_name].values, exog=section_sents_df.index.values, frac=afrac, it=3, return_sorted = True).T\n",
        "  col_lowess_frac = f'{model_name}_frac{int((afrac-int(afrac))*100)}_win{win_lowess}'\n",
        "  section_sents_df[col_lowess_frac] = sm_y\n",
        "  # _ = get_lowess(ts_df='section_sents_df', models_ls=[col_roll_str], text_unit='sentence', plot_subtitle='', alabel='', afrac=1./10, ait=5, alpha=0.5, do_plot=True, save2file=False)\n",
        "  section_crux_ls = list(get_lowess_cruxes(section_sents_df, col_series=col_lowess_frac, win_lowess=win_lowess, do_plot=False))\n",
        "  # col_lowess_frac = f'{model_name}_frac{int((afrac-int(afrac))*100)}_win{win_lowess}'\n",
        "  # print(f\"col_lowess_frac: {col_lowess_frac}\")\n",
        "  section_crux_sents_dt[col_lowess_frac] = section_crux_ls # list(zip(sm_x, sm_y))\n",
        "  # x, y = zip(*data)\n",
        "\n",
        "  # Set vertical y-axis magnification\n",
        "  y_mag = 30\n",
        "  plt.plot(sm_x, y_mag*sm_y)\n",
        "  plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nDifferent LOWESS Smoothed SMA Sentence Sentiments and Crux Points within selected Section #{Select_Section_No}')\n",
        "  plt.legend(loc='best')\n",
        "\n",
        "\n",
        "# Plot Crux Points for all LOWESS Curves on the x-axis\n",
        "crux_points_ls = []\n",
        "for key,value in section_crux_sents_dt.items():\n",
        "  model_lowess_name = key\n",
        "  crux_points_ls.extend(value)\n",
        "  plt.scatter(*zip(*value))\n",
        "\n",
        "# Plot Automatic HDBSCAN Clusters of Crux Points\n",
        "crux_points_np = np.reshape(crux_points_ls, (-1, 1))\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=3).fit(crux_points_np)\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')\n",
        "\n",
        "color_palette = sns.color_palette('deep', 9)\n",
        "cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in cluster_labels]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, clusterer.probabilities_)]\n",
        "y_np = np.zeros_like(crux_points_np) + 12.5\n",
        "plt.scatter(*crux_points_np.T, y_np, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} Sentence Crux Detection within selected Section #{Select_Section_No}\\nLOWESS Smoothed (Model: {model_lowess_name})')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\n",
        "# Plot the mean of all SMA MedianIQR Sentiment Time Series\n",
        "# col_meanroll = f'{model_name}_rollmean'\n",
        "# section_sents_df[col_meanroll] = section_sents_df[col_rolls_ls].mean(axis=1)\n",
        "# section_sents_df[col_meanroll].plot(label='mean', color='black', linewidth=3)\n",
        "\n",
        "# Plot corresponding Crux Points\n",
        "# model_name = 'vader_lnorm_medianiqr'\n",
        "section_crux_ls = get_lowess_cruxes(section_sents_df, col_series=model_name, win_lowess=10, do_plot=False) # 'vader_lnorm_medianiqr_0.07_lowess')\n",
        "section_sents_df.shape[0]\n",
        "print('\\n');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDx21JbaeL9s"
      },
      "source": [
        "!pip install kmeans1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFPbi6WqQVUs"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPcTifCyeUJh"
      },
      "source": [
        "import kmeans1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN4M4LEAeoCa"
      },
      "source": [
        "crux_points_x_ls = [x[0] for x in crux_points_ls]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htu7N02odHaU"
      },
      "source": [
        "**Enter how many Clusters of potential Crux Points you see in the Plot above**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBSVG9LXa2FD"
      },
      "source": [
        "Cluster_Count = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "clusters, centroids = kmeans1d.cluster(crux_points_x_ls, Cluster_Count)\n",
        "\n",
        "print(clusters)  \n",
        "print(centroids)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVCq0YUXBpX8"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2]\n",
        "\n",
        "# section_sents_df['vader_lnorm_medianiqr'].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot('vader_lnorm_medianiqr', data=section_sents_df)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "for my_afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  _ = get_lowess(section_sents_df, [model_name], plot_subtitle='Naive Raw + MedianIQR Midpoints', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                 afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # corpus_cruxes_dt['vader_lnorm_medianiqr'] = plot_crux_sections(model_names_ls=['vader_lnorm_medianiqr'], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL}\\n LOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpzOnRqZS3jZ"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.12, 0.14, 0.16, 0.18, 0.2]\n",
        "\n",
        "# section_sents_df[model_name].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot(model_name, data=section_sents_df)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "for my_afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  _ = get_lowess(section_parags_df, [model_name], plot_subtitle='MedianIQR Midpoints', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                 afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # corpus_cruxes_dt[model_name] = plot_crux_sections(model_names_ls=[model_name], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL}\\n LOWESS Smoothed Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O85J51__Ka-z"
      },
      "source": [
        "section_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48u_zr6-KkH-"
      },
      "source": [
        "# SMA Sentences\n",
        "\n",
        "# grid_afracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.3]\n",
        "# grid_fracs = [1./6, 1./7, 1./8, 1./9, 1./10, 1./15, 1./20]\n",
        "scale_roll = 1.\n",
        "win_lowess_per = 30\n",
        "win_lowess = int(win_lowess/100 * section_sents_df.shape[0])\n",
        "\n",
        "col_meanroll_lowess_ls = []\n",
        "\n",
        "col_meanroll = f'{model_name}_mean_roll050'\n",
        "for afrac in grid_fracs:\n",
        "  lowess_smooth_df = get_lowess(section_sents_df, [col_meanroll], plot_subtitle='SMA Mean of MedianIQR ', alabel=f'LOWESS afrac={afrac:.3f}', \n",
        "                afrac=afrac, ait=7, alpha=0.3, do_plot=True, save2file=False)\n",
        "  # print(f'type: {lowess_smooth_df.columns}')\n",
        "\n",
        "# col_lowess_str = f'{col_mean_roll}_lowess_frac{10*win_lowess}'\n",
        "col_meanroll_lowess_str = f'{col_meanroll}_frac{int((afrac-int(afrac))*100)}_win{win_lowess}'\n",
        "col_meanroll_lowess_ls.append(col_meanroll_lowess_str)\n",
        "section_sents_df[col_meanroll_lowess_str] = section_sents_df[model_name].apply(lambda x: x*scale_roll).rolling(win_lowess, center=True).mean()\n",
        "section_sents_df[col_meanroll_lowess_str].plot(alpha=0.7)\n",
        "get_lowess(section_sents_df, [col_meanroll_lowess_str], plot_subtitle='SMA Mean of MedianIQR ', alabel=f'LOWESS (afrac={my_afrac:.3f})', \n",
        "                afrac=afrac, ait=7, alpha=1.0, do_plot=True, save2file=False);\n",
        "\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KlWvNkcMI-M"
      },
      "source": [
        "**Raw Paragraph Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhRcTp4ALYUQ"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "win_sents_ls = [5,10,15,20,25]\n",
        "scale_roll = 6\n",
        "\n",
        "plt.plot(model_name, data=section_parags_df, alpha=0.3, label=f'Raw Paragraph Sentiment within selected Segment #{Select_Section_No}')\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5TcmNEsMMiE"
      },
      "source": [
        "**Length-Normed Paragraph Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BemEIw0Tknk"
      },
      "source": [
        "corpus_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyRTfn6HI50X"
      },
      "source": [
        "# Plot and Compare Naive Raw and LOWESS Smoothed Paragraph Sentiment Time Series within selected Section\n",
        "\n",
        "section_parag_lowess_df = pd.DataFrame()\n",
        "section_parag_lowess_df['parag_no'] = section_parags_df['parag_no'].copy()\n",
        "\n",
        "parags_midpoint_sentiment_ls = []\n",
        "for parags_midpoint_idx in parags_midpoint_ls:\n",
        "  parags_midpoint_sentiment_ls.append(float(corpus_parags_df[corpus_parags_df['parag_no'] == parags_midpoint_idx][model_name]))\n",
        "\n",
        "col_midapprox = f'{model_name}_midapprox'\n",
        "section_parag_lowess_df[col_midapprox] = parags_midpoint_sentiment_ls\n",
        "\n",
        "\n",
        "section_parag_lowess_df[col_midapprox].plot(label='Raw Midpoints')\n",
        "plt.xlabel(f'Niave Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "\n",
        "_ = get_lowess(section_parag_lowess_df, [col_midapprox], plot_subtitle=f'{model_base.capitalize()} Naive Raw + MedianIQR Midpoints', alabel='LOWESS Midpoints', afrac=1./4, ait=7, do_plot=True, save2file=False);\n",
        "\n",
        "# section_lowess_parags_df = get_lowess(section_sents_parags_df, ['vader_lnorm_medianiqr'], plot_subtitle='Approximate Paragraph MedianIQR', afrac=1./4, ait=7, do_plot=True, save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24M7uauXygFc"
      },
      "source": [
        "**Length-Noramlized Raw and LOWESS Smoothed Paragraph Sentiment plots within selected Section**\n",
        "\n",
        "NOTE: Horizonal x-axis narrative time axis adjusted for variable paragraph lengths - used midpoints of unequal length Paragraphs to more accurately visualize Sentiment Arc and precisely localize Crux Points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOsXfELh-tt_"
      },
      "source": [
        "# Verify details on currently selected Section\n",
        "print(f'Details on Section #{Select_Section_No}')\n",
        "print('------------------------')\n",
        "print(f' Paragraph Count: {section_parags_df.shape[0]}')\n",
        "print(f' Sentence Count:  {section_sents_df.shape[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfQS_qAo-Qdh"
      },
      "source": [
        "# %load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq2JjzKIHEdO"
      },
      "source": [
        "# Plot Raw and Rolling Sentence Sentiments within selected Section\n",
        "\n",
        "win_per = 5  # Rolling Window size in percentage of total Corpus length\n",
        "\n",
        "section_sents_parags_df.plot(x='sent_no', y='vader_lnorm_medianiqr')\n",
        "\n",
        "plt.title(f'Raw and Rolling Sentence Sentiments within selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No} (Length-Normalized in terms of Paragraphs)')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "\n",
        "section_sents_parags_df['vader_lnorm_medianiqr'].rolling(int((win_per/100)*section_sents_parags_df.shape[0])).mean().plot(label=\"Approx Paragraph VADER SMA (win=5%)\");\n",
        "\n",
        "# section_sents_parags_df.plot(x='sent_no', y='vader_lnorm_medianiqr', label='Sentence VADER MedianIQR')\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr'].rolling(int(0.05*section_sents_parags_df.shape[0])).mean().plot(label=\"Sentence VADER SMA (win=5%)\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laYb3dm101Qa"
      },
      "source": [
        "**Get Crux Points within selected Section**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peaks] to search for Peaks (unselect to search for Valley)\n",
        "\n",
        "* Pick [Crux_Rank] (1-5) to get the 1st to 5th biggest Peak or Valley Crux Paragraph\n",
        "\n",
        "* Pick [Context_Paragraphs_Each_Side] (0-5) to get n paragraphs before and n paragraphs after the selected Crux Paragraph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyekwnvX4wkj"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# ARCHIVED\n",
        "\n",
        "def get_sentnocontext(ts_df, model_name='vader', get_peaks=True, crux_rank=1, n_sideparags=1):\n",
        "  # get_cruxparags_section()\n",
        "  '''\n",
        "  Given a Section DataFrame with model_name sentiment column and crux peak/valley, rank and side paragraphs context\n",
        "  Return a list with the appropriate Crux Paragraph within this Section, and context\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "  Given a sentence number in the Corpus\n",
        "  Return the containing paragraph and n-paragraphs on either side\n",
        "  (e.g. if n=2, return 2+1+2=5 paragraphs)\n",
        "  '''\n",
        "\n",
        "  crux_parags_context_ls = []\n",
        "\n",
        "  if get_peaks == True:\n",
        "    sort_asc_flag=False\n",
        "  else:\n",
        "    sort_asc_flag=True\n",
        "\n",
        "  crux_parag_no = ts_df.sort_values(by=[model_name], ascending=sort_asc_flag).iloc[crux_rank-1]['parag_no']\n",
        "\n",
        "  print(f'crux_parag_no: {crux_parag_no}')\n",
        "\n",
        "  if n_sideparags == 0:\n",
        "    crux_parags_context_ls = list(corpus_parags_df[corpus_parags_df['parag_no'] == crux_parag_no]['parag_raw'])\n",
        "\n",
        "  else:\n",
        "    parag_start = crux_parag_no - n_sideparags\n",
        "    parag_end = crux_parag_no + n_sideparags + 1\n",
        "    crux_parags_context_ls = list(corpus_parags_df.iloc[parag_start:parag_end]['parag_raw'])\n",
        "\n",
        "  return crux_parags_context_ls\n",
        "\n",
        "# Test\n",
        "parags_context_ls = get_sentnocontext(ts_df=section_parags_df, model_name='vader_lnorm_medianiqr', get_peaks=True, crux_rank=1, n_sideparags=1)\n",
        "parags_context_ls\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxeabYXf2iqB"
      },
      "source": [
        "# def get_crux_parags_report(ts_df, model_name='vader', get_peaks=True, crux_rank=1, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence):\n",
        "\n",
        "# def get_sentnocontext_report(ts_df, model_name='vader', get_peaks=True, crux_rank=1, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence):\n",
        "'''\n",
        "Given a DataFrame with model_name sentiment column and crux peak/valley, rank and side paragraphs context\n",
        "Return a list with the appropriate Crux Paragraph, and context\n",
        "'''\n",
        "\"\"\"\n",
        "\n",
        "def get_sentnocontext_report(the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n",
        "  '''\n",
        "  Wrapper function around  get_sentnocontext()\n",
        "  Prints a nicely formatted context report\n",
        "  '''\n",
        "\n",
        "  context_noparags = the_n_sideparags*2+1\n",
        "\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f'The {context_noparags} Paragraph(s) Context around the Sentence #{Crux_Sentence_No} Crux Point:')\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f\"\\nCrux Sentence Raw Text: -------------------------------\\n\\n    {corpus_sents_df[corpus_sents_df['sent_no'] == the_sent_no]['sent_raw']}\") # iloc[the_sent_no]['sent_raw']}\")\n",
        "\n",
        "  print(f\"\\n{context_noparags} Paragraph(s) Context: ------------------------------\")\n",
        "  # context_parags_ls = get_sentnocontext(sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_parags_ls = get_sentnocontext(sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_len = len(context_parags_ls)\n",
        "  context_mid = context_len//2\n",
        "  for i, aparag in enumerate(context_parags_ls):\n",
        "    if i==context_mid:\n",
        "      # print(f'\\n>>> Paragraph #{i}: <<< Crux Point Sentence CAPITALIZED within this Paragraph\\n\\n    {aparag}')\n",
        "      print(f'\\n<*> {aparag}')\n",
        "    else:\n",
        "      # print(f'\\n    Paragraph #{i}:\\n\\n    {aparag}')\n",
        "      print(f'\\n    {aparag}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# get_sentnocontext_report(sent_no=1051, n_sideparags=1, sent_highlight=True)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxQGzYhgHEUD"
      },
      "source": [
        "# Get_Peaks = True #@param {type:\"boolean\"}\n",
        "Crux_Rank = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "\n",
        "# try:\n",
        "  \n",
        "# get_sentnocontext_report(ts_df=section_sents_df, model_name=model_name, get_peaks=Get_Peaks, crux_rank=Crux_Rank, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "get_sentnocontext_report(corpus_sents_df, the_sent_no=Crux_Rank, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "\n",
        "# except:\n",
        "#   print('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cPnJ7-QAHBp"
      },
      "source": [
        "section_parags_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga65SGcg5sM9"
      },
      "source": [
        "print(section_parags_df.sort_values(by=['vader_lnorm_medianiqr'], ascending=False).iloc[0]) # ['parag_no'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGuFadpM8QoB"
      },
      "source": [
        "parags_context_ls = get_cruxparag_context(ts_df=section_sents_parags_df, model_name='vader_lnorm_medianiqr', get_peaks=True, crux_rank=1, n_sideparags=2, sent_highlight=True)\n",
        "parags_context_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPE_ODwK2imT"
      },
      "source": [
        "section_sents_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gz8UImlq5Uy"
      },
      "source": [
        "# **Save Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NH8ltVX9SvD"
      },
      "source": [
        "## **Section, Chapter Crux DataFrames (summary stats/sentiments only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovjd7uvr2hS8"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLb7mQgV7_JK"
      },
      "source": [
        "# Create a Chapter Summary DataFrame extracting only key information (no text)\n",
        "\n",
        "corpus_chaps_summary_df = corpus_chaps_df[['chap_no','sent_no_start','sent_no_mid','char_len','token_len',\n",
        "                 'sentimentr','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'syuzhet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'bing','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'sentiword','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'senticnet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'nrc','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'afinn','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'vader','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'textblob','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'pattern','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'stanza','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 ]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSvXSwp-2ePp"
      },
      "source": [
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5s8R38P81M7"
      },
      "source": [
        "# Create a Section Summary DataFrame extracting only key information (no text)\n",
        "\n",
        "corpus_sects_summary_df = corpus_sects_df[['sect_no','sent_no_start','sent_no_mid','char_len','token_len',\n",
        "                 'sentimentr','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'syuzhet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'bing','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'sentiword','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'senticnet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'nrc','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'afinn','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'vader','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'textblob','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'pattern','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'stanza','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 ]]\n",
        "\n",
        "corpus_sects_summary_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Op-IgH9iP2"
      },
      "source": [
        "# Save the original Corpus text at 4 levels of semantic grouping: sentences, paragraphs, sections and chapters\n",
        "\n",
        "# Save Section and Chapter DataFrames Metainformation (e.g. sent_no_start) and Sentiment Values\n",
        "corpus_sects_summary_filename = f'corpus_section_summary_lexrules_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Section Summary to file: {corpus_sects_summary_filename}')\n",
        "corpus_sects_summary_df.to_csv(corpus_sects_summary_filename)\n",
        "\n",
        "corpus_chaps_summary_filename = f'corpus_chapter_summary_lexrules_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Chapters Summary to file: {corpus_chaps_summary_filename}')\n",
        "corpus_chaps_summary_df.to_csv(corpus_chaps_summary_filename)\n",
        "\n",
        "# Save Corpus Cruxes Dictionary is saved to a JSON file\n",
        "corpus_cruxes_summary_filename = f'corpus_cruxes_summary_lexrules_{author_abbr_str}_{title_str}.json' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes Summary to file: {corpus_cruxes_summary_filename}')\n",
        "with open(corpus_cruxes_summary_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSO0vT7oXOXO"
      },
      "source": [
        "# Verify exported Section Summary file\n",
        "\n",
        "!head -n 5 $corpus_sects_summary_filename\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3RIJOdsI_Ud"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0A0M6TPu_Ml"
      },
      "source": [
        "## **Compare Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpX60Cld_B7z"
      },
      "source": [
        "def drop_dupcols(df):\n",
        "  '''\n",
        "  Given a DataFrame\n",
        "  Drop repeatitive columns\n",
        "  '''\n",
        "\n",
        "  col_drop_ls = []\n",
        "\n",
        "  col_ls = list(df.columns)\n",
        "  print(f'BEFORE: Columns #{len(df.columns)}')\n",
        "\n",
        "  for i, acol in enumerate(col_ls):\n",
        "    acol_word_ls = acol.split('_')\n",
        "    # print(f'acol_word_ls: {acol_word_ls}')\n",
        "    if (len(acol_word_ls)) == len(set(acol_word_ls)):\n",
        "      continue\n",
        "    else:\n",
        "      col_drop_ls.append(acol)\n",
        "\n",
        "  df.drop(columns=col_drop_ls, inplace=True, axis=1)\n",
        "\n",
        "  print(f'AFTER: Columns #{len(df.columns)}')\n",
        "\n",
        "  return col_drop_ls\n",
        "\n",
        "dropped_cols_ls = drop_dupcols(corpus_parags_df)\n",
        "print(f'dropped: {dropped_cols_ls}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIOFkFzYinfd"
      },
      "source": [
        "corpus_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lpvN8PR9VRk"
      },
      "source": [
        "# List of Tuples (Model, Scaling Factor) to plot together with same size for comparison\n",
        "\n",
        "models_sma_ls = [('vader_lnorm_medianiqr',1),\n",
        "                 ('textblob_lnorm_medianiqr',20),\n",
        "                 ('afinn_lnorm_medianiqr',4),\n",
        "                 ('sentimentr_lnorm_medianiqr',1),\n",
        "                 ('syuzhet_lnorm_medianiqr',1),\n",
        "                 ('bing_lnorm_medianiqr',0.1),\n",
        "                 ('sentiword_lnorm_medianiqr',10),\n",
        "                 ('senticnet_lnorm_medianiqr',.5),\n",
        "                 ('nrc_lnorm_medianiqr',0.2),\n",
        "                 ('pattern_lnorm_medianiqr',20),\n",
        "                 ('stanza_lnorm_medianiqr',0.5),\n",
        "                 ('hfbert_lnorm_medianiqr',5),\n",
        "                 ('nlptown_lnorm_medianiqr',5),\n",
        "                 ('robertalg15_lnorm_medianiqr',5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1pjqF9nTQg-"
      },
      "source": [
        "# List of Tuples (Model, Scaling Factor) to plot together with same size for comparison\n",
        "\n",
        "models_sma_ls = [('vader',1),\n",
        "                 ('textblob',20),\n",
        "                 ('afinn',4),\n",
        "                 ('sentimentr',1),\n",
        "                 ('syuzhet',1),\n",
        "                 ('bing',0.1),\n",
        "                 ('sentiword',10),\n",
        "                 ('senticnet',.5),\n",
        "                 ('nrc',0.2),\n",
        "                 ('pattern',20),\n",
        "                 ('stanza',0.5),\n",
        "                 ('hfbert',5),\n",
        "                 ('nlptown',5),\n",
        "                 ('robertalg15',5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RihyT4wZFP5k"
      },
      "source": [
        "corpus_sents_df[['stanza_lnorm_medianiqr','stanza']].rolling(500,center=True).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaxzZkVlBcNf"
      },
      "source": [
        "def plot_autoscaled_ts(ts_df=corpus_sents_df, ts_ls=['vader_lnorm_medianiqr', 'textblob_lnorm_medianiqr', \n",
        "                                                     'syuzhet_lnorm_medianiqr', 'sentimentr_lnorm_medianiqr',\n",
        "                                                     'bing_lnorm_medianiqr', 'afinn_lnorm_medianiqr',\n",
        "                                                     'pattern_lnorm_medianiqr', 'stanza_lnorm_medianiqr']):\n",
        "  '''\n",
        "  Given a DataFrame and list of Columns/Time Series\n",
        "  Automatically scale all to the same range and plot together\n",
        "  '''\n",
        "\n",
        "  ts_spans_ls = []\n",
        "\n",
        "  current_min = ts_df[ts_ls[0]].min()\n",
        "  current_max = ts_df[ts_ls[0]].max()\n",
        "  ts_spans_ls.append(float(current_max - current_min))\n",
        "\n",
        "  for ats in ts_ls[1:]:\n",
        "    current_min = ts_df[ats].min()\n",
        "    current_max = ts_df[ats].max()\n",
        "    ts_spans_ls.append(float(current_max - current_min))\n",
        "\n",
        "  # find index of maximum span\n",
        "  max_index = ts_spans_ls.index(max(ts_spans_ls))\n",
        "  max_span_value = ts_spans_ls[max_index]\n",
        "  max_span_model = ts_ls[max_index]\n",
        "  print(f'max span is: {max_span_value} from {max_span_model}')\n",
        "\n",
        "  for i, ats in enumerate(ts_ls):\n",
        "    y_scaling_factor = max_span_value/ts_spans_ls[i]\n",
        "    print(f'ats={ats} with scaling={y_scaling_factor}')\n",
        "    ts_y_scaled_ser = ts_df[ats].apply(lambda x: x*y_scaling_factor)\n",
        "    # plt.plot()\n",
        "    plot_df = pd.DataFrame()\n",
        "    plot_df['x_value'] = ts_df.index\n",
        "    plot_df['y_scaled'] = ts_y_scaled_ser\n",
        "    plot_df['y_scaled_roll050'] = plot_df['y_scaled'].rolling(350, center=True).mean()\n",
        "    plot_df['y_scaled_roll050'].plot(label=f'{ats}')\n",
        "    plt.legend(loc='best')\n",
        "    # sns.lineplot(data=plot_df, x='x_value', y='y_scaled', alpha=0.5, label=f'{ats}')\n",
        "\n",
        "  return ts_spans_ls, ts_ls\n",
        "\n",
        "# Test\n",
        "ts_spans_ls, ts_ls = plot_autoscaled_ts()\n",
        "zip_ls = zip(ts_spans_ls, ts_ls)\n",
        "for aspan, amodel in zip_ls:\n",
        "  print(f'model: {amodel} with span: {aspan}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emkaDWvDILDc"
      },
      "source": [
        "ts_df=corpus_sents_df, ts_ls=['vader_lnorm_medianiqr', 'textblob_lnorm_medianiqr', \n",
        "                                                     'syuzhet_lnorm_medianiqr', 'sentimentr_lnorm_medianiqr',\n",
        "                                                     'bing_lnorm_medianiqr', 'afinn_lnorm_medianiqr',\n",
        "                                                     'pattern_lnorm_medianiqr', 'stanza_lnorm_medianiqr']):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T8pf6DsIGmq"
      },
      "source": [
        "# Test\n",
        "ts_spans_ls, ts_ls = plot_autoscaled_ts(ts_df=corpus_parags_df)\n",
        "zip_ls = zip(ts_spans_ls, ts_ls)\n",
        "for aspan, amodel in zip_ls:\n",
        "  print(f'model: {amodel} with span: {aspan}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWGdzLS4Ibr-"
      },
      "source": [
        "corpus_sents_df['pattern_lnorm_medianiqr'].hist(bins=100) # rolling(100, center=True).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42RrUksBOxfo"
      },
      "source": [
        "models_sma_ls = [('vader_lnorm_medianiqr',1),\n",
        "                 ('textblob_lnorm_medianiqr',20),\n",
        "                 ('sentimentr_lnorm_medianiqr',1),\n",
        "                 ('syuzhet_lnorm_medianiqr',1),\n",
        "                 ('stanza_lnorm_medianiqr',0.2)]\n",
        "\n",
        "win_per = 5  # 5=5% of full corpus length\n",
        "win_roll = int(corpus_sents_df.shape[0]* win_per/100)\n",
        "\n",
        "for amodel, amag in models_sma_ls:\n",
        "  corpus_parags_df[amodel].rolling(win_roll, center=True).mean().apply(lambda x: amag*x).plot(linewidth=4, label=amodel)\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQgdwsJGZN_"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpOCp1-88rrF"
      },
      "source": [
        "# **Standardize and Remove Outliers (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mul5MSZrgKsw"
      },
      "source": [
        "## **Remove Outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07hyJuT1c5rJ"
      },
      "source": [
        "### **Before Removing Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKX8f0Q1I53M"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  print(f'Plotting {model_name}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_name, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzUoTpEWeyCy"
      },
      "source": [
        "# sns.lineplot(data=corpus_sents_df, x='sent_no', y='y_scaled', legend='brief', label='y_scaled')\n",
        "      \n",
        "# plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nSMA Smoothed Sentence Sentiment Plot (windows={win_ls})')\n",
        "# plt.legend(loc='best')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr1k-3Rsc5XL"
      },
      "source": [
        "# Plot all Raw Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_LS:\n",
        "  print(f'Plotting {model_name}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_name, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FrNFec7tNij"
      },
      "source": [
        "### **Remove Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYXa7qoevyuo"
      },
      "source": [
        "# Trim outliers to max of 3*Median Abs Variance in Standardized Sentiment Time Series\n",
        "#   and overwrite results in model_name column\n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\n",
        "# Sentence\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Sentence: {col_noouts_str} --------------------')\n",
        "  corpus_sents_df[col_noouts_str] = clip_outliers(corpus_sents_df[amodel_str])\n",
        "  \n",
        "  print(f'  old Standardized max: {corpus_sents_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sents_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_sents_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_sents_df[col_noouts_str].min()}')\n",
        "  \n",
        "# col_rename_dt = rename_cols(corpus_sents_df, models_ls) # ERROR: created 1 new col with col_rename_dt dictionary name instead of mapping correctly\n",
        "# col_rename_dt\n",
        "# _ = corpus_sents_df.rename(columns=col_rename_dt, inplace=True, errors='raise');\n",
        "\n",
        "# Paragraph\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Paragraph: {col_noouts_str} --------------------')\n",
        "  corpus_parags_df[col_noouts_str] = clip_outliers(corpus_parags_df[amodel_str])\n",
        "\n",
        "  print(f'  old Standardized max: {corpus_parags_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_parags_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_parags_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_parags_df[col_noouts_str].min()}')\n",
        "\n",
        "# Section\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Section: {col_noouts_str} --------------------')\n",
        "  corpus_sects_df[col_noouts_str] = clip_outliers(corpus_sects_df[amodel_str])\n",
        "\n",
        "  print(f'  old Standardized max: {corpus_sects_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sects_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_sects_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_sects_df[col_noouts_str].min()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyPsLI9E4tbB"
      },
      "source": [
        "# Trim outliers to max of 3*Median Abs Variance in Standardized Sentiment Time Series\n",
        "#   and overwrite results in model_name column\n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\"\"\"\n",
        "# Sentences\n",
        "for amodel in MODELS_LS:\n",
        "  col_stand = amodel + '_stand'\n",
        "  col_standout = amodel + '_standout'\n",
        "  print(f'Sentences: {col_stand} --------------------')\n",
        "  # corpus_sents_df[amodel] = corpus_sents_df[col_stand]\n",
        "  corpus_sents_df[col_standout] = clip_outliers(corpus_sents_df[col_stand])\n",
        "  \n",
        "  print(f'  old Standardized max: {corpus_sents_df[col_stand].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sents_df[col_stand].min()}')\n",
        "  print(f'  new max: {corpus_sents_df[col_standout].max()}')\n",
        "  print(f'  new min: {corpus_sents_df[col_standout].min()}')\n",
        "  \n",
        "# col_rename_dt = rename_cols(corpus_sents_df, models_ls) # ERROR: created 1 new col with col_rename_dt dictionary name instead of mapping correctly\n",
        "# col_rename_dt\n",
        "# _ = corpus_sents_df.rename(columns=col_rename_dt, inplace=True, errors='raise');\n",
        "\n",
        "# Paragraphs\n",
        "for amodel in MODELS_LS:\n",
        "  col_stand = amodel + '_stand'\n",
        "  col_standout = amodel + '_standout'\n",
        "  print(f'Paragraphs: {col_stand} --------------------')\n",
        "  # corpus_parags_df[amodel] = corpus_parags_df[col_stand]\n",
        "  corpus_parags_df[col_standout] = clip_outliers(corpus_parags_df[col_stand])\n",
        "  print(f'  old Standardized max: {corpus_parags_df[col_stand].max()}')\n",
        "  print(f'  old Standardized min: {corpus_parags_df[col_stand].min()}')\n",
        "  print(f'  new max: {corpus_parags_df[col_standout].max()}')\n",
        "  print(f'  new min: {corpus_parags_df[col_standout].min()}')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPKABrvstSIo"
      },
      "source": [
        "### **After Removing Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ccxOjqghbgP"
      },
      "source": [
        "# Plot all Raw Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "# Exlopre to find which Sentiment Time Series still have outliers after initial 2.5*Median Abs Dev Clipping\n",
        "MODELS_SENTS_EXCLUDE_LS = ['nrc','bing','afinn','stanza']  # Likely these TS are not normal or heavy tailed so 2.5*MedAbsDev did not clip well\n",
        "MODELS_SENTS_CUSTOM_LS = [x for x in MODELS_LS if x not in MODELS_SENTS_EXCLUDE_LS] \n",
        "\n",
        "for model_name in MODELS_SENTS_CUSTOM_LS:\n",
        "  model_noouts = f'{model_name}_noouts'\n",
        "  print(f'Plotting {model_noouts}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_noouts, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment w/Trimmed Outliers Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJHXLvayIhOC"
      },
      "source": [
        "# Plot all Raw Paragraph Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_LS:\n",
        "  model_standout = f'{model_name}_standout'\n",
        "  print(f'Plotting {model_standout}')\n",
        "  sns.lineplot(data=corpus_parags_df, x='parag_no', y=model_standout, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Paragraph Sentiment Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNp9zwBfmgpI"
      },
      "source": [
        "## **Standardize Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ_wcgiyx-NJ"
      },
      "source": [
        "### **Before Standardizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEUbDF4hx-NN"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  model_noouts_str = f'{model_name}_noouts'\n",
        "  print(f'Plotting {model_noouts_str}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_noouts_str, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_noouts_str}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IrnbsMByTmX"
      },
      "source": [
        "### **Standardized the NoOutliers Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhkW7faSmPWh"
      },
      "source": [
        "# Standardize Sentence and Paragraphs Sentiment Time Series (Section TS was Standardized above)\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "# orig_cols_ls = list(set(corpus_all_df.columns) - set(['sent_no','parag_no','sent_raw','sent_clean']))\n",
        "# cols_ls = []\n",
        "\n",
        "# Sentences\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_sents_df[acol].values.reshape(-1,1)))\n",
        "    corpus_sents_df[acol_new] = pd.Series(temp_np.squeeze())\n",
        "\n",
        "# Paragraphs\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_parags_df[acol].values.reshape(-1,1)))\n",
        "    corpus_parags_df[acol_new] = pd.Series(temp_np.squeeze())\n",
        "\n",
        "# Paragraphs\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_sects_df[acol].values.reshape(-1,1)))\n",
        "    corpus_sects_df[acol_new] = pd.Series(temp_np.squeeze())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn6EWXYaybhj"
      },
      "source": [
        "### **After Standardizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcGzZNZJO7fd"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  model_standouts_str = f'{model_name}_standouts'\n",
        "  print(f'Plotting {model_standouts_str}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_standouts_str, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_standouts_str}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37_ztlEpzYHx"
      },
      "source": [
        "MODELS_LS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Be4bSszK-A"
      },
      "source": [
        "## **Deselect Poorly Behaved Sentiment Time Series Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY_HdLarzK-D"
      },
      "source": [
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "NCR_Arc = True #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6z5qYX3zm9n"
      },
      "source": [
        "# Create and Verify custom list of Models to include\n",
        "\n",
        "MODELS_CUSTOM_LS = []\n",
        "\n",
        "if VADER_Arc:\n",
        "  MODELS_CUSTOM_LS.append('vader')\n",
        "if TextBlob_Arc:\n",
        "  MODELS_CUSTOM_LS.append('textblob')\n",
        "if Stanza_Arc:\n",
        "  MODELS_CUSTOM_LS.append('stanza')\n",
        "if SentimentR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentimentr')\n",
        "if Syuzhet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('syuzhet')\n",
        "if AFINN_Arc:\n",
        "  MODELS_CUSTOM_LS.append('afinn')\n",
        "if Bing_Arc:\n",
        "  MODELS_CUSTOM_LS.append('bing')\n",
        "if Pattern_Arc:\n",
        "  MODELS_CUSTOM_LS.append('pattern')\n",
        "if SentiWord_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentiword')\n",
        "if SenticNet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('senticnet')\n",
        "if NCR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('nrc')\n",
        "\n",
        "print(f'Here are the Models we are using to ensemble and save:\\n   {MODELS_CUSTOM_LS}')\n",
        "\n",
        "models_incl_ls = []\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  models_incl_ls.append(amodel[:2])\n",
        "models_incl_str = ''.join(models_incl_ls)\n",
        "\n",
        "print(f'Here is a custom name abbr: {models_incl_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfiyOjMBBraW"
      },
      "source": [
        "## **Calculate Median of All (Trimmed Outliers then Standardized) Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1tX6BmT4U5h"
      },
      "source": [
        "# Create a list of models with Outliers trimmed and Standardized Time Series \n",
        "\n",
        "MODELS_CUSTOM_STANDOUTS_LS = []\n",
        "\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  model_standout_str = f'{amodel}_standouts'\n",
        "  MODELS_CUSTOM_STANDOUTS_LS.append(model_standout_str)\n",
        "\n",
        "print(f'List of NoOutliers/Standardized Models to compute Median on: {MODELS_CUSTOM_STANDOUTS_LS}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx3-Qug9Y6h0"
      },
      "source": [
        "# Disregard poorly behaved time series identified and stored in MODELS_SENTS_EXCLUDE_LS \n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\n",
        "corpus_sents_df['median_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_sents_df.head(2)\n",
        "\n",
        "corpus_sents_df['std_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_sents_df.head(2)\n",
        "\n",
        "corpus_sents_df['mean_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_sents_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2qunU4VfJ4J"
      },
      "source": [
        "# (For now) Paragraph Sentiment TS are better behaved and don't require excluding any Model\n",
        "\n",
        "corpus_parags_df['median_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_parags_df['std_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_parags_df['mean_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_parags_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtTY4t4k1QBc"
      },
      "source": [
        "# (For now) Paragraph Sentiment TS are better behaved and don't require excluding any Model\n",
        "\n",
        "corpus_sects_df['median_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_sects_df['std_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_sects_df['mean_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_parags_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp_gYLYbkjxd"
      },
      "source": [
        "## **Save Processed Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmyasWR1kjxh"
      },
      "source": [
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "corpus_sents_filename = f'corpus_sentences_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sentences to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "corpus_parags_filename = f'corpus_paragraphs_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Paragraphs to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "corpus_sects_filename = f'corpus_sections_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sections to file: {corpus_sects_filename}')\n",
        "corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "corpus_cruxes_filename = f'corpus_cruxes_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes to file: {corpus_cruxes_filename}')\n",
        "with open(corpus_cruxes_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUnbmOFckt2t"
      },
      "source": [
        "# **EDA Visualizations and Comparisons**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww1a9l8Lkjxk"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiHul8A1kjxn"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbY7D82jkjxp"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_sects_df.head(2)\n",
        "corpus_sects_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvvfzj7xkjxp"
      },
      "source": [
        "corpus_cruxes_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDiTp97_zlUm"
      },
      "source": [
        "# norm_cols_ls = ['distbertsst_norm', 'nlptown_norm','xlnet_sst5_norm','bert_imdb_norm', 'bertuc_googapps_norm', 'roberta_lg15_norm']\n",
        "\"\"\"\n",
        "\n",
        "# ARCHIVE\n",
        "\n",
        "cols_norm_ls = []\n",
        "cols_stand_ls = []\n",
        "\n",
        "for acol in corpus_all_df.columns:\n",
        "  if acol.endswith('_norm'):\n",
        "    print(f'Adding {acol} to norm_cols_ls')\n",
        "    cols_norm_ls.append(acol)\n",
        "  elif acol.endswith('_stand'):\n",
        "    print(f'Adding {acol} to stand_cols_ls')\n",
        "    cols_stand_ls.append(acol)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "print(f'\\nNormalized Columns: {cols_norm_ls}')\n",
        "\n",
        "print(f'\\nStandardized Columns: {cols_stand_ls}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O3l61be6_GQ"
      },
      "source": [
        "### **LOWESS Smoothed Single Plot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUjsXyRG81zT"
      },
      "source": [
        "**Normalized Sentiment Smoothed with LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5MYEwGuZ2vh"
      },
      "source": [
        "MODELS_STAND_LS = []\n",
        "for amodel in MODELS_LS:\n",
        "  MODELS_STAND_LS.append(f'{amodel}_stand')\n",
        "\n",
        "print(f'MODELS_STAND_LS: {MODELS_STAND_LS}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBT-Qd4o8919"
      },
      "source": [
        "**Standardized Sentiment Smoothed with LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ-Oe8icVpHn"
      },
      "source": [
        "# Plot and Compare all LOWESS Smoothed *Standardized* Sentiment Time Series\n",
        "\n",
        "corpus_lowess_stand_df = get_lowess(corpus_all_df, cols_stand_ls, plot_subtitle='Standardized', afrac=1./10, ait=5, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JizqEQE6X3oQ"
      },
      "source": [
        "### **High Level: Section View**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIwEIY4cd2PB"
      },
      "source": [
        "##### **Raw Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryWst950gpYx"
      },
      "source": [
        "# Plot Raw Section Sentiments\n",
        "\"\"\"\n",
        "\n",
        "# ARCHIVED\n",
        "\n",
        "for amodel in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  amodel_stand_str = f'{amodel}'\n",
        "  plot_raw_sentiments(model_name=amodel_stand_str, semantic_type='section', save2file=False)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx4mryQwVz7h"
      },
      "source": [
        "# Raw Standardized Section Sentiment Time Series\n",
        "\n",
        "_ = plot_stand_crux_sections(ts_df=corpus_sects_df, model_names_ls=MODELS_CUSTOM_STANDOUTS_LS, semantic_type='section', label_token_ct=5, title_xpos=0.5, title_ypos=1, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0HK0lbzjXXF"
      },
      "source": [
        "# Plot the Median of the Customized Set of NoOutliers/Standardized Section Sentiment Time Series\n",
        "\n",
        "corpus_sects_df['median_standouts_custom_lex'].plot()\n",
        "plt.title(f'{CORPUS_FULL}\\n Median of NoOutliers/Standardized Section Sentiment Time Series');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKz34IzZdxIg"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeOWF6frEm0S"
      },
      "source": [
        "Window_Width = 4 #@param {type:\"slider\", min:2, max:20, step:1}\n",
        "\n",
        "# DEFAULT 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1lngzR7XM5l"
      },
      "source": [
        "# SMA of Custom Set of NoOutliers/Standardized Section Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/Adjust the win_ls to explore different window_size by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  # print(f'Plotting {acol_name}')\n",
        "  get_smas(corpus_sects_df, model_name, text_unit='section', subtitle_str='(NOTE: different x-scale)', win_ls=[Window_Width])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5Qw6frQdz1C"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAuz19B5ntxk"
      },
      "source": [
        "# Standardized Section LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sects_df, models_ls=MODELS_STAND_LS, text_unit='section', afrac=1./4, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-u2ScQZD8Lv"
      },
      "source": [
        "LOWESS_fraction = 0.15 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 (approx 0.17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L82SbpnkO_3"
      },
      "source": [
        "# Standardized Median Section LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sects_df, models_ls=['median'], text_unit='section', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6Ipaa-lWVwP"
      },
      "source": [
        "### **Middle Level: Paragraph Views**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDYYpJYRfrZ-"
      },
      "source": [
        "##### **Raw Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxyJD-fjf4ev"
      },
      "source": [
        "# Plot Custom Set of NoOutliers/Standardized Paragraph Sentiments Time Series \n",
        "\n",
        "for amodel in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  plot_raw_sentiments(model_name=amodel, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwD9lNuJkCfo"
      },
      "source": [
        "# Plot the Median for the Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series\n",
        "\n",
        "corpus_parags_df['median_standouts_custom_lex'].plot()\n",
        "plt.title(f'{CORPUS_FULL}\\n Median of Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jii7hmhleyHo"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJn79SwJDKHt"
      },
      "source": [
        "Window_Width = 10 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "490cQGkgIBK2"
      },
      "source": [
        "# SMA Standardized Paragraph Sentiment Arcs\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots (5-20 defaults)\n",
        "\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  get_smas(corpus_parags_df, model_name, text_unit='paragraph', win_ls=[Window_Width])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDo7OHyHCtUJ"
      },
      "source": [
        "Window_Percentage = 5 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT: 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mahzCm5W9-QL"
      },
      "source": [
        "# SMA Plot the Median for the Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots\n",
        "\n",
        "win_percentage = Window_Percentage  # 5 means rolling window size is 5% of corpus length (5-20 default)\n",
        "\n",
        "win_size = int(corpus_parags_df.shape[0]*(win_percentage*0.01))\n",
        "\n",
        "plot_title = f'{CORPUS_FULL}\\n Median of Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series (win={win_percentage}%)'\n",
        "corpus_parags_df['median_standouts_custom_lex'].rolling(win_size).mean().plot(title=plot_title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2CIR3U9e4WC"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIzHfrCdCC9d"
      },
      "source": [
        "LOWESS_fraction = 0.17 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 (approx 0.17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2337wydGe4WC"
      },
      "source": [
        "# Standardized Paragraph LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots (0.10-0.20 default)\n",
        "\n",
        "_ = get_lowess(corpus_parags_df, models_ls=MODELS_CUSTOM_STANDOUTS_LS, text_unit='paragraph', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0emSLGZCh6-Y"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vq8s_aShoEJ"
      },
      "source": [
        "# Plot it\n",
        "\n",
        "_ = get_lowess(corpus_parags_df, models_ls=['median_standouts_custom_lex'], text_unit='paragraph', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzcDX02pZGbr"
      },
      "source": [
        "### **Low Level: Sentence Views**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV6spOW1fSKf"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE_-_JJ9FeVj"
      },
      "source": [
        "Window_Percentage = 10 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT: 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gYKbdptopin"
      },
      "source": [
        "# Plot all Standardized Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "models_stand_ls = []\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  get_smas(corpus_sents_df, model_name, text_unit='sentence', alpha=0.3, win_ls=[Window_Percentage])\n",
        "\n",
        "# print(f'models_stand_ls: {models_stand_ls}')\n",
        "corpus_sents_df['median_stand'] = corpus_sents_df[models_stand_ls].median()\n",
        "plt.plot(corpus_sents_df.sent_no, corpus_sents_df.median_stand, color='black', label='Median')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0fL5RXKfZG6"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-NGw8yEFnJD"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDlGY9CMfUj9"
      },
      "source": [
        "# Standardized Paragraph LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sents_df, models_ls=MODELS_CUSTOM_STANDOUTS_LS, text_unit='sentence', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9SQeAvgGFb6"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-GtGa5mlEhZ"
      },
      "source": [
        "# Median of Custom Set of NoOutlier/Standardized Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sents_df, models_ls=['median_standouts_custom_lex'], text_unit='paragraph', afrac=1./8, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1VhYgLai3RK"
      },
      "source": [
        "## **Save Standardized-NoOutliers Sentiment Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3zmDWROOEvM"
      },
      "source": [
        "# Save all Processed DataFrames\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "corpus_sents_filename = f'corpus_sentences_only_lexrules_standouts_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Paragraphs DataFrame\n",
        "corpus_parags_filename = f'corpus_paragraphs_only_lexrules_standouts_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Section DataFrame\n",
        "corpus_sects_filename = f'corpus_sections_only_lexrules_standouts_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_sects_filename}')\n",
        "corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "\n",
        "# Save Cruxes\n",
        "corpus_cruxes_filename = f'corpus_cruxes_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes to file: {corpus_cruxes_filename}')\n",
        "with open(corpus_cruxes_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vUzh39HHJXz"
      },
      "source": [
        "# **Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJI_13B0HMnU"
      },
      "source": [
        "## **Sentiment Stability**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R6SMWtSHJLK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9sEH8_IG6Qq"
      },
      "source": [
        "## **Crux Point Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRjKHnQaOTu2"
      },
      "source": [
        "### **Gather (n) Highest/Lowest Sentiment Values for Each Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG6Y0vPFOkzn"
      },
      "source": [
        "def getn_cruxes(crux_dt, model_name='vader', get_n=6):\n",
        "  '''\n",
        "  Given a Crux Dictionary, a Model item/dict within, and and integer n\n",
        "  Return the n highest and n lowest sentiment values\n",
        "  NOTE: if get_n == 0, return all Crux Points for all Models\n",
        "  '''\n",
        "\n",
        "  cruxes_all_df = pd.DataFrame\n",
        "  cruxes_n_top_df = pd.DataFrame()\n",
        "\n",
        "  cruxes_all_df = pd.DataFrame.from_dict(crux_dt[model_name])\n",
        "\n",
        "  cruxes_all_df = cruxes_all_df.transpose().reset_index().rename(columns={'index':'var'})\n",
        "\n",
        "  cruxes_all_df.rename(columns={'var':'sent_no',0:model_name,1:'sent_raw'}, inplace=True)\n",
        "  cruxes_all_df.drop(columns=['sent_raw'], inplace=True)\n",
        "  cruxes_all_df.rename(columns={model_name:'sentiment'}, inplace=True)\n",
        "  cruxes_all_df['sentiment'] = cruxes_all_df['sentiment'].astype('float')\n",
        "  cruxes_all_df['model_name'] = model_name\n",
        "  cruxes_all_df = cruxes_all_df[['sent_no','model_name','sentiment']]\n",
        "\n",
        "  if get_n > 0:\n",
        "    cruxes_n_top_df = cruxes_all_df.nlargest(get_n, 'sentiment')\n",
        "    cruxes_n_top_df = cruxes_n_top_df.append(cruxes_all_df.nsmallest(get_n, 'sentiment'))\n",
        "  elif get_n ==0:\n",
        "    cruxes_n_top_df = cruxes_all_df\n",
        "  else:\n",
        "    print(f'ERROR: argument get_n must be either 0 (return all Cruxes) or greater than 0')\n",
        "    \n",
        "  return cruxes_n_top_df\n",
        "\n",
        "# Test\n",
        "\n",
        "cruxes_n_top_df = getn_cruxes(corpus_cruxes_dt, model_name='vader', get_n=3)\n",
        "cruxes_n_top_df.head(6)\n",
        "cruxes_n_top_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H67tOfiaSaHB"
      },
      "source": [
        "# Acculumate all the Crux Points from All Models into one cruxes_n_top_all_df DataFrame\n",
        "\n",
        "cruxes_n_top_all_df = pd.DataFrame()\n",
        "\n",
        "for amodel in MODELS_LS:\n",
        "  print(f'Appending Cruxes from {amodel}')\n",
        "  cruxes_n_top_df = getn_cruxes(corpus_cruxes_dt, model_name=amodel, get_n=12)\n",
        "  cruxes_n_top_all_df = cruxes_n_top_all_df.append(cruxes_n_top_df, ignore_index=True)\n",
        "\n",
        "for amodel in MODELS_LS:\n",
        "  crux_ct = len(cruxes_n_top_all_df[cruxes_n_top_all_df['model_name'] == amodel])\n",
        "  print(f'{amodel.capitalize()} has {crux_ct} Cruxes')\n",
        "\n",
        "print(f'There are a total of {cruxes_n_top_all_df.shape[0]} in all Models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQDWi6YLYVn0"
      },
      "source": [
        "# Plot Crux Points in 2D Space: Scatterplot\n",
        "\n",
        "sns.lmplot('sent_no', 'sentiment', data=cruxes_n_top_all_df, fit_reg=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab2oHmU1Yqcl"
      },
      "source": [
        "# Plot Crux Points in 1D Space: Histogram\n",
        "\n",
        "sns.distplot(cruxes_n_top_all_df.sent_no, bins=2000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtdL2OjAJm55"
      },
      "source": [
        "sns.clustermap(cruxes_n_top_all_df.sent_no)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiTZC_X4G6EC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE8EJC1wHCEn"
      },
      "source": [
        "## **Sentiment Arc Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFKeAFHlG-vm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TfBnxjYG5_u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wjBEXGyvU4x"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRo_Tt35VW8"
      },
      "source": [
        "**Bi/Tri-Polarity Lexicons**\n",
        "\n",
        "NRC: \n",
        "* https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
        "\n",
        "MPQA (Upitt):\n",
        "* https://mpqa.cs.pitt.edu/lexicons/effect_lexicon/\n",
        "* https://github.com/nlpcl-lab/mpqa2.0-preprocessing\n",
        "* https://github.com/kvangundy/basic-sentiment-analyzer/blob/master/sentimentDict.csv\n",
        "\n",
        "SentimentAnalysis.R (20210217 124s):\n",
        "* QDAP \n",
        "* DictionaryGI: Harvard-IV dictionary asused in the General Inquirer software (2005-/1637+)\n",
        "* DictionaryLM: Loughran-McDonald Financial dictionary (2355-/354+/297?)\n",
        "* DictionaryHE: Henry's Financial Dictionary (85-/105+)\n",
        "\n",
        "Custom Lexicons:\n",
        "* https://nealcaren.org/lessons/wordlists/ \n",
        "\n",
        "Lexicons\n",
        "* https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsOKzM-RYHf5"
      },
      "source": [
        "# Smooth Raw Sentiment Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWKU0OzT_9pe"
      },
      "source": [
        "**Simple Moving Average (SMA) by Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGluyBsFkQ7C"
      },
      "source": [
        "# Line Plots of Sentiment Values\n",
        "\n",
        "# set a grey background (use sns.set_theme() if seaborn version 0.11.0 or above) \n",
        "sns.set(style=\"darkgrid\")\n",
        "# df = sns.load_dataset(\"iris\")\n",
        "\n",
        "fig, axs = plt.subplots(4, 2, figsize=(12, 18))\n",
        "\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"median\", color=\"skyblue\", ax=axs[0, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"vader_mean_roll050\", color=\"skyblue\", ax=axs[0, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"sentimentr_mean_roll050\", color=\"olive\", ax=axs[1, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"syuzhet_mean_roll050\", color=\"olive\", ax=axs[1, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"bing_mean_roll050\", color=\"gold\", ax=axs[2, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"textblob_mean_roll050\", color=\"gold\", ax=axs[2, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"sentiword_mean_roll050\", color=\"teal\", ax=axs[3, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"senticnet_mean_roll050\", color=\"teal\", ax=axs[3, 1])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySuYAv2YxCO0"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iX1nCK6ljY8"
      },
      "source": [
        "%whos dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC3r9eWywyXB"
      },
      "source": [
        "\n",
        "# g=sns.pointplot(x=0, y=1, data=df, dodge=True,plot_kws=dict(alpha=0.3))\n",
        "# plt.setp(g.collections, alpha=.3) #for the markers\n",
        "# plt.setp(g.lines, alpha=.3)       #for the lines\n",
        "\n",
        "for i, sa_model in enumerate(corpus_sents_df.columns):\n",
        "  if (sa_model.endswith('_roll050')):\n",
        "    # if (sa_model != 'sent_no'):\n",
        "    if (sa_model == 'median'):\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, color='black')\n",
        "    else:\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, alpha=0.3)\n",
        "\n",
        "# print(f'{i}: {sa_model}')\n",
        "\n",
        "'''\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='median')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='textb')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTKI4Fu2ACKP"
      },
      "source": [
        "**Exponential Moving Average**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvVMrD58AL97"
      },
      "source": [
        "# Not Necessary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wEab5F8_3Wl"
      },
      "source": [
        "**LOWESS and LOESS Smoothing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPOo6VAMmWh_"
      },
      "source": [
        "sa_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlPwfjS8mKpY"
      },
      "source": [
        "def plot_lowess(df, df_cols_ls, aplot=True, afrac=1./10, ait=5):\n",
        "  '''\n",
        "  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n",
        "  Return a DataFrame with LOWESS values\n",
        "  If 'plot=True', also output plot\n",
        "  '''\n",
        "\n",
        "  # global corpus_sents_norm_df\n",
        "\n",
        "  lowess_df = pd.DataFrame()\n",
        "\n",
        "  for i,acol in enumerate(df_cols_ls):\n",
        "    sm_x, sm_y = sm_lowess(endog=df[acol].values, exog=df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n",
        "    col_new = f'{acol}_lowess'\n",
        "    lowess_df[col_new] = pd.Series(sm_x)\n",
        "    if aplot:\n",
        "      plt.plot(sm_x, sm_y, label=acol, alpha=0.5, linewidth=2)\n",
        "\n",
        "      frac_str = str(round(100*afrac))\n",
        "      plt.title(f'{CORPUS_FULL} \\n LOWESS (frac={frac_str} Sentence Sentiment (Model: {sa_model})')\n",
        "      plt.legend(title='Sentiment Series')\n",
        "\n",
        "  return lowess_df\n",
        "\n",
        "# Test\n",
        "new_lowess_col = f'{sa_model}_lowess'\n",
        "my_frac = 1./10\n",
        "my_frac_per = round(100*my_frac)\n",
        "new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n",
        "corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], afrac=my_frac)\n",
        "corpus_sents_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qRUhRfmmyUA"
      },
      "source": [
        "corpus_sents_norm_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxbZdPVbnL6w"
      },
      "source": [
        "corpus_sents_norm_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25CDCoDJmTcI"
      },
      "source": [
        "norm_cols_ls = []\n",
        "for acol in corpus_sents_norm_df.columns:\n",
        "  if acol.endswith('_2norm'):\n",
        "    norm_cols_ls.append(acol)\n",
        "\n",
        "print(f'All norm_cols_ls')\n",
        "\n",
        "temp_cols_ls = list(set(norm_cols_ls) - set(['stanza_2norm','afinn_2norm']))\n",
        "\n",
        "print(f'Trimmed temp_cols_ls:')\n",
        "print(temp_cols_ls)\n",
        "\n",
        "\n",
        "plot_lowess(corpus_sents_norm_df, temp_cols_ls, 'Normed')\n",
        "\n",
        "'''\n",
        "for i, sa_model in enumerate(corpus_sents_df.columns):\n",
        "  if (sa_model.endswith('_roll050')):\n",
        "    # if (sa_model != 'sent_no'):\n",
        "    if (sa_model == 'median'):\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, color='black')\n",
        "    else:\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, alpha=0.3)\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7hs9FIxpugn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq8fWCKCpuaW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvKW23iwAGnk"
      },
      "source": [
        "%time\n",
        "\n",
        "plt.title(f'{BOOK_TITLE_FULL} \\n {sa_model} with statsmodels LOWESS (frac=0.1/iter=5)')\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['median'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "plt.plot(sm_x, sm_y, label='Median', color='black', linewidth=3)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['vader_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='VADER', color='royalblue', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['jockers_rinker_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='Jockers-Rinker', color='red', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['syuzhet_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='Syuzhet', color='tomato', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['huliu_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='HuLiu', color='teal', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['textblob_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='TextBlob', color='lime', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['sentiword_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='SentiWord', color='goldenrod', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['senticnet_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='SenticNet', color='forestgreen', alpha=0.5)\n",
        "\n",
        "# y_upper = corpus_sentiments_df['norm_score'].max() + 0.01\n",
        "# y_lower = corpus_sentiments_df['norm_score'].min() - 0.01\n",
        "# y_range = y_upper - y_lower + 0.02\n",
        "# plt.ylim([0.71, 0.74])\n",
        "# plt.ylim([y_lower, y_upper])\n",
        "# plt.plot(x, y, 'k.');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iORvQhmNFrw"
      },
      "source": [
        "**Discrete Cosine Transform (DCT)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917t5E9t_Pez"
      },
      "source": [
        "Libraries:\n",
        "\n",
        "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html#scipy.fft.dct\n",
        "* https://github.com/search?q=discrete+cosine \n",
        "\n",
        "Tutorial\n",
        "\n",
        "* https://realpython.com/python-scipy-fft/#the-discrete-cosine-and-sine-transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI-UtGpXwsJ6"
      },
      "source": [
        "from sktime.transformations.series.cos import CosineTransformer\n",
        "from sktime.datasets import load_airline\n",
        "y = load_airline()\n",
        "transformer = CosineTransformer()\n",
        "y_hat = transformer.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HUsqY_c3wsGA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YfHqTtxwsC4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKYGvaNxwr_m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noayFmY5v6Wc"
      },
      "source": [
        "!pip install pyts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-DLnLsjVWB-"
      },
      "source": [
        "**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n",
        "\n",
        "* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n",
        "* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n",
        "* http://futuredata.stanford.edu/asap/ \n",
        "* https://www.datadoghq.com/blog/auto-smoother-asap/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsz35wSyWlvA"
      },
      "source": [
        "import scipy.stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLFb9ZitVV3E"
      },
      "source": [
        "# ASAP Simple (Brute Force)\n",
        "def moving_average(data, _range):\n",
        "    ret = np.cumsum(data, dtype=float)\n",
        "    ret[_range:] = ret[_range:] - ret[:-_range]\n",
        "    return ret[_range - 1:] / _range\n",
        "\n",
        "def SMA(data, _range, slide):\n",
        "    ret = moving_average(data, _range)[::slide]\n",
        "    return list(ret)\n",
        "\n",
        "def kurtosis(values):\n",
        "    return scipy.stats.kurtosis(values)\n",
        "\n",
        "def roughness(vals):\n",
        "    return np.std(np.diff(vals))\n",
        "\n",
        "def smooth_simple(data, max_window=5, resolution=None):\n",
        "    data = np.array(data)\n",
        "    # Preaggregate according to resolution\n",
        "    window_size = 1\n",
        "    slide_size = 1\n",
        "    if resolution:\n",
        "        slide_size = int(len(data) / resolution)\n",
        "        if slide_size > 1:\n",
        "            data = SMA(data, slide_size, slide_size)\n",
        "    orig_kurt   = kurtosis(data)\n",
        "    min_obj     = roughness(data)\n",
        "    for w in range(2, int(len(data) / max_window + 1)):\n",
        "        smoothed = SMA(data, w, 1)\n",
        "        if kurtosis(smoothed) >= orig_kurt:\n",
        "            r = roughness(smoothed)\n",
        "            if r < min_obj:\n",
        "                min_obj = r\n",
        "                window_size = w\n",
        "    return window_size, slide_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHdCbShgVVrR"
      },
      "source": [
        "# Plot time series before and after smoothing\n",
        "def plot(data, window_size, slide_size, plot_title):\n",
        "    plt.clf()\n",
        "    plt.figure()\n",
        "    data = SMA(data, slide_size, slide_size)\n",
        "    method_names = [\"Original\", \"ASAP Smoothed\"]\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
        "    smoothed = SMA(data, window_size, 1)\n",
        "    smoothed_range = range(int(window_size/2), int(window_size/2) + len(smoothed))\n",
        "    ax1.set_xlim(0, len(data))\n",
        "    ax1.plot(data, linestyle='-', linewidth=1.5)\n",
        "    ax2.plot(smoothed_range, smoothed, linestyle='-', linewidth=1.5)\n",
        "    axes = [ax1, ax2]\n",
        "    for i in range(2):\n",
        "        axes[i].get_xaxis().set_visible(False)\n",
        "        axes[i].text(0.02, 0.8, \"%s\" %(method_names[i]),\n",
        "            verticalalignment='center', horizontalalignment='left',\n",
        "            transform=axes[i].transAxes, fontsize=25)\n",
        "\n",
        "    fig.set_size_inches(16, 12)\n",
        "    plt.tight_layout(w_pad=1)\n",
        "    plt.title(plot_title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpSktA4gWV3J"
      },
      "source": [
        "corpus_sentiments_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "710ghW5qVVoF"
      },
      "source": [
        "# Taxi\n",
        "# raw_data = load_csv('Taxi.csv')\n",
        "# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n",
        "# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n",
        "window_size, slide_size = smooth_simple(list(corpus_sentiments_df['median']), resolution=1000)\n",
        "print(\"Window Size: \", window_size)\n",
        "plot_title = f'{BOOK_TITLE_FULL} \\n Median Sentiment Smoothed with ASAP from Stanford (res=1000)'\n",
        "plot(list(corpus_sentiments_df['median']), window_size, slide_size, plot_title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wkkx80YYLCg"
      },
      "source": [
        "# Calculate Error Metrics based on Distance from Median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0K9P4jA_bDj"
      },
      "source": [
        "Libraries:\n",
        "\n",
        "* https://github.com/wannesm/dtaidistance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmSSemdcLQ9x"
      },
      "source": [
        "# sentiment_lowess_df['median']\n",
        "sentiment_lowess_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tvyv-LtJiqJ"
      },
      "source": [
        "# Rank each Sentiment Model by Error/Distance Metrics from Median\n",
        "\n",
        "sentiment_lowess_df['min'] = sentiment_lowess_df[['vader','jockers-rinker','syuzhet','huliu','textblob','sentiword','senticnet']].min(axis=1)\n",
        "sentiment_lowess_df['max'] = sentiment_lowess_df[['vader','jockers-rinker','syuzhet','huliu','textblob','sentiword','senticnet']].max(axis=1)\n",
        "sentiment_lowess_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18y52tS9JTeO"
      },
      "source": [
        "# LOWESS Smoothed Median Curve with Min/Max Confidence Intervals\n",
        "\n",
        "plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Min/Max Confidence Intervals')\n",
        "\n",
        "sns.lineplot(data=sentiment_lowess_df, x='x_value', y='median', linewidth=3, color='black')\n",
        "plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cQqCyABTkzT"
      },
      "source": [
        "# Error Metrics of each Model relative to the Median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BBTjp7nTkhK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHVeb7Jy9zaa"
      },
      "source": [
        "# Group and Classify Sentiment Arcs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xeV-709-_M2"
      },
      "source": [
        "Libraries\n",
        "\n",
        "* https://github.com/alan-turing-institute/sktime\n",
        "\n",
        "* https://github.com/johannfaouzi/pyts \n",
        "\n",
        "Code\n",
        "\n",
        "* https://colab.research.google.com/drive/1oEFfK5KTJyFQGs2xunc1cDW2OcbkZKCY\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFEtVAt29zMP"
      },
      "source": [
        "# https://colab.research.google.com/drive/1oEFfK5KTJyFQGs2xunc1cDW2OcbkZKCY\n",
        "\n",
        "# https://github.com/johannfaouzi/pyts \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "804w9PYAYP_p"
      },
      "source": [
        "# Export Manual and Automatic Sentiment Polarities and Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wnHGCQnO9aA"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwmMgQILMYba"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}