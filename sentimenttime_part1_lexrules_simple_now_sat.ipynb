{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sentimenttime_part1_lexrules_simple_now_sat.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aMLbyx6gIPqj",
        "yXwKR4gA8Ouk",
        "3YJJcvDVnUuT",
        "qjb60CVnz5SF",
        "PVCkjat0vffd",
        "YFC8GTnw6HrG",
        "j2tIua7tTSRz",
        "QZjqwTvU76AR",
        "NA3dWsnF78mi",
        "SkNZVk128jV9",
        "dUcANLM_8mtT",
        "Cn4KQYpH3glK",
        "jRTjCPLb8cbB",
        "gAEiglIPDfFI",
        "iCN4c-G48e7-",
        "G2blGfVlKb_s",
        "wsaziON_Z263",
        "AIGQgWvyOtg6"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jon-chun/sentimenttime/blob/main/sentimenttime_part1_lexrules_simple_now_sat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibHFmIWoU3Vx"
      },
      "source": [
        "# **An Analytic Methodology to Extract Narratives from Text: Using Sentiment Analysis to find the Arcs and Crux Points in Novels, Social Media and Chat Transcripts**\n",
        "\n",
        "By: Jon Chun\n",
        "12 Jun 2021\n",
        "\n",
        "References:\n",
        "\n",
        "* Coming...\n",
        "\n",
        "TODO:\n",
        "* Demo datafiles\n",
        "* Error detection around Crux points context (out of bounds)\n",
        "* lex_discrete2continous (research binary->gaussian transformation fn)\n",
        "* Text Preprocessing hints/tips/flowchart\n",
        "* Clearly document workflow and partition across notebooks/libraries\n",
        "* Code review and extraction to libraries\n",
        "* Corpus ingestion for any format\n",
        "* XAI (mlm false peak 1717SyuzhetR/1732SentimentR/1797robertalg15 adam watches war argument at dinner) \n",
        "* Centralize and Standardize Model name lists\n",
        "* Normalize model SA Series lengths\n",
        "* Standardize all SA Series with the same method\n",
        "* Seamless report generation/file saving\n",
        "* Get raw text from SentimentR\n",
        "* Filter out non-printable characters\n",
        "* Roll-over Crux-Points (SentNo+Sent/Parag) (plotly)\n",
        "* Label/Roll-over Chapter/Sect No at Boundries\n",
        "* Generate Report PDF/csv\n",
        "* Option to select raw or discrete2continous transformation (Bing)\n",
        "* Annotation functionality + Share/Collaboration of findings/reseearch\n",
        "* clusters, centroids = kmeans1d.cluster(np.array(corpus_sentimentr_df['jockers_rinker']), k)\n",
        "* plotly prefered library to save dynamic images: kaleido\n",
        "* Correlation heatmaps: Justify choice of Spearman, Pearson, or other algo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X05awke6vYdu"
      },
      "source": [
        "# Test\n",
        "\n",
        "# !pip install kmeans1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBWhXS9bxrtd"
      },
      "source": [
        "\"\"\"\n",
        "import kmeans1d\n",
        "\n",
        "k = corpus_sentimentr_df.shape[0]//500  \n",
        "\n",
        "clusters, centroids = kmeans1d.cluster(np.array(corpus_sentimentr_df['jockers_rinker']), k)\n",
        "type(clusters)\n",
        "\n",
        "[[x,clusters.count(x)] for x in set(clusters)]\n",
        "centroids\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u932nJxdh0Ac"
      },
      "source": [
        "# Configuration (Auto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_a9eQyBiiTG"
      },
      "source": [
        "**Global Configuration Constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT2MyyjpihFj"
      },
      "source": [
        "# Hardcoded Sentiment Analysis Models\n",
        "\n",
        "CORPUS_ENCODING = 'utf-8' # Default character/text encoding scheme (others: 'utf-8', but often 'iso-8859-1', 'windows-1252', 'cp1252', or 'ascii')\n",
        "\n",
        "MODELS_LS = ['vader','textblob','stanza','afinn','bing','sentimentr','syuzhet','pattern','sentiword','senticnet','nrc']\n",
        "            \n",
        "# Minimum lengths for Sentences and Paragraphs\n",
        "#   (Shorter Sents/Parags will be deleted)\n",
        "\n",
        "MIN_CHAP_LEN = 50\n",
        "MIN_SECT_LEN = 25  # Minimum char length to be included in section DataFrame\n",
        "MIN_PARAG_LEN = 2\n",
        "MIN_SENT_LEN = 2\n",
        "\n",
        "# Simple Moving Average/Rolling Mean \n",
        "roll_str = \"roll10\" # Default 10% Rolling Mean Window \n",
        "\n",
        "# Min/Max statistics on each lexicon's sentiment values applied to corpus\n",
        "corpus_lexicons_stats_dt = {}\n",
        "corpus_cruxes_dt = {}\n",
        "\n",
        "# Crux Points Dict key:model, value:list of crux point tuples (x,y)\n",
        "corpus_cruxes_all_dt = {}\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiapgHzZdm4x"
      },
      "source": [
        "groups_ls = ['models_baseline_ls',\n",
        "                'models_sentimentr_ls',\n",
        "                'models_syuzhetr_ls',\n",
        "                'models_transformer_ls']\n",
        "\n",
        "models_baseline_ls = ['sentimentr',\n",
        "                      'syuzhet',\n",
        "                      'bing',\n",
        "                      'sentiword',\n",
        "                      'senticnet',\n",
        "                      'nrc',\n",
        "                      'afinn',\n",
        "                      'vader',\n",
        "                      'textblob',\n",
        "                      'flair',\n",
        "                      'pattern',\n",
        "                      'stanza']\n",
        "\n",
        "models_sentimentr_ls = ['jockers_rinker',\n",
        "                        'jockers',\n",
        "                        'huliu',\n",
        "                        'senticnet',\n",
        "                        'sentiword',\n",
        "                        'nrc',\n",
        "                        'lmcd']\n",
        "\n",
        "models_syuzhetr_ls = ['syuzhet',\n",
        "                      'bing',\n",
        "                      'afinn',\n",
        "                      'nrc']\n",
        "\n",
        "models_transformer_ls = ['roberta15lg', \n",
        "                         'nlptown', \n",
        "                         'yelp', \n",
        "                         'hinglish',\n",
        "                         'imdb2way', \n",
        "                         'huggingface', \n",
        "                         't5imdb50k', \n",
        "                         'robertaxml8lang']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOPa6HH-OjZp"
      },
      "source": [
        "**Install Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drpZJilASHUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7f5222-973d-49da-aac5-442f44c5a0a5"
      },
      "source": [
        "# fast detection of character set encoding for text/files\n",
        "\n",
        "!pip install cchardet"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 37.6 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 40 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 102 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 153 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 163 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 204 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 215 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 256 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 263 kB 7.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: cchardet\n",
            "Successfully installed cchardet-2.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA7Nw-SA_si1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9c5006-e536-4543-ad1a-2147083cebf2"
      },
      "source": [
        "!pip install pysbd"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pysbd\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▋                           | 10 kB 34.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 20 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 30 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 40 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 51 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 61 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71 kB 4.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pysbd\n",
            "Successfully installed pysbd-0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEjSzsusOOJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "434cc0b5-5852-4f5d-9132-e23676e08863"
      },
      "source": [
        "# common ML code\n",
        "\n",
        "!pip install sklearn"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ0UVdasuTTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67dc289f-db8d-4ffa-d43a-5cb7d5a0830c"
      },
      "source": [
        "%pip install contractions"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 10.0 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.2.0-py3-none-any.whl (283 kB)\n",
            "\u001b[K     |████████████████████████████████| 283 kB 10.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85439 sha256=efbdf4ec548a2a9a0a1d1378bbc2d05d26e13eec840e759d12051968fab1082e\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.2.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmtqmvu6OlR9"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7bf4lfgwMEz"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import io\n",
        "import glob\n",
        "import json\n",
        "import contextlib"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOmyq4h7OOFi"
      },
      "source": [
        "# IMPORT LIBRARIES\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OslLdEsvOuFU"
      },
      "source": [
        "import re\n",
        "import string"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YelenXz5BcmE"
      },
      "source": [
        "from itertools import cycle  # For plotly\n",
        "\n",
        "import collections\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Suximbjnw8D"
      },
      "source": [
        "# Import libraries for logging\n",
        "\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import time                     # (TODO: check no dependencies and delete)\n",
        "from time import gmtime, strftime"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPZmScjVDYyw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac8a6655-4fc1-43dc-fa62-ae38be881f71"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Download for sentence tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download for nltk/VADER sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u34kPKO0_xF_"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm') # Load the English Model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMl2mfF8Haw8"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler   # To normalize time series\n",
        "from sklearn.preprocessing import StandardScaler # To Standardize time series: center(sub mean) and rescale within 1 SD (only for well-behaved guassian distributions)\n",
        "from sklearn.preprocessing import RobustScaler   # To Standardize time series: center(sub median) and rescale within 25%-75% (1st-3rd) IQR (better for noisy, outliers distributions)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nckwluDXwa1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "019793d4-b208-4490-b17b-6f7fc261df50"
      },
      "source": [
        "minmax_scaler = MinMaxScaler()\n",
        "mean_std_scaler = StandardScaler()\n",
        "median_iqr_scaler = RobustScaler()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U589lvKXmFV-"
      },
      "source": [
        "# Zoom interpolates new datapoints between existing datapoints to expand a time series \n",
        "\n",
        "from scipy.ndimage.interpolation import zoom"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wcZfSOuBlW7"
      },
      "source": [
        "from scipy import interpolate\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy import signal\n",
        "from scipy.signal import argrelextrema"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY3UyvYjAvDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7922afa-c1ab-4be4-9751-1bf35bc4b260"
      },
      "source": [
        "from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess\n",
        "from statsmodels import robust"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02LJQlYpgQGs"
      },
      "source": [
        "corpus_sects_df = pd.DataFrame()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcSc4jsggSy2"
      },
      "source": [
        "**Define Library-Dependent Objects**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjGN2sN3uRpN"
      },
      "source": [
        "import contractions"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AllIwMngDGC3"
      },
      "source": [
        "# Necessary to define before defining Utility Functions using these DataFrames\n",
        "\n",
        "corpus_sents_df = pd.DataFrame()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwl0MBDyOwtX"
      },
      "source": [
        "**Configure Jupyter Notebook**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APCau-T26XQ3"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def my_css():\n",
        "   display(HTML(\"\"\"<style>table.dataframe td{white-space: nowrap;}</style>\"\"\"))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', my_css)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD1cyqWsfjxA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "60ab2253-d5d1-42ec-8194-8e00afff12d1"
      },
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzfybE5kfmE-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7cf372c6-62f4-4ccf-eb7d-1686e7c3ea42"
      },
      "source": [
        "# Configure matplotlib and seaborn\n",
        "\n",
        "# Plotting pretty figures and avoid blurry images\n",
        "# %config InlineBackend.figure_format = 'retina'\n",
        "# Larger scale for plots in notebooks\n",
        "# sns.set_context('talk')\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [16, 8]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rc('figure', facecolor='white')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIIjSbyeP2fg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0aceae88-f982-4806-c5ce-0adae498b385"
      },
      "source": [
        "# Configure Jupyter\n",
        "\n",
        "# Enable multiple outputs from one code cell\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets, interactive\n",
        "\n",
        "# Configure Google Colab\n",
        "\n",
        "%load_ext google.colab.data_table"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS_El2PiQlyP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "4752cfbc-117d-4522-fc44-376110a398e3"
      },
      "source": [
        "# Text wrap\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuM_qnOHUil5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "41796e3f-0424-466d-e12d-ee9b1c8b5062"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dLkfn4KFmDf"
      },
      "source": [
        "**Configuration Details Snapshot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FNPovQBFZky",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a670ff95-19f5-4a5c-fa5a-31ff2711074d"
      },
      "source": [
        "# Snap Shot of Time, Machine, Data and Library/Version Blueprint\n",
        "# TODO:"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wiSBHxoOGZz"
      },
      "source": [
        "# Pick ONE Method (a) or (b) to Get Corpus Textfile\n",
        "\n",
        "**Choose either (a) OR (b), not both**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KRfiXXQOZcq"
      },
      "source": [
        "## **Option (a): Connect to Google gDrive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G64etjAUOOSm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "18882e0b-b2e2-4598-bce8-ef6f8117bff0"
      },
      "source": [
        "# Connect to Google gDrive\n",
        "\n",
        "# Flag to indicate first run through code \n",
        "flag_first_run = True\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0fvFZq-eFaw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c9348ff-bc2b-4996-8769-7b460a7f3c53"
      },
      "source": [
        "# Select the Corpus subdirectory on your Google gDrive\n",
        "\n",
        "# Done\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\" #@param {type:\"string\"}\n",
        "gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/ddefoe_robinsoncrusoe\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/staugustine_confessions\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/fdouglass_narrativelifeslave\" #@param {type:\"string\"}\n",
        "\n",
        "# Current\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/homer_odyssey\" #@param {type:\"string\"}\n",
        "\n",
        "# To do\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\" #@param {type:\"string\"}\n",
        "\n",
        "CORPUS_SUBDIR = gdrive_subdir\n",
        "corpus_filename = CORPUS_SUBDIR\n",
        "\n",
        "# Change to working subdirectory\n",
        "if flag_first_run == True:\n",
        "  full_path_str = gdrive_subdir\n",
        "  flag_first_run = False\n",
        "else:\n",
        "  full_path_str = f'/gdrive/MyDrive{gdrive_subdir[1:]}'\n",
        "\n",
        "%cd $full_path_str\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKV1uMBEO8TR"
      },
      "source": [
        "## **Option (b): Upload Corpus Textfile**\n",
        "\n",
        "***Only do this if your Google subdirectory doesn't already contain a plain text file of your Corpus or you wish to overwrite it and use a newer version***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH6dlB2fO6Ln"
      },
      "source": [
        "# Execute this code cell to upload plain text file of corpus\n",
        "#   Should be *.txt format with paragraphs separated by at least 2 newlines\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3LORQ4fRGBW"
      },
      "source": [
        "# Verify file was uploaded\n",
        "\n",
        "# Get uploaded filename\n",
        "corpus_filename = list(uploaded.keys())[0]\n",
        "print(f'Uploaded Corpus filename is: {corpus_filename}')\n",
        "CORPUS_FILENAME = corpus_filename\n",
        "\n",
        "!ls -al $corpus_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsm8awD4AZ8O"
      },
      "source": [
        "# **Configuration (Manual)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfilGg6Mkxnd"
      },
      "source": [
        "# Verify subdirectory change\n",
        "\n",
        "!pwd\n",
        "!ls -altr *\n",
        "\n",
        "# TODO: Intelligently automate the filling of form based upon directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP3WLEv_g5aq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "c894641a-39cf-4376-f1df-06e56c53a228"
      },
      "source": [
        "# CORPUS_TITLE = 'Robinson Crusoe' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Daniel Defoe\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"ddefoe_robinsoncrusoe_final_hand.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/ddefoe_robinsoncrusoe\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'The Odyssey' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Homer SButler\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"sbutler_odyssey.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/sbutler_odyssey\"  #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Middlemarch' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"George Eliot\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"geliot_middlemarch_wprelude.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\"  #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'Narrative Life of an American Slave' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Frederick Douglass\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"fdouglass_narrativelifeslave.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/fdouglass_narrativelifeslave\"  #@param {type:\"string\"}\n",
        "\n",
        "CORPUS_TITLE = 'To The Lighthouse' #@param {type:\"string\"}\n",
        "CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n",
        "CORPUS_FILENAME = \"vwoolf_tothelighthouse.txt\" #@param {type:\"string\"}\n",
        "CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'To The Lighthouse' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"ttl_final_hand.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n",
        "\n",
        "CHAPTER_HEADINGS = \"CHAPTER\" #@param [\"CHAPTER\", \"BOOK\"]\n",
        "CHAPTER_NUMBERING = \"Arabic (1,2,...)\" #@param [\"Arabic (1,2,...)\", \"Roman (I,II,...)\"]\n",
        "SECTION_HEADINGS = \"SECTION (ArabicNo)\" #@param [\"SECTION (ArabicNo)\", \"SECTION (RomanNo)\", \"----- (Hyphens)\", \"None\"]\n",
        "\n",
        "LEXICONS_SUBDIR = \"./research/2021/sa_book_code/books_sa/lexicons\" #@param {type:\"string\"}\n",
        "\n",
        "CORPUS_FULL = f'{CORPUS_TITLE} by: {CORPUS_AUTHOR}'\n",
        "\n",
        "PLOT_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n",
        "\n",
        "FILE_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n",
        "\n",
        "\n",
        "gdrive_subdir = CORPUS_SUBDIR\n",
        "corpus_filename = CORPUS_FILENAME\n",
        "author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "author_abbr_str = (CORPUS_AUTHOR.split(' ')[0][0]+CORPUS_AUTHOR.split(' ')[1]).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "\n",
        "print(f'\\nWorking Corpus Datafile: ------------------------------ \\n\\n    {CORPUS_SUBDIR}')\n",
        "print(f'\\nFull Corpus Title/Author: ------------------------------ \\n\\n    {CORPUS_FULL}')\n",
        "\n",
        "\n",
        "if CHAPTER_HEADINGS == 'CHAPTER':\n",
        "  if CHAPTER_NUMBERING == \"Arabic (1,2,...)\":\n",
        "    pattern_chap = r'CHAPTER [0123456789]{1,2}[.]?[^\\n]*'\n",
        "  elif CHAPTER_NUMBERING == \"Roman (I,II,...)\":\n",
        "    pattern_chap = r'CHAPTER [IVXL]{0,10}[.:]?[^\\n]*'\n",
        "  else:\n",
        "    print(f'ERROR: Illegal CHAPTER_NUMBERING value = {CHAPTER_NUMBERING}')\n",
        "\n",
        "elif CHAPTER_HEADINGS == 'BOOK':\n",
        "  if CHAPTER_NUMBERING == \"Arabic (1,2,...)\":\n",
        "    pattern_chap = r'BOOK [0123456789]{1,2}[.]?[^\\n]*'\n",
        "  elif CHAPTER_NUMBERING == \"Roman (I,II,...)\":\n",
        "    pattern_chap = r'[\\s]*BOOK [IVXL]{1,10}[.:]?[\\s]*[^\\n]*[\\n]{0,1}[^\\n]*' # [^\\n]*' # Problems with embedded 'Book'\n",
        "  else:\n",
        "    print(f'ERROR: Illegal CHAPTER_NUMBERING value = {CHAPTER_NUMBERING}')\n",
        "\n",
        "else:\n",
        "  print(f'ERROR: Illegal CHAPTER_HEADINGS value = {CHAPTER_HEADINGS}')\n",
        "\n",
        "# Default Section RegEx Pattern\n",
        "pattern_sect = 'SECTION [0123456789]{1,2}[^\\n]*'\n",
        "\n",
        "if SECTION_HEADINGS == 'SECTION (ArabicNo)':\n",
        "  # pattern_sect = r'SECTION [0-9]{1,2} [^\\n]*'\n",
        "  # TODO: [^\\n] gets parsed into [^\\\\n] causing problems, so simplify\n",
        "  pattern_sect = r'SECTION [0123456789]{1,2}[.:]?[^\\n]*'\n",
        "elif SECTION_HEADINGS == 'SECTION (RomanNo)':\n",
        "  pattern_sect = r'SECTION [IVX]{,10}[.:]?{^\\n]*' # } [A-Z \\.-:—;-’\\'\"]*[\\n]*'\n",
        "elif SECTION_HEADINGS == '----- (Hyphens)':\n",
        "  pattern_sect = r'^[- ]{3,}[^\\n]*'\n",
        "elif SECTION_HEADINGS == 'None':\n",
        "  pass\n",
        "else:\n",
        "  print(f'ERROR: Illegal SECTION_HEADING value = {SECTION_HEADINGS}')\n",
        "\n",
        "print(f'\\nCHAPTER Headings: ------------------------------ \\n\\n    {CHAPTER_HEADINGS}')\n",
        "\n",
        "print(f'\\nSECTION Headings: ------------------------------ \\n\\n    {SECTION_HEADINGS}')\n",
        "\n",
        "\n",
        "print(f'\\nCorpus file information: ------------------------------ \\n')\n",
        "!ls -al $CORPUS_FILENAME\n",
        "\n",
        "# Verify contents of Corpus File is Correctly Formatted\n",
        "#   \n",
        "# TODO: ./utils/verify_format.py\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Working Corpus Datafile: ------------------------------ \n",
            "\n",
            "    ./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\n",
            "\n",
            "Full Corpus Title/Author: ------------------------------ \n",
            "\n",
            "    To The Lighthouse by: Virginia Woolf\n",
            "\n",
            "CHAPTER Headings: ------------------------------ \n",
            "\n",
            "    CHAPTER\n",
            "\n",
            "SECTION Headings: ------------------------------ \n",
            "\n",
            "    SECTION (ArabicNo)\n",
            "\n",
            "Corpus file information: ------------------------------ \n",
            "\n",
            "-rw------- 1 root root 385299 Jul 17 01:36 vwoolf_tothelighthouse.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8owpM75RILKn"
      },
      "source": [
        "# **Utility Functions (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMLbyx6gIPqj"
      },
      "source": [
        "## **Files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBOvvP-BSIiC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e5f35d25-ffe2-492d-e0e1-a9c242125e0d"
      },
      "source": [
        "# https://dev.to/bowmanjd/character-encodings-and-detection-with-python-chardet-and-cchardet-4hj7\n",
        "\n",
        "import cchardet as chardet\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "def get_file_encoding(filename):\n",
        "    \"\"\"Detect encoding and return decoded text, encoding, and confidence level.\"\"\"\n",
        "    filepath = Path(filename)\n",
        "\n",
        "    # We must read as binary (bytes) because we don't yet know encoding\n",
        "    blob = filepath.read_bytes()\n",
        "\n",
        "    detection = chardet.detect(blob)\n",
        "    encoding = detection[\"encoding\"]\n",
        "    confidence = detection[\"confidence\"]\n",
        "    text = blob.decode(encoding)\n",
        "\n",
        "    return text, encoding, confidence"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2ZKrOg_8Pcz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b9e16405-cdef-446e-b056-8db9f7281cf5"
      },
      "source": [
        "def corpus2chapsect(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of min preprocessed raw CHAPTERs (corpus_parags_raw_temp_ls)\n",
        "  '''\n",
        "\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  # Strip out non-printing characters\n",
        "  corpus_clean_str = re.sub(f'[^{re.escape(string.printable)}]', ' ', corpus_raw_str)\n",
        "\n",
        "  print(f'BEFORE stripping out headings len: {len(corpus_raw_str)}')\n",
        "\n",
        "  print(f'Using RegEx pattern_chap: {pattern_chap}')\n",
        "  # print(f'len(corpus_raw_str) = {len(corpus_raw_str)}')\n",
        "  # corpus_chaps_ls = re.split(rf'{pattern_chap}', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  # corpus_chaps_ls = re.split(r'CHAPTER [IVX]{,6}[.]* [A-Z \\.-:—;-’\\'\"]*[\\n]*', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  # corpus_chaps_ls = re.split(r'CHAPTER [\\d]{1,2}[.]* [A-Z \\.-:—;-’\\'\"]*[\\n]*', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  corpus_chaps_raw_ls = re.split(rf'{pattern_chap}', corpus_clean_str, flags=re.I) # , flags=re.I)\n",
        "  print(f'len(corpus_chaps_raw_ls): {len(corpus_chaps_raw_ls)}')\n",
        "\n",
        "  # Strip off whitespace\n",
        "  corpus_chaps_raw_ls = [x.strip() for x in corpus_chaps_raw_ls]\n",
        "\n",
        "  # Filter out chapters that are empty or shorter than MIN_PARAG_LEN\n",
        "  corpus_chaps_raw_ls = [x for x in corpus_chaps_raw_ls if not (len(x.strip()) <= MIN_CHAP_LEN)]\n",
        "\n",
        "  # Filter out SECTION lines (could be embedded or leading)\n",
        "  corpus_chaps_clean_nosect_ls = []\n",
        "  for i, achap_raw in enumerate(corpus_chaps_raw_ls):\n",
        "    if bool(re.match(rf\"{pattern_sect}\", achap_raw)):\n",
        "      achap_raw_temp_str = re.sub(rf'{pattern_sect}', '', achap_raw).strip() # , flags=re.I)[1].strip()\n",
        "    else:\n",
        "      achap_raw_temp_str = achap_raw\n",
        "    corpus_chaps_clean_nosect_ls.append(achap_raw_temp_str)\n",
        "  print(f'In Chapters, filtered Sections with {len(corpus_chaps_clean_nosect_ls)} Chapters left')\n",
        "  corpus_chaps_clean_ls = [x for x in corpus_chaps_clean_nosect_ls]\n",
        "\n",
        "  # Start creating a clean version of Chapters\n",
        "  corpus_chaps_clean_ls = [clean_text(x) for x in corpus_chaps_clean_ls]\n",
        "\n",
        "  # Collapse multiple whitespaces down to one\n",
        "  corpus_chaps_clean_ls = [' '.join(x.split()).strip() for x in corpus_chaps_clean_ls]\n",
        "\n",
        "  # Filter out Chapters that are empty or shorter than MIN_CHAP_LEN\n",
        "  corpus_chaps_clean_ls = [x for x in corpus_chaps_clean_ls if not (len(x.strip()) <= MIN_CHAP_LEN)]\n",
        "\n",
        "  # Check for mismatch\n",
        "  if (len(corpus_chaps_clean_ls) != len(corpus_chaps_raw_ls)):\n",
        "    print('\\n')\n",
        "    print(f'ERROR: Cleaned Chapter lengths does not match original raw Chapter length:\\n')\n",
        "    print(f'DIFFERENCE:\\n\\n   list(set(corpus_chaps_clean_ls).difference(set(corpus_chaps_raw_ls)))')\n",
        "    print('\\n\\n\\n   RESOLUTION: Edit the original corpus file to clean this/these lines of text')\n",
        "    return [-99], [-99], [-99], [-99], [-99], '-99' # Return with ERROR condition\n",
        "\n",
        "  # Create list of Chapter Numbers\n",
        "  corpus_chapno_ls = list(range(len(corpus_chaps_raw_ls)))\n",
        "\n",
        "\n",
        "  sect_chapno_ls = []\n",
        "  if SECTION_HEADINGS == \"None\":\n",
        "    # No Sections, create pseudo-Sections by copying Chapters\n",
        "    corpus_sects_raw_ls = [x for x in corpus_chaps_raw_ls]\n",
        "    corpus_sects_clean_ls = [x for x in corpus_chaps_clean_ls]\n",
        "    # Create list of Chapter Numbers for each Section\n",
        "    sect_chapno_ls = [x for x in corpus_chapno_ls]\n",
        "\n",
        "  else:\n",
        "    # Sections need to be broken out separately\n",
        "\n",
        "    corpus_sects_all_raw_ls = []\n",
        "    corpus_sects_all_clean_ls = []\n",
        "    for achap_no,achap_str in enumerate(corpus_chaps_raw_ls):\n",
        "\n",
        "      corpus_sects_raw_ls = re.split(rf'{pattern_sect}', achap_str, flags=re.I) # , flags=re.I)\n",
        "\n",
        "      # Strip off whitespace\n",
        "      corpus_sects_raw_ls = [x.strip() for x in corpus_sects_raw_ls]\n",
        "\n",
        "      # Filter out Sections that are empty or shorter than MIN_SECT_LEN\n",
        "      corpus_sects_raw_ls = [x for x in corpus_sects_raw_ls if not (len(x.strip()) <= MIN_SECT_LEN)]\n",
        "\n",
        "      # Filter out the Section separator 'SECTION ' lines\n",
        "      corpus_sects_raw_ls = [x for x in corpus_sects_raw_ls if not (x.strip().startswith('SECTION '))]\n",
        "\n",
        "      # Filter out the Section separator '-----' lines\n",
        "      corpus_sects_raw_ls = [x for x in corpus_sects_raw_ls if not (re.match(r'^[ ]*[-]{1,20}[ ]*$',x))]\n",
        "\n",
        "      print('\\n')\n",
        "      print(f'Chapter #{achap_no} Ch Length: {len(achap_str)}')\n",
        "      print(f'             Sections:  {len(corpus_sects_raw_ls)}')\n",
        "      print('\\n')\n",
        "      print(f'        First section:\\n    {corpus_sects_raw_ls[0][:500]}\\n')\n",
        "      print(f'       Second section:\\n    {corpus_sects_raw_ls[1][:500]}')\n",
        "      print('\\n')\n",
        "      print(f'  Second-Last section:\\n    {corpus_sects_raw_ls[-2][:500]}\\n')\n",
        "      print(f'         Last section:\\n    {corpus_sects_raw_ls[-1][:500]}')\n",
        "      print('\\n')\n",
        "\n",
        "      corpus_sects_all_raw_ls += corpus_sects_raw_ls\n",
        "\n",
        "      # Start creating a clean version of Sections\n",
        "      # Filter out SECTION lines (could be embedded or leading)\n",
        "      corpus_sects_clean_ls = [clean_text(x) for x in corpus_sects_raw_ls]\n",
        "\n",
        "      # Filter out Sections that are empty or shorter than MIN_SECT_LEN\n",
        "      corpus_sects_clean_ls = [x for x in corpus_sects_clean_ls if not (len(x.strip()) <= MIN_CHAP_LEN)]\n",
        "\n",
        "      corpus_sects_all_clean_ls += corpus_sects_clean_ls\n",
        "\n",
        "      achap_sect_ct = len(corpus_sects_clean_ls)\n",
        "      for asect_no in (range(achap_sect_ct)):\n",
        "        print(f'appending Section #{asect_no} to Chapter #{achap_no}')\n",
        "        sect_chapno_ls.append(achap_no) \n",
        "\n",
        "    # Check for mismatch\n",
        "    if (len(corpus_sects_all_clean_ls) != len(corpus_sects_all_raw_ls)):\n",
        "      print('\\n')\n",
        "      print(f'     ERROR: Cleaned Section lengths does not match original raw Section length:\\n')\n",
        "      print(f'DIFFERENCE:\\n\\n   Raw length: {len(corpus_sects_all_raw_ls)} vs Clean length: {len(corpus_sects_all_clean_ls)}')\n",
        "      # print(f'DIFFERENCE:\\n\\n   {list(set(corpus_sects_clean_ls).difference(set(corpus_sects_raw_ls)))}')\n",
        "      print(f'RESOLUTION: Edit the original corpus file to clean this/these lines of text')\n",
        "      return [-99], [-99], [-99], [-99], [-99], '-99' # Return with ERROR condition\n",
        "\n",
        "\n",
        "  return corpus_chaps_raw_ls, corpus_chaps_clean_ls, corpus_sects_all_raw_ls, corpus_sects_all_clean_ls, sect_chapno_ls, corpus_raw_str"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_nBxtq6-J8l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2ca6374e-4602-4316-cb83-d30e53f917cd"
      },
      "source": [
        "def filter_nonprintable(text):\n",
        "    import itertools\n",
        "    # Use characters of control category\n",
        "    nonprintable = itertools.chain(range(0x00,0x20),range(0x7f,0xa0))\n",
        "    # Use translate to remove all non-printable characters\n",
        "    return text.translate({character:None for character in nonprintable})"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB3OrR5g8VYQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f1d3a1a5-dbbd-4c53-a2f6-0a21d64a1e9b"
      },
      "source": [
        "import sys\n",
        "\n",
        "# build a table mapping all non-printable characters to None\n",
        "NOPRINT_TRANS_TABLE = {\n",
        "    i: None for i in range(0, sys.maxunicode + 1) if not chr(i).isprintable()\n",
        "}\n",
        "\n",
        "def make_printable(s):\n",
        "    \"\"\"Replace non-printable characters in a string.\"\"\"\n",
        "\n",
        "    # the translate method on str removes characters\n",
        "    # that map to None from the string\n",
        "    return s.translate(NOPRINT_TRANS_TABLE)\n",
        "\n",
        "\n",
        "assert make_printable('Café') == 'Café'\n",
        "assert make_printable('\\x00\\x11Hello') == 'Hello'\n",
        "assert make_printable('') == ''\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woDWvRZHjU2E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9835f9cb-d3ca-4cbe-f894-5f7789c354d9"
      },
      "source": [
        "\"\"\"\n",
        "def corpus2chunks(corpus_filename, sent_tok='pysbd'):\n",
        "  '''\n",
        "  Given a corpus filename (assuming already %cd into correct subdir) \n",
        "    and a sentence tokeniziation method in ['pysbd'(default)|'both'|'nltk']\n",
        "  Return 6 lists and a string of the raw corpus:\n",
        "    4 Paragraph length lists -----\n",
        "    parag_raw_ls : list of raw text for each paragraph\n",
        "    parag_clean_ls : list of clean text for each sentence\n",
        "\n",
        "    parag_sentno_start_ls : list of the Sentence Number at the start of every Paragraph\n",
        "    parag_sentno_end_ls : list of the Sentence Number at the end of every Paragraph\n",
        "\n",
        "    2 Sentence length lists -----\n",
        "    sent_raw_ls : list of raw text for each sentence\n",
        "    sent_clean_ls : list of clean text for each sentence\n",
        "  '''\n",
        "\n",
        "  # Load PySBD if necessary\n",
        "  if (sent_tok == 'pysbd') | (sent_tok == 'both'):\n",
        "    from pysbd.utils import PySBDFactory\n",
        "    nlp = spacy.blank('en')\n",
        "    # explicitly adding component to pipeline\n",
        "    # (recommended - makes it more readable to tell what's going on)\n",
        "    nlp.add_pipe(PySBDFactory(nlp))\n",
        "    # pysbd = nlp.create_pipe('pysbd')\n",
        "    # nlp.add_pipe(pysbd)\n",
        "    # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n",
        "    # print(list(doc.sents))\n",
        "\n",
        "  # Read file into raw text string\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  # Split into Raw Paragraphs\n",
        "  print(f'BEFORE stripping out headings len: {len(corpus_raw_str)}')\n",
        "  corpus_parags_raw_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n",
        "  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n",
        "\n",
        "\n",
        "  # Copy/Clean Paragraphs into new list\n",
        "  # Filter out numbers(often footnotes) from Paragraphs\n",
        "  corpus_parags_ls = [re.sub(r'[0-9]','',x) for x in corpus_parags_raw_ls]\n",
        "\n",
        "  # Filter out empty lines Paragraphs\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n",
        "\n",
        "  # Strip out non-printing characters\n",
        "  corpus_parags_ls = [re.sub(f'[^{re.escape(string.printable)}]', '', x) for x in corpus_parags_ls]\n",
        "\n",
        "  print(f'   Parag count before processing sents: {len(corpus_parags_ls)}')\n",
        "  # FIRST PASS at Sentence Tokenization with PySBD\n",
        "  corpus_sents_all_ls = []\n",
        "  for i, aparag in enumerate(corpus_parags_ls):\n",
        "\"\"\";"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMA-040nIypX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "32ea8331-5f25-4e04-ab93-0d77832d8a96"
      },
      "source": [
        "def corpus2lines(corpus_filename, pysbd_only=False):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of every line defined by puncutation/NLTK.sent_tokenize or newlines [\\n]{2,}\n",
        "  '''\n",
        "\n",
        "  from pysbd.utils import PySBDFactory\n",
        "  nlp = spacy.blank('en')\n",
        "  # explicitly adding component to pipeline\n",
        "  # (recommended - makes it more readable to tell what's going on)\n",
        "  nlp.add_pipe(PySBDFactory(nlp))\n",
        "  # pysbd = nlp.create_pipe('pysbd')\n",
        "  # nlp.add_pipe(pysbd)\n",
        "  # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n",
        "  # print(list(doc.sents))\n",
        "\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  print(f'BEFORE stripping out headings len: {len(corpus_raw_str)}')\n",
        "\n",
        "  corpus_parags_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n",
        "  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n",
        "\n",
        "  # Strip off whitespace from Paragraphs\n",
        "  corpus_parags_ls = [x.strip() for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out numbers(often footnotes) from Paragraphs\n",
        "  corpus_parags_ls = [re.sub(r'[0-9]','',x) for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out empty lines Paragraphs\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n",
        "\n",
        "  # Strip out non-printing characters\n",
        "  corpus_parags_ls = [re.sub(f'[^{re.escape(string.printable)}]', '', x) for x in corpus_parags_ls]\n",
        "\n",
        "  print(f'   Parag count before processing sents: {len(corpus_parags_ls)}')\n",
        "  # FIRST PASS at Sentence Tokenization with PySBD\n",
        "  corpus_sents_all_ls = []\n",
        "  for i, aparag in enumerate(corpus_parags_ls):\n",
        "\n",
        "    # Generally PySBD outperforms NLTK and SpaCy, \n",
        "    #   but for Samuel Butler's 1900 translation of Homer's Odyssey\n",
        "    #   it failed in many cases so we combine/stack PySBD with NLTK\n",
        "    #   (exception to rule: NLTK > SpaCy for SentTokenization circa 2020\n",
        "    #    https://www.kaggle.com/questions-and-answers/130344)\n",
        "\n",
        "    # NLTK Sentence Tokenization\n",
        "    # 3605 lines with 'To the Lighthouse' by V.Woolf\n",
        "    # aparag_sents_ls = (sent_tokenize(aparag))\n",
        "    \n",
        "    # SpaCy Sentence Tokenization\n",
        "    # TODO: Speed up my specializaing pipe\n",
        "    # 3968 lines for 'To the Lighthouse' by V.Woolf\n",
        "    # doc = nlp(aparag)   \n",
        "    # aparag_sents_ls = [sent for sent in doc.sents]\n",
        "    # aparag_sents_ls = [x for x in doc]\n",
        "\n",
        "    # FIRST, tokenize with PySBD\n",
        "    # PySBD Sentence Tokenization\n",
        "    # 3457 lines for 'To the Lighthouse' by V.Woolf\n",
        "    # using pysbd and SpaCy\n",
        "    # or you can use it implicitly with keyword\n",
        "    \n",
        "\n",
        "    aparag_nonl = re.sub('[\\n]{1,}', ' ', aparag)\n",
        "    doc = nlp(aparag_nonl)\n",
        "    aparag_sents_first_ls = list(doc.sents)\n",
        "    print(f'pysbd found {len(aparag_sents_first_ls)} Sentences in Paragraph #{i}')\n",
        "\n",
        "    # Strip off whitespace from Sentences\n",
        "    aparag_sents_first_ls = [str(x).strip() for x in aparag_sents_first_ls]\n",
        "\n",
        "    # Filter out empty line Sentences\n",
        "    aparag_sents_first_ls = [x for x in aparag_sents_first_ls if (len(x.strip()) > MIN_SENT_LEN)]\n",
        "\n",
        "    print(f'      {len(aparag_sents_first_ls)} Sentences remain after cleaning')\n",
        "\n",
        "    corpus_sents_all_ls += aparag_sents_first_ls\n",
        "\n",
        "  # (OPTIONAL) SECOND PASS as Sentence Tokenization with NLTK\n",
        "  if pysbd_only == True:\n",
        "    # Only do one pass at Sentence tokenization with PySBD above\n",
        "    corpus_sents_all_ls = aparag_sents_first_ls\n",
        "  else:\n",
        "    # Do second pass, tokenize again with NLTK to catch any Sentence tokenization missed by PySBD\n",
        "    corpus_sents_all_second_ls = []\n",
        "    aparag_sents_second_ls = []\n",
        "    for asent_first in corpus_sents_all_ls:\n",
        "      aparag_sents_second_ls = sent_tokenize(asent_first)\n",
        "\n",
        "      # Strip off whitespace from Sentences\n",
        "      aparag_sents_second_ls = [str(x).strip() for x in aparag_sents_second_ls]\n",
        "\n",
        "      # Filter out empty line Sentences\n",
        "      aparag_sents_second_ls = [x for x in aparag_sents_second_ls if (len(x.strip()) > MIN_SENT_LEN)]\n",
        "\n",
        "      corpus_sents_all_second_ls += aparag_sents_second_ls\n",
        "\n",
        "    corpus_sents_all_ls = corpus_sents_all_second_ls\n",
        "\n",
        "  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n",
        "  # parag_before_punctstrip_ct = len(corpus_parags_ls)\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  # print(f'Punctuation only Paragraph Count: {len(corpus_parags_ls) - parag_before_punctstrip_ct}')\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Section separator 'SECTION ' lines\n",
        "  # for i,temp_str in enumerate(corpus_parags_ls):\n",
        "  #   if temp_str.startswith('SECTION '):\n",
        "  #     print(f'Parag #{i}: {temp_str}')\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('SECTION '))]\n",
        "\n",
        "  # Filter out any possible embedded 'SECTION ' lines\n",
        "  # for i,temp_str in enumerate(corpus_parags_ls):\n",
        "  #   if 'SECTION' in temp_str:   # .contains('SECTION '):\n",
        "  #     print(f'Parag #{i}: {temp_str}')\n",
        "  # corpus_parags_ls = del_substrs_list(corpus_parags_ls, pattern_sect) # [re.sub(rf'{pattern_sect}', '', x) for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out the Chapter separator 'CHAPTER ' lines\n",
        "  # for i,temp_str in enumerate(corpus_parags_ls):\n",
        "  #   if temp_str.startswith('CHAPTER '):\n",
        "  #     print(f'Parag #{i}: {temp_str}')\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('CHAPTER '))]\n",
        "\n",
        "  print(f'About to return corpus_sents_all_ls with len = {len(corpus_sents_all_ls)}')\n",
        "  return corpus_sents_all_ls, corpus_raw_str\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5nrHLut8AEl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6c7a287d-d110-4610-cdd0-dd9c2f135c7a"
      },
      "source": [
        "def corpus2sects(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a 3 lists: First, the list of Section text strings\n",
        "                    Second, a list of tuples that match (Sentence No, Segment No)\n",
        "                    Third, a list of Sentences that not found in any Section \n",
        "  '''\n",
        "\n",
        "  corpus_sects_ls = []\n",
        "\n",
        "\n",
        "  # encoding = CORPUS_ENCODING,  'windows-1252', 'utf-8', 'cp1252', 'iso-8859-1'\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  # pattern_sect = 'SECTION [\\d]{1,2}[.]?[^\\n]*'\n",
        "  # pattern_sect = 'SECTION [0123456789]{1,2}[^\\n]*'\n",
        "  # corpus_sects_ls = re.split(r'SECTION [\\d]{1,2}[.]* [A-Z \\.-:—;-’\\'\"]*[\\n]*', corpus_raw_str flags=re.I) # , flags=re.I)\n",
        "  corpus_sects_ls = re.split(rf'{pattern_sect}', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  # corpus_sects_ls = re.split(rf'{pattern_sect}', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  print(f'len(corpus_raw_str: {len(corpus_raw_str)}')\n",
        "  print(f'len(corpus_sects_ls): {len(corpus_sects_ls)}')\n",
        "  print(f'    First section: Length={len(corpus_sects_ls[0])}\\n    {corpus_sects_ls[0][:500]}')\n",
        "  print(f'    Second section: {corpus_sects_ls[1][:500]}')\n",
        "  print('\\n')\n",
        "  print(f'    Second-Last section: {corpus_sects_ls[-2][:500]}')\n",
        "  print(f'    Last section: {corpus_sects_ls[-1][:500]}')\n",
        "\n",
        "\n",
        "\n",
        "  # Strip off whitespace \n",
        "  corpus_sects_ls = [x.strip() for x in corpus_sects_ls]\n",
        "\n",
        "  # Filter out empty lines\n",
        "  corpus_sects_ls = [x for x in corpus_sects_ls if not (len(x.strip()) <= MIN_SECT_LEN)]\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Section separator 'SECTION ' lines\n",
        "  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('SECTION '))]\n",
        "\n",
        "  # Filter out any possible embedded 'CHAPTER ' lines\n",
        "  # TODO: Zeroing out corpus_sects_ls\n",
        "  # corpus_sects_ls = del_substrs_list(corpus_sects_ls, pattern_chap) # corpus_sects_ls = [re.sub(rf'{pattern_sect}', '', x) for x in corpus_sects_ls]\n",
        "\n",
        "  # Filter out the Chapter separator 'CHAPTER ' lines\n",
        "  # Keep for now, messy but enables proper SECTION assignments to appropraite CHAPTERs\n",
        "  # corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('CHAPTER '))]\n",
        "\n",
        "\n",
        "  print(f'About to process {len(corpus_sects_ls)} Sections')\n",
        "  # Filter out Sentences in Section that don't have a corresponding Sentence in corpus_sents_df \n",
        "  # Old Strategy: Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \n",
        "\n",
        "  sect_sentences_match_ls = []\n",
        "  sect_sentences_unmatch_ls = []\n",
        "  sect_sentences_unmatch_ct = 0\n",
        "  corpus_sentence_current = 0\n",
        "\n",
        "\n",
        "  for asect_no, asection in enumerate(corpus_sects_ls):\n",
        "    # sect_sentences_ls = []\n",
        "\n",
        "    # NLTK Sentence Tokenization\n",
        "    # 3605 lines with 'To the Lighthouse' by V.Woolf\n",
        "    # asect_sents_ls = (sent_tokenize(asect))\n",
        "    \n",
        "    # SpaCy Sentence Tokenization\n",
        "    # TODO: Speed up my specializaing pipe\n",
        "    # 3968 lines for 'To the Lighthouse' by V.Woolf\n",
        "    # doc = nlp(asect)   \n",
        "    # asect_sents_ls = [sent for sent in doc.sents]\n",
        "    # asect_sents_ls = [x for x in doc]\n",
        "\n",
        "    # PySBD Sentence Tokenization\n",
        "    # 3457 lines for 'To the Lighthouse' by V.Woolf\n",
        "    # using pysbd and SpaCy\n",
        "    from pysbd.utils import PySBDFactory\n",
        "    nlp = spacy.blank('en')\n",
        "    # explicitly adding component to pipeline\n",
        "    # (recommended - makes it more readable to tell what's going on)\n",
        "    nlp.add_pipe(PySBDFactory(nlp))\n",
        "    # or you can use it implicitly with keyword\n",
        "    # pysbd = nlp.create_pipe('pysbd')\n",
        "    # nlp.add_pipe(pysbd)\n",
        "    # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n",
        "    # print(list(doc.sents))\n",
        "    doc = nlp(asection)\n",
        "    asect_sents_ls = list(doc.sents)\n",
        "\n",
        "    # Create a normalized/no puncutation list of Corpus sentences for clean test comparisions filtered\n",
        "    corpus_sents_nopunct_ls = [re.sub(r'[^A-Za-z0-9]', ' ',x) for x in corpus_sents_ls]\n",
        "    # corpus_sents_nopunct_ls = [x for x in corpus_sents_nopunct_ls if x.isalnum()]\n",
        "    corpus_sents_nopunct_strip_ls = [x.strip() for x in corpus_sents_nopunct_ls]\n",
        "\n",
        "    for j, asection_sentence_raw in enumerate(asect_sents_ls):\n",
        "      asection_sentence_str = str(asection_sentence_raw)\n",
        "      asection_sentence_nopunct_str = re.sub(r'[^A-Za-z0-9]', ' ', asection_sentence_str)\n",
        "      asection_sentence_nopunct_strip_str = asection_sentence_nopunct_str.strip()\n",
        "      # This 'in' test is not sufficient, need to strip out punctuation/normalize\n",
        "      if asection_sentence_nopunct_strip_str in corpus_sents_nopunct_strip_ls:\n",
        "        sect_sentences_match_ls.append((asect_no, asection_sentence_str))\n",
        "        print(f'  Matched Segment Sent')\n",
        "      else:\n",
        "        sect_sentences_unmatch_ct += 1\n",
        "        print(f'  UNMATCHED Corpus Sentence #[{corpus_sentence_current}]\\n           Segment Sentence #{j}: [{asection_sentence_str}]\\n            [{asection_sentence_nopunct_strip_str}]')\n",
        "        sect_sentences_unmatch_ls.append(asection_sentence_str)\n",
        "\n",
        "      corpus_sentence_current += 1\n",
        "\n",
        "    # section_str = ' '.join(sect_sentences_ls)\n",
        "    # sect_sentences_match_ls.append(section_str)\n",
        "\n",
        "    # if re.search(rf'{pattern_chap}', asect):\n",
        "    #   print(f'In Section #{i} removing embedded CHAPTER:\\n\\n    {asect}')\n",
        "    #   asect = re.sub(rf'{pattern_chap}', ' ', asect)\n",
        "\n",
        "  return corpus_sects_ls, sect_sentences_match_ls, sect_sentences_unmatch_ls\n",
        "\n",
        "\n",
        "# return corpus_sects_ls, corpus_raw_str"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8z8ZY5n8BRs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3c3c9993-9c7a-4fd3-977e-14c79a4048ca"
      },
      "source": [
        "def corpus2parags(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of min preprocessed raw paragraphs (corpus_parags_ls)\n",
        "  '''\n",
        "\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  corpus_parags_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n",
        "  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n",
        "\n",
        "  # Strip off whitespace\n",
        "  corpus_parags_ls = [x.strip() for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out numbers(often footnotes) from Paragraphs\n",
        "  corpus_parags_ls = [re.sub(r'[0-9]',' ',x) for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  # Redundant, filed by punctuation only filter above\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Chapter/Section header lines\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('CHAPTER '))]\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('SECTION '))]\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('BOOK '))]\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (re.match(r\"^[ ]*[0-9]{1,3}[\\.]?[ ]*$\", x))]\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (re.match(r\"^[ ]*[IVXLC]{1,10}[\\.]?[ ]*$\", x))]\n",
        "\n",
        "  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n",
        "  parag_before_punctstrip_ct = len(corpus_parags_ls)\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  print(f'Punctuation only Paragraph Count: {len(corpus_parags_ls) - parag_before_punctstrip_ct}')\n",
        "\n",
        "  # Filter out empty lines Paragraphs\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n",
        "\n",
        "  # Made a deepcopy of the original raw paragraphs after simple cleaning while continuing to clean the original\n",
        "  corpus_parags_raw_ls = [x for x in corpus_parags_ls]\n",
        "\n",
        "  # Strip out non-printing characters\n",
        "  corpus_parags_ls = [re.sub(f'[^{re.escape(string.printable)}]', ' ', x) for x in corpus_parags_ls]\n",
        "\n",
        "  # Condense multiple whitespaces down into one\n",
        "  corpus_parags_ls = [' '.join(x.split()).strip() for x in corpus_parags_ls]\n",
        "\n",
        "  # Verify no Chapter/Section header lines remain\n",
        "  for i,temp_str in enumerate(corpus_parags_ls):\n",
        "    if temp_str.startswith('CHAPTER '):\n",
        "      print(f'Parag #{i}: {temp_str}')\n",
        "\n",
        "  return corpus_parags_ls, corpus_parags_raw_ls, corpus_raw_str\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxTYt6xsD5Iq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1fdbbdee-15cf-48f0-c530-31ab0bab4cb1"
      },
      "source": [
        "def sect2parags(sect_str):\n",
        "  '''\n",
        "  Given a Section as a text string\n",
        "  Return a list of raw Paragraphs and a raw Section text string\n",
        "  '''\n",
        "\n",
        "  sect_clean_str = re.sub(f'[^{re.escape(string.printable)}]', ' ', sect_str)\n",
        "  sect_parags_raw_ls = re.split(r'[\\n]{2,}', sect_clean_str)\n",
        "  # print(f'Section Paragraph Raw Count: {len(sect_parags_raw_ls)}')\n",
        "\n",
        "  # Strip off whitespace\n",
        "  sect_parags_raw_ls = [x.strip() for x in sect_parags_raw_ls]\n",
        "\n",
        "  # Filter out numbers(often footnotes) from Paragraphs\n",
        "  sect_parags_raw_ls = [re.sub(r'[0-9]',' ',x) for x in sect_parags_raw_ls]\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  # Redundant, filed by punctuation only filter above\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Chapter/Section header lines\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('CHAPTER '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('SECTION '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('BOOK '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[0-9]{1,3}[\\.]?[ ]*$\", x))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[IVXLC]{1,10}[\\.]?[ ]*$\", x))]\n",
        "\n",
        "  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n",
        "  parag_before_punctstrip_ct = len(sect_parags_raw_ls)\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  # print(f'Punctuation only Paragraph Count: {len(sect_parags_raw_ls) - parag_before_punctstrip_ct}')\n",
        "\n",
        "  # Filter out Paragraphs that are empty or shorter than MIN_PARAG_LEN\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n",
        "\n",
        "  # Made a clean copy of the original raw paragraphs after simple cleaning while continuing to clean the original\n",
        "  sect_parags_clean_ls = [clean_text(x) for x in sect_parags_raw_ls]\n",
        "\n",
        "  # Verify no Chapter/Section header lines remain\n",
        "  for i,temp_str in enumerate(sect_parags_raw_ls):\n",
        "    if temp_str.startswith('CHAPTER '):\n",
        "      print(f'ERROR: CHAPTERS not filtered\\n    Parag #{i}: {temp_str}')\n",
        "\n",
        "  return sect_parags_raw_ls, sect_clean_str\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWw618b3vQmJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9fa79dfa-a6ca-405b-b1e1-8ed989755756"
      },
      "source": [
        "from pysbd.utils import PySBDFactory\n",
        "nlp = spacy.blank('en')\n",
        "# explicitly adding component to pipeline\n",
        "# (recommended - makes it more readable to tell what's going on)\n",
        "nlp.add_pipe(PySBDFactory(nlp))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e0OnfrGJ_GB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9f141ee4-b647-49f0-8704-e763081590db"
      },
      "source": [
        "def parag2sents(parag_str, sent_tok='pysbd'):\n",
        "  '''\n",
        "  Given a Paragraph as a long string and a Sentence tokenizer engine ['pysbd'(default)|'both'|'nltk']\n",
        "  Return a list of every Sentence within the given Paragraph\n",
        "  '''\n",
        "\n",
        "  # Load PySBD if necessary\n",
        "  \"\"\"\n",
        "  if (sent_tok == 'pysbd') | (sent_tok == 'both'):\n",
        "    from pysbd.utils import PySBDFactory\n",
        "    nlp = spacy.blank('en')\n",
        "    # explicitly adding component to pipeline\n",
        "    # (recommended - makes it more readable to tell what's going on)\n",
        "    nlp.add_pipe(PySBDFactory(nlp))\n",
        "    # pysbd = nlp.create_pipe('pysbd')\n",
        "    # nlp.add_pipe(pysbd)\n",
        "    # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n",
        "    # print(list(doc.sents))\n",
        "  \"\"\";\n",
        "\n",
        "  # Minimally clean Paragraph string of non-printing characters\n",
        "  parag_clean_str = re.sub(f'[^{re.escape(string.printable)}]', ' ', parag_str).strip()\n",
        "\n",
        "  # Replace embedded \\n with spaces\n",
        "  parag_clean_str = re.sub('[\\n]{1,}', ' ', parag_clean_str)\n",
        "\n",
        "  # Replaces mulitple whitespaces with one space\n",
        "  parag_clean_str = ' '.join(parag_clean_str.split())\n",
        "\n",
        "\n",
        "  # TOKENIZE with 1 of 3 ways\n",
        " \n",
        "  # TODO: Try simple/fast RegEx Tokenizer in lieu of NTLK to complement PyBSD\n",
        "\n",
        "  # ONE: Tokenize with PyBSD and NLTK \n",
        "  if (sent_tok == 'both'):\n",
        "    doc = nlp(parag_clean_str)\n",
        "    parag_sents_first_ls = list(doc.sents)\n",
        "    for asent_temp in parag_sents_first_ls:\n",
        "      asent_tokenized_temp = sent_tokenize(asent_temp)\n",
        "      parag_sents_ls.append(asent_tokenized_temp)\n",
        "    # print(f'  BOTH: {len(parag_sents_ls)} Sentences found in Paragraph')\n",
        "\n",
        "  # TWO: Tokenize with PyBSD\n",
        "  elif (sent_tok == 'pysbd'):\n",
        "    doc = nlp(parag_clean_str)\n",
        "    parag_sents_ls = list(doc.sents)\n",
        "    # print(f' PySBD: {len(parag_sents_ls)} Sentences found in Paragraph')\n",
        "\n",
        "  # THREE: Tokenize with NLTK\n",
        "  elif (sent_tok == 'nltk'):\n",
        "    parag_sents_ls = sent_tokenize(parag_clean_str)\n",
        "    # print(f' NLTK: {len(parag_sents_ls)} Sentences found in Paragraph')\n",
        "\n",
        "  # ERROR\n",
        "  else:\n",
        "    print(f'ERROR: sent_tok={sent_tok} but must be [pysbd(default)|both|nltk]')\n",
        "\n",
        "\n",
        "  # CLEAN Sentences\n",
        "\n",
        "  # Strip off whitespace from Sentences\n",
        "  parag_sents_ls = [str(x).strip() for x in parag_sents_ls]\n",
        "\n",
        "  # Copy/Clean Sentences into new list\n",
        "  # Filter out numbers(often footnotes) from Sentences\n",
        "  parag_sents_ls = [re.sub(r'[0-9]',' ',x) for x in parag_sents_ls]\n",
        "\n",
        "  # Filter out the Chapter/Section header lines\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*CHAPTER[\\s]*$\", x))]\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*SECTION[\\s]*$\", x))]\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]BOOK[\\s]*$\", x))]\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*[0-9]{1,3}[\\.]?[\\s]*$\", x))]\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*[IVXLC]{1,10}[\\.]?[\\s]*$\", x))]\n",
        "\n",
        "  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n",
        "  parag_before_punctstrip_ct = len(parag_sents_ls)\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  # print(f'Punctuation only Paragraph Count: {len(parag_sents_ls) - parag_before_punctstrip_ct}')\n",
        "\n",
        "  # Condense multiple consecutive whitespaces down to one whitespace\n",
        "  parag_sents_ls = [' '.join(x.split()) for x in parag_sents_ls]\n",
        "\n",
        "\n",
        "  # Filter Sentences that are empty or shorter than MIN_SENT_LEN\n",
        "  parag_sents_ls = [x for x in parag_sents_ls if (len(x.strip()) >= MIN_SENT_LEN)]\n",
        "\n",
        "  return parag_sents_ls, parag_clean_str\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX1gjU7S8esY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9f27519a-a06d-4fa0-9bf5-ecddd14717c7"
      },
      "source": [
        "\"\"\"\n",
        "def parag2sents(parag_str, sent_tok='py'):\n",
        "  '''\n",
        "  Given a Paragraph as a string,\n",
        "  Return a list of raw Sentences and a raw text Paragraph string\n",
        "  '''\n",
        "\n",
        "  parag_clean_str = re.sub(f'[^{re.escape(string.printable)}]', '', parag_str)\n",
        "  parag_parags_raw_ls = re.split(r'[\\n]{2,}', parag_clean_str)\n",
        "  print(f'Section Paragraph Raw Count: {len(parag_parags_raw_ls)}')\n",
        "\n",
        "  # Strip off whitespace\n",
        "  parag_parags_raw_ls = [x.strip() for x in parag_parags_raw_ls]\n",
        "\n",
        "  # Filter out numbers(often footnotes) from Paragraphs\n",
        "  sect_parags_raw_ls = [re.sub(r'[0-9]','',x) for x in sect_parags_raw_ls]\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  # Redundant, filed by punctuation only filter above\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Chapter/Section header lines\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('CHAPTER '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('SECTION '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('BOOK '))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[0-9]{1,3}[\\.]?[ ]*$\", x))]\n",
        "  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[IVXLC]{1,10}[\\.]?[ ]*$\", x))]\n",
        "\n",
        "\n",
        "  parag_no = 0\n",
        "  # sent_base = 0\n",
        "  corpus_sents_ls = []\n",
        "  for parag_no,aparag in enumerate(corpus_parags_ls):\n",
        "    sents_ls = sent_tokenize(aparag)\n",
        "    # Delete (whitespace only) sentences\n",
        "    sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n",
        "    # Delete (punctuation only) sentences\n",
        "    sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n",
        "    # Delete numbers (int or float) sentences\n",
        "    sents_ls = [x for x in sents_ls if not (x.strip().isnumeric())]\n",
        "\n",
        "    # Filter out leading SECTION separator 'SECTION ' lines\n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      if temp_str.startswith('SECTION '):\n",
        "        print(f'Sentence #{i}: {temp_str}')\n",
        "    sents_ls = [x for x in sents_ls if not (x.startswith('SECTION '))]\n",
        "\n",
        "    # Filter out leading Chapter separator 'CHAPTER ' lines\n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      if temp_str.startswith('CHAPTER '):\n",
        "        print(f'Sentence #{i}: {temp_str}')\n",
        "    sents_ls = [x for x in sents_ls if not (x.startswith('CHAPTER '))]\n",
        "    \n",
        "    # Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      # TODO: More specific, robust filtering mechnism \n",
        "      if (re.search(rf'{pattern_sect}', temp_str)):\n",
        "        pass\n",
        "      if (re.search(rf'{pattern_chap}', temp_str)):\n",
        "        pass\n",
        "      else:\n",
        "        corpus_sents_ls.append([sent_no, parag_no, temp_str])\n",
        "        sent_no += 1\n",
        "\n",
        "    # print(f'Returning with corpus_sents_ls length = {len(corpus_sents_ls)}')\n",
        "  \n",
        "  return corpus_sents_ls\n",
        "\"\"\""
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef parag2sents(parag_str, sent_tok=\\'py\\'):\\n  \\'\\'\\'\\n  Given a Paragraph as a string,\\n  Return a list of raw Sentences and a raw text Paragraph string\\n  \\'\\'\\'\\n\\n  parag_clean_str = re.sub(f\\'[^{re.escape(string.printable)}]\\', \\'\\', parag_str)\\n  parag_parags_raw_ls = re.split(r\\'[\\n]{2,}\\', parag_clean_str)\\n  print(f\\'Section Paragraph Raw Count: {len(parag_parags_raw_ls)}\\')\\n\\n  # Strip off whitespace\\n  parag_parags_raw_ls = [x.strip() for x in parag_parags_raw_ls]\\n\\n  # Filter out numbers(often footnotes) from Paragraphs\\n  sect_parags_raw_ls = [re.sub(r\\'[0-9]\\',\\'\\',x) for x in sect_parags_raw_ls]\\n\\n  # Filter out the Section separator \\'-----\\' lines\\n  # Redundant, filed by punctuation only filter above\\n  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.strip().startswith(\\'----- \\'))]\\n\\n  # Filter out the Chapter/Section header lines\\n  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith(\\'CHAPTER \\'))]\\n  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith(\\'SECTION \\'))]\\n  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith(\\'BOOK \\'))]\\n  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[0-9]{1,3}[\\\\.]?[ ]*$\", x))]\\n  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[IVXLC]{1,10}[\\\\.]?[ ]*$\", x))]\\n\\n\\n  parag_no = 0\\n  # sent_base = 0\\n  corpus_sents_ls = []\\n  for parag_no,aparag in enumerate(corpus_parags_ls):\\n    sents_ls = sent_tokenize(aparag)\\n    # Delete (whitespace only) sentences\\n    sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\\n    # Delete (punctuation only) sentences\\n    sents_ls = [x for x in sents_ls if len((re.sub(r\\'[^\\\\w\\\\s]\\',\\'\\',x)).strip()) > MIN_SENT_LEN]\\n    # Delete numbers (int or float) sentences\\n    sents_ls = [x for x in sents_ls if not (x.strip().isnumeric())]\\n\\n    # Filter out leading SECTION separator \\'SECTION \\' lines\\n    for i,temp_str in enumerate(sents_ls):\\n      if temp_str.startswith(\\'SECTION \\'):\\n        print(f\\'Sentence #{i}: {temp_str}\\')\\n    sents_ls = [x for x in sents_ls if not (x.startswith(\\'SECTION \\'))]\\n\\n    # Filter out leading Chapter separator \\'CHAPTER \\' lines\\n    for i,temp_str in enumerate(sents_ls):\\n      if temp_str.startswith(\\'CHAPTER \\'):\\n        print(f\\'Sentence #{i}: {temp_str}\\')\\n    sents_ls = [x for x in sents_ls if not (x.startswith(\\'CHAPTER \\'))]\\n    \\n    # Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \\n    for i,temp_str in enumerate(sents_ls):\\n      # TODO: More specific, robust filtering mechnism \\n      if (re.search(rf\\'{pattern_sect}\\', temp_str)):\\n        pass\\n      if (re.search(rf\\'{pattern_chap}\\', temp_str)):\\n        pass\\n      else:\\n        corpus_sents_ls.append([sent_no, parag_no, temp_str])\\n        sent_no += 1\\n\\n    # print(f\\'Returning with corpus_sents_ls length = {len(corpus_sents_ls)}\\')\\n  \\n  return corpus_sents_ls\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRopU4e3IQ2R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2be234c7-2908-49b7-f117-fa8da77064e8"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "def parag2sents(corpus_parags_ls):\n",
        "  '''\n",
        "  Given a list of paragraphs,\n",
        "  Return a list of lists of Sentences [sent_no, parag_no, asent(text)]\n",
        "  '''\n",
        "\n",
        "  sent_no = 0\n",
        "  # sent_base = 0\n",
        "  corpus_sents_ls = []\n",
        "  for parag_no,aparag in enumerate(corpus_parags_ls):\n",
        "    sents_ls = sent_tokenize(aparag)\n",
        "    # Delete (whitespace only) sentences\n",
        "    sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n",
        "    # Delete (punctuation only) sentences\n",
        "    sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n",
        "    # Delete numbers (int or float) sentences\n",
        "    sents_ls = [x for x in sents_ls if not (x.strip().isnumeric())]\n",
        "\n",
        "    # Filter out leading SECTION separator 'SECTION ' lines\n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      if temp_str.startswith('SECTION '):\n",
        "        print(f'Sentence #{i}: {temp_str}')\n",
        "    sents_ls = [x for x in sents_ls if not (x.startswith('SECTION '))]\n",
        "\n",
        "    # Filter out leading Chapter separator 'CHAPTER ' lines\n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      if temp_str.startswith('CHAPTER '):\n",
        "        print(f'Sentence #{i}: {temp_str}')\n",
        "    sents_ls = [x for x in sents_ls if not (x.startswith('CHAPTER '))]\n",
        "    \n",
        "    # Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      # TODO: More specific, robust filtering mechnism \n",
        "      if (re.search(rf'{pattern_sect}', temp_str)):\n",
        "        pass\n",
        "      if (re.search(rf'{pattern_chap}', temp_str)):\n",
        "        pass\n",
        "      else:\n",
        "        corpus_sents_ls.append([sent_no, parag_no, temp_str])\n",
        "        sent_no += 1\n",
        "\n",
        "    # print(f'Returning with corpus_sents_ls length = {len(corpus_sents_ls)}')\n",
        "  \n",
        "  return corpus_sents_ls\n",
        "\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-adBsfM38eo0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6b2b5e37-2409-47d8-d8a6-df1ddd2e54de"
      },
      "source": [
        "# corpus_sents_ls = parag2sents(corpus_parags_ls)\n",
        "# len(corpus_sents_ls)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaRD7RL9IOeg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "632d6ee3-645d-4ead-bde2-ed4e6a55d656"
      },
      "source": [
        "# Generate full path and timestamp for new filepath/filename\n",
        "\n",
        "def gen_pathfiletime(file_str, subdir_str=''):\n",
        "\n",
        "  # Geenreate compressed author and title substrings\n",
        "  author_raw_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "  title_raw_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "\n",
        "  # Generate current/unique datetime string\n",
        "  datetime_str = str(datetime.now().strftime('%Y%m%d%H%M%S'))\n",
        "\n",
        "  # Built fullpath+filename string\n",
        "  file_base, file_ext = file_str.split('.')\n",
        "\n",
        "  author_str = re.sub('[^A-Za-z0-9]+', '', author_raw_str)\n",
        "  title_str = re.sub('[^A-Za-z0-9]+', '', title_raw_str)\n",
        "\n",
        "  full_filepath_str = f'{subdir_str}{file_base}_{author_str}_{title_str}_{datetime_str}.{file_ext}'\n",
        "\n",
        "  # print(f'Returning from gen_savepath() with full_filepath={full_filepath}')\n",
        "\n",
        "  return full_filepath_str\n",
        "\n",
        "# Test\n",
        "# pathfilename_str = gen_pathfiletime('hist_paraglen.png')\n",
        "# print(pathfilename_str)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9FDtpQ6QaEJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "626e7624-d0df-46af-b645-33e8285ee7e2"
      },
      "source": [
        "#This function converts to lower-case, removes square bracket, removes numbers and punctuation\n",
        " \n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)  # Expand contrations\n",
        "    text = re.sub(\"\\\\'s\", \" own\", text)  # After expanding normal apostrophes, expand possessive apostrophes \"Mary's car\" -> \"Mary own car\"\n",
        "\n",
        "    # TODO: More formally\n",
        "    # https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0\n",
        "    text = re.sub(\"-\\n\", \" \", text)       # Join end of line words split by continuation hyphens\n",
        "    text = re.sub(\"-\\n\\r\", \" \", text)\n",
        "    text = re.sub(\"-\\r\", \" \", text)\n",
        "    text = re.sub(\"\\[.*?\\]\", \" \", text)\n",
        "\n",
        "    text = re.sub(\"-\", \" \", text)  # Special care for hypenated words well-known: choose option (a)\n",
        "                                   # (a) 'well known', (b) 'wellknown' (c) 'well known' and 'wellknown' cf: https://datascience.stackexchange.com/questions/81072/how-to-process-the-hyphenated-english-words-for-any-nlp-problem\n",
        "    text = re.sub(\"/\", \" \", text)  # sociability/conversation/interesting -> sociability conversation interesting                             \n",
        "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \" \", text)\n",
        "    text = re.sub(\"\\w*\\d\\w*\", \" \", text)\n",
        "    text = re.sub(\"[\\n]\", \" \", text)  # Replace newline with space\n",
        "    return text"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lYUmVuvl2OK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ceae4fa0-34f3-42c2-af6a-6499eb488128"
      },
      "source": [
        "def save_dataframes():\n",
        "  '''\n",
        "  Save all Corpus DataFrames\n",
        "  '''\n",
        "\n",
        "  # Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "  # author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "  # title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "  title_str = ''.join(CORPUS_FILENAME.split('.')[0]).lower()\n",
        "  datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "\n",
        "  # Sentences Raw\n",
        "  corpus_sents_filename = f'corpus_text_raw_{title_str}.csv'\n",
        "  print(f'Saving Raw Sentences to file: {corpus_sents_filename}')\n",
        "  corpus_sents_df['sent_raw'].to_csv(corpus_sents_filename)\n",
        "\n",
        "  # Sentences Clean\n",
        "  corpus_sents_filename = f'corpus_text_clean_{title_str}.csv'\n",
        "  print(f'Saving Clean Sentences to file: {corpus_sents_filename}')\n",
        "  corpus_sents_df['sent_clean'].to_csv(corpus_sents_filename)\n",
        "\n",
        "\n",
        "  # Sentence DataFrame\n",
        "  corpus_sents_filename = f'corpus_sents_{title_str}.csv'\n",
        "  print(f'Saving Sentences DataFrame to file: {corpus_sents_filename}')\n",
        "  corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "  # Paragraph DataFrame\n",
        "  corpus_parags_filename = f'corpus_parags_{title_str}.csv'\n",
        "  print(f'Saving Paragraph DataFrame to file: {corpus_parags_filename}')\n",
        "  corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "  # if SECTION_HEADINGS != 'None':  # Even if no Sections, save dummy placeholder\n",
        "  #                                   filled with Chapter data\n",
        "  # Section DataFrame\n",
        "  corpus_sects_filename = f'corpus_sects_{title_str}.csv'\n",
        "  print(f'Saving Section DataFrame to file: {corpus_sects_filename}')\n",
        "  corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "  # Chapter DataFrame\n",
        "  corpus_chaps_filename = f'corpus_chaps_{title_str}.csv'\n",
        "  print(f'Saving Chapter DataFrame to file: {corpus_chaps_filename}')\n",
        "  corpus_chaps_df.to_csv(corpus_chaps_filename)\n",
        "\n",
        "  \"\"\"\n",
        "  if sentiment_df == True:\n",
        "\n",
        "    corpus_sentiment_filename = f'sum_sentiments_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "    print(f'Saving Corpus Sentences to file: {corpus_sentiment_filename}')\n",
        "    corpus_baseline_df.to_csv(corpus_sentiment_filename)\n",
        "\n",
        "    corpus_parags_filename = f'sum_sentiments_parags_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "    print(f'Saving Corpus Paragraphs to file: {corpus_parags_filename}')\n",
        "    corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "    corpus_sects_filename = f'sum_sentiments_sects_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "    print(f'Saving Corpus Sections to file: {corpus_sects_filename}')\n",
        "    corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "    corpus_chaps_filename = f'sum_sentiments_chaps_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "    print(f'Saving Corpus Chapters to file: {corpus_chaps_filename}')\n",
        "    corpus_chaps_df.to_csv(corpus_chaps_filename)\n",
        "  \"\"\";\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# save_dataframes()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUvKJEybUIeP"
      },
      "source": [
        "## **Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8TwYCrwwUco",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cd9e7092-bb4c-4891-e552-95f2ac471a1d"
      },
      "source": [
        "# Test to find longest String in Corpus (in terms of #tokens) \n",
        "#      must be <510 tokens for Transformers\n",
        "\n",
        "# corpus_sents_df['sent_raw'].astype(str).apply(lambda x: len(x.split())).max() # split().len()).max()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnbQ_2Xmj1uW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5ed296e4-d212-4179-a4ef-a1ec32205c83"
      },
      "source": [
        "# get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentimentr, sentiment_type='lexicon')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtxqQjU0Odos",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e1746c7c-9aa0-4234-c0f3-5f4c61772b65"
      },
      "source": [
        "# corpus_sents_df.head(2)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0vWzfQ9D4Fr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1727664c-41cf-4f82-d1d5-f5daf3ef11e7"
      },
      "source": [
        "# corpus_parags_df.columns"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOsfpOS4uHGX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "17eb9157-2d34-444a-aa19-0567362aa003"
      },
      "source": [
        "def list2stdscaler(tseries_ls):\n",
        "  '''\n",
        "  Given a list of floating point number\n",
        "  Return a pd.Series that has been Standardized Scaled\n",
        "  '''\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  tseries_np = np.array(tseries_ls)\n",
        "  \n",
        "  tseries_np = tseries_np.reshape((len(tseries_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(tseries_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  tseries_xform_np = scaler.transform(tseries_np)\n",
        "\n",
        "  return pd.Series(tseries_xform_np.flatten())"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La0qTp9BwXQV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5f963919-3064-47ad-efd1-9064ab098846"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# Test MedianIQR vs StandarizedScalar \n",
        "\n",
        "corpus_sents_df['sentimentr_medianiqr'].plot(alpha=0.1)\n",
        "corpus_sents_df['sentimentr_stdscore'].plot(alpha=0.3)\n",
        "# corpus_sents_df['sentimentr_meanstd'].plot(alpha=0.3)\n",
        "corpus_sents_df['sentimentr'].plot(alpha=0.2)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "\"\"\";"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z0jQoBlfPir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6a94ceae-565e-4652-8d94-d70831e76de5"
      },
      "source": [
        "def standardize_tsls(ts_df, col_ls):\n",
        "  '''\n",
        "  Given a DataFrame and list of Columns in that DataFrame\n",
        "  Create 4 new Standardized Columns for each given Columns\n",
        "  '''\n",
        "\n",
        "  # Create 4 new column names for each column provided\n",
        "  for amodel in col_ls:\n",
        "    # col_meanstd = f'{amodel}_meanstd'\n",
        "    col_medianiqr = f'{amodel}_medianiqr'\n",
        "    col_stdscaler = f'{amodel}_stdscaler'\n",
        "    col_lnorm_meanstd = f'{amodel}_lnorm_meanstd'\n",
        "    col_lnorm_medianiqr = f'{amodel}_lnorm_medianiqr'\n",
        "\n",
        "    # Standardize each column provided using Standard Scaler and  MedianIQRScaling\n",
        "    ts_df[col_stdscaler]  = list2stdscaler(ts_df[amodel])\n",
        "    ts_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(ts_df[amodel]).reshape(-1, 1))\n",
        "    # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "    text_len_ls = list(ts_df['token_len'])\n",
        "    text_sentiment_ls = list(ts_df[amodel])\n",
        "    textsentiment_norm_ls = [text_sentiment_ls[i]/text_len_ls[i] for i in range(len(text_len_ls))]\n",
        "    # RobustStandardize Sentence sentiment values\n",
        "    ts_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n",
        "    ts_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  return\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLASKwzZF-Lh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "dd688b8b-c48a-4624-bfc3-f2a7e2653fb5"
      },
      "source": [
        "def get_sentiments(model_base, sentiment_fn, sentiment_type='lexicon'):\n",
        "  '''\n",
        "  Given a model_base name and sentiment evaluation function\n",
        "  Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "  '''\n",
        "\n",
        "  # Calculate Sentiment Polarities\n",
        "\n",
        "  if sentiment_type == 'lexicon':\n",
        "    print(f'Processing Lexicon Sentiments/Sentences...')\n",
        "    corpus_sents_df[model_base] = corpus_sents_df['sent_raw'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon Sentiments/Paragraphs...')\n",
        "    corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon Sentiments/Sections...')\n",
        "    corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon Sentiments/Chapters...')\n",
        "    corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "  \n",
        "  elif sentiment_type == 'compound':\n",
        "    # VADER\n",
        "\n",
        "    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n",
        "    print(f'Processing Compound Sentiments/Sentences...')\n",
        "    corpus_sents_df['scores'] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Compound Sentiments/Paragraphs...')\n",
        "    corpus_parags_df['scores'] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Compound Sentiments/Sections...')\n",
        "    corpus_sects_df['scores'] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Compound Sentiments/Chapters...')\n",
        "    corpus_chaps_df['scores'] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "\n",
        "    # Extract Compound Sentiment\n",
        "    corpus_sents_df[model_base]  = corpus_sents_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_parags_df[model_base]  = corpus_parags_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_sects_df[model_base]  = corpus_sects_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_chaps_df[model_base]  = corpus_chaps_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "\n",
        "  elif sentiment_type == 'function':\n",
        "    # TextBlob\n",
        "\n",
        "    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean parag_clean\n",
        "    print(f'Processing Function Sentiments/Sentences...')\n",
        "    corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Function Sentiments/Paragraphs...')\n",
        "    corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Function Sentiments/Sections...')\n",
        "    corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Function Sentiments/Chapters...')\n",
        "    corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: sentiment_type={sentiment_type} but must be one of (lexicon, compound, function)')\n",
        "    return\n",
        "\n",
        "  # Create new column names\n",
        "  # col_meanstd = f'{model_base}_meanstd'\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_stdscaler = f'{model_base}_stdscaler'\n",
        "  # col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_stdscaler = f'{model_base}_lnorm_stdscaler'\n",
        "\n",
        "\n",
        "  print('Standardizing Chapters')\n",
        "  # Get Chapter Robust Standardization with Standard Scaler and  MedianIQRScaling\n",
        "  corpus_chaps_df[col_stdscaler]  = list2stdscaler(corpus_chaps_df[model_base])\n",
        "  corpus_chaps_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Chapter Sentiment by dividing by Chapter Length\n",
        "  chaps_len_ls = list(corpus_chaps_df['token_len'])\n",
        "  chaps_sentiment_ls = list(corpus_chaps_df[model_base])\n",
        "  chaps_sentiment_norm_ls = [chaps_sentiment_ls[i]/chaps_len_ls[i] for i in range(len(chaps_len_ls))]\n",
        "  # RobustStandardize Chapter sentiment values\n",
        "  # corpus_chaps_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  print('Standardizing Sections')\n",
        "  # Get Section Robust Standardization with Standard Scaler and  MedianIQRScaling\n",
        "  # corpus_sects_df[col_stdscaler]  = mean_std_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sects_df[col_stdscaler]  = list2stdscaler(corpus_sects_df[model_base])\n",
        "  corpus_sects_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Section Sentiment by dividing by Section Length\n",
        "  sects_len_ls = list(corpus_sects_df['token_len'])\n",
        "  sects_sentiment_ls = list(corpus_sects_df[model_base])\n",
        "  sects_sentiment_norm_ls = [sects_sentiment_ls[i]/sects_len_ls[i] for i in range(len(sects_len_ls))]\n",
        "  # RobustStandardize Section sentiment values\n",
        "  # corpus_sects_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_sects_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  print('Standardizing Paragraphs')\n",
        "  # Get Paragraph Robust Standardization with Standard Scaler and  MedianIQRScaling\n",
        "  corpus_parags_df[col_stdscaler]  = list2stdscaler(corpus_parags_df[model_base])\n",
        "  corpus_parags_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_parags_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Paragraph Sentiment by dividing by Chapter Length\n",
        "  parags_len_ls = list(corpus_parags_df['token_len'])\n",
        "  parags_sentiment_ls = list(corpus_parags_df[model_base])\n",
        "  parags_sentiment_norm_ls = [parags_sentiment_ls[i]/parags_len_ls[i] for i in range(len(parags_len_ls))]\n",
        "  # RobustStandardize Paragraph sentiment values\n",
        "  # corpus_parags_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_parags_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(corpus_parags_df[model_base]).reshape(-1, 1))\n",
        "  corpus_parags_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  print('Standardizing Sentences')\n",
        "  # Get Sentence Robust Standardization with Standard Scaler and  MedianIQRScaling\n",
        "  corpus_sents_df[col_stdscaler]  = list2stdscaler(corpus_sents_df[model_base])\n",
        "  corpus_sents_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "  sents_len_ls = list(corpus_sents_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_sents_df[model_base])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/sents_len_ls[i] for i in range(len(sents_len_ls))]\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  # corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_sents_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sents_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  return"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJNepWBIkVjm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "860eddba-fe87-4235-b03d-7821a715624e"
      },
      "source": [
        "# Read in lexicon at given path into Dict[word]=polarity\n",
        "\n",
        "def get_lexicon(lexicon_name, lexicon_format=2):\n",
        "    \"\"\"\n",
        "    Read sentiment lexicon.csv file at lexicon_path\n",
        "    into appropriate Dict[word]=polarity\n",
        "\n",
        "    1. lexicon_dt[word] = <polarity value>\n",
        "\n",
        "    Args:\n",
        "        sa_lib (str, optional): [description]. Defaults to 'syuzhet'.\n",
        "    \"\"\"\n",
        "    \n",
        "    # global lexicon_df\n",
        "\n",
        "    lexicon_df = pd.DataFrame()\n",
        "    \n",
        "    # print(os.getcwd())\n",
        "\n",
        "    \n",
        "    try:\n",
        "      lexicon_df = pd.read_csv(lexicon_name)\n",
        "      lexicon_df.info()\n",
        "      # lexicon_df = lexicon_tmp_df.copy()\n",
        "      # print(lexicon_df.head())\n",
        "      return lexicon_df\n",
        "    except:\n",
        "      print(f'ERROR: Cannot read lexicon.csv at {lexicon_name}')\n",
        "      return -1\n",
        "\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWdzLF0jwI-u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8ece059a-47b7-487c-9b17-d058400735d0"
      },
      "source": [
        "# Sentence to Sentiment Polarity according to passed in Lexicon Dictionary\n",
        "\n",
        "def text2sentiment(text_str, lexicon_dt):\n",
        "  '''\n",
        "  Given a text_str and lexicon_dt, calculate \n",
        "  the sentimety polarity.\n",
        "  '''\n",
        "\n",
        "  # Remove all not alphanumeric and whitespace characters\n",
        "  text_str = re.sub(r'[^\\w\\s]', '', text_str) \n",
        "\n",
        "  text_str = text_str.strip().lower()\n",
        "  if (len(text_str) < 1):\n",
        "      print(f\"ERROR: text2sentiment() given empty/null/invalid string: {text_str}\")\n",
        "\n",
        "  text_ls = text_str.split()\n",
        "  # print(f'text_ls: {text_ls}')\n",
        "\n",
        "  # Accumulated Total Sentiment Polarity for entire Sentence\n",
        "  text_sa_tot = 0.0\n",
        "\n",
        "  for aword in text_ls:\n",
        "      # print(f'getting sa for word: {aword}')\n",
        "      try:\n",
        "          word_sa_fl = float(lexicon_dt[aword])\n",
        "          text_sa_tot += word_sa_fl\n",
        "          # print(f\">>{aword} has a sentiment value of {word_sa_fl}\")\n",
        "      except TypeError: # KeyError:\n",
        "          # aword is not in lexicon so it adds 0 to the sentence sa sum\n",
        "          # print(f\"TypeError: cannot convert {lexicon_dt[aword]} to float\")\n",
        "          continue\n",
        "      except KeyError:\n",
        "          # print(f\"KeyError: missing key {aword} in defaultdict syuzhet_dt\")\n",
        "          continue\n",
        "      except:\n",
        "          e = sys.exc_info()[0]\n",
        "          # print(f\"ERROR {e}: sent2lex_sa() cannot catch aword indexing into syuzhet_dt error\")\n",
        "  \n",
        "  # print(f\"Leaving sent2lex_sa() with sentence sa value = {str(text_sa_tot)}\")\n",
        "  \n",
        "  return text_sa_tot\n",
        "\n",
        "\n",
        "# Test\n",
        "\n",
        "# sent2sentiment('I hate and despise and abhor and dislike and am disgusted by Mondays.', lexicon_jockersrinker_dt)\n",
        "# sent2sentiment('hate Mondays.', lexicon_jockersrinker_dt)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6AEBZzvEz4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6e7666f4-059c-4db4-f255-e7b75533cf7a"
      },
      "source": [
        "def plot_smas(section_view=True, model_name='vader', text_unit='sentence', wins_ls=[20], alpha=0.5, subtitle_str='', y_height=0, save2file=False):\n",
        "  '''\n",
        "  Given a model, text_unit\n",
        "  Plot a SMA using default values and wrapping the function get_smas()\n",
        "  '''\n",
        "\n",
        "  if (section_view == True) and not any(x == text_unit for x in ['sentence', 'paragraph']):\n",
        "    print(f'ERROR: You can only plot SMA within a Section with Sentence or Paragraph text units')\n",
        "    return -99\n",
        "\n",
        "  if text_unit == 'sentence':\n",
        "    if section_view == False:\n",
        "      ts_df = corpus_sents_df\n",
        "    else:\n",
        "      ts_df = section_sents_df\n",
        "    wins_ls = [5,10,20]\n",
        "  elif text_unit == 'paragraph':\n",
        "    if section_view == False:\n",
        "      ts_df = corpus_parags_df\n",
        "    else:\n",
        "      ts_df = section_parags_df\n",
        "    wins_ls = [5,10,20]\n",
        "  elif text_unit == 'section':\n",
        "    ts_df = corpus_sects_df\n",
        "    wins_ls=[20]\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n",
        "\n",
        "  sectno_loc = ts_df[model_name].min()\n",
        "\n",
        "  if section_view ==False:\n",
        "    # At Section boundries draw blue vertical lines \n",
        "    section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "    for i, sent_no in enumerate(section_boundries_ls):\n",
        "      plt.text(sent_no, y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n",
        "      plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "      # 'BigNews1', xy=(sent_no, 0.5), xytext=(-10, 25), textcoords='offset points',                   rotation=90, va='bottom', ha='center', annotation_clip=True)\n",
        "\n",
        "      # plt.text(sent_no, -.5, 'goodbye',rotation=90, zorder=0)\n",
        "\n",
        "    # At Chapter boundaries draw red vertical lines\n",
        "    chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n",
        "    for i, sent_no in enumerate(chapter_boundries_ls):\n",
        "      plt.axvline(sent_no, color='navy', alpha=0.1)\n",
        "      # plt.text(sent_no, .5, 'hello', rotation=90, zorder=0)\n",
        "\n",
        "  get_smas(ts_df, model_name=model_name, text_unit=text_unit, wins_ls=wins_ls, alpha=alpha, subtitle_str=subtitle_str, save2file=save2file)\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_sma_sents_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcAMyjyn8ugj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "3bdc2107-78e0-4444-fa19-3fcf0b314c53"
      },
      "source": [
        "# SMA 5% Sentiment of Sentence Sentiment\n",
        "\n",
        "def get_smas(ts_df, model_name, text_unit='sentence', wins_ls=[5,10], alpha=0.5, scale_factor=1., subtitle_str='', mean_adj=0., do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a model_name and time series DataFrame and list of win_rolls in percentages\n",
        "  Return the rolling means of the time series using the window sizes in win_rolls\n",
        "  '''\n",
        "\n",
        "  temp_roll_df = pd.DataFrame() # TODO: save sma rolling values into temp_df and return this value\n",
        "\n",
        "  win_1per = int(ts_df.shape[0]*0.01)\n",
        "  if text_unit ==  'sentence':\n",
        "    # win_1per = win_s1per\n",
        "    x_idx = 'sent_no'\n",
        "    fname_abbr = 'sents'\n",
        "  elif text_unit == 'paragraph':\n",
        "    # win_1per = win_p1per\n",
        "    x_idx = 'parag_no'\n",
        "    fname_abbr = 'parags'\n",
        "  elif text_unit == 'section':\n",
        "    win_1per = 1\n",
        "    wins_ls = [int(0.1 * corpus_sects_df.shape[0])]  # Edge case to deal with very few Section data points\n",
        "    x_idx = 'sect_no'\n",
        "    fname_abbr = 'sects'\n",
        "  else:\n",
        "    print(f'ERROR: text_unit={text_unit} but must be either sentence, paragraph or section')\n",
        "  \n",
        "  for i, awin_size in enumerate(wins_ls):\n",
        "    if len(str(awin_size)) == 1:\n",
        "      awin_str = '0' + str(awin_size)\n",
        "    else:\n",
        "      awin_str = str(awin_size) \n",
        "    col_roll_str = f'{model_name}_mean_roll{awin_str}'\n",
        "    win_size = awin_size*win_1per\n",
        "    ts_df[col_roll_str] = ts_df[model_name].rolling(window=win_size, center=True).mean()\n",
        "  \n",
        "    if do_plot == True:\n",
        "      alabel = f'{model_name} (win={awin_size})'\n",
        "      ts_df['y_scaled'] = ts_df[col_roll_str]*scale_factor + mean_adj \n",
        "      sns.lineplot(data=ts_df, x=x_idx, y='y_scaled', legend='brief', label=alabel, alpha=alpha)\n",
        "      \n",
        "  plt.title(f'{CORPUS_FULL} (Model: {model_name}: {subtitle_str}) \\nSMA Smoothed {text_unit} Sentiment Plot (windows={wins_ls})')\n",
        "  # plt.legend(loc='best')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_{fname_abbr}_sa_mean_050100sma.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return temp_roll_df"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULzfHeDK8udN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f6330012-f6e4-4b2d-e96d-76f496ea3a1b"
      },
      "source": [
        "def get_lexstats(ts_df, model_name, text_unit='sentence'):\n",
        "  '''\n",
        "  Given a model name\n",
        "  calculate, store and return time series stats\n",
        "  '''\n",
        "  \n",
        "  global corpus_lexicons_stats_dt\n",
        "\n",
        "  temp_dt = {}\n",
        "  \n",
        "  if text_unit == 'sentence':\n",
        "    stat_idx = f'{model_name}_sents'\n",
        "  elif text_unit == 'paragraph':\n",
        "    stat_idx = f'{model_name}_parags'\n",
        "  elif text_unit == 'section':\n",
        "    stat_idx = f'{model_name}_sects'\n",
        "  elif text_unit == 'chapter':\n",
        "    stat_idx = f'{model_name}_chaps'\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must either be sentence, paragraph, or section')\n",
        "\n",
        "  sentiment_min = ts_df[model_name].min()\n",
        "  sentiment_max = ts_df[model_name].max()\n",
        "\n",
        "  temp_dt = {'sentiment_min' : sentiment_min,\n",
        "             'sentiment_max' : sentiment_max}\n",
        "\n",
        "  corpus_lexicons_stats_dt[stat_idx] = temp_dt\n",
        "                                     \n",
        "  return \n",
        "\n",
        "# Test\n",
        "# get_lexstats('afinn')\n",
        "# corpus_lexicons_stats_dt"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltdJn-7ePNM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c23a349b-f58c-4d29-9c80-7c0055fe9a42"
      },
      "source": [
        "def lex_discrete2continous_sentiment(text, lexicon):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_tot = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    word_sentiment = text2sentiment(str(aword), lexicon)\n",
        "    text_sentiment_tot += word_sentiment\n",
        "  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.01)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6xMI98l8uaH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7fed4210-ac6a-492b-f2f9-d5f9b7c5e07a"
      },
      "source": [
        "\"\"\"\n",
        "def clip_outliers(floats_ser):\n",
        "  '''\n",
        "  Given a pd.Series of float values\n",
        "  Return a list with outliers removed, values limited within 3 median absolute deviations from median\n",
        "  '''\n",
        "  # https://www.statsmodels.org/stable/generated/statsmodels.robust.scale.mad.html#statsmodels.robust.scale.mad\n",
        "\n",
        "  # Old mean/std, less robust\n",
        "  # ser_std = floats_ser.std()\n",
        "  # ser_median = floats_ser.mean() # TODO: more robust: asym/outliers -> median/IQR or median/median abs deviation\n",
        "\n",
        "  floats_np = np.array(floats_ser)\n",
        "  ser_median = floats_ser.median()\n",
        "  ser_mad = robust.mad(floats_np)\n",
        "  print(f'ser_median = {ser_median}')\n",
        "  print(f'ser_mad = {ser_mad}')\n",
        "\n",
        "  if ser_mad == 0:\n",
        "    # for TS with small ranges (e.g. -1.0 to +1.0) Median Abs Deviation = 0\n",
        "    #   so pass back the original time series\n",
        "    floats_clip_ls = list(floats_ser)\n",
        "\n",
        "  else:\n",
        "    ser_oldmax = floats_ser.max()\n",
        "    ser_oldmin = floats_ser.min()\n",
        "    print(f'ser_max = {ser_oldmax}')\n",
        "    print(f'ser_min = {ser_oldmin}')\n",
        "\n",
        "    ser_upperlim = ser_median + 2.5*ser_mad\n",
        "    ser_lowerlim = ser_median - 2.5*ser_mad\n",
        "    print(f'ser_upperlim = {ser_upperlim}')\n",
        "    print(f'ser_lowerlim = {ser_lowerlim}')\n",
        "\n",
        "    # Clip outliers to max or min values\n",
        "    floats_clip_ls = np.clip(floats_np, ser_lowerlim, ser_upperlim)\n",
        "    # print(f'max floast_ls {floats_ls.max()}')\n",
        "\n",
        "    # def map2range(value, low, high, new_low, new_high):\n",
        "    #   '''map a value from one range to another'''\n",
        "    #   return value * 1.0 / (high - low + 1) * (new_high - new_low + 1)\n",
        "\n",
        "    # Map all float values to range [-1.0 to 1.0]\n",
        "    # floats_clip_sig_ls = [map2range(i, ser_oldmin, ser_oldmax, ser_upperlim, ser_lowerlim) for i in floats_clip_ls]\n",
        "\n",
        "    # listmax_fl = float(max(floats_ls))\n",
        "    # floats_ls = [i/listmax_fl for i in floats_ls]\n",
        "    #floats_ls = [1/(1+math.exp(-i)) for i in floats_ls]\n",
        "\n",
        "  return floats_clip_ls  # floats_clip_sig_ls\n",
        "\"\"\";\n",
        "\n",
        "# Test\n",
        "# Will not work on first run as corpus_sents_df is not defined yet\n",
        "'''\n",
        "data = np.array([1, 4, 4, 7, 12, 13, 16, 19, 22, 24])\n",
        "test_ls = clip_outliers(corpus_sents_df['vader'])\n",
        "print(f'new min is {min(test_ls)}')\n",
        "print(f'new max is {max(test_ls)}')\n",
        "''';"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXwKR4gA8Ouk"
      },
      "source": [
        "## **Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8Hf8nU98uXI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "37bd186f-1ce3-430e-b89f-b6bf7bd2ac89"
      },
      "source": [
        "\"\"\"\n",
        "def rename_cols(ts_df, col_old_ls, suffix_str='_raw'):\n",
        "  '''\n",
        "  Given a DataFrame, list of columns in DataFrame and a suffix,\n",
        "  Return a Dictionary mapping old col names to new col name (orig+suffix)\n",
        "  '''\n",
        "\n",
        "  col_new_ls = []\n",
        "  for acol in col_old_ls:\n",
        "    acol_new = acol + suffix_str\n",
        "    col_new_ls.append(acol_new)\n",
        "\n",
        "  # Create dict for col mapping: keys=old col names, value=new col names\n",
        "  col_rename_dt = dict(zip(col_old_ls, col_new_ls))\n",
        "\n",
        "  # ts_df.rename(columns=col_rename_dt, errors=\"raise\")\n",
        "\n",
        "  return col_rename_dt\n",
        "\n",
        "# test_ls = [col for col in corpus_sents_df.columns if not(renaming_fun(col) is None)]\n",
        "# print(f'test_ls: {test_ls}')\n",
        "\n",
        "# Test\n",
        "# col_rename_dt = rename_cols(corpus_sents_df, sentiment_only_cols_ls)\n",
        "# col_rename_dt\n",
        "\"\"\";"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YJJcvDVnUuT"
      },
      "source": [
        "## **Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIo6-zGKnZps",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "73b0c861-8602-4fba-b5b1-fba928b087cc"
      },
      "source": [
        "\"\"\"\n",
        "def norm2negpos1(data_ser):\n",
        "  '''\n",
        "  Given a series of floating number\n",
        "  Return a a list of same values normed between -1.0 and +1.0\n",
        "  '''\n",
        "  # data_np = np.matrix(data_ser)\n",
        "\n",
        "  scaler=MinMaxScaler(feature_range=(-1.0, 1.0))\n",
        "  temp_ser = scaler.fit_transform(np.matrix(data_ser))\n",
        "  \n",
        "  return temp_ser\n",
        "\"\"\";\n",
        "\n",
        "# Test\n",
        "'''\n",
        "temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n",
        "print(type(temp_np))\n",
        "temp_np.shape\n",
        "''';"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpVHeYKYnUhU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "284f731b-09d6-49f7-ed39-d66979db382c"
      },
      "source": [
        "\"\"\"\n",
        "def standardize_ts(data_ser):\n",
        "  '''\n",
        "  Given a series of floating number\n",
        "  Return a a list of same values normed between -1.0 and +1.0\n",
        "  '''\n",
        "  # data_np = np.matrix(data_ser)\n",
        "\n",
        "  std_scaler = StandardScaler()\n",
        "  df_std = std_scaler.fit_transform(np.array(data_ser))\n",
        "  \n",
        "  return df_std\n",
        "\"\"\";\n",
        "\n",
        "# Test\n",
        "'''\n",
        "temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n",
        "print(type(temp_np))\n",
        "temp_np.shape\n",
        "temp_np\n",
        "''';"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylt_kLuEFDrj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b400d4f7-927d-41ea-b597-a09650dbac03"
      },
      "source": [
        "# This must be defined AFTER the corpus_sects_df DataFrame is created in the Preprocessing Step below\n",
        "# Raw Plot of Section Sentiments (Adjusted for (x-axis) mid-Section Sentence No and (y-axis) Sentiment weighted by Section length )\n",
        "# corpus_sects_df = pd.DataFrame()  # Create empty early as required by some utility functions\n",
        "\n",
        "def plot_crux_sections(model_names_ls, semantic_type='section', subtitle_str='', label_token_ct=0, title_xpos = 0.8, title_ypos=0.2, sec_y_height=0, save2file=False):\n",
        "  '''\n",
        "  Given a Sections DataFrame, model_name and semantic type,\n",
        "  Return a Plot of the Cruxes\n",
        "  '''\n",
        "\n",
        "  crux_points_dt = {}\n",
        "  model_stand_names_ls = []\n",
        "  section_boundries_ls = []\n",
        "\n",
        "\n",
        "  # print(f'Using model_names: {model_names_ls}')\n",
        "\n",
        "  # sns.lineplot(data=ts_df, x='sent_no_mid', y=amodel_stand, markers=['o'], alpha=0.5, label=amodel_stand); # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment (Bing Lexicon)')\n",
        "\n",
        "\n",
        "  # At Section boundries draw blue vertical lines \n",
        "  section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "  for i, sent_no in enumerate(section_boundries_ls):\n",
        "    plt.text(sent_no, sec_y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1);\n",
        "\n",
        "  # At Chapter boundaries draw red vertical lines\n",
        "  chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n",
        "  for i, sent_no in enumerate(chapter_boundries_ls):\n",
        "    plt.axvline(sent_no, color='navy', alpha=0.1);\n",
        "\n",
        "  # Error check and assign DataFrame associated with each semantic_type\n",
        "  if semantic_type == 'section':\n",
        "    # Get midpoints of each Section\n",
        "    ts_df=corpus_sects_df\n",
        "    midpoints_ls = list(corpus_sects_df['sent_no_mid'])\n",
        "  elif semantic_type == 'chapter':\n",
        "    # Get midpoints of each Chapter\n",
        "    ts_df=corpus_chaps_df\n",
        "    midpoints_ls = list(corpus_chaps_df['sent_no_mid'])\n",
        "  else:\n",
        "    print(f\"ERROR: semantic_type={semantic_type} must be either 'section' or 'chapter'\")\n",
        "    return -1\n",
        "\n",
        "  # How many sentiment time series are we plotting?\n",
        "  if len(model_names_ls) == 1:\n",
        "    \n",
        "    # Plotting only one model\n",
        "    model_name_full = str(model_names_ls[0])\n",
        "    model_name_root = model_name_full.split('_')[0]\n",
        "    print(f'model_name_full: {model_name_full} and model_name_root: {model_name_root}')\n",
        "    if model_name_root in MODELS_LS:\n",
        "      # Plot\n",
        "      print(f'about to sns.lineplot model: ') # {ts_df}')\n",
        "      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "      # g._legend.remove()\n",
        "      # print(f'model_name_full={model_name_full}')\n",
        "      # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], markers=\"o\", alpha=0.5, label=model_name_full)\n",
        "    else:\n",
        "      print(f'ERROR: model_names_ls[0]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n",
        "      return -1\n",
        "\n",
        "    # If plotting only one model, add labels\n",
        "    midpoints_sentiment_ls = list(ts_df[model_name_full])\n",
        "    sect_ct = 0\n",
        "    for x,y in zip(midpoints_ls, midpoints_sentiment_ls): \n",
        "      label_token_int = int(label_token_ct)\n",
        "      if label_token_int < 0:\n",
        "        label = ''\n",
        "      elif label_token_int == 0:\n",
        "        # if arg label_token_ct == 0, just print sent_no\n",
        "        label = f\"#{x}({sect_ct})\"\n",
        "      else:\n",
        "        # if arg label_token_ct > 0, print the first label_token_ct words of sentence at crux point\n",
        "        label = f\"#{x}({sect_ct}) {' '.join(corpus_sents_df.iloc[x-1]['sent_raw'].split()[:label_token_int])}\"; # \\nPolarity: {y:.2f}'\n",
        "\n",
        "      # Save Crux point in crux_points_dt Dictionary if plotting Cruxes for a single/specific Model\n",
        "      crux_full_str = ' '.join(corpus_sents_df.iloc[x]['sent_raw'].split())\n",
        "      crux_points_dt[x] = [y, crux_full_str]\n",
        "\n",
        "      plt.annotate(label,\n",
        "                   (x,y),\n",
        "                   textcoords='offset points',\n",
        "                   xytext=(0,10),\n",
        "                   ha='center',\n",
        "                   rotation=90)\n",
        "      sect_ct += 1\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment ({model_name_full.capitalize()})\\n{subtitle_str}', x=title_xpos, y=title_ypos);\n",
        "    # Plot\n",
        "    plt.plot(midpoints_ls, midpoints_sentiment_ls, marker=\"o\", ms=6) # , markevery=[0,1])\n",
        "\n",
        "  else:\n",
        "    # If plotting multiple models\n",
        "    model_names_str = 'Multiple Models'\n",
        "    for i, model_name_full in enumerate(model_names_ls):\n",
        "      # Error check and assign correct model names\n",
        "      model_name_root = model_name_full.split('_')[0]\n",
        "      if model_name_root in MODELS_LS:\n",
        "        # Plot\n",
        "        g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "        # g._legend.remove()\n",
        "        # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], marker=\"o\", alpha=0.5, label=model_name_full)\n",
        "      else:\n",
        "        print(f'ERROR: model_names_ls[]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n",
        "        return -1\n",
        "\n",
        "      # Plot\n",
        "      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "      # g._legend.remove()\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment (Standardized Models)\\n{subtitle_str}', x=title_xpos, y=title_ypos)\n",
        "\n",
        "  # plt.legend(loc='best');\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    models_names_ls = [x[:2] for x in model_names_ls]\n",
        "    models_names_str = ''.join(models_names_ls)\n",
        "    plot_filename = f'plot_cruxes_{semantic_type}_{models_names_str}_{models_names_str}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return crux_points_dt"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUNMIlJKHyz3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a231a199-4def-4570-c326-a7568f096dd0"
      },
      "source": [
        "def plot_histogram(model_name='vader', text_unit='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a model, text_unit\n",
        "  Plot a Histogram using the default DataFrame\n",
        "  '''\n",
        "\n",
        "  if text_unit == 'sentence':\n",
        "    ts_df = corpus_sents_df\n",
        "\n",
        "  elif text_unit == 'paragraph':\n",
        "    ts_df = corpus_parags_df\n",
        "\n",
        "  elif text_unit == 'section':\n",
        "    ts_df = corpus_sects_df\n",
        "\n",
        "  elif text_unit == 'chapter':\n",
        "    ts_df = corpus_chaps_df\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n",
        "\n",
        "  sns.histplot(ts_df[model_name], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram {text_unit.capitalize()} Sentiment (Model {model_name.capitalize()})')\n",
        "  # get_smas(ts_df, model_name=model_name, text_unit=text_unit, win_ls=wins_def_ls)\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_hist_{text_unit}_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGTkfsSWFCeO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "077b4651-3d71-470b-95be-37bafcffcb8f"
      },
      "source": [
        "# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n",
        "\n",
        "def plot_raw_sections(ts_df='corpus_sents_df', model_name='vader', semantic_type='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, model_name column, semantic_type \n",
        "  Plot the raw sentiment types\n",
        "  Options to save2file\n",
        "  ''' \n",
        "  \n",
        "  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n",
        "  sns.lineplot(data=ts_df, x='sect_no', y=model_name, alpha=0.5).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_nostand_sects_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmrtifYoIjOT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f84a1280-7436-4364-de79-9a35924e3002"
      },
      "source": [
        "# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n",
        "\n",
        "def plot_raw_sentiments(model_name='vader', semantic_type='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, model_name column, semantic_type \n",
        "  Plot the raw sentiment types\n",
        "  Options to save2file\n",
        "  ''' \n",
        "  \n",
        "  if semantic_type == 'sentence':\n",
        "    ts_df = corpus_sents_df\n",
        "    x_units = 'sent_no'\n",
        "  elif semantic_type == 'paragraph':\n",
        "    ts_df = corpus_parags_df\n",
        "    x_units = 'parag_no'\n",
        "  elif (semantic_type == 'section') | (semantic_type == 'section_stand'):\n",
        "    ts_df = corpus_sects_df\n",
        "    x_units = 'sect_no'\n",
        "  elif (semantic_type == 'chapter') | (semantic_type == 'chapter_stand'):\n",
        "    ts_df = corpus_chaps_df\n",
        "    x_units = 'chap_no'\n",
        "    \n",
        "  else:\n",
        "    print(f'ERROR: {semantic_type} must be sentence, paragraph or section')\n",
        "\n",
        "\n",
        "  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n",
        "  sns.lineplot(data=ts_df, x=x_units, y=model_name, alpha=0.5, label=model_name).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n",
        "  \n",
        "  plt.legend(loc='best')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_raw_sentiments_{semantic_type}_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB8ibstaeiVd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a4473ab8-6800-4084-f2a0-dfc7011fbec1"
      },
      "source": [
        "# TODO: must plot in order to save, cannot save without first plotting\n",
        "\n",
        "def get_lowess(ts_df='corpus_parags_df', models_ls=MODELS_LS, text_unit='paragraph', plot_subtitle='', alabel='', afrac=1./10, ait=5, alpha=0.5, do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n",
        "  Return a DataFrame with LOWESS values\n",
        "  If 'plot=True', also output plot\n",
        "  '''\n",
        "\n",
        "  # global corpus_all_df\n",
        "\n",
        "  lowess_df = pd.DataFrame()\n",
        "\n",
        "  # Step 1: Calculate LOWESS smoothed values\n",
        "  for i,acol in enumerate(models_ls):\n",
        "    sm_x, sm_y = sm_lowess(endog=ts_df[acol].values, exog=ts_df.index.values, frac=afrac, it=ait, return_sorted = True).T\n",
        "    col_new = f'{acol}_lowess'\n",
        "    lowess_df[col_new] = pd.Series(sm_y)\n",
        "    # Optionally plot LOWESS for all models\n",
        "    if do_plot:\n",
        "      if alabel == '':\n",
        "        alabel == acol\n",
        "      plt.plot(sm_x, sm_y, label=alabel, alpha=alpha, linewidth=2)\n",
        "\n",
        "  lowess_df['median'] = lowess_df.median(axis=1) # sm_y # corpus_all_df[df_cols_ls].median(axis=1)\n",
        "  \n",
        "  # Step 2: Optionally plot LOWESS for median\n",
        "  if do_plot:\n",
        "    # sm_x, sm_y = sm_lowess(endog=lowess_df.median, exog=lowess_df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n",
        "    # plt.plot(sm_x, sm_y, label='median', alpha=0.9, linewidth=2, color='black')\n",
        "    \n",
        "    frac_str = str(round(100*afrac))\n",
        "    plt.title(f'{CORPUS_FULL} \\n {plot_subtitle} {text_unit} Standardized Sentiment Smoothed with LOWESS (frac={frac_str})')\n",
        "    plt.legend(title='Sentiment Model')\n",
        "\n",
        "  # Step 3: Optionally save to file\n",
        "  if save2file:\n",
        "    # Save Plot to file.\n",
        "    plot_filename = f'plot_{text_unit}_lowess_{plot_subtitle.split()[0].lower()}_{author_abbr_str}_{title_str}.png'\n",
        "    # plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plot_filename, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "\n",
        "  return lowess_df\n",
        "\n",
        "\n",
        "# Test\n",
        "'''\n",
        "new_lowess_col = f'{sa_model}_lowess'\n",
        "my_frac = 1./10\n",
        "my_frac_per = round(100*my_frac)\n",
        "new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n",
        "corpus_all_df[new_lowess_col] = plot_lowess(corpus_all_df, [sa_model], afrac=my_frac)\n",
        "corpus_all_df.head()\n",
        "''';"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cANCC2iz6nwo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b5728d3d-97ab-4422-c740-d8fe56bb5560"
      },
      "source": [
        "def get_sent2dets(sent_no):\n",
        "  '''\n",
        "  Given a Sentence Number\n",
        "  Return the corresponding Paragraph, Section and Chapter Numbers that contain it\n",
        "  '''\n",
        "\n",
        "  # Get Paragraph No containing given Sentence No\n",
        "  sent_parag_no = int(corpus_sents_df[corpus_sents_df['sent_no']==sent_no]['parag_no'])\n",
        "\n",
        "  # Get Section No containing given Sentence No.\n",
        "  corpus_sects_ls = list(corpus_sects_df['sect_no'])\n",
        "  for asect_no in corpus_sects_ls:\n",
        "    if (int(corpus_sects_df[corpus_sects_df['sect_no'] == asect_no]['sent_no_start']) > sent_no):\n",
        "      break\n",
        "    sent_sect_no = asect_no\n",
        "    # print(f'asect={asect_no}')\n",
        "\n",
        "  # Get Chapter No containing given Sentence No.\n",
        "  corpus_chaps_ls = list(corpus_chaps_df['chap_no'])\n",
        "  for achap_no in corpus_chaps_ls:\n",
        "    if (int(corpus_chaps_df[corpus_chaps_df['chap_no'] == achap_no]['sent_no_start']) > sent_no):\n",
        "      break\n",
        "    sent_chap_no = achap_no\n",
        "    # print(f'achap={achap_no}')\n",
        "\n",
        "\n",
        "  return sent_parag_no, sent_sect_no, sent_chap_no\n",
        "\n",
        "# Test\n",
        "# sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(1408)\n",
        "# print(f'sent_parag_no={sent_parag_no}\\nsent_sect_no={sent_sect_no}\\nsent_chap_no={sent_chap_no}')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6liffwhYtSw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "272fb336-8273-48d4-b1a1-20a1d6579c29"
      },
      "source": [
        "# get_sentnocontext_report(the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sijR4OknJive",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c50c0165-3c96-4a38-fec8-58e9c309f2f7"
      },
      "source": [
        "def get_sentnocontext(timeser_df, sent_no=1, n_sideparags=1, sent_highlight=True):\n",
        "  '''\n",
        "  Given a sentence number in the Corpus\n",
        "  Return the containing paragraph and n-paragraphs on either side\n",
        "  (e.g. if n=2, return 2+1+2=5 paragraphs)\n",
        "  '''\n",
        "\n",
        "  # print(f'just entered get_sentnocontext withsent_no: {sent_no} and  timeser_df:\\n\\n    {timeser_df}')\n",
        "  timeser_indx = timeser_df['sent_no'] == sent_no\n",
        "  parag_target_no = int(timeser_df[timeser_df['sent_no'] == sent_no]['parag_no'])\n",
        "  # print(f'parag_target_no = {parag_target_no} and type: {type(parag_target_no)}')\n",
        "\n",
        "  if n_sideparags == 0:\n",
        "    parags_context_ls = list(corpus_parags_df[corpus_parags_df['parag_no'] == parag_target_no]['parag_raw'])\n",
        "\n",
        "  else:\n",
        "    parag_start = parag_target_no - n_sideparags\n",
        "    parag_end = parag_target_no + n_sideparags + 1\n",
        "    parags_context_ls = list(corpus_parags_df.iloc[parag_start:parag_end]['parag_raw'])\n",
        "\n",
        "\n",
        "  if sent_highlight == True:\n",
        "    parag_match_str = str(parags_context_ls[n_sideparags])\n",
        "    # print(f'parag_match_str:\\n  {parag_match_str}') parag_no\n",
        "    sent_idx = sent_no\n",
        "    sent_str = (timeser_df[timeser_df['sent_no']==sent_idx]['sent_raw'].values)[0]\n",
        "    sent_str_up = sent_str.upper()\n",
        "    # print(f'sent_str:\\n  {sent_str}')\n",
        "    # parags_context_ls[n_sideparags] \n",
        "    parags_context_ls[n_sideparags] = parag_match_str.replace(sent_str, sent_str_up)\n",
        "\n",
        "  return parags_context_ls\n",
        "\n",
        "# Te\n",
        "# context_highlighted = get_sentnoparags(sent_no=1051, n_sideparags=1)\n",
        "# print(context_highlighted)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM_I8uDfJztH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6edf68df-9fa4-4066-a9f3-171b27b92c75"
      },
      "source": [
        "def get_sentnocontext_report(ts_df = corpus_sents_df, the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n",
        "  '''\n",
        "  Wrapper function around  get_sentnocontext()   Paragraph(s) Context\n",
        "  Prints a nicely formatted context report\n",
        "  '''\n",
        "\n",
        "  context_noparags = the_n_sideparags*2+1\n",
        "  # tsdf\n",
        "  # print('-------------------------------------------------------------')\n",
        "  print(f'The {context_noparags} Paragraph(s) Context around the Sentence #{the_sent_no} Crux Point:')\n",
        "  # print(f'ts_df = {ts_df}')\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f\"\\nCrux Sentence #{the_sent_no} Raw Text: -------------------------------\\n\\n    {str(ts_df[ts_df['sent_no'] == the_sent_no]['sent_raw'].values[0])}\\n\") # iloc[the_sent_no]['sent_raw']}\")\n",
        "\n",
        "  sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(the_sent_no)\n",
        "  print(f\"\\nCrux Sentence #{the_sent_no} is Contained in: ---------------------------\\n\\n    Paragraph #{sent_parag_no}\\n      Section #{sent_sect_no}\\n      Chapter #{sent_chap_no}\\n\")\n",
        "\n",
        "  print(f\"\\n{context_noparags} Paragraph(s) Context: ------------------------------\")\n",
        "  # print('calling get_sentnocontext')\n",
        "  context_parags_ls = get_sentnocontext(timeser_df = ts_df, sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_len = len(context_parags_ls)\n",
        "  context_mid = context_len//2\n",
        "  for i, aparag in enumerate(context_parags_ls):\n",
        "    if i==context_mid:\n",
        "      # print(f'\\n>>> Paragraph #{i}: <<< Crux Point Sentence CAPITALIZED within this Paragraph\\n\\n    {aparag}') \n",
        "      print(f'\\n<*> {aparag}')\n",
        "    else:\n",
        "      # print(f'\\n    Paragraph #{i}:\\n\\n    {aparag}')\n",
        "      print(f'\\n    {aparag}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# get_sentnocontext_report(sent_no=1051, n_sideparags=1, sent_highlight=True)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y04GiohGypNX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "68e61f75-6554-4333-c2a3-c8acea189270"
      },
      "source": [
        "def get_section_timeseries(sect_no):\n",
        "  '''\n",
        "  Given a Section No in the current Corpus\n",
        "  Return the start,mid and ending Sent No for this Section as well as the Sentiment Time Series between the start/end Sentence for this Section\n",
        "  '''\n",
        "  \n",
        "  section_count = corpus_sects_df.shape[0]\n",
        "\n",
        "  # Compute the start, mid and end Sentence numbers for the selected Section\n",
        "  if Select_Section_No >= section_count:\n",
        "    print(f'ERROR: You picked Section #{Select_Section_No}.\\n  Section for this Corpus must be between 0 and {section_count-1}')\n",
        "    return -1\n",
        "\n",
        "  else:\n",
        "\n",
        "    # Get the starting and middle Sentence No of this Section\n",
        "    sect_sent_start = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_start'].values)\n",
        "    # sect_sent_mid = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_mid'].values)\n",
        "\n",
        "    # Calculate last Sentence No of this Section\n",
        "    if Select_Section_No == (section_count-1):   \n",
        "      print(f'You selected the last Section of this Corpus')\n",
        "      sect_sent_end = corpus_sents_df.shape[0] - 1\n",
        "    else:\n",
        "      sect_sent_end = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No+1]['sent_no_start'].values) # - 1\n",
        "      \n",
        "    print(f'Section #{sect_no}:----------')\n",
        "    print(f'\\nsect_sent_start: {sect_sent_start}')\n",
        "    # print(f'sect_sent_mid: {sect_sent_mid}')\n",
        "    print(f'sect_sent_end: {sect_sent_end}')\n",
        "\n",
        "\n",
        "  # Comput the start, and end Paragraph numbers for the selected Section\n",
        "  sect_parag_start = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_start]['parag_no'].values)\n",
        "  sect_parag_end = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_end]['parag_no'].values)\n",
        "\n",
        "  print(f'\\nsect_parag_start: {sect_parag_start}')\n",
        "  print(f'sect_parag_end: {sect_parag_end}')\n",
        "\n",
        "\n",
        "  # Extract and Return both a Sentence and Paragraph DataFrame for this Section \n",
        "\n",
        "  section_sents_df = corpus_sents_df.iloc[sect_sent_start:sect_sent_end]\n",
        "\n",
        "  section_parags_df = corpus_parags_df.iloc[sect_parag_start:sect_parag_end]\n",
        "\n",
        "\n",
        "  return section_sents_df, section_parags_df\n",
        "\n",
        "# Test\n",
        "\n",
        "# section_sents_df, section_parags_df = get_section_timeseries(Select_Section_No)\n",
        "\n",
        "# section_sents_df.head()\n",
        "\n",
        "# print(f'\\nsection_sents_df.shape: {section_sents_df.shape}')\n",
        "# print(f'section_parags_df.shape: {section_parags_df.shape}')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgFMgdQ3X33F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b9b42db7-8841-45c7-e158-76c4a0b9596e"
      },
      "source": [
        "def get_lowess_cruxes(ts_df, col_series, text_type='sentence', win_lowess=5, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "\n",
        "  sent_no_min = ts_df.sent_no.min()\n",
        "  sent_no_max = ts_df.sent_no.max()\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values\n",
        "\n",
        "  half_win = int((win_lowess/100)*series_len)\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    if text_type == 'sentence':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(sent_no, sec_y_height, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "    elif text_type == 'paragraph':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(aparag_no, sec_y_height, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(aparag_no, color='blue', alpha=0.1)    \n",
        "    else:\n",
        "      print(f\"ERROR: text_type is {text_type} but must be either 'sentence' or 'paragarph'\")\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} Raw Sentence Crux Detection in Section #{Select_Section_No}\\nLOWESS Smoothed {subtitle_str} and SciPy find_peaks')\n",
        "    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig('argrelextrema.png')\n",
        "\n",
        "  return crux_coord_ls"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv376c5_bfrg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3aaf613c-77b9-42f3-ffe0-621fc1ff5b8a"
      },
      "source": [
        "def crux_sortsents(crux_ls, corpus_df=corpus_sents_df, atop_n=3, get_peaks=True, sort_key='sent_no'):\n",
        "  '''\n",
        "  Given a list of tuples (sent_no, sentiment value), atop_n cruxes to retrieve and bool flag get_peaks\n",
        "  Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n",
        "  '''\n",
        "  # print(f'Entered crux_sortsents with crux_ls={crux_ls}\\natop_n={atop_n}')\n",
        "\n",
        "  crux_old_ls = []\n",
        "  crux_new_ls = []\n",
        "\n",
        "  # TODO: Error check for null/invalid corpus_df/crus_ls sent_no\n",
        "\n",
        "  if sort_key == 'sent_no':\n",
        "    crux_old_ls = sorted(crux_ls, key=lambda tup: (tup[0]))\n",
        "  else:\n",
        "    crux_old_ls = sorted(crux_ls, key=lambda tup: (tup[1]), reverse=get_peaks)\n",
        "\n",
        "  \"\"\"\n",
        "  if get_peaks == True:\n",
        "    crux_old_ls = [x for x in crux_old_ls if x[1] > 0]\n",
        "  else:\n",
        "    crux_old_ls = [x for x in crux_old_ls if x[1] < 0]\n",
        "  \"\"\";\n",
        "\n",
        "  # Return only the n_top cruxes if more cruxes than n_top else return all cruxes\n",
        "  if (sort_key != 'sent_no') & (len(crux_old_ls) >= atop_n):\n",
        "    # trim crux list if user asked for less than total number\n",
        "    crux_old_ls = crux_old_ls[:atop_n]\n",
        "\n",
        "  # Retrieve the Sentence raw text for each Crux and add as Tuple(sent_no, sentiment_val, raw_text) to return List\n",
        "  for asent_no, asentiment_val in crux_old_ls:\n",
        "    # print(f'  Retrieving Sentence #{asent_no} with Sentiment Value {asentiment_val} from DataFrame {corpus_df}')\n",
        "    asent_int = int(asent_no)\n",
        "    # print(f\"                      asent_int is type: {type(asent_int)} and Sentence Text:\\n\\n     {corpus_df.iloc[asent_int]['sent_raw']}\")\n",
        "\n",
        "    asent_raw = str(corpus_df[corpus_df['sent_no'] == asent_int]['sent_raw'].values[0])\n",
        "    crux_new_ls.append((int(asent_no), float(f'{asentiment_val:.3f}'), str(asent_raw),)) # Append a Tuple to return List\n",
        "\n",
        "  return crux_new_ls\n",
        "\n",
        "# Test\n",
        "# crux_n_top_ls = crux_sortsents(section_crux_ls, atop_n=3, get_peaks=True)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjb60CVnz5SF"
      },
      "source": [
        "## **crux_sortsents_report**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFDjK1o8gnQ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8316106e-1434-477d-841c-8d198a8dcd02"
      },
      "source": [
        "def crux_sortsents_report(crux_ls, ts_df=corpus_sents_df, library_type='baseline', top_n=3, get_peaks=True, sort_by='sent_no', n_sideparags=1, sentence_highlight=True):\n",
        "  '''\n",
        "  Wrapper function to produce report based upon 'crux_sortsents() described as:\n",
        "    Given a list of tuples (sent_no, sentiment value), top_n cruxes to retrieve and bool flag get_peaks\n",
        "    Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n",
        "\n",
        "    # get_sentnocontext_report\n",
        "  '''\n",
        "\n",
        "  if get_peaks == True:\n",
        "    crux_label = 'Peak'\n",
        "  else:\n",
        "    crux_label = 'Valley'\n",
        "\n",
        "  # Filter and keep only the desired crux type in List crux_subset_ls\n",
        "  crux_subset_ls = []\n",
        "  for acrux_tup in crux_ls:\n",
        "    crux_type, crux_x_coord, crux_y_coord = acrux_tup\n",
        "    if crux_type.lower() == crux_label.lower():\n",
        "      crux_subset_ls.append((crux_x_coord,crux_y_coord)) # Append a Tuple to List\n",
        "\n",
        "  flag_2few_cruxes = False\n",
        "\n",
        "  # Check to see if asked for more Cruxes than were found \n",
        "  top_n_found = len(crux_subset_ls)\n",
        "  if top_n_found < top_n:\n",
        "    flag_2few_cruxes = True\n",
        "    print(f'\\n\\nWARNING: You asked for {top_n} {crux_label}s\\n         but there only {top_n_found} were found above.\\n')\n",
        "    print(f'             Displaying as many {crux_label}s as possible,')\n",
        "    print(f'             to retrieve more, go back to the previous code cells and re-run with wider Crux Window.\\n\\n')\n",
        "\n",
        "\n",
        "  # Get Sentence no and raw text for appropriate Crux subset\n",
        "  # print(f'Calling crux_n_top_ls with crux_subset_ls={crux_subset_ls}\\ntop_n={top_n}\\nget_peaks={get_peaks}')\n",
        "  crux_n_top_ls = crux_sortsents(corpus_df = ts_df, crux_ls=crux_subset_ls, atop_n=top_n, get_peaks=get_peaks, sort_key=sort_by)\n",
        "  # print(f'Returning crux_n_top_ls = {crux_n_top_ls}')\n",
        "\n",
        "  # Print appropriate header Select_Section_No sent_no\n",
        "  print('------------------------------')\n",
        "  # print(f'library_type: {library_type}')\n",
        "  if library_type in ['baseline','sentimentr','syuzhetr','transformer']:\n",
        "    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n",
        "      print(f'Library: {library_type.capitalize()} ALL Top {top_n} {crux_label}s Found\\n')\n",
        "    else:\n",
        "      print(f'Library #{library_type.capitalize()} ONLY Top {top_n_found} {crux_label}s Found\\n')\n",
        "  else:\n",
        "    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n",
        "      print(f'Section #{Select_Section_No} ALL Top {top_n} {crux_label}s Found\\n')\n",
        "    else:\n",
        "      print(f'Section #{Select_Section_No} ONLY Top {top_n_found} {crux_label}s Found\\n')\n",
        "\n",
        "  # Print summary of subset Cruxes\n",
        "  for i,crux_sent_tup in enumerate(crux_n_top_ls):\n",
        "    # crux_type, crux_x_coord, crux_y_coord = crux_sent_tup\n",
        "    crux_x_coord, crux_y_coord, crux_sent_raw = crux_sent_tup\n",
        "    print(f'   {crux_label} #{i} at Sentence #{crux_x_coord} with Sentiment Value {crux_y_coord}')\n",
        "  # print('------------------------------\\n')\n",
        "  # print('Sent_No  Sentiment   Sentence (Raw Text)\\n')\n",
        "  \n",
        "  # Print details of each Crux in subset\n",
        "  for sent_no, sent_pol, sent_raw in crux_n_top_ls: \n",
        "    sent_no = int(sent_no)\n",
        "    print('\\n\\n-------------------------------------------------------------')\n",
        "    print(f'Sentence #{sent_no}   Sentiment: {sent_pol:.3f}\\n') #     {sent_raw}\\n')\n",
        "    # print('------------------------------')\n",
        "    get_sentnocontext_report(ts_df=ts_df, the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n",
        "    # get_sentnocontext(sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJw2WDlwHH5y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3a3283bd-3dcd-4bda-8b42-2b31b469691a"
      },
      "source": [
        "# For the selected Section, create an expanded Paragraph DataFrame to match the number of Sentences in the Section\n",
        "\n",
        "def expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df', model_name='vader_lnorm_medianiqr'):\n",
        "  '''\n",
        "  Given a Corpus Paragraph DataFrame and a longer Sentence DataFrame that cover the same Section of a Corpus\n",
        "  Return an expanded version of the Paragraph DataFrame of equal length to the Sentence DataFrame so they can be plotted/compared along the same x-axis\n",
        "  '''\n",
        "\n",
        "  parag_sentiment_expanded_ls = []\n",
        "  parags_midpoint_ls = []\n",
        "  sent_sum = 0\n",
        "  parag_start = section_parags_df.parag_no.min()\n",
        "  print(f'parag_start: {parag_start}')\n",
        "  parag_end = section_parags_df.parag_no.max() + 1 # shape[0] + 3\n",
        "  print(f'parag_end: {parag_end}')\n",
        "  parags_range_ls = list(range(parag_start, parag_end, 1))\n",
        "  print(f'parags_range_ls: {parags_range_ls}')\n",
        "  for i, aparag_no in enumerate(parags_range_ls):\n",
        "    aparag_sentiment_fl = float(corpus_parags_df[corpus_parags_df['parag_no']==aparag_no][model_name])\n",
        "    sent_ct = len(corpus_sents_df[corpus_sents_df.parag_no == aparag_no])\n",
        "    parag_midpoint_int = int(sent_ct//2 + sent_sum)\n",
        "    parags_midpoint_ls.append(parag_midpoint_int)\n",
        "    for asent in range(sent_ct):\n",
        "      parag_sentiment_expanded_ls.append(aparag_sentiment_fl)\n",
        "    sent_sum += sent_ct\n",
        "    print(f'#{i}: Paragraph #{aparag_no} has {sent_ct} Sentences and Avg Sentiment: {aparag_sentiment_fl:.3f}')\n",
        "\n",
        "  print(f'\\nSentence Total: {sent_sum} vs Original section_sents_df: {section_sents_df.shape[0]}')\n",
        "  print(f'  Paragraph Sentiment length: {len(parag_sentiment_expanded_ls)}')\n",
        "\n",
        "  # section_sents_parags_df = section_sents_df.copy()\n",
        "  \n",
        "  # section_sents_parags_df.head(1);\n",
        "\n",
        "  # corpus_sents_df['']\n",
        "\n",
        "  return parag_sentiment_expanded_ls, parags_midpoint_ls\n",
        "\n",
        "# Test\n",
        "# section_sents_df['vader_lnorm_medianiqr_parag'] = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df')\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXRq54NQwI7D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "57df660c-8206-4e2d-fedf-287ef5ea775d"
      },
      "source": [
        "def get_crux_points(ts_df, col_series, text_type='sentence', win_per=5, sec_y_labels=True, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  # print('entered get_crux_points')\n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "  # print(f'series_len = {series_len}')\n",
        "\n",
        "  sent_no_min = ts_df.sent_no.min()\n",
        "  sent_no_max = ts_df.sent_no.max()\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values.flatten()\n",
        "\n",
        "  half_win = int((win_per/100)*series_len)\n",
        "  # print(f'half_win = {half_win}')\n",
        "  # print(f'sm_y type = {type(sm_y)}')\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  # print(f'peak_indexes type = {type(peak_indexes)}') # sent_no_start sent\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_x_adj_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  peak_label_ls = ['peak'] * len(peak_y_ls)\n",
        "  peak_coord_ls = tuple(zip(peak_label_ls, peak_x_adj_ls, peak_y_ls))\n",
        "\n",
        "  # peak_y_all_ls = peak_y_ls + valley_y_ls\n",
        "  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_x_adj_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  valley_label_ls = ['valley'] * len(valley_y_ls)\n",
        "  valley_coord_ls = tuple(zip(valley_label_ls, valley_x_adj_ls, valley_y_ls))\n",
        "\n",
        "  # Combine Peaks and Valley Coordinates into List of Tuples(label, x_coord, y_coord)\n",
        "  crux_coord_ls = peak_coord_ls + valley_coord_ls\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  #  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  #  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  #  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    if sec_y_labels == True:\n",
        "      section_sent_no_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "      section_no_ls = list(corpus_sects_df['sect_no'])\n",
        "      for i, asect_no in enumerate(section_sent_no_boundries_ls):\n",
        "        # Plot vertical lines for section boundries\n",
        "        plt.text(asect_no, sec_y_height, f'Section #{section_no_ls[i]}', alpha=0.2, rotation=90)\n",
        "        plt.axvline(asect_no, color='blue', alpha=0.1)    \n",
        "\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, annotation_clip=True) # xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} SMA Smoothed Sentence Sentiment Arcs Crux Detection\\n{subtitle_str} Models: {col_series}')\n",
        "    plt.xlabel(f'Sentence No') # within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n SMA Smoothed Sentence Sentiment Arcs Crux Points')\n",
        "    # plt.legend(loc='best')\n",
        "    plt.savefig(f\"{CORPUS_FILENAME.split('.')[0]}_find_peaks.png\")\n",
        "\n",
        "  return crux_coord_ls;"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNdJQGtghS_X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cb42473e-7a7b-413e-f5cd-f829ba7c6dfd"
      },
      "source": [
        "def get_standardscaler(series_name, values_ser):\n",
        "  '''\n",
        "  Given a Series of values\n",
        "  Return a list of StandardSclar transformations on that input Series\n",
        "  '''\n",
        "\n",
        "  scaler = StandardScaler()  \n",
        "\n",
        "  # Convert to np.array\n",
        "  values_np = np.array(values_ser)\n",
        "  \n",
        "  values_flat_np = values_np.reshape((len(values_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(values_flat_np)\n",
        "  print(f'Model: {series_name}\\n       Mean: {scaler.mean_}, StandardDeviation: {np.sqrt(scaler.var_)}') # % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  values_flat_xform_np = scaler.transform(values_flat_np)\n",
        "\n",
        "  return values_flat_xform_np.flatten().tolist()\n",
        "\n",
        "# Test\n",
        "# stdscaler_series_ls = get_standardscaler('vader_lnorm_medianiqr_roll100', corpus_sents_df['vader_lnorm_medianiqr_roll100'])\n",
        "# corpus_sents_df['vader_roll100_stdscaler'] = pd.Series(stdscaler_series_ls)\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABW4oQ4xJT4R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "10dcb4b3-ea09-4d19-8e69-563ecf8342a3"
      },
      "source": [
        "def plot_models(models_subset_ls, models_type='baseline', text_unit='sent_no', win_per=10):\n",
        "  '''\n",
        "  Given a DataFrame and list of correponding Models in that DataFrame\n",
        "  Plot the Sentiment Arcs for the stdscaler with the rolling window \n",
        "  '''\n",
        "\n",
        "\n",
        "  if models_type == 'baseline':\n",
        "    models_ls = models_baseline_ls\n",
        "  elif models_type == 'sentimentr':\n",
        "    models_ls = models_sentimentr_ls\n",
        "  elif models_type == 'syuzhetr':\n",
        "    models_ls = models_syuzhetr_ls\n",
        "  elif models_type == 'transformer':\n",
        "    models_ls = models_transformer_ls\n",
        "  else:\n",
        "    print(f'ERROR: model_type={model_type} must be one of: baseline, sentimentr, syuzhetr or transformer')\n",
        "\n",
        "\n",
        "  if (models_type == 'baseline') | (models_type == 'transformers'):\n",
        "    # Have Corpus data in 4 DataFrames: corpus_[sents|parags|sects|chaps]_df\n",
        "    if text_unit == 'sent_no':\n",
        "      ts_df = corpus_sents_df\n",
        "      text_raw = 'sent_raw'\n",
        "      text_type = 'Sentence'\n",
        "      win_roll = int(corpus_sents_df.shape[0]* win_per/100)\n",
        "    elif text_unit == 'parag_no':\n",
        "      ts_df = corpus_parags_df\n",
        "      text_raw = 'parag_raw'\n",
        "      text_type = 'Paragraph'\n",
        "      win_roll = int(corpus_parags_df.shape[0]* win_per/100)\n",
        "    elif text_unit == 'sect_no':\n",
        "      ts_df = corpus_sects_df\n",
        "      text_raw = 'sect_raw'\n",
        "      text_type = 'Section'\n",
        "      win_roll = int(corpus_sects_df.shape[0]* win_per/100)\n",
        "    elif text_unit == 'chap_no':\n",
        "      ts_df = corpus_chaps_df\n",
        "      text_raw = 'chap_raw'\n",
        "      text_type = 'Chapter'\n",
        "      win_roll = int(corpus_chaps_df.shape[0]* win_per/100)\n",
        "    else:\n",
        "      print(f'ERROR: text_unit={text_unit} must be one of: sent_no, parag_no, sect_no or chap_no')\n",
        "\n",
        "  elif (models_type == 'sentimentr') | (models_type == 'syuzhetr'):\n",
        "\n",
        "    # Only have Corpus Sentence data in one DataFrame: corpus_sentimentr_df or corpus_syuzhetr_df\n",
        "    text_raw = 'sent_raw'\n",
        "    text_type = 'Sentence'\n",
        "\n",
        "    if models_type == 'sentimentr':\n",
        "      ts_df = corpus_sentimentr_df\n",
        "      win_roll = int(corpus_sentimentr_df.shape[0]*win_per/100)\n",
        "    else:\n",
        "      ts_df = corpus_syuzhetr_df\n",
        "      win_roll = int(corpus_syuzhetr_df.shape[0]*win_per/100)\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: model_type = {model_type} but must be one of 4: [baseline|transformer|sentimentr|syuzhet]')\n",
        "\n",
        "\n",
        "  # Get Rolling Window String\n",
        "  if len(str(win_per)) == 1:\n",
        "    roll_str = 'roll' + '0' + str(win_per)\n",
        "  else:\n",
        "    roll_str = 'roll' + str(win_per%100)\n",
        "\n",
        "\n",
        "  # Translate base model name into _stdscaler_rollxxx derivative\n",
        "  all_stdscaler_roll_ls = []\n",
        "  subset_stdscaler_roll_ls = []\n",
        "  for amodel in models_ls:\n",
        "    # Create a Rolling SMA Series from the stdscaler version of model sentiment values\n",
        "    amodel_stdscaler = f'{amodel}_stdscaler'\n",
        "    col_stdscaler_rollwin = f'{amodel}_stdscaler_{roll_str}'\n",
        "    ts_df[col_stdscaler_rollwin] = ts_df[amodel_stdscaler].rolling(win_roll, center=True).mean()\n",
        "    all_stdscaler_roll_ls.append(col_stdscaler_rollwin)\n",
        "    if amodel in models_subset_ls:\n",
        "      subset_stdscaler_roll_ls.append(col_stdscaler_rollwin)\n",
        "    # col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "\n",
        "  # Compute the Mean of All\n",
        "  mean_all_col = 'mean_stdscaler_' + roll_str\n",
        "  col_stdscaler_roll_ls = [f'{x}_stdscaler_{roll_str}' for x in models_ls] #  if ('mean' not in x)]\n",
        "  # print(f'Computing Mean based upon:\\n    {col_stdscaler_roll_ls}')\n",
        "  ts_df[mean_all_col] = ts_df[all_stdscaler_roll_ls].mean(axis=1)\n",
        "\n",
        "\n",
        "  palette = cycle(px.colors.qualitative.Safe)\n",
        "  # palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "  my_layout = go.Layout(\n",
        "      autosize=False,\n",
        "      width=1600,\n",
        "      height=800,\n",
        "      margin=go.layout.Margin(\n",
        "          l=10,\n",
        "          r=50,\n",
        "          b=100,\n",
        "          t=100,\n",
        "          pad = 1\n",
        "      )\n",
        "  )\n",
        "\n",
        "\n",
        "  fig = go.Figure(layout=my_layout)\n",
        "\n",
        "  # add traces\n",
        "  for i,amodel_stdscaler_roll in enumerate(subset_stdscaler_roll_ls):\n",
        "    # print(f'adding trace: {amodel_stdscaler_roll}')\n",
        "    fig.add_traces(go.Line(x = ts_df[text_unit],\n",
        "                          y = ts_df[amodel_stdscaler_roll],\n",
        "                          text = ts_df[text_raw],\n",
        "                          name = amodel_stdscaler_roll,\n",
        "                          hovertemplate = \"Model: <b>\"+amodel_stdscaler_roll+\"</b><br>\"+text_type+\" #<b>%{x}</b><br>Polarity <b>%{y}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                          marker_color=next(palette)))\n",
        "  \"\"\"\n",
        "  if Mean_Subset_Arc == True:\n",
        "    mean_subset_col = 'mean_subset_'+roll_str\n",
        "    corpus_sents_df[mean_subset_col] = corpus_sents_df[model_baseline_subset_ls].mean(axis=1)\n",
        "    fig.add_traces(go.Line(x=corpus_sents_df['sent_no'],\n",
        "                          y = corpus_sents_df[mean_subset_col],\n",
        "                          line=dict(\n",
        "                                # color='#000000',\n",
        "                                width=5\n",
        "                                ),\n",
        "                          text = 'NA', # corpus_sents_df['sent_raw'],\n",
        "                          name = 'Mean of Selected Models',\n",
        "                          hovertemplate = \"Model <b>%{mean_subset_col}</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                          marker_color=next(palette)))\n",
        "\n",
        "  \"\"\";\n",
        "\n",
        "  if Mean_All_Arc == True:\n",
        "    # mean_all_col = 'mean_all_stdscaler_'+roll_str\n",
        "    # ts_df[mean_all_col] = ts_df[col_stdscaler_rollwin_ls].mean(axis=1)\n",
        "    fig.add_traces(go.Line(x=ts_df[text_unit],\n",
        "                          y = ts_df[mean_all_col],\n",
        "                          line=dict(\n",
        "                                color='#000000',\n",
        "                                width=5,\n",
        "                                dash='dot',\n",
        "                                ),\n",
        "                          text = 'NA', # ts_df['sent_raw'],\n",
        "                          name = 'Mean of All Models',\n",
        "                          hovertemplate = \"Model <b>%{mean_all_col}</b><br>Paragraph #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                          marker_color=next(palette)))\n",
        "\n",
        "\n",
        "  fig.update_layout(\n",
        "      title=f\"{CORPUS_FULL}\\n{text_type} Baseline Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "      xaxis_title=text_type + \" Number\",\n",
        "      yaxis_title=\"StdScaler Sentiment Value\",\n",
        "      hoverlabel=dict(\n",
        "          bgcolor=\"white\",\n",
        "          font_size=16,\n",
        "          font_family=\"Rockwell\"\n",
        "      ),\n",
        "      font=dict(\n",
        "          family=\"Courier New, monospace\",\n",
        "          size=18,\n",
        "          color=\"RebeccaPurple\"\n",
        "      )\n",
        "  )\n",
        "\n",
        "  fig.show();\n",
        "\n",
        "# Test\n",
        "\n",
        "# plot_models(models_subset_ls = ['vader','stanza'], models_type='baseline', text_unit='sent_no', win_per=10)\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4ehet9JjOsK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "98861329-3326-42ff-fc7e-fe0a46a7c0d5"
      },
      "source": [
        "def standardize_ts_ls(ts_df, col_ls):\n",
        "  '''\n",
        "  Given a DataFrame and list of Columns in that DataFrame\n",
        "  Create 4 new Standardized Columns for each given Columns\n",
        "  '''\n",
        "\n",
        "  # Create 4 new column names for each column provided\n",
        "  for amodel in col_ls:\n",
        "    # col_meanstd = f'{amodel}_meanstd'\n",
        "    col_medianiqr = f'{amodel}_medianiqr'\n",
        "    col_stdscaler = f'{amodel}_stdscaler'\n",
        "    col_lnorm_meanstd = f'{amodel}_lnorm_meanstd'\n",
        "    col_lnorm_medianiqr = f'{amodel}_lnorm_medianiqr'\n",
        "\n",
        "    # Standardize each column provided using Standard Scaler and  MedianIQRScaling\n",
        "    ts_df[col_stdscaler]  = list2stdscaler(ts_df[amodel])\n",
        "    ts_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(ts_df[amodel]).reshape(-1, 1))\n",
        "    # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "    text_len_ls = list(ts_df['token_len'])\n",
        "    text_sentiment_ls = list(ts_df[amodel])\n",
        "    text_sentiment_norm_ls = [text_sentiment_ls[i]/text_len_ls[i] for i in range(len(text_len_ls))]\n",
        "    # RobustStandardize Sentence sentiment values\n",
        "    ts_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n",
        "    ts_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  return\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daEzvNu6huM-"
      },
      "source": [
        "# **Either (a) Load Precomputed DataFrames or (b) Create Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnWJCkqDhA5L"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "len(sections_ls)\n",
        "min(sections_ls, key=len) \n",
        "\n",
        "# TODO: Spell check and correct common OCR errors\n",
        "\n",
        "# SymSpellPy\n",
        "# JamSpell\n",
        "# OCR - https://github.com/Alvant/MIL-OCR\n",
        "\n",
        "# !pip install -U symspellpy\n",
        "\n",
        "# Did not need these\n",
        "# dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# term_index is the column of the term and count_index is the\n",
        "# column of the term frequency\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# lookup suggestions for single-word input strings\n",
        "input_term = \"memebers\"  # misspelling of \"members\"\n",
        "input_term = \"summermorning\"\n",
        "# max edit distance per lookup\n",
        "# (max_edit_distance_lookup <= max_dictionary_edit_distance)\n",
        "suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST,\n",
        "                               max_edit_distance=2)\n",
        "# display suggestion term, term frequency, and edit distance\n",
        "for suggestion in suggestions:\n",
        "    print(suggestion)\n",
        "\n",
        "\n",
        "\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell\n",
        "\n",
        "# Set max_dictionary_edit_distance to avoid spelling correction\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# term_index is the column of the term and count_index is the\n",
        "# column of the term frequency\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# a sentence without any spaces\n",
        "input_term = \"thequickbrownfoxjumpsoverthelazydog\"\n",
        "input_term = \"summermorning\"\n",
        "result = sym_spell.word_segmentation(input_term)\n",
        "print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n",
        "                          result.log_prob_sum))\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUz1ieOU1sAF"
      },
      "source": [
        "!ls -altr *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI5Tg1bFiERD"
      },
      "source": [
        "### **(a) Load Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxCBdYNTiDrC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "38858d8d-3b4d-4a0c-b443-0a80a7aed183"
      },
      "source": [
        "# Read Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "# title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "title_str = ''.join(CORPUS_FILENAME.split('.')[0]).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "# Sentence DataFrame\n",
        "corpus_sents_filename = f'corpus_sents_{title_str}.csv'\n",
        "print(f'Reading from file: {corpus_sents_filename}')\n",
        "corpus_sents_df = pd.read_csv(corpus_sents_filename)\n",
        "\n",
        "# Paragraph DataFrame\n",
        "corpus_parags_filename = f'corpus_parags_{title_str}.csv'\n",
        "print(f'Reading from file: {corpus_parags_filename}')\n",
        "corpus_parags_df = pd.read_csv(corpus_parags_filename)\n",
        "\n",
        "# Section DataFrame\n",
        "corpus_sects_filename = f'corpus_sects_{title_str}.csv'\n",
        "print(f'Reading from file: {corpus_sects_filename}')\n",
        "corpus_sects_df = pd.read_csv(corpus_sects_filename)\n",
        "\n",
        "# Chapter DataFrame\n",
        "corpus_chaps_filename = f'corpus_chaps_{title_str}.csv'\n",
        "print(f'Reading from file: {corpus_chaps_filename}')\n",
        "corpus_chaps_df = pd.read_csv(corpus_chaps_filename)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Reading from file: corpus_sents_vwoolf_tothelighthouse.csv\n",
            "Reading from file: corpus_parags_vwoolf_tothelighthouse.csv\n",
            "Reading from file: corpus_sects_vwoolf_tothelighthouse.csv\n",
            "Reading from file: corpus_chaps_vwoolf_tothelighthouse.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqsUHiF9K_5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "45b7a4ed-8988-4e2f-c7f2-83ffcfe4fbac"
      },
      "source": [
        "# Verify Sentences\n",
        "\n",
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"\\\"Yes, of course, if it's fine tomorrow,\\\" said Mrs. Ramsay.\",\n\"yes of course if it is fine tomorrow said mrs ramsay\",\n{\n            'v': 11,\n            'f': \"11\",\n        },\n{\n            'v': 58,\n            'f': \"58\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"\\\"But you'll have to be up with the lark,\\\" she added.\",\n\"but you will have to be up with the lark she added\",\n{\n            'v': 12,\n            'f': \"12\",\n        },\n{\n            'v': 52,\n            'f': \"52\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"Unnamed: 0\"], [\"number\", \"sent_no\"], [\"number\", \"parag_no\"], [\"number\", \"chap_no\"], [\"number\", \"sect_no\"], [\"string\", \"sent_raw\"], [\"string\", \"sent_clean\"], [\"number\", \"token_len\"], [\"number\", \"char_len\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>sent_no</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>chap_no</th>\n",
              "      <th>sect_no</th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>sent_clean</th>\n",
              "      <th>token_len</th>\n",
              "      <th>char_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\"Yes, of course, if it's fine tomorrow,\" said ...</td>\n",
              "      <td>yes of course if it is fine tomorrow said mrs ...</td>\n",
              "      <td>11</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\"But you'll have to be up with the lark,\" she ...</td>\n",
              "      <td>but you will have to be up with the lark she a...</td>\n",
              "      <td>12</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  sent_no  ...  token_len  char_len\n",
              "0           0        0  ...         11        58\n",
              "1           1        1  ...         12        52\n",
              "\n",
              "[2 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3421 entries, 0 to 3420\n",
            "Data columns (total 9 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  3421 non-null   int64 \n",
            " 1   sent_no     3421 non-null   int64 \n",
            " 2   parag_no    3421 non-null   int64 \n",
            " 3   chap_no     3421 non-null   int64 \n",
            " 4   sect_no     3421 non-null   int64 \n",
            " 5   sent_raw    3421 non-null   object\n",
            " 6   sent_clean  3421 non-null   object\n",
            " 7   token_len   3421 non-null   int64 \n",
            " 8   char_len    3421 non-null   int64 \n",
            "dtypes: int64(7), object(2)\n",
            "memory usage: 240.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3fD5CpfKJnw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "2db87b16-10a7-4d20-94fb-22566e13edee"
      },
      "source": [
        "# corpus_sents_df = corpus_sents_df.loc[:, ~corpus_sents_df.columns.str.contains('^Unnamed')]\n",
        "corpus_sents_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "corpus_sents_df.head(2)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"\\\"Yes, of course, if it's fine tomorrow,\\\" said Mrs. Ramsay.\",\n\"yes of course if it is fine tomorrow said mrs ramsay\",\n{\n            'v': 11,\n            'f': \"11\",\n        },\n{\n            'v': 58,\n            'f': \"58\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"\\\"But you'll have to be up with the lark,\\\" she added.\",\n\"but you will have to be up with the lark she added\",\n{\n            'v': 12,\n            'f': \"12\",\n        },\n{\n            'v': 52,\n            'f': \"52\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"number\", \"parag_no\"], [\"number\", \"chap_no\"], [\"number\", \"sect_no\"], [\"string\", \"sent_raw\"], [\"string\", \"sent_clean\"], [\"number\", \"token_len\"], [\"number\", \"char_len\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>chap_no</th>\n",
              "      <th>sect_no</th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>sent_clean</th>\n",
              "      <th>token_len</th>\n",
              "      <th>char_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\"Yes, of course, if it's fine tomorrow,\" said ...</td>\n",
              "      <td>yes of course if it is fine tomorrow said mrs ...</td>\n",
              "      <td>11</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\"But you'll have to be up with the lark,\" she ...</td>\n",
              "      <td>but you will have to be up with the lark she a...</td>\n",
              "      <td>12</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sent_no  parag_no  ...  token_len  char_len\n",
              "0        0         0  ...         11        58\n",
              "1        1         0  ...         12        52\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl3hwS0wLEeR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "2817614a-b84c-4a87-fdbb-b2b4a1d3ca23"
      },
      "source": [
        "# Verify Paragraphs\n",
        "\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n\"\\\"Yes, of course, if it's fine tomorrow,\\\" said Mrs. Ramsay. \\\"But you'll have to be up with the lark,\\\" she added.\",\n\"yes of course if it is fine tomorrow said mrs ramsay but you will have to be up with the lark she added\",\n{\n            'v': 23,\n            'f': \"23\",\n        },\n{\n            'v': 111,\n            'f': \"111\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n\"To her son these words conveyed an extraordinary joy, as if it were settled, the expedition were bound to take place, and the wonder to which he had looked forward, for years and years it seemed, was, after a night's darkness and a day's sail, within touch. Since he belonged, even at the age of six, to that great clan which cannot keep this feeling separate from that, but must let future prospects, with their joys and sorrows, cloud what is actually at hand, since to such people even in earliest childhood any turn in the wheel of sensation has the power to crystallise and transfix the moment upon which its gloom or radiance rests, James Ramsay, sitting on the floor cutting out pictures from the illustrated catalogue of the Army and Navy stores, endowed the picture of a refrigerator, as his mother spoke, with heavenly bliss. It was fringed with joy. The wheelbarrow, the lawnmower, the sound of poplar trees, leaves whitening before rain, rooks cawing, brooms knocking, dresses rustling - \\u201dall these were so coloured and distinguished in his mind that he had already his private code, his secret language, though he appeared the image of stark and uncompromising severity, with his high forehead and his fierce blue eyes, impeccably candid and pure, frowning slightly at the sight of human frailty, so that his mother, watching him guide his scissors neatly round the refrigerator, imagined him all red and ermine on the Bench or directing a stern and momentous enterprise in some crisis of public affairs.\",\n\"to her son these words conveyed an extraordinary joy as if it were settled the expedition were bound to take place and the wonder to which he had looked forward for years and years it seemed was after a night own darkness and a day own sail within touch since he belonged even at the age of six to that great clan which cannot keep this feeling separate from that but must let future prospects with their joys and sorrows cloud what is actually at hand since to such people even in earliest childhood any turn in the wheel of sensation has the power to crystallise and transfix the moment upon which its gloom or radiance rests james ramsay sitting on the floor cutting out pictures from the illustrated catalogue of the army and navy stores endowed the picture of a refrigerator as his mother spoke with heavenly bliss it was fringed with joy the wheelbarrow the lawnmower the sound of poplar trees leaves whitening before rain rooks cawing brooms knocking dresses rustling   \\u201dall these were so coloured and distinguished in his mind that he had already his private code his secret language though he appeared the image of stark and uncompromising severity with his high forehead and his fierce blue eyes impeccably candid and pure frowning slightly at the sight of human frailty so that his mother watching him guide his scissors neatly round the refrigerator imagined him all red and ermine on the bench or directing a stern and momentous enterprise in some crisis of public affairs\",\n{\n            'v': 259,\n            'f': \"259\",\n        },\n{\n            'v': 1517,\n            'f': \"1517\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"Unnamed: 0\"], [\"number\", \"parag_no\"], [\"number\", \"sent_no_start\"], [\"number\", \"sent_no_mid\"], [\"number\", \"sent_no_end\"], [\"string\", \"parag_raw\"], [\"string\", \"parag_clean\"], [\"number\", \"token_len\"], [\"number\", \"char_len\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sent_no_start</th>\n",
              "      <th>sent_no_mid</th>\n",
              "      <th>sent_no_end</th>\n",
              "      <th>parag_raw</th>\n",
              "      <th>parag_clean</th>\n",
              "      <th>token_len</th>\n",
              "      <th>char_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>\"Yes, of course, if it's fine tomorrow,\" said ...</td>\n",
              "      <td>yes of course if it is fine tomorrow said mrs ...</td>\n",
              "      <td>23</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>To her son these words conveyed an extraordina...</td>\n",
              "      <td>to her son these words conveyed an extraordina...</td>\n",
              "      <td>259</td>\n",
              "      <td>1517</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  parag_no  ...  token_len  char_len\n",
              "0           0         0  ...         23       111\n",
              "1           1         1  ...        259      1517\n",
              "\n",
              "[2 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 493 entries, 0 to 492\n",
            "Data columns (total 9 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Unnamed: 0     493 non-null    int64 \n",
            " 1   parag_no       493 non-null    int64 \n",
            " 2   sent_no_start  493 non-null    int64 \n",
            " 3   sent_no_mid    493 non-null    int64 \n",
            " 4   sent_no_end    493 non-null    int64 \n",
            " 5   parag_raw      493 non-null    object\n",
            " 6   parag_clean    493 non-null    object\n",
            " 7   token_len      493 non-null    int64 \n",
            " 8   char_len       493 non-null    int64 \n",
            "dtypes: int64(7), object(2)\n",
            "memory usage: 34.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzP_CrO3J0gd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "2c0bb2eb-961f-4f2f-d6c4-d0c9828f17fe"
      },
      "source": [
        "# corpus_parags_df = corpus_parags_df.loc[:, ~corpus_parags_df.columns.str.contains('^Unnamed')]\n",
        "corpus_parags_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "corpus_parags_df.head(2)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n\"\\\"Yes, of course, if it's fine tomorrow,\\\" said Mrs. Ramsay. \\\"But you'll have to be up with the lark,\\\" she added.\",\n\"yes of course if it is fine tomorrow said mrs ramsay but you will have to be up with the lark she added\",\n{\n            'v': 23,\n            'f': \"23\",\n        },\n{\n            'v': 111,\n            'f': \"111\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n\"To her son these words conveyed an extraordinary joy, as if it were settled, the expedition were bound to take place, and the wonder to which he had looked forward, for years and years it seemed, was, after a night's darkness and a day's sail, within touch. Since he belonged, even at the age of six, to that great clan which cannot keep this feeling separate from that, but must let future prospects, with their joys and sorrows, cloud what is actually at hand, since to such people even in earliest childhood any turn in the wheel of sensation has the power to crystallise and transfix the moment upon which its gloom or radiance rests, James Ramsay, sitting on the floor cutting out pictures from the illustrated catalogue of the Army and Navy stores, endowed the picture of a refrigerator, as his mother spoke, with heavenly bliss. It was fringed with joy. The wheelbarrow, the lawnmower, the sound of poplar trees, leaves whitening before rain, rooks cawing, brooms knocking, dresses rustling - \\u201dall these were so coloured and distinguished in his mind that he had already his private code, his secret language, though he appeared the image of stark and uncompromising severity, with his high forehead and his fierce blue eyes, impeccably candid and pure, frowning slightly at the sight of human frailty, so that his mother, watching him guide his scissors neatly round the refrigerator, imagined him all red and ermine on the Bench or directing a stern and momentous enterprise in some crisis of public affairs.\",\n\"to her son these words conveyed an extraordinary joy as if it were settled the expedition were bound to take place and the wonder to which he had looked forward for years and years it seemed was after a night own darkness and a day own sail within touch since he belonged even at the age of six to that great clan which cannot keep this feeling separate from that but must let future prospects with their joys and sorrows cloud what is actually at hand since to such people even in earliest childhood any turn in the wheel of sensation has the power to crystallise and transfix the moment upon which its gloom or radiance rests james ramsay sitting on the floor cutting out pictures from the illustrated catalogue of the army and navy stores endowed the picture of a refrigerator as his mother spoke with heavenly bliss it was fringed with joy the wheelbarrow the lawnmower the sound of poplar trees leaves whitening before rain rooks cawing brooms knocking dresses rustling   \\u201dall these were so coloured and distinguished in his mind that he had already his private code his secret language though he appeared the image of stark and uncompromising severity with his high forehead and his fierce blue eyes impeccably candid and pure frowning slightly at the sight of human frailty so that his mother watching him guide his scissors neatly round the refrigerator imagined him all red and ermine on the bench or directing a stern and momentous enterprise in some crisis of public affairs\",\n{\n            'v': 259,\n            'f': \"259\",\n        },\n{\n            'v': 1517,\n            'f': \"1517\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"parag_no\"], [\"number\", \"sent_no_start\"], [\"number\", \"sent_no_mid\"], [\"number\", \"sent_no_end\"], [\"string\", \"parag_raw\"], [\"string\", \"parag_clean\"], [\"number\", \"token_len\"], [\"number\", \"char_len\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sent_no_start</th>\n",
              "      <th>sent_no_mid</th>\n",
              "      <th>sent_no_end</th>\n",
              "      <th>parag_raw</th>\n",
              "      <th>parag_clean</th>\n",
              "      <th>token_len</th>\n",
              "      <th>char_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>\"Yes, of course, if it's fine tomorrow,\" said ...</td>\n",
              "      <td>yes of course if it is fine tomorrow said mrs ...</td>\n",
              "      <td>23</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>To her son these words conveyed an extraordina...</td>\n",
              "      <td>to her son these words conveyed an extraordina...</td>\n",
              "      <td>259</td>\n",
              "      <td>1517</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   parag_no  sent_no_start  ...  token_len  char_len\n",
              "0         0              0  ...         23       111\n",
              "1         1              2  ...        259      1517\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZR54xIPJvm-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "051b9d00-80cd-4bd5-957d-f9a5d1d59aaa"
      },
      "source": [
        "# Verify Sections\n",
        "\n",
        "# corpus_sects_df.head(2)\n",
        "corpus_sects_df.info(2)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 43 entries, 0 to 42\n",
            "Data columns (total 9 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Unnamed: 0     43 non-null     int64 \n",
            " 1   sect_no        43 non-null     int64 \n",
            " 2   sent_no_start  43 non-null     int64 \n",
            " 3   sent_no_mid    43 non-null     int64 \n",
            " 4   sent_no_end    43 non-null     int64 \n",
            " 5   sect_raw       43 non-null     object\n",
            " 6   sect_clean     43 non-null     object\n",
            " 7   token_len      43 non-null     int64 \n",
            " 8   char_len       43 non-null     int64 \n",
            "dtypes: int64(7), object(2)\n",
            "memory usage: 3.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmlXbKnbL1N7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "631b8f00-1203-4918-b9e1-521fe5947b3f"
      },
      "source": [
        "corpus_sects_df.rename(columns={'Unnamed: 0':'sect_no','chap_raw':'sect_raw','chap_clean':'sect_clean'}, inplace=True)\n",
        "# corpus_sects_df = corpus_sects_df.loc[:, ~corpus_sects_df.columns.str.contains('^Unnamed')]\n",
        "# corpus_sects_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "corpus_sects_df = corpus_sects_df.loc[:, ~corpus_sects_df.columns.duplicated()]\n",
        "# corpus_sects_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "corpus_sects_df.info () # [: corpus_sects_df.columns.like('_no')]"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 43 entries, 0 to 42\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   sect_no        43 non-null     int64 \n",
            " 1   sent_no_start  43 non-null     int64 \n",
            " 2   sent_no_mid    43 non-null     int64 \n",
            " 3   sent_no_end    43 non-null     int64 \n",
            " 4   sect_raw       43 non-null     object\n",
            " 5   sect_clean     43 non-null     object\n",
            " 6   token_len      43 non-null     int64 \n",
            " 7   char_len       43 non-null     int64 \n",
            "dtypes: int64(6), object(2)\n",
            "memory usage: 2.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb2DYmbqMEF-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "7e6f91e4-1778-4571-e302-ed5200b0e37f"
      },
      "source": [
        "# Verfiy Chapters\n",
        "\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3 entries, 0 to 2\n",
            "Data columns (total 9 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Unnamed: 0     3 non-null      int64 \n",
            " 1   chap_no        3 non-null      int64 \n",
            " 2   sent_no_start  3 non-null      int64 \n",
            " 3   sent_no_mid    3 non-null      int64 \n",
            " 4   sent_no_end    3 non-null      int64 \n",
            " 5   chap_raw       3 non-null      object\n",
            " 6   chap_clean     3 non-null      object\n",
            " 7   token_len      3 non-null      int64 \n",
            " 8   char_len       3 non-null      int64 \n",
            "dtypes: int64(7), object(2)\n",
            "memory usage: 344.0+ bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmfsseAz2F8T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "344ae894-7877-41e9-c7c5-c03910c5ae91"
      },
      "source": [
        "# corpus_chaps_df = corpus_chaps_df.loc[:, ~corpus_chaps_df.columns.str.contains('^Unnamed')]\n",
        "corpus_chaps_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3 entries, 0 to 2\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   chap_no        3 non-null      int64 \n",
            " 1   sent_no_start  3 non-null      int64 \n",
            " 2   sent_no_mid    3 non-null      int64 \n",
            " 3   sent_no_end    3 non-null      int64 \n",
            " 4   chap_raw       3 non-null      object\n",
            " 5   chap_clean     3 non-null      object\n",
            " 6   token_len      3 non-null      int64 \n",
            " 7   char_len       3 non-null      int64 \n",
            "dtypes: int64(6), object(2)\n",
            "memory usage: 320.0+ bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9X5gfDoi_d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "5794d46d-eaeb-4dfc-918e-9dcb095f0f49"
      },
      "source": [
        "# Verify all 4 semantic unit DataFrame shapes\n",
        "\n",
        "print(f'corpus_sents_df.shape: {corpus_sents_df.shape}')\n",
        "print(f'corpus_parags_df.shape: {corpus_parags_df.shape}')\n",
        "print(f'corpus_sects_df.shape: {corpus_sects_df.shape}')\n",
        "print(f'corpus_chaps_df.shape: {corpus_chaps_df.shape}')\n",
        "\n",
        "\"\"\"\n",
        "SButler Odyssey\n",
        "\n",
        "corpus_sents_df.shape: (2445, 8)\n",
        "corpus_parags_df.shape: (1051, 8)\n",
        "corpus_sects_df.shape: (24, 8)\n",
        "corpus_chaps_df.shape: (24, 8)\n",
        "\"\"\";\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "corpus_sents_df.shape: (3421, 8)\n",
            "corpus_parags_df.shape: (493, 8)\n",
            "corpus_sects_df.shape: (43, 8)\n",
            "corpus_chaps_df.shape: (3, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQYjye-v9XMY"
      },
      "source": [
        "### **(b) Create Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6qZH_qAVPQD"
      },
      "source": [
        "#### **Try to Automatically Detected File/Text Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtWVf1cJUeSW"
      },
      "source": [
        "!pwd\n",
        "!ls -altr *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4TH13cFUErD"
      },
      "source": [
        "# Try to automatically discover Corpus text Encoding scheme (default to 'utf-8', but often 'iso-8859-1', 'windows-1252', 'cp1252', or 'ascii')\n",
        "\n",
        "CORPUS_ENCODING = 'utf-8' # Python3 default encoding\n",
        "\n",
        "corpus_str, corpus_encode, encoding_confidence = get_file_encoding(CORPUS_FILENAME)\n",
        "CORPUS_ENCODING = str(corpus_encode).lower()\n",
        "\n",
        "if encoding_confidence > 0.8:\n",
        "  print(f'Setting file/text encoding to {CORPUS_ENCODING}\\n')\n",
        "  print(f\"    {encoding_confidence*100:.2f}% confidence Encoding = '{CORPUS_ENCODING}' for '{CORPUS_FILENAME}'\")\n",
        "else:\n",
        "  print(f\"WARNING: Less than 80% confidence estimating Encoding scheme for '{CORPUS_FILENAME}'\\n\")\n",
        "  print(f\"         Only {encoding_confidence*100:.2f}% confidence Encoding = '{CORPUS_ENCODING}'\")\n",
        "  print(f\"         Manually verify corpus file '{CORPUS_FILENAME}' encoding, set as GLOBAL_CONSTATANT and rerun\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMkihxoY6T6U"
      },
      "source": [
        "#### **Create Chapter and Section DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ydj8ITnTE1D"
      },
      "source": [
        "!head -n 10 $corpus_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pwfl-0axB1X"
      },
      "source": [
        "# Parse out raw/clean Chapters/Sections from Corpus text file\n",
        "\n",
        "corpus_chaps_raw_ls, corpus_chaps_clean_ls, corpus_sects_raw_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str = corpus2chapsect(corpus_filename)\n",
        "\n",
        "# Verify Paragraph count and sample\n",
        "\n",
        "if type(corpus_chaps_raw_ls[0]) is not str:\n",
        "\n",
        "  print(f'\\nERROR: Could not parse Corpus file into Chapters and Sections correctly\\n       Edit Corpus file and re-run')\n",
        "\n",
        "else:\n",
        "\n",
        "  print(f'\\n{len(corpus_chaps_raw_ls)} Chapters found in this Corpus')\n",
        "\n",
        "  print(f'\\n{len(corpus_sects_raw_ls)} Sections found in this Corpus')\n",
        "\n",
        "\n",
        "  print(f'\\n\\n----------{len(corpus_chaps_raw_ls)} CHAPTERS ----------')\n",
        "  chap_sample_no = 0\n",
        "  print(f'First 500 character sample from Chapter #{chap_sample_no}\\n\\n     {corpus_chaps_raw_ls[chap_sample_no][:500]}')\n",
        "\n",
        "  print(f'\\n\\n----------{len(corpus_sects_raw_ls)} SECTIONS ----------')\n",
        "  sect_sample_no = 0\n",
        "  print(f'First 500 character sample from Section #{sect_sample_no}\\n\\n    {corpus_sects_raw_ls[sect_sample_no][:500]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Coq6ZS6aMkvd"
      },
      "source": [
        "# Create Chapter DataFrame\n",
        "\n",
        "chap_no_ls = list(range(len(corpus_chaps_raw_ls)))\n",
        "corpus_chaps_df = pd.DataFrame({'chap_no':chap_no_ls, 'chap_raw':corpus_chaps_raw_ls, 'chap_clean':corpus_chaps_clean_ls})\n",
        "corpus_chaps_df['chap_raw'] = corpus_chaps_df['chap_raw'].astype('string')\n",
        "corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_clean'].astype('string')\n",
        "# corpus_chaps_df.head(1)\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdKAEsWt-y_w"
      },
      "source": [
        "# Calculate length statistics for Chapters\n",
        "\n",
        "corpus_chaps_df['char_len'] = corpus_chaps_df['chap_raw'].apply(lambda x : len(x))\n",
        "corpus_chaps_df['token_len'] = corpus_chaps_df['chap_raw'].apply(lambda x : len(x.split()))\n",
        "# corpus_chaps_df.head()\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiBMyYDD88z5"
      },
      "source": [
        "# Create Section DataFrame\n",
        "\n",
        "sect_no_ls = list(range(len(corpus_sects_raw_ls)))\n",
        "corpus_sects_df = pd.DataFrame({'sect_no':sect_no_ls, 'chap_no':sect_chapno_ls, 'sect_raw':corpus_sects_raw_ls, 'sect_clean':corpus_sects_clean_ls})\n",
        "corpus_sects_df['sect_raw'] = corpus_sects_df['sect_raw'].astype('string')\n",
        "corpus_sects_df['sect_clean'] = corpus_sects_df['sect_clean'].astype('string')\n",
        "# corpus_sects_df.head(1)\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9jpNBIGmyBT"
      },
      "source": [
        "# Calculate length statistics for Chapters\n",
        "\n",
        "corpus_sects_df['char_len'] = corpus_sects_df['sect_raw'].apply(lambda x : len(x))\n",
        "corpus_sects_df['token_len'] = corpus_sects_df['sect_raw'].apply(lambda x : len(x.split()))\n",
        "# corpus_sects_df.head()\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFj2CQPD6l01"
      },
      "source": [
        "#### **Create Paragraph and Sentence DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weu1ZRsC6XCP"
      },
      "source": [
        "corpus_sectno = -1\n",
        "\n",
        "corpus_paragno = -1\n",
        "corpus_parags_ls = []\n",
        "\n",
        "corpus_sentno = -1\n",
        "corpus_sents_ls = []\n",
        "\n",
        "sects_raw_ls = list(corpus_sects_df.sect_raw)\n",
        "\n",
        "# For every Section in the Corpus\n",
        "for asectno, asect_raw in enumerate(sects_raw_ls):\n",
        "  # print(f'Section #{asectno}')\n",
        "  corpus_sectno += 1\n",
        "\n",
        "  # Split into a list of Paragraphs\n",
        "  sect_parags_raw_ls, sect_clean_str = sect2parags(asect_raw)\n",
        "  # For every Paragraph in Section \n",
        "  for aparagno, aparag_raw in enumerate(sect_parags_raw_ls):\n",
        "    # print(f'  Paragraph #{aparagno}')\n",
        "    corpus_paragno += 1\n",
        "\n",
        "    # Split into list of Sentences\n",
        "    parag_sents_raw_ls, parag_clean_str = parag2sents(aparag_raw)\n",
        "    aparag_clean = clean_text(aparag_raw)\n",
        "    corpus_parags_ls.append((corpus_paragno, corpus_sectno, aparag_raw, aparag_clean))\n",
        "\n",
        "    # For every Sentence in Paragraph\n",
        "    for asentno, asent_raw in enumerate(parag_sents_raw_ls):\n",
        "      print(f'Section #{asectno}, Paragraph #{aparagno}, Sentence #{asentno}')\n",
        "      corpus_sentno += 1\n",
        "      asent_clean = clean_text(asent_raw)\n",
        "      corpus_sents_ls.append((corpus_sentno, corpus_paragno, corpus_sectno, asent_raw, asent_clean))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4721-er6W96"
      },
      "source": [
        "# Verify\n",
        "\n",
        "sent_ct = len(corpus_sents_ls)\n",
        "print(f'{sent_ct} Sentences were found in the Corpus')\n",
        "\n",
        "parag_ct = len(corpus_parags_ls)\n",
        "print(f'{parag_ct} Paragraphs were found in the Corpus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0gltS-r0QPv"
      },
      "source": [
        "print(corpus_parags_ls[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm9fDtVJziE3"
      },
      "source": [
        "# Create Paragraph DataFrame\n",
        "\"\"\"\n",
        "parag_no_ls = list(range(len(corpus_parags_ls)))\n",
        "corpus_parags_df = pd.DataFrame({'sect_no':sect_no_ls, 'chap_no':sect_chapno_ls, 'sect_raw':corpus_sects_raw_ls, 'sect_clean':corpus_sects_clean_ls})\n",
        "corpus_parags_df['sect_raw'] = corpus_parags_df['sect_raw'].astype('string')\n",
        "corpus_parags_df['sect_clean'] = corpus_parags_df['sect_clean'].astype('string')\n",
        "corpus_parags_df.info()\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ7Xdmyt0Asb"
      },
      "source": [
        "# Create Paragraph DataFrame\n",
        "\n",
        "corpus_parags_df = pd.DataFrame(corpus_parags_ls,columns=['parag_no','sect_no','parag_raw','parag_clean'])\n",
        "corpus_parags_df['parag_raw'] = corpus_parags_df['parag_raw'].astype('string')\n",
        "corpus_parags_df['parag_clean'] = corpus_parags_df['parag_clean'].astype('string')\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arm9ZlvSl-42"
      },
      "source": [
        "# Calculate length statistics for Paragraphs\n",
        "\n",
        "corpus_parags_df['char_len'] = corpus_parags_df['parag_raw'].apply(lambda x : len(x))\n",
        "corpus_parags_df['token_len'] = corpus_parags_df['parag_raw'].apply(lambda x : len(x.split()))\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqN4aFeT3RMq"
      },
      "source": [
        "# Verify Paragraph DataFrame start and end\n",
        "\n",
        "corpus_parags_df.head(5)\n",
        "corpus_parags_df.tail(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T27_raCzWk6"
      },
      "source": [
        "# Create Sentences DataFrame\n",
        "\n",
        "corpus_sents_df = pd.DataFrame(corpus_sents_ls,columns=['sent_no', 'parag_no','sect_no','sent_raw','sent_clean'])\n",
        "corpus_sents_df['sent_raw'] = corpus_sents_df['sent_raw'].astype('string')\n",
        "corpus_sents_df['sent_clean'] = corpus_sents_df['sent_clean'].astype('string')\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glceM5TylaA8"
      },
      "source": [
        "# Compute length statistics for Sentences\n",
        "\n",
        "corpus_sents_df['char_len'] = corpus_sents_df['sent_raw'].apply(lambda x : len(x))\n",
        "corpus_sents_df['token_len'] = corpus_sents_df['sent_raw'].apply(lambda x : len(x.split()))\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4WWrd1J2uz_"
      },
      "source": [
        "# Verify Sentence DataFrame start and end\n",
        "\n",
        "corpus_sents_df.head(10)\n",
        "corpus_sents_df.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgJKnHHk4f3F"
      },
      "source": [
        "#### **For each Section, insert [start|mid|end] Sentence numbers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnwy-rFW4swr"
      },
      "source": [
        "# Calculate the start, mid and end Sentence No for each Section\n",
        "\n",
        "sect_sent_no_start_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].min())\n",
        "sect_sent_no_end_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].max())\n",
        "\n",
        "def my_mid(anum, bnum):\n",
        "  mid_no = (anum - bnum)//2 + bnum\n",
        "\n",
        "  return mid_no\n",
        "\n",
        "print('\\nSection start sentence no: -----')\n",
        "print(sect_sent_no_start_ls)\n",
        "\n",
        "sect_sent_no_mid_ls = list(map(my_mid, sect_sent_no_end_ls, sect_sent_no_start_ls))\n",
        "print('\\nSection mid-sentence no: -----')\n",
        "print(sect_sent_no_mid_ls)\n",
        "\n",
        "print('\\nSection end sentence no: -----')\n",
        "print(sect_sent_no_end_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MNNcN_d48xa"
      },
      "source": [
        "# Insert 3 new columns on start,mid and end Sentence No for each Section\n",
        "\n",
        "corpus_sects_df.insert(2, 'sent_no_start', sect_sent_no_start_ls)\n",
        "corpus_sects_df.insert(3, 'sent_no_mid', sect_sent_no_mid_ls)\n",
        "corpus_sects_df.insert(4, 'sent_no_end', sect_sent_no_end_ls)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuxMQpOY2uwF"
      },
      "source": [
        "# Verfiy Section \n",
        "\n",
        "corpus_sects_df.filter(like='_no')\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z48WKV1W6KdA"
      },
      "source": [
        "#### **For Each Chapter, Insert [start|mid|end] Sentence numbers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGNlqV5G7Tk-"
      },
      "source": [
        "corpus_chaps_clean_ls[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70ntj-6X8PbU"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWFbBuEGQ4fH"
      },
      "source": [
        "search_str = 'within touch'\n",
        "\n",
        "corpus_sents_df[corpus_sents_df['sent_clean'].str.contains(search_str)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWvddQwkCjKY"
      },
      "source": [
        "corpus_sents_df.iloc[1056]['sent_clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuPPyK2gUZmS"
      },
      "source": [
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DylkEa8cCwG"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vrZVqCRVOwF"
      },
      "source": [
        "achap_sentstart_clean_ls = []\n",
        "\n",
        "for indx, achap_tup in corpus_chaps_df.iterrows():\n",
        "  achap_no, achap_raw, achap_clean, achap_charlen, achap_tokenlen = achap_tup\n",
        "  achap_sentstart_clean_ls.append(achap_clean[:100])\n",
        "\n",
        "print(achap_sentstart_clean_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4_18dyRVOqk"
      },
      "source": [
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2HcU3WLaHpV"
      },
      "source": [
        "!pip install fuzzywuzzy[speedup]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU_11lWzaI-I"
      },
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbxpUygQaL3g"
      },
      "source": [
        " fuzz.ratio(\"this is a test\", \"this is a test!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6B0n3hdbLyp"
      },
      "source": [
        "corpus_sects_df.info()\n",
        "\n",
        "type(corpus_sects_df['sect_clean'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtGurlnd6-Qt"
      },
      "source": [
        "# Calculate the start, mid and end Sentence No for each Section\n",
        "\n",
        "min_charlen = 80  # How many characters to match at the start of start/end Sentences\n",
        "\n",
        "\n",
        "# First, find which Sections align with each Chapter and save Section's sent_no_start as the same for Chapter\n",
        "chaps_sentstart_ls = []\n",
        "sects_sentstart_ls = list(corpus_sects_df['sect_clean'].apply(lambda x: x[:min_charlen].strip()))\n",
        "for achap_indx, achap_tup in corpus_chaps_df.iterrows():\n",
        "  achap_no, achap_raw, achap_clean, achap_charlen, achap_tokenlen = achap_tup\n",
        "  achap_sentstart_str = achap_clean[:min_charlen].strip()\n",
        "  achap_sentstart_str = ' '.join(achap_sentstart_str.split())\n",
        "\n",
        "  sent_startno_found = False\n",
        "  # Loop over all the Sections to see if the starting text matches the Chapter starting text\n",
        "  for asect_indx, asect_sentstart_str in enumerate(sects_sentstart_ls):\n",
        "    asect_sentstart_str = ' '.join(asect_sentstart_str.split())\n",
        "    achap_sentstart_len = len(achap_sentstart_str)\n",
        "    asect_sentstart_len = len(asect_sentstart_str)\n",
        "    if (achap_sentstart_len > asect_sentstart_len):\n",
        "      len_diff = achap_sentstart_len - asect_sentstart_len\n",
        "      achap_sentstart_str = achap_sentstart_str[:-len_diff]\n",
        "    elif (achap_sentstart_len < asect_sentstart_len):\n",
        "      len_diff = asect_sentstart_len - achap_sentstart_len\n",
        "      asect_sentstart_str = asect_sentstart_str[:-len_diff]\n",
        "    else:\n",
        "      # Both Chapter and Section starting Sentence strings are equal length\n",
        "      pass\n",
        "\n",
        "    print(f'Fuzzy Compare:\\n\\n    Chapter Start: {achap_sentstart_str}\\n    Section Start: {asect_sentstart_str}')\n",
        "    if fuzz.ratio(achap_sentstart_str, asect_sentstart_str) > 97:\n",
        "      sent_startno_ls = list(corpus_sects_df[corpus_sects_df['sect_no'] == asect_indx]['sent_no_start'])\n",
        "      # print(f'type(sent_startno): {sent_startno_ls[0]}')\n",
        "      chaps_sentstart_ls.append(sent_startno_ls[0])\n",
        "      sent_startno_found = True\n",
        "      break\n",
        "  # If no match found, enter ERROR code -1 in the Sentence start no for the current Chapter\n",
        "  if sent_startno_found == False:\n",
        "    chaps_sentstart_ls.append((achap_indx, -1))\n",
        "\n",
        "print(f'\\n\\nchaps_sentstart_ls: {chaps_sentstart_ls}')\n",
        "\n",
        "\n",
        "\n",
        "# Second, get the end Sentence for Each Chapter by rotating Sentence Start No left and pushing on the last Sentence No\n",
        "chaps_sentend_ls = []\n",
        "chaps_sentend_ls = [x-1 for x in chaps_sentstart_ls]\n",
        "corpus_sentlast = corpus_sents_df.shape[0] - 1\n",
        "\n",
        "chaps_sentend_ls.pop(0)\n",
        "chaps_sentend_ls.append(corpus_sentlast)\n",
        "\n",
        "print(f'\\n\\nchaps_sentend_ls: {chaps_sentend_ls}')\n",
        "\n",
        "\n",
        "# Third, calculate the Sentence No in the middle of each Chapter\n",
        "\n",
        "chaps_sentmid_ls = [(((chaps_sentend_ls[i] - chaps_sentstart_ls[i])//2)+chaps_sentstart_ls[i]) for i in range(len(chaps_sentend_ls))]\n",
        "\n",
        "print(f'\\n\\nchaps_sentmid_ls: {chaps_sentmid_ls}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpOzOahxhgvU"
      },
      "source": [
        "# Verify boundry cases\n",
        "\n",
        "corpus_sents_df.iloc[268]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN_GXkBekH0P"
      },
      "source": [
        "# corpus_chaps_df.drop(columns=['sent_no_start','sent_no_mid','sent_no_end'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZTduIsl6KdB"
      },
      "source": [
        "# Insert 3 new columns on start,mid and end Sentence No for each Chapter\n",
        "\n",
        "corpus_chaps_df.insert(1, 'sent_no_start', chaps_sentstart_ls)\n",
        "corpus_chaps_df.insert(2, 'sent_no_mid', chaps_sentmid_ls)\n",
        "corpus_chaps_df.insert(3, 'sent_no_end', chaps_sentend_ls)\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTW3Gxmf6KdC"
      },
      "source": [
        "# Verfiy Chapter \n",
        "\n",
        "corpus_chaps_df.filter(like='_no')\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK27hN0KkvBI"
      },
      "source": [
        "### **END HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVCkjat0vffd"
      },
      "source": [
        "#### **Get All (non-null) Raw Lines**\n",
        "\n",
        "* NOTE: Corpus textfile needs to be Preprocessed before running this\n",
        "- Remove all Curve Parenthesis that span multiple sentences or paragraphs\n",
        "- Remove all Square Parenthesis\n",
        "- Filter out non-printing characters if they exist (or use proper encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgqqpwagWPS7"
      },
      "source": [
        "len(corpus_lines_ls)\n",
        "print('\\n')\n",
        "print(corpus_lines_ls[11])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBl_hrhPZo29"
      },
      "source": [
        "min(corpus_lines_ls, key=lambda word: len(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthUz3kpZ8_K"
      },
      "source": [
        "corpus_lines_ls.sort(key=lambda s: len(s))\n",
        "corpus_lines_ls[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an7SzlAuvXA0"
      },
      "source": [
        "# NOTE: ~3-15 minutes (one pass with PySBD)\n",
        "#             minutes (two passes with PySBD+NLTK)\n",
        "\n",
        "# Time consuming so only set pysbd_only=True (second NLTK sentence tokenizer pass) \n",
        "#   if necessary (e.g. Samuel Butler's 1900 trans. of Homer's Odyssey)\n",
        "#                 PySBD: 1075 lines, PySBD+NLTK: 3905 lines, NLTK: 3109 lines\n",
        "#                        Why? a 3 Sentence Paragraph enclosed in double quotes is counted as one Sentence by PySBD\n",
        "#           w/spec char strip: PySBD+NLTK: 3926\n",
        "#           w/o digits/footnotes: PySBD+NLTK: 3925\n",
        "\n",
        "corpus_lines_ls, lines_raw_str = corpus2lines(corpus_filename, pysbd_only=False)\n",
        "\n",
        "# Verify\n",
        "print(f'\\n\\nRead raw corpus lines: character count: {len(lines_raw_str)}')\n",
        "print(f'                        raw line count:  {len(corpus_lines_ls)}\\n\\n')\n",
        "\n",
        "line_ct = 10\n",
        "print(f'First {line_ct} raw lines: --------------------\\n')\n",
        "for i,aline in enumerate(corpus_lines_ls[:line_ct]):\n",
        "  print(f'Line #{i}:\\n    {aline}')\n",
        "print(f'\\n\\nLast {line_ct} raw lines: -------------------\\n')\n",
        "for i,aline in enumerate(corpus_lines_ls[-line_ct:]):\n",
        "  print(f'Line #{i}: {aline}')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "BEFORE stripping out headings len: 610949 (605835 w/2nd pass)\n",
        "Corpus Paragraph Raw Count: 1075\n",
        "   Parag count before processing sents: 1075\n",
        "About to return corpus_sents_raw_ls with len = 2469\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b6ORN6A08Dv"
      },
      "source": [
        "#### **Create Sentence DataFrame: [corpus_sents_df]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BeGt1ot0_dK"
      },
      "source": [
        "# Filter out all the structural/metatag and blank/punctuation only lines\n",
        "#    and save all semantically meaningful Sentences in corpus_sents_ls\n",
        "\n",
        "corpus_sents_ls = []\n",
        "\n",
        "for i, aline in enumerate(corpus_lines_ls):\n",
        "\n",
        "    # print(f'Examing line #{i}: {aline}')\n",
        "\n",
        "    aline_clean = aline.strip()\n",
        "    # Skip/delete whitespace only sentences\n",
        "    if len(aline_clean) == 0:\n",
        "      continue\n",
        "    \n",
        "    # Skip/delete any sentences starting with CHAPTER RegEx Pattern\n",
        "    if aline_clean.startswith('CHAPTER '):\n",
        "      continue\n",
        "    \n",
        "    # Skip/delete any sentences starting with SECTION RegEx Pattern\n",
        "    if aline_clean.startswith('SECTION '):\n",
        "      continue\n",
        "\n",
        "    # Skip/delete any sentences starting with SECTION RegEx Pattern\n",
        "    if aline_clean.startswith('BOOK '):\n",
        "      continue\n",
        "\n",
        "    # Skip/delete any sentence no alpha/numeric charcters (e.g. only punctuation)\n",
        "    if (re.match('^[^a-zA-Z]+$', aline_clean)):\n",
        "      print(f'No alnum line #{i}: {aline}')\n",
        "      continue\n",
        "\n",
        "    # If passed through all previous filters, save as genuine Sentence\n",
        "    corpus_sents_ls.append(aline_clean)\n",
        "\n",
        "# Test\n",
        "print(f'Raw Lines length: {len(corpus_lines_ls)}')\n",
        "print(f' Clean Sentences: {len(corpus_sents_ls)}')\n",
        "\n",
        "line_ct = 10\n",
        "print(f'First {line_ct} clean Sentences : --------------------\\n')\n",
        "for i,aline in enumerate(corpus_sents_ls[:line_ct]):\n",
        "  print(f'Line #{i}:\\n    {aline}')\n",
        "print(f'\\n\\nLast {line_ct} clean Sentences: -------------------\\n')\n",
        "for i,aline in enumerate(corpus_sents_ls[-line_ct:]):\n",
        "  print(f'Line #{i}: {aline}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8ERYqQW7U9q"
      },
      "source": [
        "# Create Sentence DataFrame\n",
        "\n",
        "sent_no_ls = list(range(len(corpus_sents_ls)))\n",
        "\n",
        "corpus_sents_df = pd.DataFrame({'sent_no': sent_no_ls, 'sent_raw': corpus_sents_ls})\n",
        "corpus_sents_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70VO_w4Oql1a"
      },
      "source": [
        "# Compute Sentence text statistics\n",
        "\n",
        "corpus_sents_df['sent_clean'] = corpus_sents_df['sent_raw'].apply(lambda x: clean_text(x))\n",
        "corpus_sents_df['token_len'] = corpus_sents_df['sent_clean'].apply(lambda x: len(x.split()))\n",
        "corpus_sents_df['char_len'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x))\n",
        "corpus_sents_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFC8GTnw6HrG"
      },
      "source": [
        "#### **Create Paragraph DataFrame: [corpus_parags_df]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGog8ZW16bbr"
      },
      "source": [
        "# Read Corpus into a single string then split into raw Paragraphs\n",
        "\n",
        "corpus_parags_ls, corpus_parags_raw_ls, corpus_raw_str = corpus2parags(CORPUS_FILENAME)\n",
        "print(f'Found #{len(corpus_parags_ls)} paragraphs\\n')\n",
        "\n",
        "print('\\nThe first 5 Paragraphs of the Corpus (first 10 chars):')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_ls[:5][:10]\n",
        "print('\\n')\n",
        "print('\\n\\nThe last 5 Paragraphs of the Corpus (first 10 chars):')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_ls[-5:][:10]\n",
        "print('\\n')\n",
        "\n",
        "n_shortest = 10\n",
        "print(f'The {n_shortest} shortest Paragraphs in the Corpus are:')\n",
        "print('--------------------------------------------')\n",
        "temp_parags_ls = sorted(corpus_parags_ls, key=lambda x: (len(x), x))\n",
        "for i, asent in enumerate(temp_parags_ls[:n_shortest]):\n",
        "  print(f'Shortest Paragraph #{i}: {asent}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZdfH-rTXwwT"
      },
      "source": [
        "# Verify Paragraphs found and Sentence-Paragraph matches\n",
        "\n",
        "# TODO: Upgrade if warranted (requires updating x2parags functions)\n",
        "\"\"\"\n",
        "print(f'{len(corpus_parags_ls)} Paragraphs found in Corpus')\n",
        "print(f'{len(sentences_section_ls)} Sentences found in a Section')\n",
        "\n",
        "sentences_nosection_ct = len(sentences_nosection_ls)\n",
        "\n",
        "if (sentences_nosection_ct > 0):\n",
        "  print(f'\\n    WARNING: The following {sentences_nosection_ct} Sentences were NOT FOUND in any Section')\n",
        "  print(f'             If these are important/numerous, go back and edit/correct source Corpus text file and rerun this notebook\\n')\n",
        "\n",
        "  for i, asent in enumerate(sentences_nosection_ls):\n",
        "    print(f'    #{i}: {asent}\\n')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAwfcbYJnwtq"
      },
      "source": [
        "# Verify Paragraph count and sample\n",
        "\n",
        "len(corpus_parags_ls)\n",
        "print('\\n')\n",
        "\n",
        "parag_no_ls = range(len(corpus_parags_ls))\n",
        "len(corpus_parags_ls)\n",
        "print('\\n')\n",
        "\n",
        "corpus_parags_ls[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnJQvKkyHfVc"
      },
      "source": [
        "# Create DataFrame from list of Paragraphs extracted from Corpus\n",
        "\n",
        "corpus_parags_df = pd.DataFrame({'parag_no':parag_no_ls, 'parag_raw':corpus_parags_raw_ls, 'parag_clean':[clean_text(x) for x in corpus_parags_ls]})\n",
        "corpus_parags_df['parag_raw'] = corpus_parags_df['parag_raw'].astype('string')\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.tail(2)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOvIFAsgdgK2"
      },
      "source": [
        "corpus_parags_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrQ4YmkK6HrH"
      },
      "source": [
        "# For each Paragraph, compute the start, mid and end Sentence Number\n",
        "\n",
        "# NOTE: ~5 minutes runtime\n",
        "\n",
        "# NOTE: May fail on any paragraph/sentence with dirty text and require\n",
        "#       multiple iterations of fix/run cycles\n",
        "\n",
        "# Used raw paragraph data to update Master Corpus DataFrame with Paragraph Info\n",
        "\n",
        "# Filter out all the structural/metatag and blank/punctuation only lines\n",
        "#    and save all semantically meaningful Sentences in corpus_sents_ls\n",
        "\n",
        "corpus_sents2parag_ls = []\n",
        "corpus_sents2parag_reject_ls = []\n",
        "parag_sents_tup_ls = []\n",
        "sent_no_current = 0\n",
        "\n",
        "parag_no_current = 0\n",
        "sent_no_current = 0\n",
        "\n",
        "flag_previous_miss = False\n",
        "\n",
        "corpus_parag_ct = len(corpus_parags_ls)\n",
        "\n",
        "def get_sent2paragno(asent_no, asent_raw_str):\n",
        "  '''\n",
        "  Given a sent_no and sent_raw_str\n",
        "  Search and return the corresponding parag_no that contains the sent_raw_str\n",
        "      (return -1 if not found)\n",
        "  '''\n",
        "\n",
        "  global parag_no_current\n",
        "  global flag_previous_miss\n",
        "  # NOTE: Dependencies on local vars outside this def and global vars corpus_parags_ls\n",
        "  # global parag_no_current\n",
        "\n",
        "  # Loop over every paragraph until we find matching sentence (or fail)\n",
        "  while parag_no_current < corpus_parag_ct:\n",
        "\n",
        "    print(f'Sentence #{asent_no}, Text: {asent_raw_str}')\n",
        "    print(f'    Starting search at Paragraph #{parag_no_current}')\n",
        "    print(f'    Paragraph Text:\\n    {corpus_parags_ls[parag_no_current]}')\n",
        "    parag_str = corpus_parags_ls[parag_no_current]\n",
        "\n",
        "    # Search for Sentence string in current Paragraph string\n",
        "    # if re.search(asent_raw_str, re.escape(parag_str)):\n",
        "    # problems with embedded parenthesis\n",
        "    # noparen_table = str.maketrans({'(':' ', ')':' ', '[':' ', ']':' ', '?':' ', '\"':' ', \"'\":\" \"})\n",
        "    # asent_noparens_str = asent_raw_str.translate(noparen_table)\n",
        "    # parag_noparens_str = parag_str.translate(noparen_table)\n",
        "    asent_noparens_str = re.sub('[^0-9a-zA-Z]+', ' ', asent_raw_str)\n",
        "    parag_noparens_str = re.sub('[^0-9a-zA-Z]+', ' ', parag_str)\n",
        "\n",
        "    if re.search(asent_noparens_str, parag_noparens_str):\n",
        "      print(f'    Found it!')\n",
        "      flag_previous_miss = False\n",
        "      return parag_no_current\n",
        "    else:\n",
        "      if flag_previous_miss == True:\n",
        "        # Sentence not found in 2 consecutive Paragraphs, skip this Sentence\n",
        "        print(f'    Miss: Skip this Sentence and go set current paragraph back 1')\n",
        "        parag_no_current -= 1\n",
        "        flag_previous_miss = False\n",
        "        return -1\n",
        "      else:\n",
        "        # Sentence not found in current Paragraph, so try next one\n",
        "        print(f'     Miss: Sentence not found in current Paragraph, try next one')\n",
        "        parag_no_current += 1\n",
        "        flag_previous_miss = True\n",
        "\n",
        "\n",
        "  # At this point we searched both the current and next Paragraphs for Sentence \n",
        "  #     without success so return error code\n",
        "  return -1 \n",
        "\n",
        "# Step through every Sentence and find the Paragraph Number it lies within\n",
        "# corpus_sents_df['parag_no'] = corpus_sents_df.apply(lambda x: get_sent2paragno(x.sent_no, x.sent_raw), axis=1)\n",
        "parag_no_last_match = 0\n",
        "for idx, row in corpus_sents_df.iterrows():\n",
        "  parag_no_current = parag_no_last_match\n",
        "  asent_no = row['sent_no']\n",
        "  asent_str = row['sent_raw']\n",
        "  print(f'idx #{idx}, sent_no: {asent_no}\\n    {asent_str}')\n",
        "  \n",
        "  asent_parag_no = get_sent2paragno(asent_no, asent_str)\n",
        "  print(f'back from searching for Sentence in all Paragraphs with result: {asent_parag_no}')\n",
        "  if asent_parag_no < 0:\n",
        "    # print(f'FAIL: Did not find current Sentence so skip to next Sentence\\n\\n')\n",
        "    corpus_sents2parag_reject_ls.append((asent_no, corpus_sents_ls[asent_no]))\n",
        "    parag_no_current = parag_no_last_match\n",
        "  else:\n",
        "    # print(f'SUCCESS: Sentence #{asent_no} found in Paragraph #{asent_parag_no}\\n\\n')\n",
        "    corpus_sents2parag_ls.append((asent_no, asent_parag_no))\n",
        "    parag_no_last_match = parag_no_current\n",
        "\n",
        "  # if idx > 20:\n",
        "  #   break\n",
        "\n",
        "# Test\n",
        "\"\"\"\n",
        "print(f'Raw Lines length: {len(corpus_lines_ls)}')\n",
        "print(f' Clean Sentences: {len(corpus_sents_ls)}')\n",
        "\n",
        "line_ct = 10\n",
        "print(f'First {line_ct} clean Sentences : --------------------\\n')\n",
        "for i,aline in enumerate(corpus_sents_ls[:line_ct]):\n",
        "  print(f'Line #{i}:\\n    {aline}')\n",
        "print(f'\\n\\nLast {line_ct} clean Sentences: -------------------\\n')\n",
        "for i,aline in enumerate(corpus_sents_ls[-line_ct:]):\n",
        "  print(f'Line #{i}: {aline}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJxi6BUWqj-l"
      },
      "source": [
        "# Any Sentences not found in Corpus Paragraphs?\n",
        "#  If not empty list, manually check and edit corpus if many/significant Sentences rejected\n",
        "\n",
        "sentences_noparag_ct = len(corpus_sents2parag_reject_ls)\n",
        "\n",
        "if sentences_noparag_ct > 0:\n",
        "  print(f'\\n    WARNING: The following {sentences_noparag_ct} Sentences were NOT FOUND in any Paragraph')\n",
        "  print(f'             If these are important/numerous, go back and edit/correct source Corpus text file and rerun this notebook\\n')\n",
        "\n",
        "  for i, asent in enumerate(corpus_sents2parag_reject_ls):\n",
        "    print(f'    #{i}: {asent}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9FrWNj80PkD"
      },
      "source": [
        "corpus_sents_df.iloc[200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UAnLu0k1PAr"
      },
      "source": [
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGqcTKBtF4QU"
      },
      "source": [
        "# Create a list of the Sentence Nos associated with each Paragraph in the Corpus\n",
        "\n",
        "sent_parag_no_ls = [sentnoparagno_tp[1] for sentnoparagno_tp in corpus_sents2parag_ls]\n",
        "\n",
        "corpus_sent_ct = corpus_sents_df.shape[0]\n",
        "parags_sent_ct = len(sent_parag_no_ls)\n",
        "if (corpus_sent_ct == parags_sent_ct):\n",
        "  print(f'GOOD, all {corpus_sent_ct} Sentences were matched in one of the {corpus_parags_df.shape[0]} Paragraphs\\n')\n",
        "else:\n",
        "  print(f'WARNING: only {parags_sent_ct} Sentences were matched in one of the {corpus_parags_df.shape[0]} Paragraphs')\n",
        "  print(f'         {corpus_sent_ct - parags_sent_ct} Sentences were not matched\\n')\n",
        "\n",
        "print(f'First 10 Sentences belong to these Paragraph:\\n    {sent_parag_no_ls[:10]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7-1oBkJKeJH"
      },
      "source": [
        "# Verfiy the data about to be used to update the master corpus_all_df DataFrame with Paragraph No data\n",
        "\n",
        "corpus_sents2parag_ls[3233:3237]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2vZk07BkeNw"
      },
      "source": [
        "# corpus_all_df.drop(columns=['parag_no'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29tMZKR4KERa"
      },
      "source": [
        "# Update the master corpus_all_df DataFrame with Paragraph No for each Sentence\n",
        "\n",
        "# WARNING: Only execute once (insert Column/Series into DataFrame)\n",
        "\n",
        "parag_no_ser = pd.Series(parag_no_ls)\n",
        "corpus_sents_df.insert(loc=1, column='parag_no', value=sent_parag_no_ls)\n",
        "\n",
        "corpus_sents_df.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsOgtjM71eKE"
      },
      "source": [
        "corpus_sents_df.iloc[300:320]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK5kKZbVl3jR"
      },
      "source": [
        "# corpus_parags_df.drop(columns=['sent_no_start', 'sent_no_mid','sent_no_end'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmLLzsNtECDt"
      },
      "source": [
        "corpus_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVkStzTrlxll"
      },
      "source": [
        "# Verify the Paragraph only DataFrame\n",
        "\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkHIEWqEKz66"
      },
      "source": [
        "# Calculate the start, mid and end Sentence No for each Paragraph\n",
        "\n",
        "parag_sent_no_start_ls = np.array(corpus_sents_df.groupby('parag_no')['sent_no'].min())\n",
        "parag_sent_no_end_ls = np.array(corpus_sents_df.groupby('parag_no')['sent_no'].max())\n",
        "\n",
        "def my_mid(anum, bnum):\n",
        "  mid_no = (anum - bnum)//2 + bnum\n",
        "\n",
        "  return mid_no\n",
        "\n",
        "print('\\nParagraph Start Sentence no: -----')\n",
        "print(parag_sent_no_start_ls)\n",
        "\n",
        "parag_sent_no_mid_ls = list(map(my_mid, parag_sent_no_end_ls, parag_sent_no_start_ls))\n",
        "print('\\nParagraph Mid-Sentence no: -----')\n",
        "print(parag_sent_no_mid_ls)\n",
        "\n",
        "print('\\nParagraph End Sentence no: -----')\n",
        "print(parag_sent_no_end_ls)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbSi16c1Kz6_"
      },
      "source": [
        "# If necessary, delete prior columns to update DataFrame with new data\n",
        "\n",
        "# corpus_parags_df.drop(columns=['parag_no_start'], inplace=True)\n",
        "# corpus_parags_df.drop(columns=['parag_no_mid'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkdeR-S_Kz7B"
      },
      "source": [
        "# Insert 3 new columns on start,mid and end Sentence No for each Paragraph\n",
        "\n",
        "corpus_parags_df.insert(1, 'sent_no_start', parag_sent_no_start_ls)\n",
        "corpus_parags_df.insert(2, 'sent_no_mid', parag_sent_no_mid_ls)\n",
        "corpus_parags_df.insert(3, 'sent_no_end', parag_sent_no_end_ls)\n",
        "\n",
        "corpus_parags_df.head()\n",
        "                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9uIWYnCLSr7"
      },
      "source": [
        "# Create a clean text version of each Paragraph\n",
        "\n",
        "# corpus_parags_df = pd.DataFrame({'parag_no':parag_no_ls, 'parag_raw':corpus_parags_ls})\n",
        "corpus_parags_df['parag_clean'] = corpus_parags_df['parag_raw'].apply(lambda x: clean_text(x))\n",
        "corpus_parags_df['parag_clean'] = corpus_parags_df['parag_clean'].astype('string')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gza5pqW3LSsF"
      },
      "source": [
        "# Compute Paragraph text Statistics\n",
        "\n",
        "corpus_parags_df['token_len'] = corpus_parags_df['parag_clean'].apply(lambda x: len(x.split()))\n",
        "corpus_parags_df['char_len'] = corpus_parags_df['parag_raw'].apply(lambda x: len(x))\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOjESpbc6Izc"
      },
      "source": [
        "#### **Create Section DataFrame: [corpus_sects_df]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vITL5uo1ABR"
      },
      "source": [
        "# If Corpus has Sections, read Corpus and split into raw Sections\n",
        "#   else just copy Chapter data as pseudo Sections\n",
        "\n",
        "corpus_sects_ls = []    # List of all Section Numbers\n",
        "sent_sectno_ls = []     # Section Number for EVERY Matching Sentence in Corpus found in a Section\n",
        "sent_no_sectno_ls = []  # Sentence Number for for ANY Unmatched Sentence in Corpus NOT found in any Section\n",
        "\n",
        "\n",
        "if SECTION_HEADINGS == 'None':\n",
        "  # Just copy Chapter info\n",
        "  corpus_sects_ls = [x for x in corpus_chaps_ls]\n",
        "  print(f'No Sections in {CORPUS_FULL},\\n    so using Chapters as pseudo-Sections.')\n",
        "  sent_sectno_ls = sent_chap_no_ls # Every Sentence in Corpus belongs to the same SectionNo as CorpusNo\n",
        "\n",
        "else:\n",
        "\n",
        "  # Read corpus into a single string then split into raw Section\n",
        "\n",
        "  corpus_sects_ls, sent_sectno_ls, sent_no_sectno_ls = corpus2sects(CORPUS_FILENAME)\n",
        "  print(f'Found #{len(sent_sectno_ls)} Section\\n')\n",
        "\n",
        "  print('\\nThe first 5 Section of the Corpus (first 10 chars):')\n",
        "  print('-----------------------------------\\n')\n",
        "  sent_sectno_ls[:5][:10]\n",
        "  print('\\n')\n",
        "  print('\\n\\nThe last 5 Section of the Corpus (first 10 chars):')\n",
        "  print('-----------------------------------\\n')\n",
        "  sent_sectno_ls[-5:][:10]\n",
        "  print('\\n')\n",
        "\n",
        "  n_shortest = 10\n",
        "  print(f'The {n_shortest} shortest Section in the Corpus are:')\n",
        "  print('--------------------------------------------')\n",
        "  temp_sects_ls = sorted(sent_sectno_ls, key=lambda x: (len(x), x))\n",
        "  for i, asent in enumerate(temp_sects_ls[:n_shortest]):\n",
        "    print(f'Shortest Section #{i}: {asent}')\n",
        "\n",
        "\n",
        "  # Calculate Section Informationif SECTION_HEADINGS != 'None':\n",
        "  # encoding = 'windows-1252', 'utf-8', 'cp1252', 'iso-8859-1'\n",
        "  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n",
        "  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n",
        "  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n",
        "\n",
        "  \"\"\"\n",
        "  with open(corpus_filename, \"r\", encoding='utf-8') as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  len(corpus_raw_str)\n",
        "\n",
        "  # Extract and process Sections from Corpus\n",
        "  corpus_sects_ls, corpus_str_raw = corpus2sects(corpus_filename)\n",
        "\n",
        "  print('\\n\\nAFTER ----------')\n",
        "  print(f'len(corpus_raw_str): {len(corpus_raw_str)}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'len(corpus_sects_ls): {len(corpus_sects_ls)}\\n\\n')\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[0]:\\n\\n    {corpus_sects_ls[0]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[1]:\\n\\n    {corpus_sects_ls[1]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[2]:\\n\\n    {corpus_sects_ls[2]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[-2]:\\n\\n    {corpus_sects_ls[-2]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[-1]:\\n\\n    {corpus_sects_ls[-1]}')\n",
        "  \"\"\";\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1MJfVfpKOU-"
      },
      "source": [
        "# Create a list of the Sentence Nos associated with each Section in the Corpus\n",
        "\n",
        "# sent_chap_no_ls = [sentnochapno_tp[1] for sentnochapno_tp in corpus_sents2chap_ls]\n",
        "\n",
        "corpus_sent_ct = corpus_sents_df.shape[0]\n",
        "sect_sent_ct = len(sent_sectno_ls)\n",
        "if (corpus_sent_ct == sect_sent_ct):\n",
        "  print(f'GOOD, all {corpus_sent_ct} Sentences were matched in one of the {corpus_sects_df.shape[0]} Sections\\n')\n",
        "else:\n",
        "  print(f'WARNING: only {sect_sent_ct} Sentences were matched in one of the {corpus_sects_df.shape[0]} Sections')\n",
        "  print(f'         {corpus_sent_ct - sect_sent_ct} Sentences were not matched\\n')\n",
        "\n",
        "\n",
        "# print(f'There are {corpus_sents_df.shape[0]} Sentences in the Corpus')\n",
        "# print(f'{len(sent_chap_no_ls)} Sentences have been associated with {corpus_chaps_df.shape[0]} Sections\\n')\n",
        "\n",
        "print(f'First 10 Sentences belong to these Sections:\\n    {sent_sectno_ls[:10]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqKwpPs9JC0R"
      },
      "source": [
        "# Verify Sections found and Sentence-Section matches\n",
        "\n",
        "print(f'{len(corpus_sects_ls)} Sections found in Corpus')\n",
        "print(f'{len(sent_sectno_ls)} Sentences found in a Section')\n",
        "\n",
        "sentences_nosection_ct = len(sent_no_sectno_ls)\n",
        "\n",
        "if (sentences_nosection_ct > 0):\n",
        "  print(f'\\n    WARNING: The following {sentences_nosection_ct} Sentences were NOT FOUND in any Section')\n",
        "  print(f'             If these are important/numerous, go back and edit/correct source Corpus text file and rerun this notebook\\n')\n",
        "\n",
        "  for i, asent in enumerate(sentences_nosection_ls):\n",
        "    print(f'    #{i}: {asent}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JFQTmEgK66I"
      },
      "source": [
        "# Create DataFrame from list of Sections extracted from Corpus\n",
        "\n",
        "sect_no_ls = list(range(len(corpus_sects_ls)))\n",
        "corpus_sects_df = pd.DataFrame({'sect_no':sect_no_ls, 'sect_raw':corpus_sects_ls})\n",
        "corpus_sects_df['sect_raw'] = corpus_sects_df['sect_raw'].astype('string')\n",
        "corpus_sects_df.head(1)\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjN6omWrifWm"
      },
      "source": [
        "# Create a list of the Sentence Nos associated with each Section in the Corpus\n",
        "\n",
        "# sentno_sectno_ls = [sentno2sectno_tp[0] for sentno2sectno_tp in sentences_section_ls]\n",
        "# sentstr_sectno_ls = [sentno2sectno_tp[1] for sentno2sectno_tp in sentences_section_ls]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KM-JEKECfSa"
      },
      "source": [
        "# corpus_sents_df.drop(columns=['sect_no'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpAlcbpQru0t"
      },
      "source": [
        "# Add Section No for each Sentence in Master DataFrame corpus_all_df\n",
        "# ONLY RUN THIS CODE CELL ONCE\n",
        "\n",
        "# Test if already exists, if not execute\n",
        "corpus_sents_df.insert(3, 'sect_no', sent_sectno_ls)  # This can only be run once\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "353_Nn7pM1FX"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.tail(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpE0UiORYZy6"
      },
      "source": [
        "# Verfiy correct Section Nos using the Sentence No boundaries found in previous code cell\n",
        "#   Search for iloc index ranges containing 1 or more Section boundaries to check correctness\n",
        "\n",
        "corpus_sents_df.iloc[300:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVNIhlQpNZiV"
      },
      "source": [
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yG53MVwkNHB"
      },
      "source": [
        "# Calculate the start, mid and end Sentence No for each Section\n",
        "\n",
        "sect_sent_no_start_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].min())\n",
        "sect_sent_no_end_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].max())\n",
        "\n",
        "def my_mid(anum, bnum):\n",
        "  mid_no = (anum - bnum)//2 + bnum\n",
        "\n",
        "  return mid_no\n",
        "\n",
        "print('\\nSection start sentence no: -----')\n",
        "print(sect_sent_no_start_ls)\n",
        "\n",
        "sect_sent_no_mid_ls = list(map(my_mid, sect_sent_no_end_ls, sect_sent_no_start_ls))\n",
        "print('\\nSection mid-sentence no: -----')\n",
        "print(sect_sent_no_mid_ls)\n",
        "\n",
        "print('\\nSection end sentence no: -----')\n",
        "print(sect_sent_no_end_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUofWkm0kNHC"
      },
      "source": [
        "# If necessary, delete prior columns to update DataFrame with new data\n",
        "\n",
        "# corpus_chaps_df.drop(columns=['parag_no_start'], inplace=True)\n",
        "# corpus_chaps_df.drop(columns=['parag_no_mid'], inplace=True)\n",
        "# corpus_chaps_df.drop(columns=['parag_no_end'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HxCqi2XkNHC"
      },
      "source": [
        "# Verify DataFrame before update\n",
        "\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhfgZuiPkNHD"
      },
      "source": [
        "# Insert 3 new columns on start,mid and end Sentence No for each Section\n",
        "\n",
        "corpus_sects_df.insert(1, 'sent_no_start', sect_sent_no_start_ls)\n",
        "corpus_sects_df.insert(2, 'sent_no_mid', sect_sent_no_mid_ls)\n",
        "corpus_sects_df.insert(3, 'sent_no_end', sect_sent_no_end_ls)\n",
        "\n",
        "corpus_sects_df.loc[:, corpus_sects_df.columns != 'sect_raw']\n",
        "# corpus_sects_df.loc[:, ['sect_no', 'sect_no_start', 'sect_no_mid', 'sect_no_end']]\n",
        "corpus_sects_df.info()\n",
        "\n",
        "                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iObHkdhmkNHF"
      },
      "source": [
        "# Create a clean text version of each Paragraph\n",
        "\n",
        "corpus_sects_df['sect_clean'] = corpus_sects_df['sect_raw'].apply(lambda x: clean_text(x))\n",
        "corpus_sects_df['sect_clean'] = corpus_sects_df['sect_clean'].astype('string')\n",
        "\n",
        "# corpus_sects_df.loc[:, corpus_sects_df.columns != 'sect_raw']\n",
        "corpus_sects_df.filter(like='_no')\n",
        "# corpus_sects_df.loc[:, ['sect_no', 'sect_no_start', 'sect_no_mid', 'sect_no_end']]\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFDwD6M-kNHH"
      },
      "source": [
        "# Compute Paragraph text Statistics\n",
        "\n",
        "corpus_sects_df['token_len'] = corpus_sects_df['sect_clean'].apply(lambda x: len(x.split()))\n",
        "corpus_sects_df['char_len'] = corpus_sects_df['sect_raw'].apply(lambda x: len(x))\n",
        "corpus_sects_df.loc[:, list(set(corpus_sects_df.columns) - set(['sect_raw', 'sect_clean']))]\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SdZ4e3ALYdD"
      },
      "source": [
        "### **Add Descriptive Statistics and Clean Raw Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWu8_C6PQ4iE"
      },
      "source": [
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7au9Zy2kR0NK"
      },
      "source": [
        "# TODO: Verfiy and eal with any NaN entries\n",
        "#   all Sentences with NaN or '' Raw Text\n",
        "\"\"\"\n",
        "\n",
        "# Sentences\n",
        "# Let's take a look at the updated text\n",
        "corpus_sents_df['sent_clean'] = corpus_sents_df['sent_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Textcorpus_sents_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_sents_df.dropna(how='any', axis=0, subset=['sent_raw'], inplace=True)\n",
        "corpus_sents_df.dropna(how='any', axis=0, subset=['sent_clean'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Sentences:')\n",
        "print('--------------------------------------')\n",
        "corpus_sents_df.head(2)\n",
        "\n",
        "\n",
        "# Paragraphs\n",
        "# Let's take a look at the updated text\n",
        "corpus_parags_df['parag_clean'] = corpus_parags_df['parag_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Text\n",
        "corpus_parags_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_parags_df.dropna(how='any', axis=0, subset=['parag_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Paragraphs:')\n",
        "print('--------------------------------------')\n",
        "corpus_parags_df.head(2)\n",
        "\n",
        "\n",
        "# Sections\n",
        "# Let's take a look at the updated text\n",
        "corpus_sects_df['sect_clean'] = corpus_sects_df['sect_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Text\n",
        "corpus_sects_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_sects_df.dropna(how='any', axis=0, subset=['sect_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Sections:')\n",
        "print('--------------------------------------')\n",
        "# corpus_sects_df.head(2)\n",
        "\n",
        "\n",
        "# Chapters\n",
        "# Let's take a look at the updated text\n",
        "corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Chapters with NaN or '' Raw Text\n",
        "corpus_chaps_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_chaps_df.dropna(how='any', axis=0, subset=['chap_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Chapters:')\n",
        "print('--------------------------------------')\n",
        "# corpus_sects_df.head(2)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf99apfwKPAO"
      },
      "source": [
        "# Verify shapes of all 4 Baseline 4 Models\n",
        "\n",
        "print(f'corpus_sents_df.shape: {corpus_sents_df.shape}')\n",
        "print(f'corpus_parags_df.shape: {corpus_parags_df.shape}')\n",
        "print(f'corpus_sects_df.shape: {corpus_sects_df.shape}')\n",
        "print(f'corpus_chaps_df.shape: {corpus_chaps_df.shape}')\n",
        "\n",
        "\"\"\"\n",
        "SButler Odyssey\n",
        "\n",
        "corpus_sents_df.shape: (2445, 8)\n",
        "corpus_parags_df.shape: (1051, 8)\n",
        "corpus_sects_df.shape: (24, 8)\n",
        "corpus_chaps_df.shape: (24, 8)\n",
        "\"\"\";\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHscLkclSYqN"
      },
      "source": [
        "##**Save Preprocess Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEzo8eltSvWS"
      },
      "source": [
        "# Save Corpus DataFrames\n",
        "\n",
        "save_dataframes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8R_ANtfYG6"
      },
      "source": [
        "# (Optional) EDA Raw Text Features: Interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1njcRD-jgGJh"
      },
      "source": [
        "**(Optional) Can Skip Ahead to: 'EDA of Raw Text and Extracted Features'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ti9jQK7grxO"
      },
      "source": [
        "# Review Cleaned Up Sentences\n",
        "\n",
        "corpus_sents_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TN5ksy55kXy"
      },
      "source": [
        "# Summary Statistics\n",
        "\n",
        "corpus_sents_df.describe()\n",
        "corpus_sents_df['token_len'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxvxkbUGzkfy"
      },
      "source": [
        "# Create histogram of Paragraph lengths\n",
        "\n",
        "sns.histplot(data=corpus_sents_df['char_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Lengths');\n",
        "\n",
        "if (PLOT_OUTPUT == 'All'):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'hist_paraglen.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqUtGz8UjY2b"
      },
      "source": [
        "# Plot histogram of Sentence lengths\n",
        "\n",
        "sns.histplot(data=corpus_sents_df['token_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Lengths')\n",
        "\n",
        "if (PLOT_OUTPUT == 'All'):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'hist_sentlen.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OoBrucYR9Xc"
      },
      "source": [
        "# SELECT CORPUS TYPE\n",
        "# TODO: Customized Preprocessing (e.g. Tweets) by Corpus Type\n",
        "\n",
        "# Novel, Tweets, Chat Transcript\n",
        "\n",
        "# Processing Options\n",
        "\n",
        "# Apply first level cleaning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2tIua7tTSRz"
      },
      "source": [
        "# (Optional) Manually Create Sentiment Arc Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU11XOZIPalR"
      },
      "source": [
        "***Can skip to Section [Load Sentiment Polarities...] or [Calculate VADER...]***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cke3OowdWk6I"
      },
      "source": [
        "**Interactively Enter Cruxes and Edge Cases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv7Foe3oTMmz"
      },
      "source": [
        "# Setup data structures for endpoints of Sentiment Time Series\n",
        "\n",
        "#   [-1.0 to +1.0] = [v.neg, neg, neutral, pos, v.pos]\n",
        "\n",
        "corpus_man_crux_ols = []  # working datastructure to dynamically build ordered list of manually selected Crux Points\n",
        "corpus_man_cruxes_odt = OrderedDict() # Once all manual Crux points selected, this will be working data structure\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0] - 1\n",
        "\n",
        "corpus_parags_len = corpus_sents_df.parag_no.max() # make sure no omissions/repeats/skips\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-_ylgiAc6Aw"
      },
      "source": [
        "# <INPUT> Set the Begining and Ending Sentiment Values (Manual Versions)\n",
        "\n",
        "# Start of Corpus Sentiment Analysis Time Series\n",
        "Corpus_Starting_Sentiment = -0.1 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "# corpus_sa_begin = Corpus_Starting_Sentiment\n",
        "\n",
        "# End of Corpus Sentiment Analysis Time Series\n",
        "Corpus_Ending_Sentiment = -1 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "# corpus_sa_end = Corpus_Ending_Sentiment\n",
        "\n",
        "corpus_man_crux_ols = [tuple((0, Corpus_Starting_Sentiment)), tuple((corpus_sents_len, Corpus_Ending_Sentiment))]\n",
        "# corpus_man_cruxes_dt[0.] = corpus_sa_begin\n",
        "# corpus_man_cruxes_dt[float(corpus_sents_len)] = corpus_sa_end\n",
        "\n",
        "print(f'Manual Cruxes with Start/End: {corpus_man_crux_ols}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmcLrzyUw93d"
      },
      "source": [
        "**Seach for Key Words that suggest Min/Max Sentiment Crux**\n",
        "* Specific to the Novel: Introduction of Pivotal Character, Scene, Factual Reveal, McGuffin, etc...\n",
        "* General to Events/Themes: Death, Birth, Fight, Accident, Money, Sex, etc... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UESgwwIAY28T"
      },
      "source": [
        "# <INPUT> Search Corpus for Line No of Peaks/Valleys\n",
        "# TODO: Better Vis\n",
        "Search_String = \"Death\" #@param {type:\"string\"}\n",
        "if (Search_String == \"\"):\n",
        "  search_str = \"accident\"\n",
        "else:\n",
        "  search_str = Search_String.lower()\n",
        "\n",
        "# search the list of cleaned paragraphs\n",
        "# results_ls = [x for x in search_match_ls if re.search(subs, x)]\n",
        "\n",
        "# creating and passsing series to new column\n",
        "match_sents_ser = corpus_sents_df[\"sent_clean\"].str.find(search_str)\n",
        "\n",
        "# print(f'Found #{len(match_index>0)} Matches')\n",
        "match_sents_df = corpus_sents_df.loc[match_sents_ser > 0]\n",
        "print(f'Found #{match_sents_df.shape[0]} Matching Sentences')\n",
        "print('------------------------------------')\n",
        "# print(f'  {match_sents_df}')\n",
        "match_sents_df[['sent_no', 'parag_no', 'sent_raw', 'token_len']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oVYBfz6-EVo"
      },
      "source": [
        "**Get Context for Matched Sentence by Retrieving Surrounding Paragraph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AvYQqT5TsdV"
      },
      "source": [
        "# Extract Surrounding Paragraphs for context on matching Sentences\n",
        "\n",
        "def get_parag4sentno(asent_no):\n",
        "  '''\n",
        "  Return the original raw paragraph containing a \n",
        "  given sentence number.\n",
        "  '''\n",
        "  # parag_df = pd.DataFrame()\n",
        "  # print(f'Passed in sent_no: {asent_no}')\n",
        "  aparag_no = int(corpus_sents_df.loc[corpus_sents_df['sent_no'] == asent_no]['parag_no'])\n",
        "  # print(f'  This sent_no {asent_no} is in parag_no: {aparag_no}')\n",
        "  aparag_str = corpus_sents_df.loc[corpus_sents_df['parag_no'] == aparag_no]['sent_raw'].str.cat() # ['sent_clean']\n",
        "  # sentno_parag_df = corpus_sents_df[corpus_sents_df['sent_no']==asent_no]\n",
        "  # print(f'Sent #{asent_no} is in the paragraph: ')\n",
        "  # print(aparag)\n",
        "  # print(f'returning aparag_no: [{aparag_no}]: {aparag}')\n",
        "  return aparag_no, aparag_str\n",
        "\n",
        "'''\n",
        "# Testing\n",
        "asent_no = 7\n",
        "print(f'Searching for paragraph containing Sentence #{asent_no}')\n",
        "\n",
        "aparag_no, aparag_str = get_parag4sentno(asent_no)\n",
        "print(f'\\n  Found in Paragraph #{aparag_no} \\n\\n{aparag_str}')\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnHz08NyDroK"
      },
      "source": [
        "# Extract Surrounding Paragraphs for context on matching Sentences\n",
        "\n",
        "def get_parag_str(aparag_no):\n",
        "  '''\n",
        "  Return the original raw paragraph containing a \n",
        "  given sentence number.\n",
        "  '''\n",
        "  # parag_df = pd.DataFrame()\n",
        "  # print(f'Passed in sent_no: {asent_no}')\n",
        "  # aparag_no = int(corpus_sents_df.loc[corpus_sents_df['sent_no'] == asent_no]['parag_no'])\n",
        "  # print(f'  This sent_no {asent_no} is in parag_no: {aparag_no}')\n",
        "  aparag_str = corpus_sents_df.loc[corpus_sents_df['parag_no'] == aparag_no]['sent_raw'].str.cat() # ['sent_clean']\n",
        "  # sentno_parag_df = corpus_sents_df[corpus_sents_df['sent_no']==asent_no]\n",
        "  # print(f'Sent #{asent_no} is in the paragraph: ')\n",
        "  # print(aparag)\n",
        "  # print(f'returning aparag_no: [{aparag_no}]: {aparag}')\n",
        "  return aparag_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByKLYZDsK123"
      },
      "source": [
        "# Summarize current status of manually selected Crux Points\n",
        "# TODO:\n",
        "\n",
        "def crux_sum_short():\n",
        "  print(f'\\nOrdered list of all manually selected Crux Points')\n",
        "  print('---------------------------------------')\n",
        "  for i, acrux_tp in enumerate(corpus_man_crux_ols):\n",
        "    asent_no, asent_pol = acrux_tp\n",
        "    asent_str = corpus_sents_df[corpus_sents_df.sent_no==asent_no].sent_raw.str.cat()\n",
        "    # print(f'Type: {type(asent_str)}')\n",
        "    print(f'Sent No {asent_no:4d}: Polarity: {asent_pol}\\nText: {asent_str}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz0FLzt2UeSe"
      },
      "source": [
        "# Summarize current manually selected Crux Points\n",
        "\n",
        "def crux_summary():\n",
        "  print(f'\\nOrdered list of all manually selected Crux Points')\n",
        "  print('---------------------------------------\\n\\n')\n",
        "  for i, acrux_tp in enumerate(corpus_man_crux_ols):\n",
        "    asent_no, asent_pol = acrux_tp\n",
        "    asent_str = corpus_sents_df[corpus_sents_df.sent_no==asent_no].sent_raw.str.cat()\n",
        "    # print(f'Type: {type(asent_str)}')\n",
        "    print(f'Sent No {asent_no:4d}: Polarity: {asent_pol}')\n",
        "    print('------------------------------')\n",
        "    print(f'Text: {asent_str}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiUmseNw3CkN"
      },
      "source": [
        "# View the Paragraph containing your Matching Sentence:\n",
        "\n",
        "def get_nparags_context(crux_sent_no, parag_ct):\n",
        "\n",
        "  parag_win = int(parag_ct)\n",
        "  parag_crux_str = ''\n",
        "\n",
        "  parag_crux_no = 0\n",
        "\n",
        "  if (crux_sent_no < 0) | (crux_sent_no > corpus_sents_len):\n",
        "    print(f'ERROR: Pick a Sentence No between 0-{corpus_sents_len-1}')\n",
        "  else:\n",
        "    # get_sent_no = crux_sent_no\n",
        "    # print(f'Retrieving Sentence No: {get_sent_no}')\n",
        "    # print('----------')\n",
        "\n",
        "    parag_crux_no, aparag_str = get_parag4sentno(crux_sent_no)\n",
        "    if parag_win == 1:\n",
        "      print(f'Match #{i}: Sentence No. {asent_no} found in Paragraph No. {parag_crux_no}')\n",
        "      print('----------------------------')\n",
        "      print(f'Sentence:\\n')\n",
        "      # print(f'     {corpus_sents_df[corpus_sents_df.sent_no == crux_sent_no]}\\n\\n')\n",
        "      corpus_sents_df[corpus_sents_df.sent_no == crux_sent_no]\n",
        "      print('----------------------------')\n",
        "      print(f'Paragraph Context:\\n')\n",
        "      print(f'     {aparag_str}\\n\\n')\n",
        "    else:\n",
        "      parag_half_win = int((parag_win-1)/2)\n",
        "      parag_start = parag_crux_no - parag_half_win\n",
        "      parag_end = parag_crux_no + parag_half_win\n",
        "      print(f'Retrieving {parag_ct} Contextual Paragraphs Nos {parag_start} to {parag_end}')\n",
        "      print(f'  for Crux Point centered on Sentence No {crux_sent_no}')\n",
        "      for i in range(parag_start, parag_end + 1, 1):\n",
        "        if i == parag_crux_no:\n",
        "          print(f'\\n   ---------------------------------------------------------')\n",
        "          print(f'** Crux Point Paragraph #{i} with Sentence No. {crux_sent_no} **')\n",
        "          print(f'   ---------------------------------------------------------')\n",
        "          parag_crux_str = get_parag_str(i)\n",
        "          print(parag_crux_str)\n",
        "        else:\n",
        "          print(f'\\n   ----------------------')\n",
        "          print(f'   Regular Paragraph #{i}')\n",
        "          print(f'   ----------------------')\n",
        "          print(get_parag_str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjoXC8Fg3lBP"
      },
      "source": [
        "# Insert new crux point into ordered list: corpus_man_crux_ls\n",
        "\n",
        "# NOTE: For very long lists, use Python simple bisect library (at cost of additional dependency)\n",
        "\n",
        "\n",
        "def insert_ord_tp_list(crux_ord_ols, crux_tp):\n",
        "  '''\n",
        "  Insert new crux tuple: crux_tp = (sent_no, sentiment_polarity)\n",
        "  into ordered list of tuples while maintaining sent_no order\n",
        "  '''\n",
        "  sent_no, senti_pol = crux_tp\n",
        "\n",
        "  # Searching for the position\n",
        "  for i in range(len(crux_ord_ols)):\n",
        "    if crux_ord_ols[i][0] == sent_no:\n",
        "      # Attempting to insert duplicate\n",
        "      return crux_ord_ols\n",
        "    elif crux_ord_ols[i][0] < sent_no:\n",
        "      insert_idx = i\n",
        "    else:\n",
        "      break\n",
        "      \n",
        "  # Inserting n in the list\n",
        "  list = crux_ord_ols[:i] + [crux_tp] + crux_ord_ols[i:]\n",
        "  return list\n",
        "\n",
        "'''\n",
        "# Test\n",
        "crux_test_ls = [(1,0), (5,1), (10,-1)]\n",
        "crux_test_tp = (3,10)\n",
        "  \n",
        "print(insert_ord_tp_list(crux_test_ls, crux_test_tp))\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PPOLJDZ1wHc"
      },
      "source": [
        "**Start of Human in the Loop Manual Crux Point Identification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1MLexX57Guc"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "print('Enter a Sentence number based upon your search above to see the ')\n",
        "print('  surrounding Paragraph context.')\n",
        "print('----------------------------------------')\n",
        "print(f'(Enter an integer between 0 and {corpus_sents_len-1})\\n\\n')\n",
        "\n",
        "print('\\n')\n",
        "print('Enter an ODD NUMBER for the Number of surrounding Paragraphs ')\n",
        "print('  around the Sentence No to give Context.')\n",
        "print('----------------------------------------')\n",
        "print(f'(Enter an integer: 3, 5, 7\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exxWtaUPC30l"
      },
      "source": [
        "# Input your Context Retrieval Parameters\n",
        "\n",
        "Sentence_No =  2692#@param {type:\"integer\"}\n",
        "No_Paragraphs_Context = \"3\" #@param [\"1\", \"3\", \"5\"]\n",
        "\n",
        "get_nparags_context(Sentence_No, No_Paragraphs_Context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30fBikIrrgMe"
      },
      "source": [
        "**Add Crux to Manually Generated Sentiment Arc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgljeT6ypNbo"
      },
      "source": [
        "# Instructions to add current sentence as a Crux Point\n",
        "\n",
        "crux_summary()\n",
        "\n",
        "print('--------------------------------------------------------')\n",
        "print(f'INSTRUCTIONS To current Sentence No: {Sentence_No} as a Crux Point')\n",
        "print('--------------------------------------------------------')\n",
        "\n",
        "print(\"\\nCheck this box if you want to add the Sentence/Paragraph above \")\n",
        "print(\"  as a new Min/Max Crux Point with your approximation \")\n",
        "print(\"  for a Sentiment Polarity value between -1.0 to +1.0\\n\\n\")\n",
        "\n",
        "print(f\"Crux Sentence No: {Sentence_No} in Paragraph No: {parag_crux_no}\\n\")\n",
        "print(parag_crux_str)\n",
        "\n",
        "print(\"\\n\\nLeave Add_Sentence_Crux 'unchecked' to not add\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia1IXChRmyr3"
      },
      "source": [
        "# <INPUT> Option to add this Sentence/Paragraph as a Min/Max Crux Point\n",
        "\n",
        "Sentiment_Polarity = -0.4 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "Add_Sentence_Crux = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "# Add Crux if selected and give current summary status\n",
        "\n",
        "if Add_Sentence_Crux == True:\n",
        "  crux_new_tp = tuple((Sentence_No, Sentiment_Polarity))\n",
        "  corpus_man_crux_ols = insert_ord_tp_list(corpus_man_crux_ols, crux_new_tp)\n",
        "  if (corpus_man_crux_ols):\n",
        "    print(f'Successfully inserted new Crux = {crux_new_tp}')\n",
        "    print(f'Added Crux at Sentence No={Sentence_No} with Polarity={Sentiment_Polarity}')\n",
        "    # corpus_man_cruxes_dt[Sentence_No] = Sentiment_Polarity\n",
        "  else:\n",
        "    print(f'ERROR: Could not insert new Crux = {crux_new_tp}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lps67T-lUTi2"
      },
      "source": [
        "# Summary of current Crux Points after addition\n",
        "\n",
        "print('\\n------------------------------------------------------------')\n",
        "print(f'After addition of new Crux Point (Sentence No {Sentence_No})')\n",
        "print('------------------------------------------------------------\\n')\n",
        "\n",
        "crux_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjZT9_0CrzFk"
      },
      "source": [
        "**Delete Manually Selected Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTX30nokNPBp"
      },
      "source": [
        "len(corpus_man_crux_ols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxS-J_hPOfkI"
      },
      "source": [
        "crux_tp = (1, 2)\n",
        "a, b = crux_tp\n",
        "print(f'a is {a} and b is {b}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi7Cu1iErvzc"
      },
      "source": [
        "# Insert new crux point into ordered list: corpus_man_crux_ls\n",
        "\n",
        "# FIX: 20210616 and move to utility functions\n",
        "\n",
        "# NOTE: For very long lists, use Python simple bisect library (at cost of additional dependency)\n",
        "\n",
        "\n",
        "def del_ord_tp_list(acorpus_man_crux_ols, crux_tp):\n",
        "  '''\n",
        "  Insert new crux tuple: crux_tp = (sent_no, sentiment_polarity)\n",
        "  into ordered list of tuples while maintaining sent_no order\n",
        "  '''\n",
        "  crux_ct = len(acorpus_man_crux_ols)\n",
        "  sent_no = crux_tp[0]\n",
        "  print(f'Deleting sent_no: {sent_no} over crux_ls len={len(acorpus_man_crux_ols)}')\n",
        "\n",
        "  # Searching for the positionk\n",
        "  del_idx = -1\n",
        "  for i in range(len(acorpus_man_crux_ols)):\n",
        "    acrux_sent_no = acorpus_man_crux_ols[i][0]\n",
        "    print(f'Crux #{i} is sent_no={acrux_sent_no}')\n",
        "    if acrux_sent_no == sent_no:\n",
        "      print(f'Matching index at {i}')\n",
        "      del_idx = i\n",
        "      \n",
        "  # Delete n in the list\n",
        "  print(f'Deletion index = {del_idx}')\n",
        "  if del_idx == 0:\n",
        "    # Delete the first Crux\n",
        "    list = acorpus_man_crux_ols[1:]\n",
        "    return list\n",
        "  elif del_idx == crux_ct -1:\n",
        "    # Delete the last Crux\n",
        "    list = acorpus_man_crux_ols[:-1]\n",
        "    return list    \n",
        "  elif (del_idx > 0) & (del_idx < crux_ct):\n",
        "    # Delete an interior Crux\n",
        "    before_idx = i - 1\n",
        "    after_idx = i\n",
        "    list = acorpus_man_crux_ols[:before_idx] + acorpus_man_crux_ols[after_idx:]\n",
        "    print(f'Returning list: {list}')\n",
        "    return list\n",
        "  else:\n",
        "    print('No matching Crux tuple found')\n",
        "    return acorpus_man_crux_ols\n",
        "  \n",
        "\n",
        "# Test\n",
        "crux_test_ls = [(1,0), (5,1), (10,-1)]\n",
        "crux_test_tp = (5,5)\n",
        "  \n",
        "print(del_ord_tp_list(crux_test_ls, crux_test_tp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI5hZqmSLg5R"
      },
      "source": [
        "# Instructions to Delete a Crux Point\n",
        "\n",
        "crux_summary()\n",
        "\n",
        "print('--------------------------------------------------------')\n",
        "print('INSTRUCTIONS To Delete a Crux Point')\n",
        "print('--------------------------------------------------------')\n",
        "\n",
        "print(\"\\nEnter the Sentence No of a Crux you want to delete.\\n\")\n",
        "print(f'     Current Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n",
        "print(\" Skip this if you want to keep all manually selected Crux Points.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzowFY-I8U1_"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "print(\"\\nEnter the Sentence No of a Crux you want to delete.\\n\")\n",
        "print(f'     Current Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n",
        "print(\" Skip this if you want to keep all manually selected Crux Points.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ShFCfOsrvsv"
      },
      "source": [
        "Delete_Sent_No =  777#@param {type:\"integer\"}\n",
        "# Select a Crux to Delete\n",
        "# TODO: Drop down list\n",
        "\n",
        "# corpus_man_crux_ols\n",
        "corpus_man_crux_temp_ols = []\n",
        "\n",
        "crux_sent_set = set([x[0] for x in corpus_man_crux_ols])\n",
        "if not(Delete_Sent_No in crux_sent_set):\n",
        "  print(f'ERROR: {Delete_Sent_No} is not a Crux Point Sentence No')\n",
        "else:\n",
        "  # Keep the same tuple format for uniformity and future features\n",
        "  crux_del_tp = tuple((Delete_Sent_No, 'dummy_sentence'))\n",
        "  print(f'Selected {(crux_del_tp)} to delete')\n",
        "  # corpus_man_crux_temp_ols = \n",
        "  print(f'WTF: {del_ord_tp_list(corpus_man_crux_ols, crux_del_tp)}')\n",
        "  corpus_man_crux_old = del_ord_tp_list(corpus_man_crux_ols, crux_del_tp)\n",
        "  print(f\"corpus_man_crux_ols: {corpus_man_crux_ols}\")\n",
        "  # get_sent_no = Sentence_No\n",
        "  # print(f'Retrieving Sentence No: {get_sent_no}')\n",
        "  # print('----------')\n",
        "  print(f'Updated Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJdNu-oYrwHv"
      },
      "source": [
        "**Review Summary of all Manually Selected Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmpggAWuvMb1"
      },
      "source": [
        "# Generate Report Summary of All Manually Selected Cruxes\n",
        "\n",
        "f = io.StringIO()\n",
        "with contextlib.redirect_stdout(f):\n",
        "    crux_summary()\n",
        "crux_summary = f.getvalue()\n",
        "\n",
        "# Print Manual Crux Report Summary to Screen\n",
        "\n",
        "# print(crux_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ULFfHbxwDfu"
      },
      "source": [
        "# Save Manual Crux Summary Report\n",
        "\n",
        "plot_filename = 'man_cruxes.txt'\n",
        "plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "\n",
        "with open(plotpathfilename_str, 'a+') as outfp:\n",
        "  outfp.write(crux_summary)\n",
        "\n",
        "# Verify \n",
        "\n",
        "!ls -alt $plotpathfilename_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG_hGPgjxY7X"
      },
      "source": [
        "# Verify Report Content\n",
        "\n",
        "!cat man_cruxes_fscottfitzgerald_thegreatgatsby_20210616214050.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zPYRKCex8-U"
      },
      "source": [
        "**Clean and Organize Manual Crux Points into new Data Structures**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YICJNt3AXI2d"
      },
      "source": [
        "print(corpus_sents_df[corpus_sents_df['sent_no']==5]['sent_raw'].squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2bLtpweWQt4"
      },
      "source": [
        "# Convert and assemble all the Crux values in lists to save in a new Crux DataFrame\n",
        "\n",
        "\n",
        "pol_val_ls = [x[1] for x in corpus_man_crux_ols]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "parag_no_ls = [(get_parag4sentno(x))[0] for x in sent_no_ls]\n",
        "parag_str_ls = [get_parag_str(x) for x in parag_no_ls]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "sent_raw_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_raw'].squeeze() for x in sent_no_ls]\n",
        "sent_raw_ls\n",
        "sent_clean_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_clean'].squeeze() for x in sent_no_ls]\n",
        "sent_clean_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFEYg7Gfbj9i"
      },
      "source": [
        "# Create a Dict of Crux Points to Tuples (Polarity, Raw Sentence)\n",
        "\n",
        "# First Create the Tuples for each Sentence No (Sentiment Polarity, Raw Text)\n",
        "def merge(list1, list2):\n",
        "    merged_list = tuple(zip(list1, list2)) \n",
        "    return merged_list\n",
        "      \n",
        "crux_tp_ls = merge(pol_val_ls, sent_raw_ls)\n",
        "\n",
        "# Second, Create the Dictionary C\n",
        "corpus_man_cruxes_dt = {sent_no_ls[i]: crux_tp_ls[i] for i in range(len(crux_tp_ls))}\n",
        "\n",
        "# Verify\n",
        "corpus_man_cruxes_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtIHecpMdbXa"
      },
      "source": [
        "corpus_sents_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8qmpFRaTfu2"
      },
      "source": [
        "**Plot Interpolated Manual Sentiment Arc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ASg7uUZWf3"
      },
      "source": [
        "corpus_man_sa_df = pd.DataFrame({'sent_no':xn, 'sentiment':yn, 'sent_raw':corpus_sents_df.sent_raw.values})\n",
        "corpus_man_sa_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36VfGi4a47Yl"
      },
      "source": [
        "# Hermite Interpolation with SciPy\n",
        "\n",
        "pol_val_ls = [x[1] for x in corpus_man_crux_ols]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "parag_no_ls = [(get_parag4sentno(x))[0] for x in sent_no_ls]\n",
        "parag_str_ls = [get_parag_str(x) for x in parag_no_ls]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "sent_raw_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_raw'].squeeze() for x in sent_no_ls]\n",
        "sent_raw_ls\n",
        "sent_clean_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_clean'].squeeze() for x in sent_no_ls]\n",
        "sent_clean_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElJKawWfZWbv"
      },
      "source": [
        "sent_no_ls\n",
        "pol_val_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xufA403s3lbF"
      },
      "source": [
        "corpus_man_crux_np = np.asarray(corpus_man_crux_ols)\n",
        "corpus_man_crux_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdgKbjpi63a9"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMkgU-JhwudG"
      },
      "source": [
        "x2 = np.array(sent_no_ls)\n",
        "y2 = np.array(pol_val_ls)\n",
        "\n",
        "xn = np.linspace(0, corpus_sents_len, corpus_sents_len)\n",
        "yn = interpolate.pchip_interpolate(x2, y2, xn)\n",
        "\n",
        "crux_man_df = pd.DataFrame(\n",
        "    {'sent_no': sent_no_ls,\n",
        "     'pol_val': pol_val_ls\n",
        "     }\n",
        ")\n",
        "\n",
        "# plt.plot(x2, y2, 'ok', label='True values')\n",
        "# plt.plot(xn, yn, label='Hermite Interpolation')\n",
        "\n",
        "# plt.plot(xn, yn4, label='Spline order 4')\n",
        "# plt.plot(xn, yn5, label='Spline order 5')\n",
        "# plt.plot(xn, yn6, label='Spline order 6')\n",
        "# plt.plot(xn, yn7, label='Spline order 7')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# sns.histplot(data=corpus_sents_df['char_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Lengths')\n",
        "sns.histplot(data=crux_man_df, x='sent_no', y='pol_val', kde=True).set_title(f'{CORPUS_FULL} \\n Manual Cruxes with Hermite Smoothing')\n",
        "\n",
        "\n",
        "# Save graph to file.\n",
        "plot_filename = 'man_crux_plot.png'\n",
        "plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "plt.savefig(plotpathfilename_str, format='png', dpi=300)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW-VkS2s_ogy"
      },
      "source": [
        "**Gaussian Process Regression**\n",
        "\n",
        "* https://blog.dominodatalab.com/fitting-gaussian-process-models-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLFFYDfYpPjn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0BukxX9ZKYA"
      },
      "source": [
        "# (Optional) Load Sentiment Polarities: Interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhPJ9e-V9NMu"
      },
      "source": [
        "***If you upload a file of Sentiment Values you don't have to Calculate them in the following sections***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpx7viqf9AAJ"
      },
      "source": [
        "!ls -altr *.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw8_mUHI9sdr"
      },
      "source": [
        "# Test\n",
        "\n",
        "files.download('sentiments_raw_all_virginiawoolf_tothelighthouse_20210618161224.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90axHClm9cGZ"
      },
      "source": [
        "# Upload your precomputed Sentiment Values\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02NH5nnto2AC"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL9JCyCI-Sic"
      },
      "source": [
        "# Verify the file was uploaded correctly\n",
        "\n",
        "newest_csvfile = get_recentfile().split('/')[-1]\n",
        "print(f'The most recently updated *.csv file is: {newest_csvfile}')\n",
        "\n",
        "!head -n 10 $newest_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTfBj8qdi_vZ"
      },
      "source": [
        "%whos DataFrame\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgjz-E-mYgzI"
      },
      "source": [
        "# Upload file into DataFrame\n",
        "\n",
        "corpus_test_df = pd.read_csv(newest_csvfile)\n",
        "corpus_test_df['sent_clean'] = corpus_test_df['sent_clean'].astype('string')\n",
        "corpus_test_df['sent_raw'] = corpus_test_df['sent_raw'].astype('string')\n",
        "corpus_test_df.head()\n",
        "corpus_test_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ajyb8Jpmzw"
      },
      "source": [
        "***Skip to Section <Calculate Median of All...> if SA Loaded***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGA6sQy6RPDC"
      },
      "source": [
        "corpus_lexicons_stats_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7CfH00OFkQV"
      },
      "source": [
        "# **Either (a) Load Precomputed Sentiment Series or (b) Calculate Sentiment Values**\n",
        "\n",
        "Sentiment Models\n",
        "\n",
        "* VADER [-1.0 to 1.0] zero peak\n",
        "* TextBlob [-1.0 to 1.0] zero peak\n",
        "* Stanza outliers [-1.0 to 199.0] pos, outliers(+peak)\n",
        "* AFINN [-14 (-8 to 8) 20] discrete\n",
        "* SentimentR 11,710 [-5.4 to 8.8] norm\n",
        "* Syuzhet [-5.4 to 8.8] norm\n",
        "* Bing [-100.0 (-20.0 to 20.0) 100] discrete, outliers\n",
        "* Pattern [-1.0 to 1.0] norm\n",
        "* SentiWord [-3.8 to 4.4] norm\n",
        "* SenticNet [-3.8 to 10] norm\n",
        "* NRC [-100.0 (-5.0 to 5.0) 100] zero, outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ueo8fTSqqaak"
      },
      "source": [
        "## **(a) Load Previously Computed Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEq8eWslgu28"
      },
      "source": [
        "!ls -altr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RH8TY-LgCVf"
      },
      "source": [
        "### **Baseline 12 Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVX1gXQajWNG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "709642ac-6b34-461a-cec2-657cc75a99f1"
      },
      "source": [
        "# ARCHIVE: Discontinue use of redundant corpus_baseline_df (just use corpus_sents_df)\n",
        "\n",
        "# Baseline 12 Models: Read Computed sentiment data saved from previous run of this notebook\n",
        "corpus_sents_df = pd.read_csv('corpus_sents_baselines_vwoolf_tothelighthouse.csv')\n",
        "corpus_parags_df = pd.read_csv('sum_sentiments_parags_baselines_vwoolf_tothelighthouse.csv')\n",
        "corpus_sects_df = pd.read_csv('sum_sentiments_sects_baselines_vwoolf_tothelighthouse.csv')\n",
        "corpus_chaps_df = pd.read_csv('sum_sentiments_chaps_baselines_tothelighthouse.csv')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "corpus_baseline_df = pd.read_csv('sum_sentiments_sents_baselines_hsbutler_theodyssey.csv')\n",
        "corpus_baseline_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "corpus_baseline_df['sent_raw'] = corpus_baseline_df['sent_raw'].astype('string')\n",
        "corpus_baseline_df['sent_clean'] = corpus_baseline_df['sent_clean'].astype('string')\n",
        "\n",
        "corpus_baseline_df.info()\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vfl-NIhZeNzT",
        "outputId": "15fba887-bfc8-413d-c3cd-4f9b02da7489"
      },
      "source": [
        "# Clean Sentences DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_sents_df.columns:\n",
        "  corpus_sents_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  \n",
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sect_no</th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>sent_clean</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>sentimentr</th>\n",
              "      <th>sentimentr_stdscaler</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_stdscaler</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>syuzhet</th>\n",
              "      <th>syuzhet_stdscaler</th>\n",
              "      <th>syuzhet_medianiqr</th>\n",
              "      <th>syuzhet_lnorm_stdscaler</th>\n",
              "      <th>syuzhet_lnorm_medianiqr</th>\n",
              "      <th>bing</th>\n",
              "      <th>bing_stdscaler</th>\n",
              "      <th>bing_medianiqr</th>\n",
              "      <th>bing_lnorm_stdscaler</th>\n",
              "      <th>bing_lnorm_medianiqr</th>\n",
              "      <th>sentiword</th>\n",
              "      <th>sentiword_stdscaler</th>\n",
              "      <th>sentiword_medianiqr</th>\n",
              "      <th>sentiword_lnorm_stdscaler</th>\n",
              "      <th>sentiword_lnorm_medianiqr</th>\n",
              "      <th>senticnet</th>\n",
              "      <th>senticnet_stdscaler</th>\n",
              "      <th>senticnet_medianiqr</th>\n",
              "      <th>senticnet_lnorm_stdscaler</th>\n",
              "      <th>senticnet_lnorm_medianiqr</th>\n",
              "      <th>nrc</th>\n",
              "      <th>nrc_stdscaler</th>\n",
              "      <th>nrc_medianiqr</th>\n",
              "      <th>nrc_lnorm_stdscaler</th>\n",
              "      <th>nrc_lnorm_medianiqr</th>\n",
              "      <th>afinn</th>\n",
              "      <th>afinn_stdscaler</th>\n",
              "      <th>afinn_medianiqr</th>\n",
              "      <th>afinn_lnorm_stdscaler</th>\n",
              "      <th>afinn_lnorm_medianiqr</th>\n",
              "      <th>scores</th>\n",
              "      <th>vader</th>\n",
              "      <th>vader_stdscaler</th>\n",
              "      <th>vader_medianiqr</th>\n",
              "      <th>vader_lnorm_stdscaler</th>\n",
              "      <th>vader_lnorm_medianiqr</th>\n",
              "      <th>textblob</th>\n",
              "      <th>textblob_stdscaler</th>\n",
              "      <th>textblob_medianiqr</th>\n",
              "      <th>textblob_lnorm_stdscaler</th>\n",
              "      <th>textblob_lnorm_medianiqr</th>\n",
              "      <th>pattern</th>\n",
              "      <th>pattern_stdscaler</th>\n",
              "      <th>pattern_medianiqr</th>\n",
              "      <th>pattern_lnorm_stdscaler</th>\n",
              "      <th>pattern_lnorm_medianiqr</th>\n",
              "      <th>stanza</th>\n",
              "      <th>stanza_stdscaler</th>\n",
              "      <th>stanza_medianiqr</th>\n",
              "      <th>stanza_lnorm_stdscaler</th>\n",
              "      <th>stanza_lnorm_medianiqr</th>\n",
              "      <th>flair</th>\n",
              "      <th>flair_stdscaler</th>\n",
              "      <th>flair_medianiqr</th>\n",
              "      <th>flair_lnorm_stdscaler</th>\n",
              "      <th>flair_lnorm_medianiqr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\"Yes, of course, if it's fine tomorrow,\" said ...</td>\n",
              "      <td>yes  of course  if it is fine tomorrow   said...</td>\n",
              "      <td>58</td>\n",
              "      <td>10</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.923508</td>\n",
              "      <td>1.076923</td>\n",
              "      <td>0.923508</td>\n",
              "      <td>2.316794</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.937938</td>\n",
              "      <td>1.3125</td>\n",
              "      <td>0.937938</td>\n",
              "      <td>2.433944</td>\n",
              "      <td>29.429983</td>\n",
              "      <td>0.985287</td>\n",
              "      <td>1.536056</td>\n",
              "      <td>0.985287</td>\n",
              "      <td>10.913232</td>\n",
              "      <td>0.25000</td>\n",
              "      <td>0.228119</td>\n",
              "      <td>0.377358</td>\n",
              "      <td>0.228119</td>\n",
              "      <td>0.610906</td>\n",
              "      <td>-0.702</td>\n",
              "      <td>-0.782225</td>\n",
              "      <td>-0.623452</td>\n",
              "      <td>-0.782225</td>\n",
              "      <td>-0.970125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.138453</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.138453</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.201011</td>\n",
              "      <td>1.254104</td>\n",
              "      <td>3.289638</td>\n",
              "      <td>1.254104</td>\n",
              "      <td>10.181998</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'comp...</td>\n",
              "      <td>0.5423</td>\n",
              "      <td>1.213553</td>\n",
              "      <td>1.358297</td>\n",
              "      <td>1.213553</td>\n",
              "      <td>4.016552</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>1.428187</td>\n",
              "      <td>3.120567</td>\n",
              "      <td>1.428187</td>\n",
              "      <td>10.101883</td>\n",
              "      <td>0.173042</td>\n",
              "      <td>0.645640</td>\n",
              "      <td>1.607919</td>\n",
              "      <td>0.645640</td>\n",
              "      <td>4.36339</td>\n",
              "      <td>4.804044</td>\n",
              "      <td>-0.338601</td>\n",
              "      <td>-0.016109</td>\n",
              "      <td>-0.338601</td>\n",
              "      <td>0.544555</td>\n",
              "      <td>0.9820</td>\n",
              "      <td>1.317700</td>\n",
              "      <td>0.947648</td>\n",
              "      <td>1.317700</td>\n",
              "      <td>0.917937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\"But you'll have to be up with the lark,\" she ...</td>\n",
              "      <td>but you will have to be up with the lark   sh...</td>\n",
              "      <td>52</td>\n",
              "      <td>11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.070987</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.070987</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.081466</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.081466</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.34375</td>\n",
              "      <td>-0.591756</td>\n",
              "      <td>-0.518868</td>\n",
              "      <td>-0.591756</td>\n",
              "      <td>-0.763632</td>\n",
              "      <td>0.713</td>\n",
              "      <td>0.235461</td>\n",
              "      <td>0.407138</td>\n",
              "      <td>0.235461</td>\n",
              "      <td>0.604208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.138453</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.138453</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.097142</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.097142</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.153663</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.153663</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.228419</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.228419</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.272384</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.272384</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>4.642334</td>\n",
              "      <td>-0.374651</td>\n",
              "      <td>-0.053533</td>\n",
              "      <td>-0.374651</td>\n",
              "      <td>0.244125</td>\n",
              "      <td>0.9052</td>\n",
              "      <td>1.232525</td>\n",
              "      <td>0.907642</td>\n",
              "      <td>1.232525</td>\n",
              "      <td>0.802564</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sent_no  parag_no  ...  flair_lnorm_stdscaler flair_lnorm_medianiqr\n",
              "0        0         0  ...               1.317700              0.917937\n",
              "1        1         0  ...               1.232525              0.802564\n",
              "\n",
              "[2 rows x 68 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3403 entries, 0 to 3402\n",
            "Data columns (total 68 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   sent_no                     3403 non-null   int64  \n",
            " 1   parag_no                    3403 non-null   int64  \n",
            " 2   sect_no                     3403 non-null   int64  \n",
            " 3   sent_raw                    3403 non-null   object \n",
            " 4   sent_clean                  3403 non-null   object \n",
            " 5   char_len                    3403 non-null   int64  \n",
            " 6   token_len                   3403 non-null   int64  \n",
            " 7   sentimentr                  3403 non-null   float64\n",
            " 8   sentimentr_stdscaler        3403 non-null   float64\n",
            " 9   sentimentr_medianiqr        3403 non-null   float64\n",
            " 10  sentimentr_lnorm_stdscaler  3403 non-null   float64\n",
            " 11  sentimentr_lnorm_medianiqr  3403 non-null   float64\n",
            " 12  syuzhet                     3403 non-null   float64\n",
            " 13  syuzhet_stdscaler           3403 non-null   float64\n",
            " 14  syuzhet_medianiqr           3403 non-null   float64\n",
            " 15  syuzhet_lnorm_stdscaler     3403 non-null   float64\n",
            " 16  syuzhet_lnorm_medianiqr     3403 non-null   float64\n",
            " 17  bing                        3403 non-null   float64\n",
            " 18  bing_stdscaler              3403 non-null   float64\n",
            " 19  bing_medianiqr              3403 non-null   float64\n",
            " 20  bing_lnorm_stdscaler        3403 non-null   float64\n",
            " 21  bing_lnorm_medianiqr        3403 non-null   float64\n",
            " 22  sentiword                   3403 non-null   float64\n",
            " 23  sentiword_stdscaler         3403 non-null   float64\n",
            " 24  sentiword_medianiqr         3403 non-null   float64\n",
            " 25  sentiword_lnorm_stdscaler   3403 non-null   float64\n",
            " 26  sentiword_lnorm_medianiqr   3403 non-null   float64\n",
            " 27  senticnet                   3403 non-null   float64\n",
            " 28  senticnet_stdscaler         3403 non-null   float64\n",
            " 29  senticnet_medianiqr         3403 non-null   float64\n",
            " 30  senticnet_lnorm_stdscaler   3403 non-null   float64\n",
            " 31  senticnet_lnorm_medianiqr   3403 non-null   float64\n",
            " 32  nrc                         3403 non-null   float64\n",
            " 33  nrc_stdscaler               3403 non-null   float64\n",
            " 34  nrc_medianiqr               3403 non-null   float64\n",
            " 35  nrc_lnorm_stdscaler         3403 non-null   float64\n",
            " 36  nrc_lnorm_medianiqr         3403 non-null   float64\n",
            " 37  afinn                       3403 non-null   float64\n",
            " 38  afinn_stdscaler             3403 non-null   float64\n",
            " 39  afinn_medianiqr             3403 non-null   float64\n",
            " 40  afinn_lnorm_stdscaler       3403 non-null   float64\n",
            " 41  afinn_lnorm_medianiqr       3403 non-null   float64\n",
            " 42  scores                      3403 non-null   object \n",
            " 43  vader                       3403 non-null   float64\n",
            " 44  vader_stdscaler             3403 non-null   float64\n",
            " 45  vader_medianiqr             3403 non-null   float64\n",
            " 46  vader_lnorm_stdscaler       3403 non-null   float64\n",
            " 47  vader_lnorm_medianiqr       3403 non-null   float64\n",
            " 48  textblob                    3403 non-null   float64\n",
            " 49  textblob_stdscaler          3403 non-null   float64\n",
            " 50  textblob_medianiqr          3403 non-null   float64\n",
            " 51  textblob_lnorm_stdscaler    3403 non-null   float64\n",
            " 52  textblob_lnorm_medianiqr    3403 non-null   float64\n",
            " 53  pattern                     3403 non-null   float64\n",
            " 54  pattern_stdscaler           3403 non-null   float64\n",
            " 55  pattern_medianiqr           3403 non-null   float64\n",
            " 56  pattern_lnorm_stdscaler     3403 non-null   float64\n",
            " 57  pattern_lnorm_medianiqr     3403 non-null   float64\n",
            " 58  stanza                      3403 non-null   float64\n",
            " 59  stanza_stdscaler            3403 non-null   float64\n",
            " 60  stanza_medianiqr            3403 non-null   float64\n",
            " 61  stanza_lnorm_stdscaler      3403 non-null   float64\n",
            " 62  stanza_lnorm_medianiqr      3403 non-null   float64\n",
            " 63  flair                       3403 non-null   float64\n",
            " 64  flair_stdscaler             3403 non-null   float64\n",
            " 65  flair_medianiqr             3403 non-null   float64\n",
            " 66  flair_lnorm_stdscaler       3403 non-null   float64\n",
            " 67  flair_lnorm_medianiqr       3403 non-null   float64\n",
            "dtypes: float64(60), int64(5), object(3)\n",
            "memory usage: 1.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0WnLPxjefkhE",
        "outputId": "70e247db-b667-4856-bbbd-d49f82296aa2"
      },
      "source": [
        "# Clean Paragraphs DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_parags_df.columns:\n",
        "  corpus_parags_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  \n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sect_no</th>\n",
              "      <th>parag_raw</th>\n",
              "      <th>parag_clean</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>sentimentr</th>\n",
              "      <th>sentimentr_stdscaler</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_stdscaler</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>syuzhet</th>\n",
              "      <th>syuzhet_stdscaler</th>\n",
              "      <th>syuzhet_medianiqr</th>\n",
              "      <th>syuzhet_lnorm_stdscaler</th>\n",
              "      <th>syuzhet_lnorm_medianiqr</th>\n",
              "      <th>bing</th>\n",
              "      <th>bing_stdscaler</th>\n",
              "      <th>bing_medianiqr</th>\n",
              "      <th>bing_lnorm_stdscaler</th>\n",
              "      <th>bing_lnorm_medianiqr</th>\n",
              "      <th>sentiword</th>\n",
              "      <th>sentiword_stdscaler</th>\n",
              "      <th>sentiword_medianiqr</th>\n",
              "      <th>sentiword_lnorm_stdscaler</th>\n",
              "      <th>sentiword_lnorm_medianiqr</th>\n",
              "      <th>senticnet</th>\n",
              "      <th>senticnet_stdscaler</th>\n",
              "      <th>senticnet_medianiqr</th>\n",
              "      <th>senticnet_lnorm_stdscaler</th>\n",
              "      <th>senticnet_lnorm_medianiqr</th>\n",
              "      <th>nrc</th>\n",
              "      <th>nrc_stdscaler</th>\n",
              "      <th>nrc_medianiqr</th>\n",
              "      <th>nrc_lnorm_stdscaler</th>\n",
              "      <th>nrc_lnorm_medianiqr</th>\n",
              "      <th>afinn</th>\n",
              "      <th>afinn_stdscaler</th>\n",
              "      <th>afinn_medianiqr</th>\n",
              "      <th>afinn_lnorm_stdscaler</th>\n",
              "      <th>afinn_lnorm_medianiqr</th>\n",
              "      <th>scores</th>\n",
              "      <th>vader</th>\n",
              "      <th>vader_stdscaler</th>\n",
              "      <th>vader_medianiqr</th>\n",
              "      <th>vader_lnorm_stdscaler</th>\n",
              "      <th>vader_lnorm_medianiqr</th>\n",
              "      <th>textblob</th>\n",
              "      <th>textblob_stdscaler</th>\n",
              "      <th>textblob_medianiqr</th>\n",
              "      <th>textblob_lnorm_stdscaler</th>\n",
              "      <th>textblob_lnorm_medianiqr</th>\n",
              "      <th>pattern</th>\n",
              "      <th>pattern_stdscaler</th>\n",
              "      <th>pattern_medianiqr</th>\n",
              "      <th>pattern_lnorm_stdscaler</th>\n",
              "      <th>pattern_lnorm_medianiqr</th>\n",
              "      <th>stanza</th>\n",
              "      <th>stanza_stdscaler</th>\n",
              "      <th>stanza_medianiqr</th>\n",
              "      <th>stanza_lnorm_stdscaler</th>\n",
              "      <th>stanza_lnorm_medianiqr</th>\n",
              "      <th>flair</th>\n",
              "      <th>flair_stdscaler</th>\n",
              "      <th>flair_medianiqr</th>\n",
              "      <th>flair_lnorm_stdscaler</th>\n",
              "      <th>flair_lnorm_medianiqr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\"Yes, of course, if it's fine tomorrow,\" said ...</td>\n",
              "      <td>yes  of course  if it is fine tomorrow   said...</td>\n",
              "      <td>111</td>\n",
              "      <td>21</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.152527</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>0.152527</td>\n",
              "      <td>1.479757</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.135961</td>\n",
              "      <td>0.225806</td>\n",
              "      <td>0.135961</td>\n",
              "      <td>1.431332</td>\n",
              "      <td>24.180907</td>\n",
              "      <td>0.460669</td>\n",
              "      <td>0.383272</td>\n",
              "      <td>0.460669</td>\n",
              "      <td>2.064597</td>\n",
              "      <td>-0.09375</td>\n",
              "      <td>-0.306765</td>\n",
              "      <td>-0.187793</td>\n",
              "      <td>-0.306765</td>\n",
              "      <td>-0.357505</td>\n",
              "      <td>0.011</td>\n",
              "      <td>-0.588253</td>\n",
              "      <td>-0.33606</td>\n",
              "      <td>-0.588253</td>\n",
              "      <td>-0.446880</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.330163</td>\n",
              "      <td>-0.253389</td>\n",
              "      <td>-0.330163</td>\n",
              "      <td>-0.189939</td>\n",
              "      <td>0.927215</td>\n",
              "      <td>0.330868</td>\n",
              "      <td>0.448610</td>\n",
              "      <td>0.330868</td>\n",
              "      <td>3.385920</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.866, 'pos': 0.134, 'comp...</td>\n",
              "      <td>0.3071</td>\n",
              "      <td>0.124453</td>\n",
              "      <td>-0.029760</td>\n",
              "      <td>0.124453</td>\n",
              "      <td>1.518997</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>1.798513</td>\n",
              "      <td>1.856515</td>\n",
              "      <td>1.798513</td>\n",
              "      <td>13.520357</td>\n",
              "      <td>0.132465</td>\n",
              "      <td>-0.241890</td>\n",
              "      <td>-0.027664</td>\n",
              "      <td>-0.241890</td>\n",
              "      <td>1.494511</td>\n",
              "      <td>7.417723</td>\n",
              "      <td>-0.936427</td>\n",
              "      <td>-0.561232</td>\n",
              "      <td>-0.936427</td>\n",
              "      <td>1.875289</td>\n",
              "      <td>0.9752</td>\n",
              "      <td>1.550735</td>\n",
              "      <td>1.077276</td>\n",
              "      <td>1.550735</td>\n",
              "      <td>3.287672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>To her son these words conveyed an extraordina...</td>\n",
              "      <td>to her son these words conveyed an extraordina...</td>\n",
              "      <td>1517</td>\n",
              "      <td>258</td>\n",
              "      <td>7.60</td>\n",
              "      <td>2.159185</td>\n",
              "      <td>2.534483</td>\n",
              "      <td>2.159185</td>\n",
              "      <td>0.836448</td>\n",
              "      <td>6.60</td>\n",
              "      <td>1.866646</td>\n",
              "      <td>2.016129</td>\n",
              "      <td>1.866646</td>\n",
              "      <td>0.682967</td>\n",
              "      <td>76.256384</td>\n",
              "      <td>1.321976</td>\n",
              "      <td>1.208679</td>\n",
              "      <td>1.321976</td>\n",
              "      <td>0.529954</td>\n",
              "      <td>0.90625</td>\n",
              "      <td>0.133308</td>\n",
              "      <td>0.262911</td>\n",
              "      <td>0.133308</td>\n",
              "      <td>-0.011668</td>\n",
              "      <td>13.999</td>\n",
              "      <td>2.412627</td>\n",
              "      <td>2.67535</td>\n",
              "      <td>2.412627</td>\n",
              "      <td>0.911565</td>\n",
              "      <td>12.856091</td>\n",
              "      <td>0.180823</td>\n",
              "      <td>0.225195</td>\n",
              "      <td>0.180823</td>\n",
              "      <td>0.009366</td>\n",
              "      <td>2.474885</td>\n",
              "      <td>1.384653</td>\n",
              "      <td>1.510809</td>\n",
              "      <td>1.384653</td>\n",
              "      <td>0.589919</td>\n",
              "      <td>{'neg': 0.067, 'neu': 0.804, 'pos': 0.129, 'co...</td>\n",
              "      <td>0.9607</td>\n",
              "      <td>1.134361</td>\n",
              "      <td>0.561465</td>\n",
              "      <td>1.134361</td>\n",
              "      <td>0.182824</td>\n",
              "      <td>0.150317</td>\n",
              "      <td>0.322447</td>\n",
              "      <td>0.376798</td>\n",
              "      <td>0.322447</td>\n",
              "      <td>0.131057</td>\n",
              "      <td>0.782783</td>\n",
              "      <td>1.580919</td>\n",
              "      <td>1.643233</td>\n",
              "      <td>1.580919</td>\n",
              "      <td>0.507033</td>\n",
              "      <td>47.906706</td>\n",
              "      <td>1.024510</td>\n",
              "      <td>0.993513</td>\n",
              "      <td>1.024510</td>\n",
              "      <td>-0.317796</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.579534</td>\n",
              "      <td>1.091143</td>\n",
              "      <td>1.579534</td>\n",
              "      <td>0.553997</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   parag_no  sect_no  ... flair_lnorm_stdscaler flair_lnorm_medianiqr\n",
              "0         0        0  ...              1.550735              3.287672\n",
              "1         1        0  ...              1.579534              0.553997\n",
              "\n",
              "[2 rows x 67 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 493 entries, 0 to 492\n",
            "Data columns (total 67 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   parag_no                    493 non-null    int64  \n",
            " 1   sect_no                     493 non-null    int64  \n",
            " 2   parag_raw                   493 non-null    object \n",
            " 3   parag_clean                 493 non-null    object \n",
            " 4   char_len                    493 non-null    int64  \n",
            " 5   token_len                   493 non-null    int64  \n",
            " 6   sentimentr                  493 non-null    float64\n",
            " 7   sentimentr_stdscaler        493 non-null    float64\n",
            " 8   sentimentr_medianiqr        493 non-null    float64\n",
            " 9   sentimentr_lnorm_stdscaler  493 non-null    float64\n",
            " 10  sentimentr_lnorm_medianiqr  493 non-null    float64\n",
            " 11  syuzhet                     493 non-null    float64\n",
            " 12  syuzhet_stdscaler           493 non-null    float64\n",
            " 13  syuzhet_medianiqr           493 non-null    float64\n",
            " 14  syuzhet_lnorm_stdscaler     493 non-null    float64\n",
            " 15  syuzhet_lnorm_medianiqr     493 non-null    float64\n",
            " 16  bing                        493 non-null    float64\n",
            " 17  bing_stdscaler              493 non-null    float64\n",
            " 18  bing_medianiqr              493 non-null    float64\n",
            " 19  bing_lnorm_stdscaler        493 non-null    float64\n",
            " 20  bing_lnorm_medianiqr        493 non-null    float64\n",
            " 21  sentiword                   493 non-null    float64\n",
            " 22  sentiword_stdscaler         493 non-null    float64\n",
            " 23  sentiword_medianiqr         493 non-null    float64\n",
            " 24  sentiword_lnorm_stdscaler   493 non-null    float64\n",
            " 25  sentiword_lnorm_medianiqr   493 non-null    float64\n",
            " 26  senticnet                   493 non-null    float64\n",
            " 27  senticnet_stdscaler         493 non-null    float64\n",
            " 28  senticnet_medianiqr         493 non-null    float64\n",
            " 29  senticnet_lnorm_stdscaler   493 non-null    float64\n",
            " 30  senticnet_lnorm_medianiqr   493 non-null    float64\n",
            " 31  nrc                         493 non-null    float64\n",
            " 32  nrc_stdscaler               493 non-null    float64\n",
            " 33  nrc_medianiqr               493 non-null    float64\n",
            " 34  nrc_lnorm_stdscaler         493 non-null    float64\n",
            " 35  nrc_lnorm_medianiqr         493 non-null    float64\n",
            " 36  afinn                       493 non-null    float64\n",
            " 37  afinn_stdscaler             493 non-null    float64\n",
            " 38  afinn_medianiqr             493 non-null    float64\n",
            " 39  afinn_lnorm_stdscaler       493 non-null    float64\n",
            " 40  afinn_lnorm_medianiqr       493 non-null    float64\n",
            " 41  scores                      493 non-null    object \n",
            " 42  vader                       493 non-null    float64\n",
            " 43  vader_stdscaler             493 non-null    float64\n",
            " 44  vader_medianiqr             493 non-null    float64\n",
            " 45  vader_lnorm_stdscaler       493 non-null    float64\n",
            " 46  vader_lnorm_medianiqr       493 non-null    float64\n",
            " 47  textblob                    493 non-null    float64\n",
            " 48  textblob_stdscaler          493 non-null    float64\n",
            " 49  textblob_medianiqr          493 non-null    float64\n",
            " 50  textblob_lnorm_stdscaler    493 non-null    float64\n",
            " 51  textblob_lnorm_medianiqr    493 non-null    float64\n",
            " 52  pattern                     493 non-null    float64\n",
            " 53  pattern_stdscaler           493 non-null    float64\n",
            " 54  pattern_medianiqr           493 non-null    float64\n",
            " 55  pattern_lnorm_stdscaler     493 non-null    float64\n",
            " 56  pattern_lnorm_medianiqr     493 non-null    float64\n",
            " 57  stanza                      493 non-null    float64\n",
            " 58  stanza_stdscaler            493 non-null    float64\n",
            " 59  stanza_medianiqr            493 non-null    float64\n",
            " 60  stanza_lnorm_stdscaler      493 non-null    float64\n",
            " 61  stanza_lnorm_medianiqr      493 non-null    float64\n",
            " 62  flair                       493 non-null    float64\n",
            " 63  flair_stdscaler             493 non-null    float64\n",
            " 64  flair_medianiqr             493 non-null    float64\n",
            " 65  flair_lnorm_stdscaler       493 non-null    float64\n",
            " 66  flair_lnorm_medianiqr       493 non-null    float64\n",
            "dtypes: float64(60), int64(4), object(3)\n",
            "memory usage: 258.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RMNQI76jfkdu",
        "outputId": "79462c9b-ac96-4225-9cc6-0d9a0c484d0c"
      },
      "source": [
        "# Clean Section DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_sects_df.columns:\n",
        "  corpus_sects_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  \n",
        "corpus_sects_df.head(2)\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sect_no</th>\n",
              "      <th>chap_no</th>\n",
              "      <th>sent_no_start</th>\n",
              "      <th>sent_no_mid</th>\n",
              "      <th>sent_no_end</th>\n",
              "      <th>sect_raw</th>\n",
              "      <th>sect_clean</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>sentimentr</th>\n",
              "      <th>sentimentr_stdscaler</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>syuzhet</th>\n",
              "      <th>syuzhet_stdscaler</th>\n",
              "      <th>syuzhet_medianiqr</th>\n",
              "      <th>syuzhet_lnorm_medianiqr</th>\n",
              "      <th>bing</th>\n",
              "      <th>bing_stdscaler</th>\n",
              "      <th>bing_medianiqr</th>\n",
              "      <th>bing_lnorm_medianiqr</th>\n",
              "      <th>sentiword</th>\n",
              "      <th>sentiword_stdscaler</th>\n",
              "      <th>sentiword_medianiqr</th>\n",
              "      <th>sentiword_lnorm_medianiqr</th>\n",
              "      <th>senticnet</th>\n",
              "      <th>senticnet_stdscaler</th>\n",
              "      <th>senticnet_medianiqr</th>\n",
              "      <th>senticnet_lnorm_medianiqr</th>\n",
              "      <th>nrc</th>\n",
              "      <th>nrc_stdscaler</th>\n",
              "      <th>nrc_medianiqr</th>\n",
              "      <th>nrc_lnorm_medianiqr</th>\n",
              "      <th>afinn</th>\n",
              "      <th>afinn_stdscaler</th>\n",
              "      <th>afinn_medianiqr</th>\n",
              "      <th>afinn_lnorm_medianiqr</th>\n",
              "      <th>scores</th>\n",
              "      <th>vader</th>\n",
              "      <th>vader_stdscaler</th>\n",
              "      <th>vader_medianiqr</th>\n",
              "      <th>vader_lnorm_medianiqr</th>\n",
              "      <th>textblob</th>\n",
              "      <th>textblob_stdscaler</th>\n",
              "      <th>textblob_medianiqr</th>\n",
              "      <th>textblob_lnorm_medianiqr</th>\n",
              "      <th>pattern</th>\n",
              "      <th>pattern_stdscaler</th>\n",
              "      <th>pattern_medianiqr</th>\n",
              "      <th>pattern_lnorm_medianiqr</th>\n",
              "      <th>stanza</th>\n",
              "      <th>stanza_stdscaler</th>\n",
              "      <th>stanza_medianiqr</th>\n",
              "      <th>stanza_lnorm_medianiqr</th>\n",
              "      <th>flair</th>\n",
              "      <th>flair_stdscaler</th>\n",
              "      <th>flair_medianiqr</th>\n",
              "      <th>flair_lnorm_medianiqr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>\"Yes, of course, if it's fine tomorrow,\" said ...</td>\n",
              "      <td>yes  of course  if it is fine tomorrow   said...</td>\n",
              "      <td>22117</td>\n",
              "      <td>3964</td>\n",
              "      <td>24.75</td>\n",
              "      <td>0.808178</td>\n",
              "      <td>1.251948</td>\n",
              "      <td>0.279594</td>\n",
              "      <td>25.50</td>\n",
              "      <td>0.793791</td>\n",
              "      <td>1.274639</td>\n",
              "      <td>0.224012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.192396</td>\n",
              "      <td>0.206867</td>\n",
              "      <td>0.290906</td>\n",
              "      <td>16.279199</td>\n",
              "      <td>0.611713</td>\n",
              "      <td>1.036790</td>\n",
              "      <td>0.107713</td>\n",
              "      <td>115.877</td>\n",
              "      <td>2.200457</td>\n",
              "      <td>2.175486</td>\n",
              "      <td>0.665869</td>\n",
              "      <td>158.543183</td>\n",
              "      <td>0.516371</td>\n",
              "      <td>0.928599</td>\n",
              "      <td>-0.105946</td>\n",
              "      <td>6.911718</td>\n",
              "      <td>0.536248</td>\n",
              "      <td>0.813295</td>\n",
              "      <td>0.067521</td>\n",
              "      <td>{'neg': 0.093, 'neu': 0.793, 'pos': 0.114, 'co...</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.666473</td>\n",
              "      <td>0.049175</td>\n",
              "      <td>-0.222048</td>\n",
              "      <td>0.113539</td>\n",
              "      <td>0.140717</td>\n",
              "      <td>0.419254</td>\n",
              "      <td>-0.305411</td>\n",
              "      <td>4.939393</td>\n",
              "      <td>1.620292</td>\n",
              "      <td>1.882152</td>\n",
              "      <td>0.254535</td>\n",
              "      <td>477.027684</td>\n",
              "      <td>1.431156</td>\n",
              "      <td>1.615162</td>\n",
              "      <td>-0.605728</td>\n",
              "      <td>0.9988</td>\n",
              "      <td>2.615883</td>\n",
              "      <td>12.106968</td>\n",
              "      <td>0.616349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>133</td>\n",
              "      <td>133</td>\n",
              "      <td>134</td>\n",
              "      <td>\"No going to the Lighthouse, James,\" he said, ...</td>\n",
              "      <td>no going to the lighthouse  james   he said  ...</td>\n",
              "      <td>210</td>\n",
              "      <td>37</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>-0.284324</td>\n",
              "      <td>-0.041558</td>\n",
              "      <td>-0.357983</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>-0.309147</td>\n",
              "      <td>-0.073587</td>\n",
              "      <td>-0.403280</td>\n",
              "      <td>21.687656</td>\n",
              "      <td>0.301772</td>\n",
              "      <td>0.325799</td>\n",
              "      <td>2.754839</td>\n",
              "      <td>-0.375000</td>\n",
              "      <td>-0.476475</td>\n",
              "      <td>-0.263711</td>\n",
              "      <td>-1.228707</td>\n",
              "      <td>-1.481</td>\n",
              "      <td>-0.862420</td>\n",
              "      <td>-0.518745</td>\n",
              "      <td>-3.190248</td>\n",
              "      <td>7.347043</td>\n",
              "      <td>-0.475872</td>\n",
              "      <td>-0.278440</td>\n",
              "      <td>1.798463</td>\n",
              "      <td>-0.269475</td>\n",
              "      <td>-0.498430</td>\n",
              "      <td>-0.363935</td>\n",
              "      <td>-1.976717</td>\n",
              "      <td>{'neg': 0.058, 'neu': 0.942, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.296</td>\n",
              "      <td>-1.004518</td>\n",
              "      <td>-1.667307</td>\n",
              "      <td>-7.398068</td>\n",
              "      <td>-0.243750</td>\n",
              "      <td>-3.045612</td>\n",
              "      <td>-4.064478</td>\n",
              "      <td>-74.311707</td>\n",
              "      <td>-0.134634</td>\n",
              "      <td>-0.898741</td>\n",
              "      <td>-0.558063</td>\n",
              "      <td>-5.430419</td>\n",
              "      <td>9.431629</td>\n",
              "      <td>-1.058971</td>\n",
              "      <td>-0.901579</td>\n",
              "      <td>4.123516</td>\n",
              "      <td>-1.0000</td>\n",
              "      <td>-0.490487</td>\n",
              "      <td>-0.110636</td>\n",
              "      <td>-19.494819</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sect_no  chap_no  ...  flair_medianiqr  flair_lnorm_medianiqr\n",
              "0        0        0  ...        12.106968               0.616349\n",
              "1        1        0  ...        -0.110636             -19.494819\n",
              "\n",
              "[2 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 43 entries, 0 to 42\n",
            "Data columns (total 58 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   sect_no                     43 non-null     int64  \n",
            " 1   chap_no                     43 non-null     int64  \n",
            " 2   sent_no_start               43 non-null     int64  \n",
            " 3   sent_no_mid                 43 non-null     int64  \n",
            " 4   sent_no_end                 43 non-null     int64  \n",
            " 5   sect_raw                    43 non-null     object \n",
            " 6   sect_clean                  43 non-null     object \n",
            " 7   char_len                    43 non-null     int64  \n",
            " 8   token_len                   43 non-null     int64  \n",
            " 9   sentimentr                  43 non-null     float64\n",
            " 10  sentimentr_stdscaler        43 non-null     float64\n",
            " 11  sentimentr_medianiqr        43 non-null     float64\n",
            " 12  sentimentr_lnorm_medianiqr  43 non-null     float64\n",
            " 13  syuzhet                     43 non-null     float64\n",
            " 14  syuzhet_stdscaler           43 non-null     float64\n",
            " 15  syuzhet_medianiqr           43 non-null     float64\n",
            " 16  syuzhet_lnorm_medianiqr     43 non-null     float64\n",
            " 17  bing                        43 non-null     float64\n",
            " 18  bing_stdscaler              43 non-null     float64\n",
            " 19  bing_medianiqr              43 non-null     float64\n",
            " 20  bing_lnorm_medianiqr        43 non-null     float64\n",
            " 21  sentiword                   43 non-null     float64\n",
            " 22  sentiword_stdscaler         43 non-null     float64\n",
            " 23  sentiword_medianiqr         43 non-null     float64\n",
            " 24  sentiword_lnorm_medianiqr   43 non-null     float64\n",
            " 25  senticnet                   43 non-null     float64\n",
            " 26  senticnet_stdscaler         43 non-null     float64\n",
            " 27  senticnet_medianiqr         43 non-null     float64\n",
            " 28  senticnet_lnorm_medianiqr   43 non-null     float64\n",
            " 29  nrc                         43 non-null     float64\n",
            " 30  nrc_stdscaler               43 non-null     float64\n",
            " 31  nrc_medianiqr               43 non-null     float64\n",
            " 32  nrc_lnorm_medianiqr         43 non-null     float64\n",
            " 33  afinn                       43 non-null     float64\n",
            " 34  afinn_stdscaler             43 non-null     float64\n",
            " 35  afinn_medianiqr             43 non-null     float64\n",
            " 36  afinn_lnorm_medianiqr       43 non-null     float64\n",
            " 37  scores                      43 non-null     object \n",
            " 38  vader                       43 non-null     float64\n",
            " 39  vader_stdscaler             43 non-null     float64\n",
            " 40  vader_medianiqr             43 non-null     float64\n",
            " 41  vader_lnorm_medianiqr       43 non-null     float64\n",
            " 42  textblob                    43 non-null     float64\n",
            " 43  textblob_stdscaler          43 non-null     float64\n",
            " 44  textblob_medianiqr          43 non-null     float64\n",
            " 45  textblob_lnorm_medianiqr    43 non-null     float64\n",
            " 46  pattern                     43 non-null     float64\n",
            " 47  pattern_stdscaler           43 non-null     float64\n",
            " 48  pattern_medianiqr           43 non-null     float64\n",
            " 49  pattern_lnorm_medianiqr     43 non-null     float64\n",
            " 50  stanza                      43 non-null     float64\n",
            " 51  stanza_stdscaler            43 non-null     float64\n",
            " 52  stanza_medianiqr            43 non-null     float64\n",
            " 53  stanza_lnorm_medianiqr      43 non-null     float64\n",
            " 54  flair                       43 non-null     float64\n",
            " 55  flair_stdscaler             43 non-null     float64\n",
            " 56  flair_medianiqr             43 non-null     float64\n",
            " 57  flair_lnorm_medianiqr       43 non-null     float64\n",
            "dtypes: float64(48), int64(7), object(3)\n",
            "memory usage: 19.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NFfc4HTPfy5c",
        "outputId": "58c43442-7c60-4558-d00e-c3c2c5f33c7e"
      },
      "source": [
        "# Clean Chapter DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_chaps_df.columns:\n",
        "  corpus_chaps_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  \n",
        "corpus_chaps_df.head(2)\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chap_no</th>\n",
              "      <th>sent_no_start</th>\n",
              "      <th>sent_no_mid</th>\n",
              "      <th>sent_no_end</th>\n",
              "      <th>chap_raw</th>\n",
              "      <th>chap_clean</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>sentimentr</th>\n",
              "      <th>sentimentr_stdscaler</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_stdscaler</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>syuzhet</th>\n",
              "      <th>syuzhet_stdscaler</th>\n",
              "      <th>syuzhet_medianiqr</th>\n",
              "      <th>syuzhet_lnorm_stdscaler</th>\n",
              "      <th>syuzhet_lnorm_medianiqr</th>\n",
              "      <th>bing</th>\n",
              "      <th>bing_stdscaler</th>\n",
              "      <th>bing_medianiqr</th>\n",
              "      <th>bing_lnorm_stdscaler</th>\n",
              "      <th>bing_lnorm_medianiqr</th>\n",
              "      <th>sentiword</th>\n",
              "      <th>sentiword_stdscaler</th>\n",
              "      <th>sentiword_medianiqr</th>\n",
              "      <th>sentiword_lnorm_stdscaler</th>\n",
              "      <th>sentiword_lnorm_medianiqr</th>\n",
              "      <th>senticnet</th>\n",
              "      <th>senticnet_stdscaler</th>\n",
              "      <th>senticnet_medianiqr</th>\n",
              "      <th>senticnet_lnorm_stdscaler</th>\n",
              "      <th>senticnet_lnorm_medianiqr</th>\n",
              "      <th>nrc</th>\n",
              "      <th>nrc_stdscaler</th>\n",
              "      <th>nrc_medianiqr</th>\n",
              "      <th>nrc_lnorm_stdscaler</th>\n",
              "      <th>nrc_lnorm_medianiqr</th>\n",
              "      <th>afinn</th>\n",
              "      <th>afinn_stdscaler</th>\n",
              "      <th>afinn_medianiqr</th>\n",
              "      <th>afinn_lnorm_stdscaler</th>\n",
              "      <th>afinn_lnorm_medianiqr</th>\n",
              "      <th>scores</th>\n",
              "      <th>vader</th>\n",
              "      <th>vader_stdscaler</th>\n",
              "      <th>vader_medianiqr</th>\n",
              "      <th>vader_lnorm_stdscaler</th>\n",
              "      <th>vader_lnorm_medianiqr</th>\n",
              "      <th>textblob</th>\n",
              "      <th>textblob_stdscaler</th>\n",
              "      <th>textblob_medianiqr</th>\n",
              "      <th>textblob_lnorm_stdscaler</th>\n",
              "      <th>textblob_lnorm_medianiqr</th>\n",
              "      <th>pattern</th>\n",
              "      <th>pattern_stdscaler</th>\n",
              "      <th>pattern_medianiqr</th>\n",
              "      <th>pattern_lnorm_stdscaler</th>\n",
              "      <th>pattern_lnorm_medianiqr</th>\n",
              "      <th>stanza</th>\n",
              "      <th>stanza_stdscaler</th>\n",
              "      <th>stanza_medianiqr</th>\n",
              "      <th>stanza_lnorm_stdscaler</th>\n",
              "      <th>stanza_lnorm_medianiqr</th>\n",
              "      <th>flair</th>\n",
              "      <th>flair_stdscaler</th>\n",
              "      <th>flair_medianiqr</th>\n",
              "      <th>flair_lnorm_stdscaler</th>\n",
              "      <th>flair_lnorm_medianiqr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>972</td>\n",
              "      <td>1944</td>\n",
              "      <td>SECTION 1\\n\\n\"Yes, of course, if it's fine tom...</td>\n",
              "      <td>yes of course if it is fine tomorrow said mrs ...</td>\n",
              "      <td>229459</td>\n",
              "      <td>42064</td>\n",
              "      <td>360.80</td>\n",
              "      <td>1.414214</td>\n",
              "      <td>1.999506</td>\n",
              "      <td>1.328243</td>\n",
              "      <td>1.302889</td>\n",
              "      <td>375.10</td>\n",
              "      <td>1.414202</td>\n",
              "      <td>1.990782</td>\n",
              "      <td>1.345262</td>\n",
              "      <td>1.369259</td>\n",
              "      <td>1296.115178</td>\n",
              "      <td>1.384853</td>\n",
              "      <td>1.572971</td>\n",
              "      <td>1.238515</td>\n",
              "      <td>1.034327</td>\n",
              "      <td>239.642564</td>\n",
              "      <td>1.336978</td>\n",
              "      <td>1.335946</td>\n",
              "      <td>1.051000</td>\n",
              "      <td>0.631906</td>\n",
              "      <td>927.665</td>\n",
              "      <td>1.293674</td>\n",
              "      <td>1.18732</td>\n",
              "      <td>0.990183</td>\n",
              "      <td>0.517715</td>\n",
              "      <td>2372.858467</td>\n",
              "      <td>1.368168</td>\n",
              "      <td>1.475109</td>\n",
              "      <td>1.240750</td>\n",
              "      <td>1.040014</td>\n",
              "      <td>98.230738</td>\n",
              "      <td>1.407192</td>\n",
              "      <td>1.781619</td>\n",
              "      <td>1.362399</td>\n",
              "      <td>1.446083</td>\n",
              "      <td>{'neg': 0.083, 'neu': 0.79, 'pos': 0.127, 'com...</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.748270</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>-0.862156</td>\n",
              "      <td>-0.284799</td>\n",
              "      <td>0.123033</td>\n",
              "      <td>1.373863</td>\n",
              "      <td>1.50584</td>\n",
              "      <td>-0.666487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>39.832747</td>\n",
              "      <td>1.388801</td>\n",
              "      <td>1.600511</td>\n",
              "      <td>1.352813</td>\n",
              "      <td>1.401618</td>\n",
              "      <td>3925.043548</td>\n",
              "      <td>1.248187</td>\n",
              "      <td>1.059208</td>\n",
              "      <td>-0.995092</td>\n",
              "      <td>-0.526797</td>\n",
              "      <td>0.9988</td>\n",
              "      <td>1.414213</td>\n",
              "      <td>1.999199</td>\n",
              "      <td>1.085300</td>\n",
              "      <td>0.698426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1945</td>\n",
              "      <td>2066</td>\n",
              "      <td>2187</td>\n",
              "      <td>SECTION 1 \\n\\n\"Well, we must wait for the futu...</td>\n",
              "      <td>well we must wait for the future to show said ...</td>\n",
              "      <td>32655</td>\n",
              "      <td>5785</td>\n",
              "      <td>-44.25</td>\n",
              "      <td>-0.706845</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.084631</td>\n",
              "      <td>-0.697111</td>\n",
              "      <td>-37.15</td>\n",
              "      <td>-0.712001</td>\n",
              "      <td>-0.009218</td>\n",
              "      <td>-1.050390</td>\n",
              "      <td>-0.630741</td>\n",
              "      <td>-951.624379</td>\n",
              "      <td>-0.444157</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.210493</td>\n",
              "      <td>-0.965673</td>\n",
              "      <td>-13.051524</td>\n",
              "      <td>-1.067698</td>\n",
              "      <td>-0.664054</td>\n",
              "      <td>-1.344981</td>\n",
              "      <td>-1.368094</td>\n",
              "      <td>33.023</td>\n",
              "      <td>-1.141615</td>\n",
              "      <td>-0.81268</td>\n",
              "      <td>-1.369536</td>\n",
              "      <td>-1.482285</td>\n",
              "      <td>16.070556</td>\n",
              "      <td>-0.994062</td>\n",
              "      <td>-0.524891</td>\n",
              "      <td>-1.208087</td>\n",
              "      <td>-0.959986</td>\n",
              "      <td>-1.938939</td>\n",
              "      <td>-0.825490</td>\n",
              "      <td>-0.218381</td>\n",
              "      <td>-1.009683</td>\n",
              "      <td>-0.553917</td>\n",
              "      <td>{'neg': 0.084, 'neu': 0.823, 'pos': 0.093, 'co...</td>\n",
              "      <td>0.9974</td>\n",
              "      <td>-1.413399</td>\n",
              "      <td>-1.923077</td>\n",
              "      <td>1.401910</td>\n",
              "      <td>1.715201</td>\n",
              "      <td>0.073103</td>\n",
              "      <td>-0.396455</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.413450</td>\n",
              "      <td>1.925499</td>\n",
              "      <td>3.616010</td>\n",
              "      <td>-0.925538</td>\n",
              "      <td>-0.399489</td>\n",
              "      <td>-0.319445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>655.703673</td>\n",
              "      <td>-1.199872</td>\n",
              "      <td>-0.940792</td>\n",
              "      <td>1.367801</td>\n",
              "      <td>1.473203</td>\n",
              "      <td>-0.9997</td>\n",
              "      <td>-0.707531</td>\n",
              "      <td>-0.000801</td>\n",
              "      <td>-1.327884</td>\n",
              "      <td>-1.301574</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   chap_no  sent_no_start  ...  flair_lnorm_stdscaler  flair_lnorm_medianiqr\n",
              "0        0              0  ...               1.085300               0.698426\n",
              "1        1           1945  ...              -1.327884              -1.301574\n",
              "\n",
              "[2 rows x 69 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3 entries, 0 to 2\n",
            "Data columns (total 69 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   chap_no                     3 non-null      int64  \n",
            " 1   sent_no_start               3 non-null      int64  \n",
            " 2   sent_no_mid                 3 non-null      int64  \n",
            " 3   sent_no_end                 3 non-null      int64  \n",
            " 4   chap_raw                    3 non-null      object \n",
            " 5   chap_clean                  3 non-null      object \n",
            " 6   char_len                    3 non-null      int64  \n",
            " 7   token_len                   3 non-null      int64  \n",
            " 8   sentimentr                  3 non-null      float64\n",
            " 9   sentimentr_stdscaler        3 non-null      float64\n",
            " 10  sentimentr_medianiqr        3 non-null      float64\n",
            " 11  sentimentr_lnorm_stdscaler  3 non-null      float64\n",
            " 12  sentimentr_lnorm_medianiqr  3 non-null      float64\n",
            " 13  syuzhet                     3 non-null      float64\n",
            " 14  syuzhet_stdscaler           3 non-null      float64\n",
            " 15  syuzhet_medianiqr           3 non-null      float64\n",
            " 16  syuzhet_lnorm_stdscaler     3 non-null      float64\n",
            " 17  syuzhet_lnorm_medianiqr     3 non-null      float64\n",
            " 18  bing                        3 non-null      float64\n",
            " 19  bing_stdscaler              3 non-null      float64\n",
            " 20  bing_medianiqr              3 non-null      float64\n",
            " 21  bing_lnorm_stdscaler        3 non-null      float64\n",
            " 22  bing_lnorm_medianiqr        3 non-null      float64\n",
            " 23  sentiword                   3 non-null      float64\n",
            " 24  sentiword_stdscaler         3 non-null      float64\n",
            " 25  sentiword_medianiqr         3 non-null      float64\n",
            " 26  sentiword_lnorm_stdscaler   3 non-null      float64\n",
            " 27  sentiword_lnorm_medianiqr   3 non-null      float64\n",
            " 28  senticnet                   3 non-null      float64\n",
            " 29  senticnet_stdscaler         3 non-null      float64\n",
            " 30  senticnet_medianiqr         3 non-null      float64\n",
            " 31  senticnet_lnorm_stdscaler   3 non-null      float64\n",
            " 32  senticnet_lnorm_medianiqr   3 non-null      float64\n",
            " 33  nrc                         3 non-null      float64\n",
            " 34  nrc_stdscaler               3 non-null      float64\n",
            " 35  nrc_medianiqr               3 non-null      float64\n",
            " 36  nrc_lnorm_stdscaler         3 non-null      float64\n",
            " 37  nrc_lnorm_medianiqr         3 non-null      float64\n",
            " 38  afinn                       3 non-null      float64\n",
            " 39  afinn_stdscaler             3 non-null      float64\n",
            " 40  afinn_medianiqr             3 non-null      float64\n",
            " 41  afinn_lnorm_stdscaler       3 non-null      float64\n",
            " 42  afinn_lnorm_medianiqr       3 non-null      float64\n",
            " 43  scores                      3 non-null      object \n",
            " 44  vader                       3 non-null      float64\n",
            " 45  vader_stdscaler             3 non-null      float64\n",
            " 46  vader_medianiqr             3 non-null      float64\n",
            " 47  vader_lnorm_stdscaler       3 non-null      float64\n",
            " 48  vader_lnorm_medianiqr       3 non-null      float64\n",
            " 49  textblob                    3 non-null      float64\n",
            " 50  textblob_stdscaler          3 non-null      float64\n",
            " 51  textblob_medianiqr          3 non-null      float64\n",
            " 52  textblob_lnorm_stdscaler    3 non-null      float64\n",
            " 53  textblob_lnorm_medianiqr    3 non-null      float64\n",
            " 54  pattern                     3 non-null      float64\n",
            " 55  pattern_stdscaler           3 non-null      float64\n",
            " 56  pattern_medianiqr           3 non-null      float64\n",
            " 57  pattern_lnorm_stdscaler     3 non-null      float64\n",
            " 58  pattern_lnorm_medianiqr     3 non-null      float64\n",
            " 59  stanza                      3 non-null      float64\n",
            " 60  stanza_stdscaler            3 non-null      float64\n",
            " 61  stanza_medianiqr            3 non-null      float64\n",
            " 62  stanza_lnorm_stdscaler      3 non-null      float64\n",
            " 63  stanza_lnorm_medianiqr      3 non-null      float64\n",
            " 64  flair                       3 non-null      float64\n",
            " 65  flair_stdscaler             3 non-null      float64\n",
            " 66  flair_medianiqr             3 non-null      float64\n",
            " 67  flair_lnorm_stdscaler       3 non-null      float64\n",
            " 68  flair_lnorm_medianiqr       3 non-null      float64\n",
            "dtypes: float64(60), int64(6), object(3)\n",
            "memory usage: 1.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdYoS50pgJtz"
      },
      "source": [
        "### **SentimentR 7 Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "EBk7sJltaE43",
        "outputId": "b6394707-beae-4ab1-9208-9f0324b40f55"
      },
      "source": [
        "# SentimentR 7 Models: Read Computed sentiment data saved from previous run of this notebook\n",
        "\n",
        "corpus_sentimentr_df = pd.read_csv('sum_sentiments_sentimentR_7models_vwoolf_tothelighthouse.csv')\n",
        "\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "j5RkpbLrgWNQ",
        "outputId": "6b5d47c3-4b70-4883-c915-1a7cf91b314c"
      },
      "source": [
        "# Clean Chapter DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_sentimentr_df.columns:\n",
        "  corpus_sentimentr_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  \n",
        "corpus_sentimentr_df.head(2)\n",
        "corpus_sentimentr_df.info()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"CHAPTER 1 THE WINDOW  SECTION 1  \\\"Yes, of course, if it's fine tomorrow,\\\" said Mrs. Ramsay.\",\n{\n            'v': 0.280624304008046,\n            'f': \"0.280624304008046\",\n        },\n{\n            'v': 0.280624304008046,\n            'f': \"0.280624304008046\",\n        },\n{\n            'v': 0.26726124191242395,\n            'f': \"0.26726124191242395\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': -0.0465034560927619,\n            'f': \"-0.0465034560927619\",\n        },\n{\n            'v': 0.0668153104781061,\n            'f': \"0.0668153104781061\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"\\\"But you'll have to be up with the lark,\\\" she added.\",\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.483699574538877,\n            'f': \"0.483699574538877\",\n        },\n{\n            'v': -0.18608903298158802,\n            'f': \"-0.18608903298158802\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"sent_raw\"], [\"number\", \"jockers_rinker\"], [\"number\", \"jockers\"], [\"number\", \"huliu\"], [\"number\", \"lmcd\"], [\"number\", \"nrc\"], [\"number\", \"senticnet\"], [\"number\", \"sentiword\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>jockers_rinker</th>\n",
              "      <th>jockers</th>\n",
              "      <th>huliu</th>\n",
              "      <th>lmcd</th>\n",
              "      <th>nrc</th>\n",
              "      <th>senticnet</th>\n",
              "      <th>sentiword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CHAPTER 1 THE WINDOW  SECTION 1  \"Yes, of cour...</td>\n",
              "      <td>0.280624</td>\n",
              "      <td>0.280624</td>\n",
              "      <td>0.267261</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.046503</td>\n",
              "      <td>0.066815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"But you'll have to be up with the lark,\" she ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.483700</td>\n",
              "      <td>-0.186089</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sent_raw  ...  sentiword\n",
              "0  CHAPTER 1 THE WINDOW  SECTION 1  \"Yes, of cour...  ...   0.066815\n",
              "1  \"But you'll have to be up with the lark,\" she ...  ...  -0.186089\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3511 entries, 0 to 3510\n",
            "Data columns (total 8 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   sent_raw        3511 non-null   object \n",
            " 1   jockers_rinker  3511 non-null   float64\n",
            " 2   jockers         3511 non-null   float64\n",
            " 3   huliu           3511 non-null   float64\n",
            " 4   lmcd            3511 non-null   float64\n",
            " 5   nrc             3511 non-null   float64\n",
            " 6   senticnet       3511 non-null   float64\n",
            " 7   sentiword       3511 non-null   float64\n",
            "dtypes: float64(7), object(1)\n",
            "memory usage: 219.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXotYX02g-Zf"
      },
      "source": [
        "# TODO: If missing, generate derived values for each model, lnorm/not, stdscaler/medianiqr, roll10/not"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krB0DGVMgNbW"
      },
      "source": [
        "### **SyuzhetR 4 Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "a9GOO2EseGTV",
        "outputId": "b7e00b04-b303-48e6-f6cb-36c71c2990e9"
      },
      "source": [
        "# SyuzhetR 4 Models: Read Computed sentiment data saved from previous run of this notebook\n",
        "\n",
        "corpus_syuzhetr_df = pd.read_csv('sum_sentiments_syuzhetR_4models_vwoolf_tothelighthouse.csv')"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "1EAxGBWwgdPz",
        "outputId": "a0406c18-3555-4dcb-f44a-546ba18c81c4"
      },
      "source": [
        "# Clean Chapter DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_syuzhetr_df.columns:\n",
        "  corpus_syuzhetr_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n",
        "  \n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n\"CHAPTER 1 THE WINDOW  SECTION 1  \\\"Yes, of course, if it's fine tomorrow,\\\" said Mrs. Ramsay.\",\n{\n            'v': 1.05,\n            'f': \"1.05\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 16,\n            'f': \"16\",\n        },\n{\n            'v': 91,\n            'f': \"91\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n\"\\\"But you'll have to be up with the lark,\\\" she added.\",\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 11,\n            'f': \"11\",\n        },\n{\n            'v': 52,\n            'f': \"52\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"string\", \"sent_raw\"], [\"number\", \"syuzhet\"], [\"number\", \"bing\"], [\"number\", \"afinn\"], [\"number\", \"nrc\"], [\"number\", \"token_len\"], [\"number\", \"char_len\"], [\"number\", \"syuzhet_roll10\"], [\"number\", \"bing_roll10\"], [\"number\", \"syuzhet_roll10\"], [\"number\", \"bing_roll10\"], [\"number\", \"syuzhet_roll10\"], [\"number\", \"bing_roll10\"], [\"number\", \"syuzhet_roll20\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>syuzhet</th>\n",
              "      <th>bing</th>\n",
              "      <th>afinn</th>\n",
              "      <th>nrc</th>\n",
              "      <th>token_len</th>\n",
              "      <th>char_len</th>\n",
              "      <th>syuzhet_roll10</th>\n",
              "      <th>bing_roll10</th>\n",
              "      <th>syuzhet_roll10</th>\n",
              "      <th>bing_roll10</th>\n",
              "      <th>syuzhet_roll10</th>\n",
              "      <th>bing_roll10</th>\n",
              "      <th>syuzhet_roll20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>CHAPTER 1 THE WINDOW  SECTION 1  \"Yes, of cour...</td>\n",
              "      <td>1.05</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>91</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>\"But you'll have to be up with the lark,\" she ...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>52</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sent_no  ... syuzhet_roll20\n",
              "0        1  ...            NaN\n",
              "1        2  ...            NaN\n",
              "\n",
              "[2 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3511 entries, 0 to 3510\n",
            "Data columns (total 15 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   sent_no         3511 non-null   int64  \n",
            " 1   sent_raw        3511 non-null   object \n",
            " 2   syuzhet         3511 non-null   float64\n",
            " 3   bing            3511 non-null   int64  \n",
            " 4   afinn           3511 non-null   int64  \n",
            " 5   nrc             3511 non-null   int64  \n",
            " 6   token_len       3511 non-null   int64  \n",
            " 7   char_len        3511 non-null   int64  \n",
            " 8   syuzhet_roll10  3161 non-null   float64\n",
            " 9   bing_roll10     3161 non-null   float64\n",
            " 10  syuzhet_roll10  3161 non-null   float64\n",
            " 11  bing_roll10     3161 non-null   float64\n",
            " 12  syuzhet_roll10  3161 non-null   float64\n",
            " 13  bing_roll10     3161 non-null   float64\n",
            " 14  syuzhet_roll20  2810 non-null   float64\n",
            "dtypes: float64(8), int64(6), object(1)\n",
            "memory usage: 411.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "yHujhBBNlaQy",
        "outputId": "f5b5f1cd-7e0a-4324-f571-179f07ec5b8b"
      },
      "source": [
        "def process_timeseries(ts_df, col_models_ls, col_mod):\n",
        "  '''\n",
        "  Given a DataFrame, a list of columns to process and a modification to perform on these columns\n",
        "\n",
        "  Return a new DataFrame with the following new columns inserted from each original Model column:\n",
        "\n",
        "    1. Length-Normed (col_mod='lnorm'])\n",
        "       a. {base_model}_lnorm\n",
        "\n",
        "    2. Standardized (col_mod='std')\n",
        "       a. {base_model}_stdscaler\n",
        "       b. {base_model}_medianiqr \n",
        "       a. {base_model_lnorm}_stdscaler\n",
        "       b. {base_model_lnorm}_medianiqr \n",
        "\n",
        "    3. Length-Normed Standardized (col_mod='roll[dd]') # where dd = 01 to 20 indicating rolling window width as % of corpus length\n",
        "       a. {base_model}_roll\n",
        "       b. {base_model}_roll \n",
        "       a. {base_model_lnorm}_roll\n",
        "       b. {base_model_lnorm}_roll \n",
        "       a. {base_model_lnorm_stdscaler}_roll\n",
        "       b. {base_model_lnorm_medianiqr}_roll\n",
        " \n",
        "  '''\n",
        "\n",
        "  temp_df = pd.DataFrame()  # ts_df.filter(['sent_no','sent_raw'], axis=1)\n",
        "  print(f'temp_df is {temp_df}')\n",
        "\n",
        "  # Prerequisite: token_len column must exists\n",
        "  if 'token_len' not in ts_df.columns:\n",
        "    ts_df['token_len'] = ts_df['sent_raw'].apply(lambda x: len(x.strip().split()))\n",
        "    ts_df['char_len'] = ts_df['sent_raw'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "  # OPOTION 1: Apply (token) Length-Normalization to specified time series\n",
        "  if col_mod == 'lnorm':\n",
        "\n",
        "    # Apply lnorm operation on all specificed columns\n",
        "    for anold_model_no, anold_model in enumerate(col_models_ls):\n",
        "\n",
        "      # lnorm (Length normalization of sentiment values using token_len) operation can only be applied to base model time series\n",
        "      # rule: disallowed substrings in anold_model name if applying lnorm (anew_mod)\n",
        "      rule_badsubstr_lnorm = ['lnorm', 'stdscaler', 'medianiqr', 'roll']\n",
        "      if any(map(anold_model.__contains__, rule_badsubstr_lnorm)):\n",
        "        print(f'ERROR: Length-Normalization (lnorm) operation cannot be applied to {anold_model}\\n    any columns containing {rule_badsubstr_lnorm}')\n",
        "        return -99  # Return ERROR condition\n",
        "      else:\n",
        "        # Normalize specified sentiment time series by dividing by text length\n",
        "\n",
        "        # TODO: replace 'sent_raw'/'sent_clean' with 'text_raw'/'text_clean' for Sentence, Paragraph, Section and Chapter DataFrames\n",
        "\n",
        "        text_len_ls = list(ts_df['token_len'])\n",
        "        text_sentiment_ls = list(ts_df[anold_model])\n",
        "        text_sentiment_norm_ls = [text_sentiment_ls[i]/text_len_ls[i] for i in range(len(text_len_ls))]\n",
        "        col_mod_name = f'{anold_model}_{col_mod}'\n",
        "        # ts_df = ts_df.assign(acol_mod_name = pd.Series(text_sentiment_norm_ls).values)\n",
        "        temp_df[col_mod_name] = pd.Series(text_sentiment_norm_ls)\n",
        "\n",
        "\n",
        "  # OPTION 2: Apply (Robust) Standardization to specified time series\n",
        "  if col_mod == 'std':\n",
        "\n",
        "    # Apply lnorm operation on all specificed columns\n",
        "    for anold_model_no, anold_model in enumerate(col_models_ls):\n",
        "\n",
        "      # std (Standardization) operation can only be applied to base model time series\n",
        "      # rule: disallowed substrings in anold_model name if applying std to existing rolling averages or already standardized time series\n",
        "      rule_badsubstr_std = ['stdscaler', 'medianiqr', 'roll']\n",
        "      if any(map(anold_model.__contains__, rule_badsubstr_std)):\n",
        "        print(f'ERROR: Standardization (std) operation cannot be applied to {anold_model}\\n    any columns containing {rule_badsubstr_std}')\n",
        "        return -99  # Return ERROR condition\n",
        "      else:\n",
        "        # Standardize specificied sentiment time series by dividing by applying (Robust)Standization also configured in code below\n",
        "\n",
        "        # TODO: replace 'sent_raw'/'sent_clean' with 'text_raw'/'text_clean' for Sentence, Paragraph, Section and Chapter DataFrames\n",
        "\n",
        "        col_mod_minmax = f'{anold_model}_minmax'\n",
        "        temp_vals_np = ts_df[anold_model].values\n",
        "        print(f'type(temp_vals_np): {type(temp_vals_np)}')\n",
        "        print(f'shape temp_vals_np): {temp_vals_np.shape}')\n",
        "        print(f'reshaped temp_vals_np: {temp_vals_np.reshape(-1,1).shape}')\n",
        "        temp_df[col_mod_minmax] = minmax_scaler.fit_transform(temp_vals_np.reshape(-1, 1))\n",
        "        col_mod_stdscaler = f'{anold_model}_stdscaler'\n",
        "        temp_df[col_mod_stdscaler] = mean_std_scaler.fit_transform(np.array(ts_df[anold_model]).reshape(-1, 1))\n",
        "        col_mod_medianiqr = f'{anold_model}_medianiqr'\n",
        "        temp_df[col_mod_medianiqr] = median_iqr_scaler.fit_transform(np.array(ts_df[anold_model]).reshape(-1, 1))\n",
        "\n",
        "\n",
        "  # OPTION 3: Apply Simple Rolling Mean to specified time series\n",
        "  if col_mod.startswith('roll'):\n",
        "\n",
        "    roll_per = 0\n",
        "\n",
        "    # Check for roll argument against several rules and return detailed error message if fail to pass any combination of rules\n",
        "    roll_err_ls = []\n",
        "    roll_arg_len = len(col_mod)\n",
        "    if (roll_arg_len != 6):\n",
        "      roll_err_ls.append(f'ERROR: argument roll[dd]={col_mod} is too long with {roll_arg_len} characters (must be 6)')\n",
        "    roll_arg_2last = col_mod[-2:]\n",
        "\n",
        "    if (roll_arg_2last.isdigit() == False):\n",
        "      roll_err_ls.append(f'ERROR: argument roll[dd]={col_mod} last 2 chars {roll_arg_2last} must be both be digits [0-9]')\n",
        "    else:\n",
        "      print(f'BEFORE: roll_arg_2last={roll_arg_2last}')\n",
        "      roll_per = int(roll_arg_2last)\n",
        "      print(f'AFTER: roll_per={roll_per}')\n",
        "      if (roll_per > 20):\n",
        "        roll_err_ls.append(f'ERROR: argument roll[dd]={col_mod} last 2 chars {roll_arg_2last} must be integers between 01 and 20')\n",
        "\n",
        "    if len(roll_err_ls) > 0:\n",
        "      print(f'ERROR in process_timeseries() due to invalid argument roll[dd] = {col_mod}\\n\\n')\n",
        "      for anerror_str in roll_err_ls:\n",
        "        print(f'    {anerror_str}')\n",
        "        return -99  # Return ERROR condition\n",
        "\n",
        "    # Apply lnorm operation on all specificed columns\n",
        "    for anold_model_no, anold_model in enumerate(col_models_ls):\n",
        "\n",
        "      # roll (Simple Rolling Average with specificed window size as 2-digit percentage of corpus length (01-20%)\n",
        "      # rule: disallowed substrings in anold_model name applying roll operation more than once to existing time series\n",
        "      rule_badsubstr_roll = ['roll']\n",
        "      if any(map(anold_model.__contains__, rule_badsubstr_roll)):\n",
        "        print(f'ERROR: Rolling Mean (roll) operation cannot be applied to {anold_model}\\n    any columns containing {rule_badsubstr_roll}')\n",
        "        return -99 # Return ERROR condition\n",
        "\n",
        "      else:\n",
        "          # Compute Rolling Mean with the given window size (extracted above as an int in roll_per) for the specificied sentiment time series\n",
        "\n",
        "          # TODO: replace 'sent_raw'/'sent_clean' with 'text_raw'/'text_clean' for Sentence, Paragraph, Section and Chapter DataFrames\n",
        "\n",
        "          col_mod_roll = f'{anold_model}_{col_mod}'\n",
        "          print(f'  for col_mod: {col_mod} and roll_per: {roll_per}')\n",
        "          roll_win = int(ts_df.shape[0]*roll_per/100)\n",
        "          print(f'    and roll_win: {roll_win}')\n",
        "          temp_df[col_mod_roll] = ts_df[anold_model].rolling(roll_win, center=True).mean()\n",
        "\n",
        "  return temp_df  # Return SUCCESS condition\n",
        "\n",
        "# Test\n",
        "\n",
        "# Clean up DataFrame by dropping all modified columns\n",
        "# test_df = corpus_syuzhetr_df.filter(['sent_no', 'sent_raw'], axis=1)\n",
        "# test_df.reset_index(inplace=True, drop=True)\n",
        "# corpus_syuzhetr_df.reset_index(inplace=True, drop=True)\n",
        "# corpus_syuzhetr_df = corpus_syuzhetr_df[corpus_syuzhetr_df.columns.drop(list(corpus_syuzhetr_df.filter(regex='(minmax|stdscaler|medianiqr|lnorm|roll)')))]\n",
        "\n",
        "# test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=models_syuzhetr_ls, col_mod='lnorm')\n",
        "# corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "# corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=['syuzhet','bing', 'afinn'], col_mod='std')\n",
        "corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "# test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=['syuzhet','bing'], col_mod='roll10')\n",
        "# corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "# corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "# test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=['syuzhet_lnorm','bing_lnorm'], col_mod='roll10')\n",
        "# corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "# corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "# test_df = process_timeseries(ts_df=corpus_syuzhetr_df, col_models_ls=['syuzhet'], col_mod='roll20')\n",
        "# corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "# corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "# test_df.head() # filter([corpus_syuzhetr_df[corpus_syuzhetr_df.columns in ['sent_raw']]])\n",
        "# test_df.columns\n",
        "# corpus_syuzhetr_df = pd.concat([corpus_syuzhetr_df, test_df], axis=1)\n",
        "# corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "\n",
        "\n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()\n"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "temp_df is Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "type(temp_vals_np): <class 'numpy.ndarray'>\n",
            "shape temp_vals_np): (3511,)\n",
            "reshaped temp_vals_np: (3511, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-225-8d2b6643556c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;31m# corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_timeseries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_syuzhetr_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_models_ls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'syuzhet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'afinn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_mod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'std'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0mcorpus_syuzhetr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_syuzhetr_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0mcorpus_syuzhetr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_syuzhetr_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mcorpus_syuzhetr_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-225-8d2b6643556c>\u001b[0m in \u001b[0;36mprocess_timeseries\u001b[0;34m(ts_df, col_models_ls, col_mod)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'shape temp_vals_np): {temp_vals_np.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'reshaped temp_vals_np: {temp_vals_np.reshape(-1,1).shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mtemp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_mod_minmax\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminmax_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_vals_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mcol_mod_stdscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{anold_model}_stdscaler'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mtemp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_mod_stdscaler\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_std_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manold_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3043\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m         \"\"\"\n\u001b[0;32m-> 3119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ensure_valid_index\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   3168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3170\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3171\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3172\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data must be 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Data must be 1-dimensional"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "GPuuUo3aZMWD",
        "outputId": "c5757e95-79cc-4387-9457-94a85f93130d"
      },
      "source": [
        "test_df = corpus_syuzhetr_df.filter(['sent_no', 'sent_raw'], axis=1)\n",
        "test_df.head()"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n\"CHAPTER 1 THE WINDOW  SECTION 1  \\\"Yes, of course, if it's fine tomorrow,\\\" said Mrs. Ramsay.\"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n\"\\\"But you'll have to be up with the lark,\\\" she added.\"],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"To her son these words conveyed an extraordinary joy, as if it were settled, the expedition were bound to take place, and the wonder to which he had looked forward, for years and years it seemed, was, after a night's darkness and a day's sail, within touch.\"],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n\"Since he belonged, even at the age of six, to that great clan which cannot keep this feeling separate from that, but must let future prospects, with their joys and sorrows, cloud what is actually at hand, since to such people even in earliest childhood any turn in the wheel of sensation has the power to crystallise and transfix the moment upon which its gloom or radiance rests, James Ramsay, sitting on the floor cutting out pictures from the illustrated catalogue of the Army and Navy stores, endowed the picture of a refrigerator, as his mother spoke, with heavenly bliss.\"],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n\"It was fringed with joy.\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"string\", \"sent_raw\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>sent_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>CHAPTER 1 THE WINDOW  SECTION 1  \"Yes, of cour...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>\"But you'll have to be up with the lark,\" she ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>To her son these words conveyed an extraordina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Since he belonged, even at the age of six, to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>It was fringed with joy.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sent_no                                           sent_raw\n",
              "0        1  CHAPTER 1 THE WINDOW  SECTION 1  \"Yes, of cour...\n",
              "1        2  \"But you'll have to be up with the lark,\" she ...\n",
              "2        3  To her son these words conveyed an extraordina...\n",
              "3        4  Since he belonged, even at the age of six, to ...\n",
              "4        5                           It was fringed with joy."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "HXdacd0FXWTq",
        "outputId": "209d8b01-6173-4515-96b0-7ad0af2f6433"
      },
      "source": [
        "corpus_syuzhetr_df.info()\n",
        "corpus_syuzhetr_df = corpus_syuzhetr_df.loc[:,~corpus_syuzhetr_df.columns.duplicated()]\n",
        "corpus_syuzhetr_df.info()"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3511 entries, 0 to 3510\n",
            "Data columns (total 21 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   sent_no               3511 non-null   int64  \n",
            " 1   sent_raw              3511 non-null   object \n",
            " 2   syuzhet               3511 non-null   float64\n",
            " 3   bing                  3511 non-null   int64  \n",
            " 4   afinn                 3511 non-null   int64  \n",
            " 5   nrc                   3511 non-null   int64  \n",
            " 6   token_len             3511 non-null   int64  \n",
            " 7   char_len              3511 non-null   int64  \n",
            " 8   syuzhet_roll10        3161 non-null   float64\n",
            " 9   bing_roll10           3161 non-null   float64\n",
            " 10  syuzhet_roll10        3161 non-null   float64\n",
            " 11  bing_roll10           3161 non-null   float64\n",
            " 12  syuzhet_roll10        3161 non-null   float64\n",
            " 13  bing_roll10           3161 non-null   float64\n",
            " 14  syuzhet_roll20        2810 non-null   float64\n",
            " 15  syuzhet_lnorm         3511 non-null   float64\n",
            " 16  bing_lnorm            3511 non-null   float64\n",
            " 17  afinn_lnorm           3511 non-null   float64\n",
            " 18  nrc_lnorm             3511 non-null   float64\n",
            " 19  syuzhet_lnorm_roll10  3161 non-null   float64\n",
            " 20  bing_lnorm_roll10     3161 non-null   float64\n",
            "dtypes: float64(14), int64(6), object(1)\n",
            "memory usage: 576.1+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3511 entries, 0 to 3510\n",
            "Data columns (total 17 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   sent_no               3511 non-null   int64  \n",
            " 1   sent_raw              3511 non-null   object \n",
            " 2   syuzhet               3511 non-null   float64\n",
            " 3   bing                  3511 non-null   int64  \n",
            " 4   afinn                 3511 non-null   int64  \n",
            " 5   nrc                   3511 non-null   int64  \n",
            " 6   token_len             3511 non-null   int64  \n",
            " 7   char_len              3511 non-null   int64  \n",
            " 8   syuzhet_roll10        3161 non-null   float64\n",
            " 9   bing_roll10           3161 non-null   float64\n",
            " 10  syuzhet_roll20        2810 non-null   float64\n",
            " 11  syuzhet_lnorm         3511 non-null   float64\n",
            " 12  bing_lnorm            3511 non-null   float64\n",
            " 13  afinn_lnorm           3511 non-null   float64\n",
            " 14  nrc_lnorm             3511 non-null   float64\n",
            " 15  syuzhet_lnorm_roll10  3161 non-null   float64\n",
            " 16  bing_lnorm_roll10     3161 non-null   float64\n",
            "dtypes: float64(10), int64(6), object(1)\n",
            "memory usage: 466.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "nQ6ohlgKELtN",
        "outputId": "fb8b64ba-9cc1-48e5-f711-b3d1007a4ea5"
      },
      "source": [
        "plt.clf()\n",
        "corpus_syuzhetr_df[['syuzhet_roll10', 'bing_roll10', 'syuzhet_lnorm_roll10','bing_lnorm_roll10']].plot()"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb5a9237e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x800 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1600x800 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2i_kQdBsMC4"
      },
      "source": [
        "corpus_syuzhetr_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz3XdlUJnIRp"
      },
      "source": [
        "  print('Standardizing Sentences')\n",
        "  # Get Sentence Robust Standardization with Standard Scaler and  MedianIQRScaling\n",
        "  corpus_sents_df[col_stdscaler]  = list2stdscaler(corpus_sents_df[model_base])\n",
        "  corpus_sents_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "  sents_len_ls = list(corpus_sents_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_sents_df[model_base])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/sents_len_ls[i] for i in range(len(sents_len_ls))]\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  # corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_sents_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sents_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is7GsGTCgQik"
      },
      "source": [
        "### **Transformer 8 Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuGaejHXeGP2"
      },
      "source": [
        "# Transformer 8 Models: Read Computed sentiment data saved from previous run of this notebook\n",
        "\n",
        "corpus_trans_sents_df = pd.read_csv('sum_sentiments_sents_trans_vwoolf_tothelighthouse.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm2Lsk4zgoce"
      },
      "source": [
        "# Clean Chapter DataFrame\n",
        "\n",
        "if 'Unnamed: 0' in corpus_trans_sents_df.columns:\n",
        "  corpus_trans_sents_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "  \n",
        "corpus_trans_sents_df.head(2)\n",
        "corpus_trans_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfyebD6HhQIU"
      },
      "source": [
        "# TODO: If missing, generate derived values for each model, lnorm/not, stdscaler/medianiqr, roll10/not"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d-AM_WvbqFu"
      },
      "source": [
        "\n",
        "# Verfiy there are no NaN or Empty strings that passed the cleaning process\n",
        "\n",
        "# Only execute if previously datafile/corpus_sents_df\n",
        "\n",
        "\n",
        "# corpus_sents_df[corpus_sents_df['sent_clean'].isnull()]\n",
        "\n",
        "# corpus_sents_df[corpus_sents_df['sent_clean'].apply(lambda x: len(str(x)) <= 0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAAQ0SwfZynO"
      },
      "source": [
        "# Verify that hyphenated words are correctly handled (e.g. 'summer-mroning' -> 'summer morning')\n",
        "\n",
        "# corpus_sents_df[corpus_sents_df['sent_clean'].str.contains('summer', na=False)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsEbvCoCX7HY"
      },
      "source": [
        "## **(b) Compute Baseline Sentiments (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZK1gA47xzkS"
      },
      "source": [
        "### **Select Sentiment Models (Manual)**\n",
        "\n",
        "NOTE:\n",
        "\n",
        "* Stanza (Stanford OpenNLP) can take upto 50 minutes to run\n",
        "\n",
        "* Listed in increasing order of (approx) run time\n",
        "\n",
        "* MPQA/SentiStrength not yet implemented (placeholders only for now)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0pStg1gJTZA"
      },
      "source": [
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "NRC_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "# MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "# SentiStrength_Arc = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0IACAN5JTZC"
      },
      "source": [
        "# Create and Verify custom list of Models to include\n",
        "\n",
        "MODELS_CUSTOM_LS = []\n",
        "\n",
        "if VADER_Arc:\n",
        "  MODELS_CUSTOM_LS.append('vader')\n",
        "if TextBlob_Arc:\n",
        "  MODELS_CUSTOM_LS.append('textblob')\n",
        "if Flair_Arc:\n",
        "  MODELS_CUSTOM_LS.append('flair')\n",
        "if Stanza_Arc:\n",
        "  MODELS_CUSTOM_LS.append('stanza')\n",
        "if SentimentR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentimentr')\n",
        "if Syuzhet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('syuzhet')\n",
        "if AFINN_Arc:\n",
        "  MODELS_CUSTOM_LS.append('afinn')\n",
        "if Bing_Arc:\n",
        "  MODELS_CUSTOM_LS.append('bing')\n",
        "if Pattern_Arc:\n",
        "  MODELS_CUSTOM_LS.append('pattern')\n",
        "if SentiWord_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentiword')\n",
        "if SenticNet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('senticnet')\n",
        "if NRC_Arc:\n",
        "  MODELS_CUSTOM_LS.append('nrc')\n",
        "\n",
        "print(f'Here are the Models we are using to ensemble and save:\\n\\n   {MODELS_CUSTOM_LS}')\n",
        "\n",
        "\"\"\"\n",
        "models_incl_ls = []\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  models_incl_ls.append(amodel[:2])\n",
        "models_incl_str = ''.join(models_incl_ls)\n",
        "\n",
        "print(f'Here is a custom name abbr: {models_incl_str}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I9EVfxnffkG"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw0JPNe6T2ap"
      },
      "source": [
        "# Calculate (win_(x)1per) 1% of Corpus length for smallest (odd-valued) rolling window\n",
        "\n",
        "# Sentences\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "win_raw_s1per = int(corpus_sents_len * 0.01)\n",
        "# print(f'1% Rolling Window: {win_raw_s1per}')\n",
        "\n",
        "if win_raw_s1per % 2:\n",
        "  win_s1per = win_raw_s1per\n",
        "else:\n",
        "  win_s1per = win_raw_s1per + 1\n",
        "\n",
        "# Paragraphs\n",
        "# corpus_parags_df = corpus_all_df\n",
        "corpus_parags_len = len(corpus_sents_df['parag_no'].unique())\n",
        "\n",
        "win_raw_p1per = int(corpus_parags_len * 0.01)\n",
        "# print(f'1% Rolling Window: {win_raw_1per}')\n",
        "\n",
        "if win_raw_p1per % 2:\n",
        "  win_p1per = win_raw_p1per\n",
        "else:\n",
        "  win_p1per = win_raw_p1per + 1\n",
        "\n",
        "\n",
        "# Sections\n",
        "\n",
        "# NO NEED FOR SLIDING WINDOW ON SECTIONS\n",
        "\n",
        "\n",
        "print(f'Sentence 1 Percent window: {win_s1per}')\n",
        "print(f'Paragraph 1 Percent window: {win_p1per}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGjfPK9u21HH"
      },
      "source": [
        "# Verify Sentiment Lexicon hash files are accessable\n",
        "\n",
        "lexicons_path = f'/gdrive/MyDrive/{LEXICONS_SUBDIR[1:]}/hash*.csv'\n",
        "!pwd\n",
        "!ls $lexicons_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tAxuAxU7ueg"
      },
      "source": [
        "### **Calculate SentimentR (Jockers-Rinker) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foQeDPQpvyB3"
      },
      "source": [
        "if SentimentR_Arc == True:\n",
        "  model_base = 'sentimentr'\n",
        "\n",
        "  \"\"\"\n",
        "  model_name = 'sentimentr_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n",
        "  \"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEnJj5rIMcSj"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_sentimentr.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq7dImOJozm5"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "\n",
        "  lexicon_sentimentr_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_sentimentr.csv')\n",
        "  lexicon_sentimentr_df['x'] = lexicon_sentimentr_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_sentimentr_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_sentimentr_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_sentimentr_df.head()\n",
        "    lexicon_sentimentr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25_Zjja0o_Hx"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "\n",
        "  id = lexicon_sentimentr_df.word.values\n",
        "  values = lexicon_sentimentr_df.polarity.values\n",
        "\n",
        "  lexicon_sentimentr_dt = dict(zip(id, values))\n",
        "  # lexicon_sentimentr_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_sentimentr_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQYYP2hiO53j"
      },
      "source": [
        "# Verify\n",
        "\n",
        "# corpus_sents_df['sent_clean'].isna().any()\n",
        "# corpus_chaps_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVSImJ3ETSYt"
      },
      "source": [
        "corpus_parags_df.columns\n",
        "print('\\n')\n",
        "corpus_sects_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxe15XjNwByS"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_sentimentr(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_sentimentr_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SentimentR_Arc == True:\n",
        "  model_base = 'sentimentr'\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentimentr, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVX6rg5KwBvF"
      },
      "source": [
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pkrp6TwY7OI"
      },
      "source": [
        "# Verify there are no empty Sentences\n",
        "\n",
        "corpus_sents_df[corpus_sents_df['token_len'] == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbGz2Yi5wByY"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4aDdJcrzlE1"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOSiIyhbN2-8"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "  # corpus_sents_df['sentimentr'].plot(alpha=0.3)\n",
        "  corpus_sents_df['sentimentr_stdscaler'].plot(alpha=0.1)\n",
        "  corpus_sents_df['sentimentr_medianiqr'].plot(alpha=0.3)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8POYGujomVt"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lvswx9yovMV"
      },
      "source": [
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZjqwTvU76AR"
      },
      "source": [
        "### **Calculate Syuzhet (Jockers) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjOXGQX54lbX"
      },
      "source": [
        "# Define Model names\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "  model_base = 'syuzhet'\n",
        "  model_name = 'syuzhet_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB5fBNQ-QNwk"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_syuzhet.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkhaGcuy4lbg"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "\n",
        "  lexicon_syuzhet_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_syuzhet.csv')\n",
        "  lexicon_syuzhet_df['word'] = lexicon_syuzhet_df['word'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_syuzhet_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_syuzhet_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_syuzhet_df.head()\n",
        "    lexicon_syuzhet_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9IuINuR4lbi"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "\n",
        "  id = lexicon_syuzhet_df.word.values\n",
        "  values = lexicon_syuzhet_df.value.values\n",
        "\n",
        "  lexicon_syuzhet_dt = dict(zip(id, values))\n",
        "  # lexicon_sentimentr_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_syuzhet_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO-txaFL4lbo"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_syuzhet(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_syuzhet_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_syuzhet, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfsLZSo04lbp"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9VFBSXD0vJo"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-BwrbyXR5Hd"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "  # corpus_sents_df['syuzhet'].plot(alpha=0.3)\n",
        "  corpus_sents_df['syuzhet_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['syuzhet_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA3dWsnF78mi"
      },
      "source": [
        "### **Calculate Bing (HuLiu) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wvq1prK7n1k"
      },
      "source": [
        "if Bing_Arc == True:\n",
        "  model_base = 'bing'\n",
        "  model_name = 'bing_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDfB2f0olded"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_bing.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZELGxS8y9PiS"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if Bing_Arc == True:\n",
        "  \n",
        "  lexicon_bing_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_bing.csv')\n",
        "  lexicon_bing_df['x'] = lexicon_bing_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_bing_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_bing_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_bing_df.head()\n",
        "    lexicon_bing_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJQTaeue9VjI"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if Bing_Arc == True:\n",
        "\n",
        "  id = lexicon_bing_df.word.values\n",
        "  values = lexicon_bing_df.polarity.values\n",
        "\n",
        "  lexicon_bing_dt = dict(zip(id, values))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Ffzu1m7n10"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_bing(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_bing_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Bing_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_bing, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWITnL2a9rrx"
      },
      "source": [
        "# Calculate Bing Sentiment [0,1,2]\n",
        "\n",
        "def bing_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += lex_discrete2continous_sentiment(str(aword), lexicon_bing_dt)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+1)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xpzm3hi7n1x"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Bing_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(bing_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UFEItnO-i7h"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Bing_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=bing_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4thQWz3-i7k"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if Bing_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKlKnqmWmBEP"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Bing_Arc == True:\n",
        "  # corpus_sents_df['bing'].plot(alpha=0.3)\n",
        "  corpus_sents_df['bing_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['bing_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkNZVk128jV9"
      },
      "source": [
        "### **Calculate SentiWord Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnL4ORNp8MgB"
      },
      "source": [
        "if SentiWord_Arc == True:\n",
        "  model_base = 'sentiword'\n",
        "  model_name = 'sentiword_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ootf8xdbnHPx"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_sentiword.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpVmt0fK8xi_"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "\n",
        "  lexicon_sentiword_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_sentiword.csv')\n",
        "  lexicon_sentiword_df['x'] = lexicon_sentiword_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_sentiword_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_sentiword_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_sentiword_df.head()\n",
        "    lexicon_sentiword_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwtK2M9h879M"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "\n",
        "  id = lexicon_sentiword_df.word.values\n",
        "  values = lexicon_sentiword_df.polarity.values\n",
        "\n",
        "  lexicon_sentiword_dt = dict(zip(id, values))\n",
        "  # lexicon_sentiword_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_sentiword_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I-QwB478MgP"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_sentiword(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_sentiword_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SentiWord_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentiword, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbEF88568MgS"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9GfZ5RQnn4G"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "  # corpus_sents_df['sentiword'].plot(alpha=0.3)\n",
        "  corpus_sents_df['sentiword_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['sentiword_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUcANLM_8mtT"
      },
      "source": [
        "### **Calculate SenticNet Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://sentic.net/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OqhSberA1hZ"
      },
      "source": [
        "if SenticNet_Arc == True:\n",
        "  model_base = 'senticnet'\n",
        "  model_name = 'senticnet_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsnCzsfFnx7B"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_senticnet.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmMfvQvYBBoM"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "\n",
        "  lexicon_senticnet_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_senticnet.csv')\n",
        "  lexicon_senticnet_df['x'] = lexicon_senticnet_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_senticnet_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_senticnet_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_senticnet_df.head()\n",
        "    lexicon_senticnet_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2oS71STBIUS"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "\n",
        "  id = lexicon_senticnet_df.word.values\n",
        "  values = lexicon_senticnet_df.polarity.values\n",
        "\n",
        "  lexicon_senticnet_dt =dict(zip(id, values))\n",
        "  # lexicon_jockersrinker_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  text2sentiment(sent_test, lexicon_senticnet_dt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHQwXBl5BlRM"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_senticnet(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on senticnet lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_senticnet_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SenticNet_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_senticnet, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raHUj3a4A1hs"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9lSo0Kmn_T4"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "  # corpus_sents_df['senticnet'].plot(alpha=0.3)\n",
        "  corpus_sents_df['senticnet_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['senticnet_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn4KQYpH3glK"
      },
      "source": [
        "### **Calculate NRC Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USx6LRmoCFV8"
      },
      "source": [
        "if NRC_Arc == True:\n",
        "  model_base = 'nrc'\n",
        "  model_name = 'nrc_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EdOTv3KoFEV"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_nrc.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFEszSc13glL"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if NRC_Arc == True:\n",
        "\n",
        "  lexicon_nrc_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_nrc.csv')\n",
        "  lexicon_nrc_df['x'] = lexicon_nrc_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_nrc_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_nrc_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_nrc_df.head()\n",
        "    lexicon_nrc_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nAkx4Mv3glL"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if NRC_Arc == True:\n",
        "\n",
        "  id = lexicon_nrc_df.word.values\n",
        "  values = lexicon_nrc_df.polarity.values\n",
        "\n",
        "  lexicon_nrc_dt =dict(zip(id, values))\n",
        "  # lexicon_jockersrinker_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv1nVSATb7z4"
      },
      "source": [
        "# Calculate NRC Sentiment [0,1,2]\n",
        "\n",
        "def nrc_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += lex_discrete2continous_sentiment(str(aword), lexicon_nrc_dt)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+10)\n",
        "\n",
        "  return text_sentiment_norm\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss3Xixo_CX2q"
      },
      "source": [
        "# Test\n",
        "\n",
        "if NRC_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(nrc_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhqnljMSCX2w"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if NRC_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=nrc_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2q97Dm-CX2z"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if NRC_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBK7LQx4oXI-"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if NRC_Arc == True:\n",
        "  # corpus_sents_df['nrc'].plot(alpha=0.3)\n",
        "  corpus_sents_df['nrc_stdscaler'].plot(alpha=0.1)\n",
        "  corpus_sents_df['nrc_medianiqr'].plot(alpha=0.3)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRTjCPLb8cbB"
      },
      "source": [
        "### **Calculate Afinn Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://github.com/fnielsen/afinn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcnxqnyzDCde"
      },
      "source": [
        "if AFINN_Arc == True:\n",
        "  model_base = 'afinn'\n",
        "  model_name = 'afinn_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDG2KxvNBdj6"
      },
      "source": [
        "if AFINN_Arc == True:\n",
        "  !pip install afinn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evnkXWL58CcX"
      },
      "source": [
        "# Install and configure for English\n",
        "\n",
        "if AFINN_Arc == True:\n",
        "  from afinn import Afinn\n",
        "  afinn = Afinn(language='en')\n",
        "\n",
        "  # Test\n",
        "\n",
        "  # afinn.score('I had the worst day.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKbeW_cMXhmI"
      },
      "source": [
        "# Calculate AFINN Sentiment [0,1,2]\n",
        "\n",
        "def afinn_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += afinn.score(aword)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+0.1)\n",
        "\n",
        "  return float(text_sentiment_norm)  # return float vs np.float64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRCd4VpwDO0Q"
      },
      "source": [
        "# Test\n",
        "\n",
        "if AFINN_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(afinn_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrP_wxB3DO0S"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if AFINN_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=afinn_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgZaMPYKDO0T"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if AFINN_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clUzylGOog5h"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if AFINN_Arc == True:\n",
        "  # corpus_sents_df['afinn'].plot(alpha=0.3)\n",
        "  corpus_sents_df['afinn_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['afinn_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAEiglIPDfFI"
      },
      "source": [
        "### **Calculate VADER Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgkvefzpDgx-"
      },
      "source": [
        "if VADER_Arc == True:\n",
        "  model_base = 'vader'\n",
        "  model_name = 'vader_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wodGtjXhDmZN"
      },
      "source": [
        "if VADER_Arc == True:\n",
        "  # Sentiment evaluation function\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "  # Test\n",
        "  sid.polarity_scores('hello world')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS8e25MkDmZP"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "if VADER_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sid.polarity_scores, sentiment_type='compound')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Azyv2lDmZP"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if VADER_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq8KNwpjom4X"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if VADER_Arc == True:\n",
        "  # corpus_sents_df['vader'].plot(alpha=0.3)\n",
        "  corpus_sents_df['vader_stdscaler'].plot(alpha=0.1)\n",
        "  corpus_sents_df['vader_medianiqr'].plot(alpha=0.3)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCN4c-G48e7-"
      },
      "source": [
        "### **Calculate TextBlob Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MfVWZ34Vg8U"
      },
      "source": [
        "if TextBlob_Arc == True:\n",
        "  model_base = 'textblob'\n",
        "  model_name = 'textblob_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "118Blghk7fjp"
      },
      "source": [
        "if TextBlob_Arc == True:\n",
        "  from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhJsYxPoVhY4"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "def textblob_sentiment(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return a sentiment value between -1.0 to +1.0 using TextBlob\n",
        "  '''\n",
        "  return TextBlob(text_str).sentiment.polarity\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if TextBlob_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=textblob_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlHRf2aFVhY7"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if TextBlob_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_name, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_name, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_name, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_name, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMWewkTgord1"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if TextBlob_Arc == True:\n",
        "  # corpus_sents_df['textblob'].plot(alpha=0.3)\n",
        "  corpus_sents_df['textblob_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['textblob_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2blGfVlKb_s"
      },
      "source": [
        "### **Calculate Pattern Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU60-nqpCsl7"
      },
      "source": [
        "if Pattern_Arc == True:\n",
        "  model_base = 'pattern'\n",
        "  model_name = 'pattern_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KxnLfHoL3Fy"
      },
      "source": [
        "if Pattern_Arc == True:\n",
        "  !pip install pattern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtwmIrSOKZRm"
      },
      "source": [
        "if Pattern_Arc == True:\n",
        "  from pattern.en import sentiment as pattern_sa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vwtm_jBKZM2"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  pattern_sa(sent_test)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXq-jDPxasVY"
      },
      "source": [
        "# Calculate Pattern Sentiment [0,1,2]\n",
        "\n",
        "def pattern_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += pattern_sa(str(aword))[0]\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+0.01)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-sXRBNWC08o"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(pattern_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db0hezLKC08p"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Pattern_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=pattern_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGHixqpOC08q"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clydVLby2KhG"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  # corpus_sents_df['pattern'].plot(alpha=0.3)\n",
        "  corpus_sents_df['pattern_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['pattern_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-X3xNWkoyLS"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  corpus_sents_df['pattern'].rolling(10*win_s1per, center=True).mean().plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQwX3APKnhjP"
      },
      "source": [
        "corpus_sents_df['pattern'].plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM1OmuaiVQIH"
      },
      "source": [
        "# Check Pattern Series for Outliers\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  print('Furthest Positive Outlier in Pattern Time Series:')\n",
        "  corpus_sents_df[corpus_sents_df['pattern'] == corpus_sents_df['pattern'].max()][['sent_no', 'pattern', 'sent_raw']]\n",
        "\n",
        "  pattern_max_sentno = int(corpus_sents_df[corpus_sents_df['pattern'] == corpus_sents_df['pattern'].max()][['sent_no']].max())\n",
        "  print(f'Max Outlier for Pattern Sentiment Model is at Sentence #{pattern_max_sentno}')\n",
        "\n",
        "  print('Furthest Negative Outlier in Pattern Time Series:')\n",
        "  corpus_sents_df[corpus_sents_df['pattern'] == corpus_sents_df['pattern'].min()][['sent_no', 'pattern', 'sent_raw']]\n",
        "\n",
        "  pattern_min_sentno = int(corpus_sents_df[corpus_sents_df['pattern'] == corpus_sents_df['pattern'].min()][['sent_no']].min())\n",
        "  print(f'Max Outlier for Pattern Sentiment Model is at Sentence #{pattern_min_sentno}')\n",
        "\n",
        "  print('\\n')\n",
        "  print('Median Absolute Deviation (MAD) for Pattern Time Series:')\n",
        "  robust.mad(corpus_sents_df['pattern'])\n",
        "\n",
        "  temp_df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny19347PZ85l"
      },
      "source": [
        "# Verify what is happening in the neighborhood of the Maximum Sentiment Outlier for Pattern\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  outlier_halfwin = 10\n",
        "  outlier_winstart = pattern_max_sentno - outlier_halfwin\n",
        "  outlier_winend = pattern_max_sentno + outlier_halfwin\n",
        "\n",
        "  corpus_sents_df.iloc[outlier_winstart:outlier_winend]['pattern'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRDctGTdUfKk"
      },
      "source": [
        "# Pattern library has a bug wherein a/several Sentences have far outlying Sentiments\n",
        "#   we need to clip these to within n * MAD\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  MAD_Clip_Boundary = 5.2 #@param {type:\"slider\", min:1.0, max:10, step:0.1}\n",
        "\n",
        "  # find the limits of 2.5 Median Absolute Deviation of Pattern Time Series\n",
        "\n",
        "  clip_25mad = MAD_Clip_Boundary * robust.mad(corpus_sents_df['pattern'])\n",
        "  print(f'Clip Pattern Series at 2.5 x Median Absolute Deviation (MAD) = {clip_25mad}')\n",
        "\n",
        "  # Create a Temporary DataFrame to test/find best MAD Clipping multiplier for Pattern\n",
        "  temp_df['pattern'] = pd.Series(corpus_sents_df['pattern'].clip(upper=clip_25mad))\n",
        "  temp_df['pattern'].clip(lower=-clip_25mad, inplace=True)\n",
        "\n",
        "  # Verify \n",
        "  temp_df['pattern'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co3jaquyZom7"
      },
      "source": [
        "# Once a good Clip_MAD_multiplier if found, update Pattern Time Series with it\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  corpus_sents_df['pattern'] = temp_df['pattern']\n",
        "  corpus_sents_df['pattern'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsaziON_Z263"
      },
      "source": [
        "### **Calculate Stanza/OpenNLP Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://github.com/piyushpathak03/NLP-using-STANZA/blob/main/Stanza.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZgGfcCuFnmI"
      },
      "source": [
        "if Stanza_Arc == True:\n",
        "  model_base = 'stanza'\n",
        "  model_name = 'stanza_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoZUi2AwZ_7L"
      },
      "source": [
        "if Stanza_Arc == True:\n",
        "  !pip install stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5txTb6aIZ2tN"
      },
      "source": [
        "%time\n",
        "\n",
        "import stanza\n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  stanza.download('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NORYbxsxZ2qg"
      },
      "source": [
        "if Stanza_Arc == True:\n",
        "  nlp = stanza.Pipeline('en', processors='tokenize,sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtOtBfYwZ2na"
      },
      "source": [
        "# Test stanza directly\n",
        "\n",
        "# doc = nlp('Ram is a bad boy')\n",
        "# for i, sentence in enumerate(doc.sentences):\n",
        "#     print(i, sentence.sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKnox67kayod"
      },
      "source": [
        "# Calculate Stanza Sentiment [0,1,2]\n",
        "\n",
        "def stanza_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_tot = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    adoc = nlp(aword)\n",
        "    for i, sentence in enumerate(adoc.sentences):\n",
        "      text_sentiment_tot += float(sentence.sentiment)\n",
        "  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.1)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNngkBBAF26C"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(stanza_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrY2mrhVF26D"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# NOTE: requires about 30-50mins (20210708 at 0730) Colab Pro: GPU+RAM \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Stanza_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=stanza_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSiYC73nF26D"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5tUYZA1DExu"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  # corpus_sents_df['stanza'].plot(alpha=0.3)\n",
        "  corpus_sents_df['stanza_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['stanza_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIGQgWvyOtg6"
      },
      "source": [
        "### **Calculate Flair Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWbcaaDDO1J1"
      },
      "source": [
        "if Flair_Arc == True:\n",
        "  model_base = 'flair'\n",
        "  model_name = 'flair_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooLdktLHO1J3"
      },
      "source": [
        "if Flair_Arc == True:\n",
        "  !pip install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77-3mVyRPJbb"
      },
      "source": [
        "if Flair_Arc == True:\n",
        "  from flair.models import TextClassifier\n",
        "  from flair.data import Sentence\n",
        "\n",
        "  classifier = TextClassifier.load('en-sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJOopGZhO1J5"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Flair_Arc == True:\n",
        "  sentence = Sentence('The food was great!')\n",
        "  classifier.predict(sentence)\n",
        "\n",
        "  # print sentence with predicted labels\n",
        "  print('Sentence above is: ', sentence.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7fZT10HQDqd"
      },
      "source": [
        "def get_flairsentiment(text_str):\n",
        "  # TODO: For efficiency, combine sentences in batches as arrays (if possible)\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return a floating point -1.0 to 1.0 value for Sentiment\n",
        "  '''\n",
        "\n",
        "  text_tokenized_obj = Sentence(text_str)\n",
        "  classifier.predict(text_tokenized_obj)\n",
        "\n",
        "  sentiment_str = str(text_tokenized_obj.labels[0])\n",
        "\n",
        "  sentiment_ls = sentiment_str.split(' ')\n",
        "  \n",
        "  sentiment_sign = sentiment_ls[0]\n",
        "\n",
        "  if sentiment_sign.lower() == 'positive':\n",
        "    sign_multiplier = 1.0\n",
        "  else:\n",
        "    sign_multiplier = -1.0\n",
        "\n",
        "  sentiment_abs = float(sentiment_ls[1][1:-1])\n",
        "\n",
        "  sentiment_fl = sign_multiplier * sentiment_abs \n",
        "\n",
        "  return sentiment_fl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbgQVQlNSdcE"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Flair_Arc == True:\n",
        "  get_flairsentiment('it is.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX0PDn5AO1J9"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Flair_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=get_flairsentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vlK-dyHO1J-"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if Flair_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VycWC2gDO1J_"
      },
      "source": [
        "# Verify \n",
        "\n",
        "if Flair_Arc == True:\n",
        "  corpus_sents_df['flair'].plot(alpha=0.3)\n",
        "  corpus_sents_df['flair_stdscaler'].plot(alpha=0.3)\n",
        "  corpus_sents_df['flair_medianiqr'].plot(alpha=0.1)\n",
        "  plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfMwbhVMwgXw"
      },
      "source": [
        "### **(Optional) Calculate SentimentR and SyuzhetR Sentiments in RStudio**\n",
        "\n",
        "**NOTE** Process in RStudio with the following R Script\n",
        "```\n",
        "# Setup\n",
        "library('syuzhet')\n",
        "# getwd()\n",
        "# list.files(pattern=’*.txt’)\n",
        "\n",
        "\n",
        "# Set Input Corpus textfile\n",
        "# corpus_input = 'vwoolf_tothelighthouse.txt'\n",
        "\n",
        "###\n",
        "corpus_input = 'vwoolf_tothelighthouse.txt'\n",
        "###\n",
        "\n",
        "\n",
        "# corpus_input = 'confessions_staugustine'\n",
        "# Set Output Sentiments Datafile names\n",
        "#  corpus_output = 'sum_sentiments_vwoolf_tothelighthouse_4models.csv'\n",
        "\n",
        "###\n",
        "syuzhet_output = 'sum_sentimentsvwoolf_tothelighthouse_syuzhetR_4models.csv'\n",
        "###\n",
        "\n",
        "# Use 4 Models in Syuzhet to parse Corpus and generate 4 Sentiment Time Series\n",
        "syuzhet_str <- syuzhet::get_text_as_string(syuzhet_input)\n",
        "syuzhet_sents_v <- syuzhet::get_sentences(syuzhet_str)\n",
        "\n",
        "syuzhet_all_df <- data.frame(sent_raw =syuzhet_sents_v)\n",
        "\n",
        "syuzhet_all_df$syuzhet <- syuzhet::get_sentiment(corpus_sents_v, method='syuzhet')\n",
        "syuzhet_all_df$bing <- syuzhet::get_sentiment(corpus_sents_v, method='bing')\n",
        "syuzhet_all_df$afinn <- syuzhet::get_sentiment(corpus_sents_v, method='afinn')\n",
        "syuzhet_all_df$nrc <- syuzhet::get_sentiment(corpus_sents_v, method='nrc')\n",
        "\n",
        "\n",
        "# Save Results    \n",
        "# write.csv(rc_ddefoe_all_df, ‘sum_sentiments_rc_ddefoe_syuzhetR_4models.csv’)\n",
        "write.csv(syuzhet_all_df, syuzhet_output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "library('sentimentr')\n",
        "# conflict with syuzhet::get_sentences\n",
        "# getwd()\n",
        "# list.files(pattern=’*.txt’)\n",
        "\n",
        "# Set Input Corpus textfile\n",
        "# corpus_input = 'ddefoe_robinsoncrusoe_final_hand.txt'\n",
        "# sentimentr_input = 'ttl_final_hand.txt'\n",
        "sentimentr_input = corpus_input\n",
        "\n",
        "# Set Output Sentiments Datafile names\n",
        "\n",
        "###\n",
        "sentimentr_output = 'sum_sentiments_vwoolf_tothelighthouse_sentimentR_7models.csv'\n",
        "###\n",
        "\n",
        "# continuing from syuzhet code above we inherit global var: syuzhet_sents_v\n",
        "# SentimentR recommends reparsing these with sentimentr::get_sentences(rc_ddefoe_sents_v)\n",
        "sentimentr_sents_v <- sentimentr::get_sentences(syuzhet_sents_v)\n",
        "\n",
        "# Create data.frame with jockers_rinker sentiments\n",
        "sentimentr_all_df <- data.frame(sent_raw = syuzhet_sents_v)\n",
        "\n",
        "# Add other lexicon sentiments\n",
        "sentimentr_all_df$jockers_rinker <- sentimentr::sentiment(sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_jockers_rinker, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$jockers <- sentimentr::sentiment(sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_jockers, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$huliu <- sentimentr::sentiment(sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_huliu, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$lmcd <- sentimentr::sentiment(sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_loughran_mcdonald, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$nrc <- sentimentr::sentiment(sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_nrc, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$senticnet <- sentimentr::sentiment(sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_senticnet, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "sentimentr_all_df$sentiword <- sentimentr::sentiment(sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_sentiword, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n",
        "\n",
        "write.csv(sentimentr_all_df, sentimentr_output)\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNCGf1KZEpld"
      },
      "source": [
        "## **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q5JJa0TqlEE"
      },
      "source": [
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FblNRdEbMm_b"
      },
      "source": [
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMRog9JWk40F"
      },
      "source": [
        "# Save Corpus Text DataFrames and Sentiment Values\n",
        "\n",
        "save_dataframes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNkAJzWhEpld"
      },
      "source": [
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_FILENAME.split('.')[0]).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "\"\"\"\n",
        "# Save Sentences of original Raw and Cleaned Corpus\n",
        "corpus_text_sentences_raw_filename = f'corpus_text_sentences_raw_{title_str}.csv' # _{datetime_now}.txt'\n",
        "print(f'Saving Corpus text raw sentences to file: {corpus_text_sentences_raw_filename}')\n",
        "corpus_sents_df['sent_raw'].to_csv(corpus_text_sentences_raw_filename)\n",
        "\n",
        "corpus_text_sentences_clean_filename = f'corpus_text_sentences_clean_{title_str}.csv' # _{datetime_now}.txt'\n",
        "print(f'Saving Corpus text clean sentences to file: {corpus_text_sentences_clean_filename}')\n",
        "corpus_sents_df['sent_clean'].to_csv(corpus_text_sentences_clean_filename)\n",
        "\n",
        "\n",
        "# Save Paragraphs of original Raw and Cleaned Corpus\n",
        "corpus_text_paragraphs_raw_filename = f'corpus_text_paragraphs_raw_{title_str}.csv' # _{datetime_now}.txt'\n",
        "print(f'Saving Corpus text raw paragraphs to file: {corpus_text_paragraphs_raw_filename}')\n",
        "corpus_parags_df['parag_raw'].to_csv(corpus_text_paragraphs_raw_filename)\n",
        "\n",
        "corpus_text_paragraphs_clean_filename = f'corpus_text_paragraphs_clean_{title_str}.csv' # _{datetime_now}.txt'\n",
        "print(f'Saving Corpus text clean sentences to file: {corpus_text_paragraphs_clean_filename}')\n",
        "corpus_parags_df['parag_clean'].to_csv(corpus_text_paragraphs_clean_filename)\n",
        "\n",
        "\"\"\";\n",
        "\n",
        "# Save DataFrames that divide Corpus at Sentence, Paragraph, Section and Chapter Levels\n",
        "corpus_sents_filename = f'corpus_sents_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sentences to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "corpus_parags_filename = f'sum_sentiments_parags_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Paragraphs to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "corpus_sects_filename = f'sum_sentiments_sects_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sections to file: {corpus_sects_filename}')\n",
        "corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "corpus_chaps_filename = f'sum_sentiments_chaps_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Chapters to file: {corpus_chaps_filename}')\n",
        "corpus_chaps_df.to_csv(corpus_chaps_filename)\n",
        "\n",
        "\"\"\"\n",
        "# Save Sentiment Values from Models\n",
        "\n",
        "corpus_sents_filename = f'sum_sentiments_sents_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sentences to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "corpus_parags_filename = f'sum_sentiments_parags_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Paragraphs to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "corpus_sects_filename = f'sum_sentiments_sects_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sections to file: {corpus_sects_filename}')\n",
        "corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "corpus_chaps_filename = f'sum_sentiments_chaps_baselines_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Chapters to file: {corpus_chaps_filename}')\n",
        "corpus_chaps_df.to_csv(corpus_chaps_filename)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0s6LIujwDD2"
      },
      "source": [
        "# **Explore Sentiment Models and Arcs**\n",
        "\n",
        "Baseline Models\n",
        "\n",
        "* VADER [-1.0 to 1.0] zero peak\n",
        "* TextBlob [-1.0 to 1.0] zero peak\n",
        "* Stanza outliers [-1.0 to 199.0] pos, outliers(+peak)\n",
        "* AFINN [-14 (-8 to 8) 20] discrete\n",
        "* SentimentR 11,710 [-5.4 to 8.8] norm\n",
        "* Syuzhet [-5.4 to 8.8] norm\n",
        "* Bing [-100.0 (-20.0 to 20.0) 100] discrete, outliers\n",
        "* Pattern [-1.0 to 1.0] norm\n",
        "* SentiWord [-3.8 to 4.4] norm\n",
        "* SenticNet [-3.8 to 10] norm\n",
        "* NRC [-100.0 (-5.0 to 5.0) 100] zero, outliers\n",
        "\n",
        "SentimentR Models\n",
        "\n",
        "* Jockers_Rinker\n",
        "* Jockers\n",
        "* HuLiu\n",
        "* NRC\n",
        "* Loughran-McDonald\n",
        "* SenticNet\n",
        "* SentiWord\n",
        "\n",
        "SyuzhetR Models\n",
        "\n",
        "* Syuzhet\n",
        "* Bing\n",
        "* AFINN\n",
        "* NRC\n",
        "\n",
        "Tranformer Models\n",
        "\n",
        "* NLPTown\n",
        "* RoBERTa Large 15 Datasets\n",
        "* BERT Yelp Dataset\n",
        "* BERT Code Switching Hinglish\n",
        "* IMDB 2-way \n",
        "* Huggingface Default (Distilled BERT)\n",
        "* T5 IMDB 50k Dataset\n",
        "* RoBERTa XML 8 Languages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGqpjXvRwS20"
      },
      "source": [
        "## **EDA Baseline Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8gYxSS0GRWk"
      },
      "source": [
        "# Create SMA roll=10% for all models_stdscaler as baseline\n",
        "\n",
        "win_s1per = int(corpus_sents_df.shape[0] * 1/100)\n",
        "\n",
        "col_stdscaler_roll_ls = []\n",
        "for amodel in models_baseline_ls:\n",
        "  col_stdscaler = f'{amodel}_stdscaler'\n",
        "  col_stdscaler_roll = f'{amodel}_stdscaler_{roll_str}'\n",
        "  corpus_sents_df[col_stdscaler_roll] = corpus_sents_df[col_stdscaler].rolling(10*win_s1per, center=True).mean()\n",
        "  col_stdscaler_roll_ls.append(col_stdscaler_roll)\n",
        "\n",
        "col_stdscaler_roll_mean = col_stdscaler_roll + '_mean'\n",
        "corpus_sents_df[col_stdscaler_roll_mean] = corpus_sents_df[col_stdscaler_roll_ls].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZYRLVQYH6__"
      },
      "source": [
        "corpus_sents_df[col_stdscaler_roll_ls].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJJNNaxR0iJ0"
      },
      "source": [
        "##### **Sentence SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqmU4QRLOClI"
      },
      "source": [
        "# Sentence Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "NRC_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = True #@param {type:\"boolean\"}\n",
        "# Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "# MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "# SentiStrength_Arc = False #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  models_subset_ls.append('sentimentr')\n",
        "if Syuzhet_Arc == True:\n",
        "  models_subset_ls.append('syuzhet')\n",
        "if Bing_Arc == True:\n",
        "  models_subset_ls.append('bing')\n",
        "if SenticNet_Arc == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentiWord_Arc == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if NRC_Arc == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if AFINN_Arc == True:\n",
        "  models_subset_ls.append('afinn')\n",
        "if VADER_Arc == True:\n",
        "  models_subset_ls.append('vader')\n",
        "if TextBlob_Arc == True:\n",
        "  models_subset_ls.append('textblob')\n",
        "if Flair_Arc == True:\n",
        "  models_subset_ls.append('flair')\n",
        "if Pattern_Arc == True:\n",
        "  models_subset_ls.append('pattern')\n",
        "if Stanza_Arc == True:\n",
        "  models_subset_ls.append('stanza')\n",
        "if Mean_All_Arc == True:\n",
        "  models_subset_ls.append('mean_all')\n",
        "\n",
        "if len(str(SMA_Window_Percent)) == 1:\n",
        "  roll_str = 'roll0' + str(SMA_Window_Percent)\n",
        "else:\n",
        "  roll_str = 'roll' + str(SMA_Window_Percent%100)\n",
        "\n",
        "print(f'Rolling Window: {roll_str}')\n",
        "\n",
        "plot_models(models_subset_ls, models_type='baseline', text_unit='sent_no', win_per=SMA_Window_Percent)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bafsX4zKd-wT"
      },
      "source": [
        "##### **(ABOVE) Plotly SMA Sentence, (BELOW) Correlation Heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxRGGdcgHj2S"
      },
      "source": [
        "# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n",
        "# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n",
        "\n",
        "Correlation_Algo = \"spearman\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n",
        "# corr_methods_ls = ['pearson', 'spearman', 'kendall']\n",
        "\n",
        "col_stdscaler_rollwin_ls = []\n",
        "for amodel in models_baseline_ls:\n",
        "  col_amodel_stdscaler_rollwin = f'{amodel}_stdscaler_{roll_str}'\n",
        "  col_stdscaler_rollwin_ls.append(col_amodel_stdscaler_rollwin)\n",
        "print(f'col_stdscaler_rollwin_ls: {col_stdscaler_rollwin_ls}')\n",
        "\n",
        "corr_df = corpus_sents_df[col_stdscaler_rollwin_ls].dropna(axis=0, how='any').corr(method=Correlation_Algo)\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df, # corpus_sents_df[col_stdscaler_rollwin_ls].dropna(axis=0, how='any').corr(method=corr_method),\n",
        "                    row_cluster=True,\n",
        "                    col_cluster=True,\n",
        "                    figsize=(10, 10))\n",
        "\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment for Baseline Model Sentiments\\n {Correlation_Algo.capitalize()} Correlation - StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxfultUPZr-J"
      },
      "source": [
        "##### **Sentence Sentiment DTW Hierarichal Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1rU88PlFkss"
      },
      "source": [
        "# Dynamic Time Series Clustering\n",
        "\n",
        "# !pip install dtaidistance[all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Hys86nFkox"
      },
      "source": [
        "from dtaidistance import dtw\n",
        "from dtaidistance import dtw_visualisation as dtwvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMq5XOXTFkjL"
      },
      "source": [
        "s1 = np.array([0., 0, 1, 2, 1, 0, 1, 0, 0, 2, 1, 0, 0])\n",
        "s2 = np.array([0., 1, 2, 3, 1, 0, 0, 0, 2, 1, 0, 0, 0])\n",
        "path = dtw.warping_path(s1, s2)\n",
        "dtwvis.plot_warping(s1, s2, path, filename=\"warp.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mra_zDRSF0LN"
      },
      "source": [
        "from dtaidistance import dtw\n",
        "import numpy as np\n",
        "series = np.matrix([\n",
        "    [0.0, 0, 1, 2, 1, 0, 1, 0, 0],\n",
        "    [0.0, 1, 2, 0, 0, 0, 0, 0, 0],\n",
        "    [0.0, 0, 1, 2, 1, 0, 0, 0, 0]])\n",
        "ds = dtw.distance_matrix_fast(series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGTnnTynGYJR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wycvgBTtvJYY"
      },
      "source": [
        "##### **Explore Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8QX8tSKM376"
      },
      "source": [
        "**Search Corpus for Substring**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* In [Search_for_Substring] enter a Substring to search for in the Corpus\n",
        "\n",
        "* Enter a Substring long enough/unique enough so only a reasonable number of Sentences will be returned\n",
        "\n",
        "* Substring can contain spaces/punctuation, for example: 'in the garden'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJYjOu9Ks_pL"
      },
      "source": [
        "# Search Corpus Sentences for Substring\n",
        "\n",
        "Search_for_Substring = \"combat\" #@param {type:\"string\"}\n",
        "\n",
        "sentno_matching_ls = corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Search_for_Substring, regex=False)]['sent_no']\n",
        "\n",
        "for i, asentno in enumerate(sentno_matching_ls):\n",
        "  # sentno, sentraw = asent\n",
        "  print(f\"\\n\\nMatch #{i}: Sentence #{asentno}\\n\\n\")\n",
        "  sent_highlight = re.sub(Search_for_Substring, Search_for_Substring.upper(), corpus_sents_df.iloc[asentno]['sent_raw'])\n",
        "  print(f'    {sent_highlight}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap_K_gpH0FTm"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHPx5a0xvJYb"
      },
      "source": [
        "Crux_Window_Percent = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Baseline_SMA_Model = \"VADER\" #@param [\"SentimentR\", \"SyuzhetR\", \"Bing\", \"SenticNet\", \"SentiWord\", \"NRC\", \"AFINN\", \"VADER\", \"TextBlob\", \"Flair\", \"Pattern\", \"Stanza\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Vertical_Labels = True #@param {type:\"boolean\"}\n",
        "Vertical_Labels_Height = -0.1 #@param {type:\"slider\", min:-50, max:50, step:0.1}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Baseline_SMA_Model == 'SentimentR':\n",
        "  model_selected = f'sentimentr'\n",
        "if Baseline_SMA_Model == 'SyuzhetR':\n",
        "  model_selected = f'syuzhet'\n",
        "if Baseline_SMA_Model == 'Bing':\n",
        "  model_selected = f'bing'\n",
        "if Baseline_SMA_Model == 'SenticNet':\n",
        "  model_selected = f'senticnet'\n",
        "if Baseline_SMA_Model == 'SentiWord':\n",
        "  model_selected = f'sentiword'\n",
        "if Baseline_SMA_Model == 'NRC':\n",
        "  model_selected = f'nrc'\n",
        "if Baseline_SMA_Model == 'AFINN':\n",
        "  model_selected = f'afinn'\n",
        "if Baseline_SMA_Model == 'VADER':\n",
        "  model_selected = f'vader'\n",
        "if Baseline_SMA_Model == 'TextBlob':\n",
        "  model_selected = f'textblob'\n",
        "if Baseline_SMA_Model == 'Flair':\n",
        "  model_selected = f'flair'\n",
        "if Baseline_SMA_Model == 'Pattern':\n",
        "  model_selected = f'pattern'\n",
        "if Baseline_SMA_Model == 'Stanza':\n",
        "  model_selected = f'stanza'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_stdscaler_{roll_str}'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_selected_ls = [model_selected_fullname]\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "for amodel in corpus_models_selected_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_df, \n",
        "                                         col_series=corpus_models_selected_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_labels=Vertical_Labels,\n",
        "                                         sec_y_height=Vertical_Labels_Height, \n",
        "                                         subtitle_str= '5% Crux ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False);\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6PU1zR8vJYf"
      },
      "source": [
        "**Get Top-n Crux Peaks/Valleys with surrounding Context**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUoJz_nyvJYh"
      },
      "source": [
        "# Crux Point Details\n",
        "Get_Peak_Cruxes = False #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        ts_df = corpus_sents_df,\n",
        "                        library_type='baselines', \n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes,\n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "else:\n",
        "  # import sys\n",
        "  # with open('filename.txt', 'w') as f:\n",
        "  #   print('This message will be written to a file.', file=f)\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1EnCO6sZx_0"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJlFM5kFJDdA"
      },
      "source": [
        "asent_no = 124\n",
        "corpus_df = corpus_sents_df\n",
        "asent_raw = str(corpus_df[corpus_df['sent_no'] == int(asent_no)]['sent_raw'].values[0])\n",
        "asent_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-XtE7xovJYj"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1By1LTGvJYk"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  200#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 4 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmXIEIpY3jrX"
      },
      "source": [
        "##### **Selected Sentence Interactive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTD3VvWj3jrY"
      },
      "source": [
        "# Multiple Sentence Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_Arc = False #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = False #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = False #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = False #@param {type:\"boolean\"}\n",
        "NRC_Arc = False #@param {type:\"boolean\"}\n",
        "AFINN_Arc = False #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  models_subset_ls.append('sentimentr')\n",
        "if Syuzhet_Arc == True:\n",
        "  models_subset_ls.append('syuzhet')\n",
        "if Bing_Arc == True:\n",
        "  models_subset_ls.append('bing')\n",
        "if SenticNet_Arc == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentiWord_Arc == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if NRC_Arc == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if AFINN_Arc == True:\n",
        "  models_subset_ls.append('afinn')\n",
        "if VADER_Arc == True:\n",
        "  models_subset_ls.append('vader')\n",
        "if TextBlob_Arc == True:\n",
        "  models_subset_ls.append('textblob')\n",
        "if Flair_Arc == True:\n",
        "  models_subset_ls.append('flair')\n",
        "if Pattern_Arc == True:\n",
        "  models_subset_ls.append('pattern')\n",
        "if Stanza_Arc == True:\n",
        "  models_subset_ls.append('stanza')\n",
        "if Mean_All_Arc == True:\n",
        "  models_subset_ls.append('mean_all')\n",
        "\n",
        "plot_models(models_subset_ls, models_type='baseline', text_unit='sent_no', win_per=SMA_Window_Percent)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuvAAR2x0x1B"
      },
      "source": [
        "##### **Selected Paragraph Interactive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhh0wT5yGaa0"
      },
      "source": [
        "# Paragraph Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_Arc = False #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = False #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = False #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = False #@param {type:\"boolean\"}\n",
        "NRC_Arc = False #@param {type:\"boolean\"}\n",
        "AFINN_Arc = False #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  models_subset_ls.append('sentimentr')\n",
        "if Syuzhet_Arc == True:\n",
        "  models_subset_ls.append('syuzhet')\n",
        "if Bing_Arc == True:\n",
        "  models_subset_ls.append('bing')\n",
        "if SenticNet_Arc == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentiWord_Arc == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if NRC_Arc == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if AFINN_Arc == True:\n",
        "  models_subset_ls.append('afinn')\n",
        "if VADER_Arc == True:\n",
        "  models_subset_ls.append('vader')\n",
        "if TextBlob_Arc == True:\n",
        "  models_subset_ls.append('textblob')\n",
        "if Flair_Arc == True:\n",
        "  models_subset_ls.append('flair')\n",
        "if Pattern_Arc == True:\n",
        "  models_subset_ls.append('pattern')\n",
        "if Stanza_Arc == True:\n",
        "  models_subset_ls.append('stanza')\n",
        "if Mean_All_Arc == True:\n",
        "  models_subset_ls.append('mean_all')\n",
        "\n",
        "plot_models(models_subset_ls, models_type='baseline', text_unit='parag_no', win_per=SMA_Window_Percent)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYt6fgEsapFj"
      },
      "source": [
        "##### **Selected Section Interactive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "culyTw0tapFk"
      },
      "source": [
        "# Paragraph Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_Arc = False #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = False #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = False #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = False #@param {type:\"boolean\"}\n",
        "NRC_Arc = False #@param {type:\"boolean\"}\n",
        "AFINN_Arc = False #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  models_subset_ls.append('sentimentr')\n",
        "if Syuzhet_Arc == True:\n",
        "  models_subset_ls.append('syuzhet')\n",
        "if Bing_Arc == True:\n",
        "  models_subset_ls.append('bing')\n",
        "if SenticNet_Arc == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentiWord_Arc == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if NRC_Arc == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if AFINN_Arc == True:\n",
        "  models_subset_ls.append('afinn')\n",
        "if VADER_Arc == True:\n",
        "  models_subset_ls.append('vader')\n",
        "if TextBlob_Arc == True:\n",
        "  models_subset_ls.append('textblob')\n",
        "if Flair_Arc == True:\n",
        "  models_subset_ls.append('flair')\n",
        "if Pattern_Arc == True:\n",
        "  models_subset_ls.append('pattern')\n",
        "if Stanza_Arc == True:\n",
        "  models_subset_ls.append('stanza')\n",
        "if Mean_All_Arc == True:\n",
        "  models_subset_ls.append('mean_all')\n",
        "\n",
        "plot_models(models_subset_ls, models_type='baseline', text_unit='sect_no', win_per=SMA_Window_Percent)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRt69D1Zbem0"
      },
      "source": [
        "##### **Chapter SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjlr8LEybem0"
      },
      "source": [
        "# Paragraph Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_Arc = False #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = False #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = False #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = False #@param {type:\"boolean\"}\n",
        "NRC_Arc = False #@param {type:\"boolean\"}\n",
        "AFINN_Arc = False #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Flair_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  models_subset_ls.append('sentimentr')\n",
        "if Syuzhet_Arc == True:\n",
        "  models_subset_ls.append('syuzhet')\n",
        "if Bing_Arc == True:\n",
        "  models_subset_ls.append('bing')\n",
        "if SenticNet_Arc == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentiWord_Arc == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if NRC_Arc == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if AFINN_Arc == True:\n",
        "  models_subset_ls.append('afinn')\n",
        "if VADER_Arc == True:\n",
        "  models_subset_ls.append('vader')\n",
        "if TextBlob_Arc == True:\n",
        "  models_subset_ls.append('textblob')\n",
        "if Flair_Arc == True:\n",
        "  models_subset_ls.append('flair')\n",
        "if Pattern_Arc == True:\n",
        "  models_subset_ls.append('pattern')\n",
        "if Stanza_Arc == True:\n",
        "  models_subset_ls.append('stanza')\n",
        "if Mean_All_Arc == True:\n",
        "  models_subset_ls.append('mean_all')\n",
        "\n",
        "plot_models(models_subset_ls, models_type='baseline', text_unit='chap_no', win_per=SMA_Window_Percent)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TszUw6HFQ6lU"
      },
      "source": [
        "##### **Comparison of Sentence Baseline Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSNxgw5TjU8L"
      },
      "source": [
        "# Compare Sentence Baseline Length-Normed Standardized Sentiment Values\n",
        "\n",
        "\"\"\"\n",
        "model_baselines_ls = ['sentimentr', 'syuzhet', 'bing',\n",
        "                  'sentiword', 'senticnet', 'nrc',\n",
        "                  'afinn', 'vader', 'textblob',\n",
        "                  'flair', 'pattern', 'stanza']\n",
        "\"\"\";\n",
        "\n",
        "\"\"\"\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# col_roll_ls = []\n",
        "model_base_standardized_roll_ls = []\n",
        "for amodel in model_baselines_ls:\n",
        "  # Create the simple model_rollxxx rolling mean\n",
        "  col_roll = f'{amodel}_{roll_str}'\n",
        "  corpus_sents_df['col_roll'] = corpus_sents_df[amodel].rolling(10*win_s1per, center=True).mean()\n",
        "\n",
        "  # Create list of column names for model_lnorm_medianiqr_rollxxx\n",
        "  # col_name = f'{amodel}_lnorm_medianiqr_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here                                                   # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  col_name = f'{amodel}_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here                                                   # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_base_standardized_roll_ls.append(col_name)\n",
        "\n",
        "  # for i,amodel in enumerate(model_base_standardized_roll_ls):\n",
        "\n",
        "  col_name_roll_stand = f'{col_roll}_stdscale'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_sents_df[col_roll])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_sents_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_sents_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentences for Baseline Model Sentiments\\nMean StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YsG5H1xway0"
      },
      "source": [
        "## **EDA SentimentR Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kGpC98FInPh"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# Create SMA roll=10% for all models_stdscaler as baseline\n",
        "\n",
        "win_s1per = int(corpus_sentimentr_df.shape[0] * 1/100)\n",
        "\n",
        "col_stdscaler_roll_ls = []\n",
        "for amodel in models_sentimentr_ls:\n",
        "  col_stdscaler = f'{amodel}_stdscaler'\n",
        "  col_stdscaler_roll = f'{amodel}_stdscaler_{roll_str}'\n",
        "  corpus_sentimentr_df[col_stdscaler_roll] = corpus_sentimentr_df[col_stdscaler].rolling(10*win_s1per, center=True).mean()\n",
        "  col_stdscaler_roll_ls.append(col_stdscaler_roll)\n",
        "\n",
        "col_stdscaler_roll_mean = col_stdscaler_roll + '_mean'\n",
        "corpus_sentimentr_df[col_stdscaler_roll_mean] = corpus_sentimentr_df[col_stdscaler_roll_ls].mean()\n",
        "\n",
        "# Test\n",
        "\n",
        "corpus_sentimentr_df[col_stdscaler_roll_ls].plot()\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id_VKOPiR7Mg"
      },
      "source": [
        "#### **Import SentimentR Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTPbrXDM0Gm1"
      },
      "source": [
        "# Verify SentimentR Sentiment Files exported from RStudio\n",
        "!pwd\n",
        "!ls -altr sum_sentiments_*.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlONG-ZwuuVb"
      },
      "source": [
        "# Get SentimentR Sentiment Datafile (with data on 7 Models)\n",
        "\n",
        "SentimentR_sentiment_datafile = 'sum_sentiments_sentimentR_7models_vwoolf_tothelighthouse.csv' #@param {type:\"string\"}\n",
        "\n",
        "sum_sentiments_sentimentr_filename = SentimentR_sentiment_datafile\n",
        "\n",
        "!head -n 3 $sum_sentiments_sentimentr_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u39nsvJb5YD-"
      },
      "source": [
        "corpus_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJp4DgcIwa7A"
      },
      "source": [
        "# (Optional) Read Sentiment Series generated in RStudio by SentimentR into DataFrame: corpus_sents_sentimentr_df\n",
        "#            SKIP if no SyuzhetR sentiment datafile to read in\n",
        "\n",
        "# MANUALLY: copy and paste the filename above into the quotes below for sum_sentiment_sentimentR_filename\n",
        "\n",
        "corpus_sentimentr_df = pd.read_csv(sum_sentiments_sentimentr_filename, encoding = 'unicode_escape', engine ='python')\n",
        "\n",
        "# Rename columns if necessary\n",
        "corpus_sentimentr_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n",
        "corpus_sentimentr_df['sent_raw'] = corpus_sentimentr_df['sent_raw'].astype('string')\n",
        "corpus_sentimentr_df['sent_raw'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : re.sub(f'[^{re.escape(string.printable)}]', '', x))\n",
        "corpus_sentimentr_df['sent_raw'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : filter_nonprintable(x))\n",
        "\n",
        "\n",
        "# create a clean version of Sentence in sent_clean column\n",
        "corpus_sentimentr_df['sent_clean'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : clean_text(x))\n",
        "corpus_sentimentr_df['sent_clean'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : re.sub(f'[^{re.escape(string.printable)}]', '', x))\n",
        "corpus_sentimentr_df['sent_clean'] = corpus_sentimentr_df['sent_raw'].apply(lambda x : filter_nonprintable(x))\n",
        "\n",
        "corpus_sentimentr_df['sent_clean'] = corpus_sentimentr_df['sent_clean'].astype('string')\n",
        "\n",
        "corpus_sentimentr_df.head(2)\n",
        "corpus_sentimentr_df.info()\n",
        "corpus_sentimentr_df.columns\n",
        "\n",
        "corpus_sents_sentimentr_len = corpus_sentimentr_df.shape[0]\n",
        "\n",
        "# BUG FIX: SentimentR can create many additional rows that must be deleted \n",
        "#          to enable it to be merged with other Sentiment Models on the same Corpus\n",
        "#          OR just leave SentimentR unmerged and analyze separately (preferred)\n",
        "#\n",
        "# POSSIBLE SOLUTIONS (from worst/easiest to better)\n",
        "#     \n",
        "#   1) Trim extra n rows from head/tail\n",
        "#   2) Naive Downsampling of Series\n",
        "#   3) Clustering and distribute deletes of near-medians from longest runs, avoiding outliers, start/end\n",
        "#   4) DTW character-preserving Compression\n",
        "# \n",
        "#          Simplification, 1D cluster jockers_rinker column as proxy for full interrow distance features\n",
        "#                          and delete rows near the median from the largest cluster (vs taking into account\n",
        "#                          all features in Euclidian or other distance metric)\n",
        "\n",
        "# import kmeans1d\n",
        "\n",
        "# Approximate k cluster number as 1 cluster for every 500 sentences in Corpus\n",
        "# k = corpus_sentimentr_df.shape[0]//500  \n",
        "# clusters, centroids = kmeans1d.cluster(np.array(corpus_sentimentr_df['jockers_rinker']), k)\n",
        "\n",
        "def del_oneincluster(df, cluster_per=1):\n",
        "  '''\n",
        "  TODO: Skip for now and use kmeans1d instead\n",
        "  Given a DataFrame and a Cluster Percent to calculate a sliding window\n",
        "  Return DataFrame with one row removed within a sliding window cluster with most self-similiar rows\n",
        "  '''\n",
        "\n",
        "  # Compute sliding window for cluster size\n",
        "  win_cluster_len = int(cluster_per/100 * df.shape[0])\n",
        "  win_start = 0\n",
        "  win_stop = df.shape[0] - win_cluster_len\n",
        "\n",
        "  # Get numeric columns\n",
        "  numeric_df = df.select_dtypes(include=numerics)\n",
        "\n",
        "  most_selfsimilar_value = 0\n",
        "  most_selfsimilar_index = 0\n",
        "  for i in range(win_start, win_stop, 1):\n",
        "    selfsim_score = selfsim_metric(numeric_df.iloc[i:win_cluster_len+1])\n",
        "    if selfsim_score > most_selfsimilar_value:\n",
        "      most_selfsimilar_index = i\n",
        "\n",
        "  oneless_df = del_onerow(most_selfsimilar_index)\n",
        "\n",
        "  return oneless_df\n",
        "\n",
        "# BAD SOLUTION, just trim the last n rows of corpus_sentimentr_df to make lengths match for merging\n",
        "# corpus_sentimentr_df = corpus_sentimentr_df.iloc[:-n,:]\n",
        "\n",
        "corpus_sentimentr_len = corpus_sentimentr_df.shape[0]\n",
        "if corpus_sentimentr_len != corpus_sents_df.shape[0]:\n",
        "  print('\\n\\n\\n======================================================================\\n')\n",
        "  print(f'ERROR: sentence sentiment values read into corpus_syuzhetr (len={corpus_sents_sentimentr_len})')\n",
        "  print(f'       is not the same length as corpus_sents_df (len={corpus_sents_df.shape[0]}) ')\n",
        "  print(f'\\nRECOMMENDATION: Use the preprocessed corpus output created by this notebook ')\n",
        "  print(f'                as input to SentimentR in RStudio to generate sentiment series')\n",
        "  print(f'                and then retry importing')\n",
        "  print('\\n======================================================================\\n');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bpDK9m_5Ea6"
      },
      "source": [
        "# Add summary statistics\n",
        "\n",
        "corpus_sentimentr_df['char_len'] = corpus_sentimentr_df['sent_clean'].apply(lambda x: len(x))\n",
        "corpus_sentimentr_df['token_len'] = corpus_sentimentr_df['sent_clean'].apply(lambda x: len(x.split())) \n",
        "\n",
        "# Verify\n",
        "\n",
        "corpus_sentimentr_df.head(2)\n",
        "corpus_sentimentr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnLz5_Gzh-DO"
      },
      "source": [
        "# Create 4 Standardized versions of each Model: stdscaler, medianiqr both lnormed and not\n",
        "\n",
        "print('\\nBefore Standardization ----------')\n",
        "corpus_sentimentr_df.columns\n",
        "\n",
        "standardize_ts_ls(corpus_sentimentr_df, models_sentimentr_ls)\n",
        "\n",
        "print('\\nAfter Standardization ----------')\n",
        "corpus_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ0IX-wJ-1fv"
      },
      "source": [
        "# Create SMA roll=10% for all models_stdscaler as baseline\n",
        "\n",
        "win_s1per = int(corpus_sentimentr_df.shape[0] * 1/100)\n",
        "\n",
        "col_stdscaler_roll_ls = []\n",
        "for amodel in models_sentimentr_ls:\n",
        "  col_stdscaler = f'{amodel}_stdscaler'\n",
        "  col_stdscaler_roll = f'{amodel}_stdscaler_{roll_str}'\n",
        "  corpus_sentimentr_df[col_stdscaler_roll] = corpus_sentimentr_df[col_stdscaler].rolling(10*win_s1per, center=True).mean()\n",
        "  col_stdscaler_roll_ls.append(col_stdscaler_roll)\n",
        "\n",
        "col_stdscaler_roll_mean = col_stdscaler_roll + '_mean'\n",
        "corpus_sentimentr_df[col_stdscaler_roll_mean] = corpus_sentimentr_df[col_stdscaler_roll_ls].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpoUy0Dm5Ea8"
      },
      "source": [
        "# Standardize all values with MedianIQR\n",
        "\"\"\"\n",
        "model_sentimentr_ls = ['jockers_rinker', 'jockers', 'huliu', 'lmcd', 'nrc', 'senticnet', 'sentiword']\n",
        "\n",
        "for model_sentimentr in models_sentimentr_ls:\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing Sentiment by Sentence Length\n",
        "  sents_len_ls = list(corpus_sentimentr_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_sentimentr_df[model_sentimentr])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/(sents_len_ls[i]+0.01) for i in range(len(sents_len_ls))]\n",
        "\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_sentimentr_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  col_medianiqr = f'{model_sentimentr}_medianiqr'\n",
        "  corpus_sentimentr_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sentimentr_df[model_sentimentr]).reshape(-1, 1))\n",
        "  col_lnorm_medianiqr = f'{model_sentimentr}_lnorm_medianiqr'\n",
        "  corpus_sentimentr_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\"\"\";\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqrVGplu3txO"
      },
      "source": [
        "#### **Sentence SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuOLcZq4dmFL"
      },
      "source": [
        "corpus_sentimentr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFV8VTo1dtf4"
      },
      "source": [
        "# Sentence Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "SentimentR_JockersRinker = True #@param {type:\"boolean\"}\n",
        "SentimentR_Jockers = True #@param {type:\"boolean\"}\n",
        "SentimentR_HuLiu = True #@param {type:\"boolean\"}\n",
        "SentimentR_SenticNet = True #@param {type:\"boolean\"}\n",
        "SentimentR_SentiWord = True #@param {type:\"boolean\"}\n",
        "SentimentR_NRC = True #@param {type:\"boolean\"}\n",
        "SentimentR_LoughranMcDonald = True #@param {type:\"boolean\"}\n",
        "\n",
        "models_subset_ls = []\n",
        "\n",
        "if SentimentR_JockersRinker == True:\n",
        "  models_subset_ls.append('jockers_rinker')\n",
        "if SentimentR_Jockers == True:\n",
        "  models_subset_ls.append('jockers')\n",
        "if SentimentR_HuLiu == True:\n",
        "  models_subset_ls.append('huliu')\n",
        "if SentimentR_SenticNet == True:\n",
        "  models_subset_ls.append('senticnet')\n",
        "if SentimentR_SentiWord == True:\n",
        "  models_subset_ls.append('sentiword')\n",
        "if SentimentR_NRC == True:\n",
        "  models_subset_ls.append('nrc')\n",
        "if SentimentR_LoughranMcDonald == True:\n",
        "  models_subset_ls.append('lmcd')\n",
        "\n",
        "print(f'models_subset_ls:\\n\\n    {models_subset_ls}')\n",
        "plot_models(models_subset_ls, models_type='sentimentr', text_unit='sent_no', win_per=SMA_Window_Percent);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP-Moy3DdrRr"
      },
      "source": [
        "#### **(ABOVE) Plotly SMA Sentence, (BELOW) Correlation Heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaY7ZYNSm0MM"
      },
      "source": [
        "# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n",
        "# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n",
        "\n",
        "Correlation_Algo = \"spearman\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n",
        "# corr_methods_ls = ['pearson', 'spearman', 'kendall']\n",
        "\n",
        "col_stdscaler_rollwin_ls = []\n",
        "for amodel in models_sentimentr_ls:\n",
        "  col_amodel_stdscaler_rollwin = f'{amodel}_stdscaler_{roll_str}'\n",
        "  col_stdscaler_rollwin_ls.append(col_amodel_stdscaler_rollwin)\n",
        "print(f'col_stdscaler_rollwin_ls: {col_stdscaler_rollwin_ls}')\n",
        "\n",
        "corr_df = corpus_sentimentr_df[col_stdscaler_rollwin_ls].dropna(axis=0, how='any').corr(method=Correlation_Algo)\n",
        "corr_df\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df, # corpus_sents_df[col_stdscaler_rollwin_ls].dropna(axis=0, how='any').corr(method=corr_method),\n",
        "                    row_cluster=True,\n",
        "                    col_cluster=True,\n",
        "                    figsize=(10, 10))\n",
        "\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment for SentimentR Model Sentiments\\n {Correlation_Algo.capitalize()} Correlation - StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yaxyt-MqOxdV"
      },
      "source": [
        "#### **Sentence Sentiment DTW Hierarichal Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa2RSwSBnqxd"
      },
      "source": [
        "# Compare Sentence SentimentR Standardized Sentiment Values\n",
        "\"\"\"\n",
        "model_sentimentr_ls = ['jockers_rinker', 'jockers', 'huliu', 'senticnet', 'sentiword', 'nrc', 'lmcd']\n",
        "\n",
        "model_sentimentr_standardized_roll_ls = []\n",
        "for amodel in model_sentimentr_ls:\n",
        "  col_name = f'{amodel}_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_sentimentr_standardized_roll_ls.append(col_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,amodel in enumerate(model_sentimentr_standardized_roll_ls):\n",
        "  col_name_roll_stand = f'{amodel}_stdscale'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_sentimentr_df[amodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_sentimentr_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_sentimentr_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence SentimentR Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEUx7KmlZw_o"
      },
      "source": [
        "corpus_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uvlleo0avaZ"
      },
      "source": [
        "models_sentimentr_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S83JA_esuSxc"
      },
      "source": [
        "#### **Explore Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgETYM8xMGGW"
      },
      "source": [
        "**Search Corpus for Substring**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* In [Search_for_Substring] enter a Substring to search for in the Corpus\n",
        "\n",
        "* Enter a Substring long enough/unique enough so only a reasonable number of Sentences will be returned\n",
        "\n",
        "* Substring can contain spaces/punctuation, for example: 'in the garden'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nyUrE2_L_yr"
      },
      "source": [
        "# Search Corpus Sentences for Substring\n",
        "\n",
        "Search_for_Substring = \"Euryclea\" #@param {type:\"string\"}\n",
        "\n",
        "sentno_matching_ls = corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Search_for_Substring, regex=False)]['sent_no']\n",
        "\n",
        "for i, asentno in enumerate(sentno_matching_ls):\n",
        "  # sentno, sentraw = asent\n",
        "  print(f\"\\n\\nMatch #{i}: Sentence #{asentno}\\n\\n\")\n",
        "  sent_highlight = re.sub(Search_for_Substring, Search_for_Substring.upper(), corpus_sents_df.iloc[asentno]['sent_raw'])\n",
        "  print(f'    {sent_highlight}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7TRTSm34mnl"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc4KP-oz-y4z"
      },
      "source": [
        "corpus_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8SRc-bSKZiO"
      },
      "source": [
        "Crux_Window_Percent = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SentimentR_SMA_Model = \"Jockers-Rinker\" #@param [\"Jockers-Rinker\", \"Jockers\", \"Hu-Liu\", \"SenticNet\", \"SentiWord\", \"NRC\", \"Loughan-McDonald\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Vertical_Labels = True #@param {type:\"boolean\"}\n",
        "Vertical_Labels_Height = -0.2 #@param {type:\"slider\", min:-5, max:5, step:0.05}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SentimentR_SMA_Model == 'Jockers-Rinker':\n",
        "  model_selected = f'jockers_rinker'\n",
        "if SentimentR_SMA_Model == 'Jockers':\n",
        "  model_selected = f'jockers_rinker'\n",
        "if SentimentR_SMA_Model == 'Hu-Liu':\n",
        "  model_selected = f'huliu'\n",
        "if SentimentR_SMA_Model == 'SenticNet':\n",
        "  model_selected = f'senticnet'\n",
        "if SentimentR_SMA_Model == 'SentiWord':\n",
        "  model_selected = f'sentiword'\n",
        "if SentimentR_SMA_Model == 'NRC':\n",
        "  model_selected = f'nrc'\n",
        "if SentimentR_SMA_Model == 'Loughran-McDonald':\n",
        "  model_selected = f'lmcd'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_stdscaler_{roll_str}'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_selected_ls = [model_selected_fullname]\n",
        "print(f'corpus_models_selected_ls: {corpus_models_selected_ls}')\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "\n",
        "for amodel in corpus_models_selected_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sentimentr_df, \n",
        "                                         col_series=corpus_models_selected_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_labels=Vertical_Labels,\n",
        "                                         sec_y_height=Vertical_Labels_Height, \n",
        "                                         subtitle_str='5% Crux - ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_FRnxmUtRIY"
      },
      "source": [
        "**Get Top-n Crux Peaks/Valleys with surrounding Context**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U74EzFO7848r"
      },
      "source": [
        "noisy_str = make_printable(corpus_sentimentr_df.iloc[2504]['sent_raw'])\n",
        "print(noisy_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wPbBI0T66cI"
      },
      "source": [
        "corpus_sentimentr_df.iloc[2503:2506]\n",
        "corpus_sentimentr_df.iloc[2560:2563]\n",
        "corpus_sentimentr_df.iloc[2561]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FeWNQLqscUy"
      },
      "source": [
        "corpus_sentimentr_df['sent_raw'].isna().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyeQ7Jpfszhq"
      },
      "source": [
        "# Crux Details\n",
        "Get_Peak_Cruxes = False #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 1 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = False #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "  \n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        ts_df = corpus_sentimentr_df,\n",
        "                        library_type='sentimentr',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rK7kMax60GN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fHyY7YBsbcn"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W7iWQErsbco"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  2400#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 0 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, \n",
        "                         the_n_sideparags=No_Paragraphs_on_Each_Side, \n",
        "                         the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lbzxj1zwdC6"
      },
      "source": [
        "## **EDA Syuzhet Plots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1d2Q3zqSES1"
      },
      "source": [
        "#### **Import SyuzhetR Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npwz74KeliXE"
      },
      "source": [
        "# Verify SentimentR Sentiment Files exported from RStudio\n",
        "!pwd\n",
        "!ls -altr sum_sentiments*syuzhet*.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVZB9Iff0_Nh"
      },
      "source": [
        "# Get SyuzhetR Sentiment Datafile (with data on 4 Models)\n",
        "\n",
        "SyuzhetR_sentiment_datafile = 'sum_sentiments_syuzhetR_4models_vwoolf_tothelighthouse.csv' #@param {type:\"string\"}\n",
        "\n",
        "sum_sentiments_syuzhetr_filename = SyuzhetR_sentiment_datafile\n",
        "\n",
        "!head -n 3 $sum_sentiments_syuzhetr_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrxrUOf0w4o7"
      },
      "source": [
        "# (Optional) Read Sentiment Series generated in RStudio by SyuzhetR into DataFrame: corpus_syuzhetr_df\n",
        "#            SKIP if no SyuzhetR sentiment datafile to read in\n",
        "\n",
        "corpus_syuzhetr_df = pd.read_csv(sum_sentiments_syuzhetr_filename, encoding = 'unicode_escape', engine ='python')\n",
        "\n",
        "# Rename columns if necessary\n",
        "\"\"\"\n",
        "col_rename_map = {'Unnamed: 0' : 'sent_no'}\n",
        "                  'ttl_sents_syuzhet_vec' : 'syuzhet',\n",
        "                  'ttl_sents_bing_vec' : 'bing',\n",
        "                  'ttl_sents_afinn_vec' : 'afinn',\n",
        "                  'ttl_sents_nrc_vec' : 'nrc'}\n",
        "\"\"\";\n",
        "# corpus_syuzhetr_df.rename(columns=col_rename_map,inplace=True)\n",
        "corpus_syuzhetr_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n",
        "corpus_syuzhetr_df['sent_raw'] = corpus_syuzhetr_df['sent_raw'].astype('string')\n",
        "\n",
        "# create a clean version of Sentence in sent_clean column\n",
        "corpus_syuzhetr_df['sent_clean'] = corpus_syuzhetr_df['sent_raw'].apply(lambda x : clean_text(x))\n",
        "corpus_syuzhetr_df['sent_clean'] = corpus_syuzhetr_df['sent_clean'].astype('string')\n",
        "\n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()\n",
        "corpus_syuzhetr_df.columns\n",
        "\n",
        "corpus_syuzhetr_len = corpus_syuzhetr_df.shape[0]\n",
        "\n",
        "if corpus_syuzhetr_len != corpus_sents_df.shape[0]:\n",
        "  print('\\n\\n\\n======================================================================\\n')\n",
        "  print(f'ERROR: sentence sentiment values read into corpus_syuzhetr (len={corpus_syuzhetr_len})')\n",
        "  print(f'       is not the same length as corpus_sents_df (len={corpus_sents_df.shape[0]}) ')\n",
        "  print(f'\\nRECOMMENDATION: Use the preprocessed corpus output created by this notebook ')\n",
        "  print(f'                as input to SyuzhetR in RStudio to generate sentiment series')\n",
        "  print(f'                and then retry importing')\n",
        "  print('\\n======================================================================\\n');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfJkHlYp4XGZ"
      },
      "source": [
        "# Add summary statistics\n",
        "\n",
        "corpus_syuzhetr_df['char_len'] = corpus_syuzhetr_df['sent_raw'].apply(lambda x: len(x))\n",
        "corpus_syuzhetr_df['token_len'] = corpus_syuzhetr_df['sent_raw'].apply(lambda x: len(x.split())) \n",
        "\n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlJ6MvuOEV5o"
      },
      "source": [
        "roll_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLjXnXsV4XGc"
      },
      "source": [
        "# Standardize all values with MedianIQR\n",
        "\n",
        "# model_syuzhetr_ls = ['syuzhet', 'bing', 'afinn', 'nrc']\n",
        "\n",
        "for model_syuzehtr in models_syuzhetr_ls:\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing Sentiment by Sentence Length\n",
        "  sents_len_ls = list(corpus_syuzhetr_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_syuzhetr_df[model_syuzehtr])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/(sents_len_ls[i]+0.01) for i in range(len(sents_len_ls))]\n",
        "\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_syuzhetr_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  col_medianiqr = f'{model_syuzehtr}_medianiqr'\n",
        "  corpus_syuzhetr_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_syuzhetr_df[model_syuzehtr]).reshape(-1, 1))\n",
        "  col_lnorm_medianiqr = f'{model_syuzehtr}_lnorm_medianiqr'\n",
        "  corpus_syuzhetr_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "# Verify\n",
        "\n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgqSba8FKcMd"
      },
      "source": [
        "# Create 4 Standardized versions of each Model: stdscaler, medianiqr both lnormed and not\n",
        "\n",
        "print('\\nBefore Standardization ----------')\n",
        "corpus_syuzhetr_df.columns\n",
        "\n",
        "standardize_ts_ls(corpus_syuzhetr_df, models_syuzhetr_ls)\n",
        "\n",
        "print('\\nAfter Standardization ----------')\n",
        "corpus_syuzhetr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjq8yt2I_x0u"
      },
      "source": [
        "# Create SMA roll=10% for all models_stdscaler as baseline\n",
        "\n",
        "win_s1per = int(corpus_syuzhetr_df.shape[0] * 1/100)\n",
        "\n",
        "col_stdscaler_roll_ls = []\n",
        "for amodel in models_syuzhetr_ls:\n",
        "  col_stdscaler = f'{amodel}_stdscaler'\n",
        "  col_stdscaler_roll = f'{amodel}_stdscaler_{roll_str}'\n",
        "  corpus_sents_df[col_stdscaler_roll] = corpus_sents_df[col_stdscaler].rolling(10*win_s1per, center=True).mean()\n",
        "  col_stdscaler_roll_ls.append(col_stdscaler_roll)\n",
        "\n",
        "col_stdscaler_roll_mean = col_stdscaler_roll + '_mean'\n",
        "corpus_sents_df[col_stdscaler_roll_mean] = corpus_sents_df[col_stdscaler_roll_ls].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AoBKOFwKD7r"
      },
      "source": [
        "# Create SMA roll=10% for all models_stdscaler as baseline\n",
        "\"\"\"\n",
        "win_s1per = int(corpus_syuzhetr_df.shape[0] * 1/100)\n",
        "\n",
        "col_stdscaler_roll_ls = []\n",
        "for amodel in models_syuzhetr_ls:\n",
        "  col_stdscaler = f'{amodel}_stdscaler'\n",
        "  col_stdscaler_roll = f'{amodel}_stdscaler_{roll_str}'\n",
        "  corpus_syuzhetr_df[col_stdscaler_roll] = corpus_syuzhetr_df[col_stdscaler].rolling(10*win_s1per, center=True).mean()\n",
        "  col_stdscaler_roll_ls.append(col_stdscaler_roll)\n",
        "\n",
        "col_stdscaler_roll_mean = col_stdscaler_roll + '_mean'\n",
        "corpus_syuzhetr_df[col_stdscaler_roll_mean] = corpus_syuzhetr_df[col_stdscaler_roll_ls].mean()\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IDpH20z3lhu"
      },
      "source": [
        "#### **Sentence Syuzhet SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rb2-8evEaxt"
      },
      "source": [
        "roll_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acNruMABNvte"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# display(corpus_sentimentr_df.head())\n",
        "\n",
        "win_per = SMA_Window_Percent             \n",
        "win_roll = int(win_per/100 * corpus_sentimentr_df.shape[0])\n",
        "\n",
        "model_syuzhetr_ls = ['syuzhet', 'bing', 'afinn', 'nrc']\n",
        "\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_syuzhetr_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  if len(str(win_per)) == 1:\n",
        "    roll_str = 'roll0' + str(win_per)\n",
        "  else:\n",
        "    roll_str = 'roll' + str(win_per)\n",
        "  col_name_roll = f'{amodel}_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  corpus_syuzhetr_df[col_name_roll] = corpus_syuzhetr_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_' + roll_str\n",
        "corpus_syuzhetr_df[col_mean_roll] = corpus_syuzhetr_df[col_name_roll_ls].mean(axis=1)\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Bold)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "# add traces\n",
        "\n",
        "model = 'mean_' + roll_str\n",
        "fig.add_traces(go.Line(x=corpus_syuzhetr_df['sent_no'],\n",
        "                       y = corpus_syuzhetr_df[model],\n",
        "                       line=dict(\n",
        "                            color='#000000',\n",
        "                            width=5\n",
        "                            ),\n",
        "                       text = corpus_syuzhetr_df.index.values,\n",
        "                       name = model,\n",
        "                       hovertemplate = \"Model <b>Mean: \"+str(win_per)+\"%</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                       marker_color=next(palette)))\n",
        "\n",
        "\n",
        "for amodel in model_syuzhetr_ls:\n",
        "  model_roll = f'{amodel}_' + roll_str\n",
        "  fig.add_traces(go.Line(x=corpus_syuzhetr_df['sent_no'],\n",
        "                        y = corpus_syuzhetr_df[model_roll],\n",
        "                        text = corpus_syuzhetr_df['sent_raw'],\n",
        "                        name = model_roll,\n",
        "                        hovertemplate = \"Model <b>\"+model_roll+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b><br>Index: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"SyuzhetR Sentence Sentiment Models <b><i>\" + roll_str.upper() + '</i></b>',\n",
        "    xaxis_title=\"Sentence Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aUtGZvXd406"
      },
      "source": [
        "#### **(ABOVE) Plotly SMA Sentence Syuzhet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMIE4UF_PlxL"
      },
      "source": [
        "#### **Comparison of Sentence SyuzhetR Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crwKAbkgEdYr"
      },
      "source": [
        "roll_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0iHrpqpmnuE"
      },
      "source": [
        "# Compare Sentence SyuzhetR Standardized Sentiment Values\n",
        "\n",
        "# model_syuzhetr_ls = ['syuzhet', 'bing', 'afinn', 'nrc']\n",
        "\n",
        "model_syuzhetr_standardized_roll_ls = []\n",
        "for amodel in models_syuzhetr_ls:\n",
        "  col_roll_name = f'{amodel}_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_syuzhetr_standardized_roll_ls.append(col_roll_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,arollmodel in enumerate(model_syuzhetr_standardized_roll_ls):\n",
        "  print(f'Processing model: {arollmodel}')\n",
        "  col_name_roll_stand = f'{arollmodel}_stdscale'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_syuzhetr_df[arollmodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  print(f'  Adding StdScaler Column: {col_name_roll_stand}')\n",
        "  corpus_syuzhetr_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_syuzhetr_df[col_name_roll_stand].plot(label=arollmodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence SyuzhetR Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onf1ah-vPlAT"
      },
      "source": [
        "# Create a comparison DataFrame of SentimentR Sentence Models\n",
        "# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n",
        "# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n",
        "\n",
        "Correlation_Algo = \"kendall\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n",
        "\n",
        "# syuzhetr_corr_models_ls = ['syuzhet', 'bing', 'afinn', 'nrc']\n",
        "\n",
        "corr_df = corpus_syuzhetr_df[models_syuzhetr_ls].corr(method=Correlation_Algo)\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment for SyuzhetR Model Sentiments\\n {Correlation_Algo.capitalize()} Correlation - StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OemcAFXd6JD5"
      },
      "source": [
        "#### **Explore Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oGL8mXZNBUw"
      },
      "source": [
        "**Search Corpus for Substring**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* In [Search_for_Substring] enter a Substring to search for in the Corpus\n",
        "\n",
        "* Enter a Substring long enough/unique enough so only a reasonable number of Sentences will be returned\n",
        "\n",
        "* Substring can contain spaces/punctuation, for example: 'in the garden'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMelwTuTNBUx"
      },
      "source": [
        "# Search Corpus Sentences for Substring\n",
        "\n",
        "Search_for_Substring = \"death.\" #@param {type:\"string\"}\n",
        "\n",
        "sentno_matching_ls = corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Search_for_Substring, regex=False)]['sent_no']\n",
        "\n",
        "for i, asentno in enumerate(sentno_matching_ls):\n",
        "  # sentno, sentraw = asent\n",
        "  print(f\"\\n\\nMatch #{i}: Sentence #{asentno}\\n\\n\")\n",
        "  sent_highlight = re.sub(Search_for_Substring, Search_for_Substring.upper(), corpus_sents_df.iloc[asentno]['sent_raw'])\n",
        "  print(f'    {sent_highlight}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "262TERB06JD8"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRnCoDR_Btgd"
      },
      "source": [
        "roll_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhmPgogw6JEA"
      },
      "source": [
        "Crux_Window_Percent = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SyuzhetR_SMA_Model = \"Syuzhet\" #@param [\"Syuzhet\", \"Bing\", \"AFINN\", \"NRC\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Vertical_Labels = True #@param {type:\"boolean\"}\n",
        "Vertical_Labels_Height = -0.15 #@param {type:\"slider\", min:-5.0, max:5.0, step:0.05}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SyuzhetR_SMA_Model == 'Syuzhet':\n",
        "  model_selected = f'syuzhet'\n",
        "if SyuzhetR_SMA_Model == 'Bing':\n",
        "  model_selected = f'bing'\n",
        "if SyuzhetR_SMA_Model == 'AFINN':\n",
        "  model_selected = f'afinn'\n",
        "if SyuzhetR_SMA_Model == 'NRC':\n",
        "  model_selected = f'nrc'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_{roll_str}'\n",
        "  print(f'model_selected_fullname: {model_selected_fullname}')\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_selected_ls = [model_selected_fullname]\n",
        "print(f'corpus_models_selected_ls: {corpus_models_selected_ls}')\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "for amodel in corpus_models_selected_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_syuzhetr_df, \n",
        "                                         col_series=corpus_models_selected_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_labels=Vertical_Labels,\n",
        "                                         sec_y_height=Vertical_Labels_Height, \n",
        "                                         subtitle_str='5% Crux - ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "print(f'model_crux_ls: {model_crux_ls}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhx-E08q6JEF"
      },
      "source": [
        "**Get Top-n Crux Peaks/Valleys with surrounding Context**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx47zojLBr6f"
      },
      "source": [
        "roll_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxq3Irr36JEK"
      },
      "source": [
        "# Crux Details\n",
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        ts_df = corpus_syuzhetr_df,\n",
        "                        library_type='syuzhetr',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FE5BtcJ6JEN"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsfMfACJ6JES"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  200#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 5 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, \n",
        "                         the_n_sideparags=No_Paragraphs_on_Each_Side, \n",
        "                         the_sent_highlight=Highlight_Crux_Sentence)\n",
        "\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, \n",
        "                         the_n_sideparags=No_Paragraphs_on_Each_Side, \n",
        "                         the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpHVFidu7whr"
      },
      "source": [
        "#### **Compare Sentence SentimentR vs Syuzhet Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdtWV6ViKnWH"
      },
      "source": [
        "roll_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzzmTp93CTiE"
      },
      "source": [
        "corpus_syuzhetr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLwVnxNdBx-W"
      },
      "source": [
        "corpus_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lb8TqPsEN4A"
      },
      "source": [
        "roll_str = 'roll10'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXY05cztTMl_"
      },
      "source": [
        "# Compare Sentence SentimentR vs SyuzhetR SMA smoothed series\n",
        "\"\"\"\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Standardize SentimentR Mean Rolling\n",
        "sentimentr_mean_roll_np = np.array(corpus_sentimentr_df[f'mean_{roll_str}'])\n",
        "sentimentr_mean_roll_np = sentimentr_mean_roll_np.reshape((len(sentimentr_mean_roll_np), 1))\n",
        "\n",
        "scaler = scaler.fit(sentimentr_mean_roll_np)\n",
        "print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "# standardization the dataset and print the first 5 rows\n",
        "sentimentr_mean_roll_norm_np = scaler.transform(sentimentr_mean_roll_np)\n",
        "\n",
        "# Standardize SyuzhetR Mean Rolling\n",
        "syuzhetr_mean_roll_np = np.array(corpus_syuzhetr_df[f'mean_{roll_str}'])\n",
        "syuzhetr_mean_roll_np = syuzhetr_mean_roll_np.reshape((len(syuzhetr_mean_roll_np), 1))\n",
        "\n",
        "scaler = scaler.fit(syuzhetr_mean_roll_np)\n",
        "print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "# standardization the dataset and print the first 5 rows\n",
        "syuzhetr_mean_roll_norm_np = scaler.transform(syuzhetr_mean_roll_np)\n",
        "\n",
        "\n",
        "# Plot normalized Series\n",
        "plt.plot(sentimentr_mean_roll_norm_np, label=\"SentimentR\")\n",
        "plt.plot(syuzhetr_mean_roll_norm_np, label=\"SyuzhetR\")\n",
        "# plt.plot(transformer_mean_roll_norm_np, label=\"SyuzhetR\")\n",
        "\"\"\";\n",
        "\n",
        "# corpus_syuzhetr_df[f'mean_{roll_str}'].apply(lambda x: Scale_SyuzhetR*x).plot(label='SyuzhetR')\n",
        "col_mean_stdscaler_roll = f'mean_stdscaler_{roll_str}'\n",
        "\n",
        "col_all_sentimentr_stdscaler_roll_ls = []\n",
        "for x in models_sentimentr_ls:\n",
        "  col_name = f'{x}_stdscaler_{roll_str}'\n",
        "  col_all_sentimentr_stdscaler_roll_ls.append(col_name)\n",
        "corpus_sentimentr_df[col_mean_stdscaler_roll] = corpus_sentimentr_df[col_all_sentimentr_stdscaler_roll_ls].mean()\n",
        "\n",
        "col_all_syuzhetr_stdscaler_roll_ls = []\n",
        "for x in models_syuzhetr_ls:\n",
        "  col_name = f'{x}_stdscaler_{roll_str}'\n",
        "  col_all_syuzhetr_stdscaler_roll_ls.append(col_name)\n",
        "corpus_syuzhetr_df[col_mean_stdscaler_roll] = corpus_syuzhetr_df[col_all_syuzhetr_stdscaler_roll_ls].mean()\n",
        "\n",
        "# Plot\n",
        "plt.plot(corpus_sentimentr_df[col_mean_stdscaler_roll], label=f\"SentimentR Mean StdScaler {roll_str}\")\n",
        "plt.plot(corpus_syuzhetr_df[col_mean_stdscaler_roll], label=f\"SyuzhetR Mean StdScaler {roll_str}\")\n",
        "\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Sentence SentimentR vs Syuzhet Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzAonJx6FDna"
      },
      "source": [
        "roll_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twq2IuGz7vlR"
      },
      "source": [
        "# Compare Sentence SentimentR vs SyuzhetR SMA smoothed series\n",
        "# TODO: Delete or convert to fine grained/multi-model DTW/correlation\n",
        "\n",
        "# Create a unified DataFrame of Mean Roll_{win_per} from\n",
        "#     SentimentR and SyuzhetR\n",
        "\n",
        "col_mean_roll = f'mean_{roll_str}'\n",
        "\n",
        "compare_sentimentr_syuzhetr_df = pd.concat([\n",
        "    corpus_sentimentr_df[col_mean_roll],\n",
        "    corpus_syuzhetr_df[col_mean_roll]],\n",
        "    axis=1)\n",
        "\n",
        "col_sentimentr_mean_roll = f'sentimentr_{col_mean_roll}'\n",
        "col_syuzhetr_mean_roll = f'syuzhet_{col_mean_roll}'\n",
        "\n",
        "col_mapping = {\n",
        "    compare_sentimentr_syuzhetr_df.columns[0]:'sentimentr_mean_roll', \n",
        "    compare_sentimentr_syuzhetr_df.columns[1]:'syuzhet_mean_roll'\n",
        "}\n",
        "\n",
        "compare_sentimentr_syuzhetr_df.rename(columns=col_mapping,\n",
        "                                      inplace=True)\n",
        "\n",
        "# compare_sentimentr_syuzhetr_df.iloc[1000:1005]\n",
        "\n",
        "\n",
        "# Get correlation matrix of the comparison DataFrame\n",
        "corr_df = compare_sentimentr_syuzhetr_df.corr(method='spearman')\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yF6RbP2wegA"
      },
      "source": [
        "## **EDA Transformer Plots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8wOUSyOSIVs"
      },
      "source": [
        "#### **Import Transformer Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5Trl2jZ1Yi8"
      },
      "source": [
        "# Verify SentimentR Sentiment Files exported from RStudio\n",
        "!pwd\n",
        "!ls -altr sum_sentiments*trans*.csv\n",
        "# sum_sentiments_sents_trans_vwoolf_tothelighthouse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnyvNjy91YjF"
      },
      "source": [
        "# Get SyuzhetR Sentiment Datafile (with data on 4 Models)\n",
        "\n",
        "Transformer_sentiment_datafile = 'sum_sentiments_sents_trans_vwoolf_tothelighthouse.csv' #@param {type:\"string\"}\n",
        "\n",
        "sum_sentiments_transformer_filename = Transformer_sentiment_datafile\n",
        "\n",
        "!head -n 3 $sum_sentiments_transformer_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp_L394e1ngv"
      },
      "source": [
        "# (Optional) Read Sentiment Series generated in RStudio by SyuzhetR into DataFrame: corpus_transformer_df\n",
        "#            SKIP if no SyuzhetR sentiment datafile to read in\n",
        "\n",
        "corpus_transformer_df = pd.read_csv(sum_sentiments_transformer_filename, encoding = 'unicode_escape', engine ='python')\n",
        "\n",
        "# Rename columns if necessary\n",
        "\"\"\"\n",
        "col_rename_map = {'Unnamed: 0' : 'sent_no'}\n",
        "                  'ttl_sents_syuzhet_vec' : 'syuzhet',\n",
        "                  'ttl_sents_bing_vec' : 'bing',\n",
        "                  'ttl_sents_afinn_vec' : 'afinn',\n",
        "                  'ttl_sents_nrc_vec' : 'nrc'}\n",
        "\"\"\";\n",
        "# corpus_transformer_df.rename(columns=col_rename_map,inplace=True)\n",
        "corpus_transformer_df['sent_raw'] = corpus_transformer_df['sent_raw'].astype('string')\n",
        "# corpus_transformer_df['sent_clean'] = corpus_transformer_df['sent_clean'].astype('string')\n",
        "corpus_transformer_df.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "\n",
        "corpus_transformer_df.head(2)\n",
        "corpus_transformer_df.info()\n",
        "corpus_transformer_df.columns\n",
        "\n",
        "corpus_transformer_len = corpus_transformer_df.shape[0]\n",
        "\n",
        "if corpus_transformer_len != corpus_transformer_df.shape[0]:\n",
        "  print('\\n\\n\\n======================================================================\\n')\n",
        "  print(f'ERROR: sentence sentiment values read into corpus_syuzhetr (len={corpus_transformer_len})')\n",
        "  print(f'       is not the same length as corpus_transformer_df (len={corpus_transformer_df.shape[0]}) ')\n",
        "  print(f'\\nRECOMMENDATION: Use the preprocessed corpus output created by this notebook ')\n",
        "  print(f'                as input to SyuzhetR in RStudio to generate sentiment series')\n",
        "  print(f'                and then retry importing')\n",
        "  print('\\n======================================================================\\n');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYZ7xFFDQrzD"
      },
      "source": [
        "# Add summary statistics\n",
        "\n",
        "corpus_transformer_df['sent_clean'] = corpus_transformer_df['sent_raw'].apply(lambda x: clean_text(x))\n",
        "corpus_transformer_df['char_len'] = corpus_transformer_df['sent_raw'].apply(lambda x: len(x))\n",
        "corpus_transformer_df['token_len'] = corpus_transformer_df['sent_clean'].apply(lambda x: len(x.split())) \n",
        "\n",
        "corpus_transformer_df.head(2)\n",
        "corpus_transformer_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgzWVkAYKqS-"
      },
      "source": [
        "# Create 4 Standardized versions of each Model: stdscaler, medianiqr both lnormed and not\n",
        "\n",
        "print('\\nBefore Standardization ----------')\n",
        "corpus_transformer_df.columns\n",
        "\n",
        "standardize_ts_ls(corpus_transformer_df, models_transformer_ls)\n",
        "\n",
        "print('\\nAfter Standardization ----------')\n",
        "corpus_transformer_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP5laVK8Ee9a"
      },
      "source": [
        "# Create SMA roll=10% for all models_stdscaler as baseline\n",
        "\n",
        "win_s1per = int(corpus_transformer_df.shape[0] * 1/100)\n",
        "\n",
        "col_stdscaler_roll_ls = []\n",
        "for amodel in models_transformer_ls:\n",
        "  col_stdscaler = f'{amodel}_stdscaler'\n",
        "  col_stdscaler_roll = f'{amodel}_stdscaler_{roll_str}'\n",
        "  corpus_transformer_df[col_stdscaler_roll] = corpus_transformer_df[col_stdscaler].rolling(10*win_s1per, center=True).mean()\n",
        "  col_stdscaler_roll_ls.append(col_stdscaler_roll)\n",
        "\n",
        "col_stdscaler_roll_mean = col_stdscaler_roll + '_mean'\n",
        "corpus_transformer_df[col_stdscaler_roll_mean] = corpus_transformer_df[col_stdscaler_roll_ls].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_iGanTXZSGB"
      },
      "source": [
        "# Standardize all values with MedianIQR\n",
        "\"\"\"\n",
        "model_transformers_ls = ['nlptown', 'roberta15lg', 'yelp', 'hinglish', 'imdb2way', 'huggingface', 't5imdb50k', 'robertaxml8lang']\n",
        "\n",
        "for model_transformer in model_transformers_ls:\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing Sentiment by Sentence Length\n",
        "  sents_len_ls = list(corpus_transformer_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_transformer_df[model_transformer])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/(sents_len_ls[i]+0.01) for i in range(len(sents_len_ls))]\n",
        "\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_transformer_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  col_medianiqr = f'{model_transformer}_medianiqr'\n",
        "  corpus_transformer_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_transformer_df[model_transformer]).reshape(-1, 1))\n",
        "  col_lnorm_medianiqr = f'{model_transformer}_lnorm_medianiqr'\n",
        "  corpus_transformer_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  # Verify\n",
        "\n",
        "corpus_transformer_df.head()\n",
        "corpus_transformer_df.info()\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNeuepF33Zyu"
      },
      "source": [
        "#### **Sentence Transformers SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfwwefDOXqq8"
      },
      "source": [
        "RoBERTaLg15_Arc = True #@param {type:\"boolean\"}\n",
        "T5IMDB50k_Arc = True #@param {type:\"boolean\"}\n",
        "Huggingface_Arc = True #@param {type:\"boolean\"}\n",
        "NLPTown_Arc = True #@param {type:\"boolean\"}\n",
        "RoBERTaXML8lang_Arc = True #@param {type:\"boolean\"}\n",
        "IMDB2way_Arc = True #@param {type:\"boolean\"}\n",
        "Hinglish_Arc = True #@param {type:\"boolean\"}\n",
        "Yelp_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqYU6I5uWeGc"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "win_per = SMA_Window_Percent\n",
        "win_roll = int(win_per/100 * corpus_transformer_df.shape[0])\n",
        "\n",
        "if len(str(win_per)) == 1:\n",
        "  roll_str = 'roll0' + str(win_per)\n",
        "else:\n",
        "  roll_str = 'roll' + str(win_per)\n",
        "\n",
        "# display(corpus_transformer_df.head())\n",
        "\n",
        "model_transformers_ls = ['nlptown', 'roberta15lg',\n",
        "                         'yelp', 'hinglish',\n",
        "                         'imdb2way', 'huggingface',\n",
        "                         't5imdb50k', 'robertaxml8lang']\n",
        "\n",
        "# list of (scale, center) adjustments for each model so they can be compared on same graph\n",
        "# Scaling Dictionary for each plot in form of tuple (scale, center) \n",
        "#     so they can be compared on same graph\n",
        "model_transformers_scale_dt = {'nlptown' : (0.5, -1),\n",
        "                               'roberta15lg' : (1,0),\n",
        "                               'yelp' : (1.0, 0.5),\n",
        "                               'hinglish' : (1.0, 0.5),\n",
        "                               'imdb2way' : (1.0, 0.5),\n",
        "                               'huggingface' : (1.0, 0.5),\n",
        "                               't5imdb50k' : (1.0, 0.5),\n",
        "                               'robertxml8lang' : (1.0, 0.5)}\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_transformers_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  # col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "  col_name_roll = f'{amodel}_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  # print(f'creating: {col_name_roll}')\n",
        "  corpus_transformer_df[col_name_roll] = corpus_transformer_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "  col_name_roll_stdscale = f'{col_name_roll}_stdscale'\n",
        "  corpus_transformer_df[col_name_roll_stdscale] = get_standardscaler(col_name_roll, corpus_transformer_df[col_name_roll])\n",
        "\n",
        "\n",
        "col_mean_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "# model_transformers_lnorm_medianiqr_ls = []\n",
        "corpus_transformer_df[col_mean_roll] = corpus_transformer_df[col_name_roll_ls].mean(axis=1)\n",
        "\n",
        "col_mean_lnorm_median_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "model_transformers_lnorm_medianiqr_ls = []\n",
        "for acol_name in model_transformers_ls:\n",
        "  # model_transformers_lnorm_medianiqr_ls.append(acol_name+'_lnorm_medianiqr_'+roll_str)\n",
        "  model_transformers_lnorm_medianiqr_ls.append(acol_name+ '_' +roll_str + '_stdscale')\n",
        "corpus_transformer_df[col_mean_lnorm_median_roll] = corpus_transformer_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\n",
        "\n",
        "# display(corpus_transformer_df.head())\n",
        "\n",
        "\n",
        "model_transformers_subset_ls = []\n",
        "\n",
        "if NLPTown_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'nlptown_{roll_str}_stdscale')\n",
        "if T5IMDB50k_Arc == True:\n",
        "  model_transformers_subset_ls.append(f't5imdb50k_{roll_str}_stdscale')\n",
        "if Huggingface_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'huggingface_{roll_str}_stdscale')\n",
        "if RoBERTaLg15_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'roberta15lg_{roll_str}_stdscale')\n",
        "if RoBERTaXML8lang_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'robertaxml8lang_{roll_str}_stdscale')\n",
        "  #                                     robertaxml8lang_roll100_stdscale\n",
        "if IMDB2way_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'imdb2way_{roll_str}_stdscale')\n",
        "if Hinglish_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'hinglish_{roll_str}_stdscale')\n",
        "if Yelp_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'yelp_{roll_str}_stdscale')\n",
        "\n",
        "for i,amodel in enumerate(model_transformers_subset_ls):\n",
        "  print(f'Plot model: {amodel}')\n",
        "  # corpus_transformer_df[amodel].plot()\n",
        "\n",
        "print(f'model_transformers_subset_ls: {model_transformers_subset_ls}')\n",
        "\n",
        "\"\"\"\n",
        "col_mean_subset_roll = f'mean_subset_{roll_str}'\n",
        "corpus_transformer_df[col_mean_subset_roll] = corpus_transformer_df[model_transformers_subset_ls].mean(axis=1)\n",
        "if Mean_Subset_Arc == True:\n",
        "  model_transformers_subset_ls.append(col_mean_subset_roll)\n",
        "\"\"\";\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Bold)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "# add traces\n",
        "# old: y = model_transformers_subset_ls[i][0]*corpus_transformer_df[amodel]+model_baselines_scale_ls[i][1],\n",
        "for amodel in model_transformers_subset_ls:\n",
        "  # print(f'Plot model: {amodel}')\n",
        "  # corpus_transformer_df[amodel].plot()\n",
        "  fig.add_traces(go.Line(x = corpus_transformer_df['sent_no'],\n",
        "                        y = corpus_transformer_df[amodel],\n",
        "                        text = corpus_transformer_df['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model: <b>\"+amodel+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "if Mean_Subset_Arc == True:\n",
        "  mean_subset_col = 'mean_subset_'+roll_str\n",
        "  corpus_transformer_df[mean_subset_col] = corpus_transformer_df[model_transformers_subset_ls].mean(axis=1)\n",
        "  fig.add_traces(go.Line(x=corpus_transformer_df['sent_no'],\n",
        "                        y = 0.1*corpus_transformer_df[mean_subset_col],\n",
        "                        line=dict(\n",
        "                              # color='#000000',\n",
        "                              width=5\n",
        "                              ),\n",
        "                        text = 'NA', # corpus_transformer_df['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model <b>%{mean_subset_col}</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\"\"\";\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Transformer Sentence Sentiment Models <b><i>\" + roll_str.upper() + '</i></b>',\n",
        "    xaxis_title=\"Sentence Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16QqsYHTdYqY"
      },
      "source": [
        "#### **(ABOVE) Plotly SMA Sentence Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UWlGJ1IQG9t"
      },
      "source": [
        "#### **Comparison of Sentence Transformer Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr0GPiwjKGuD"
      },
      "source": [
        "corpus_transformer_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzDn0Z83FQXi"
      },
      "source": [
        "roll_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD3H_yQcLu8O"
      },
      "source": [
        "# Create a comparison DataFrame of Transformer Sentence Models\n",
        "# Create a comparison DataFrame of Transformer Sentence Models\n",
        "# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n",
        "# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n",
        "\n",
        "Correlation_Algo = \"kendall\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n",
        "\n",
        "Correlation_Algo\n",
        "\n",
        "corr_df = corpus_transformer_df[models_transformer_ls].corr(method=Correlation_Algo)\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment for Transformer Model Sentiments\\n {Correlation_Algo.capitalize()} Correlation - StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqnwt6ygQJws"
      },
      "source": [
        "# Compare Sentence Transformer Standardized Sentiment Values\n",
        "\"\"\"\n",
        "model_transformers_ls = ['nlptown', 'roberta15lg',\n",
        "                         'yelp', 'hinglish',\n",
        "                         'imdb2way', 'huggingface',\n",
        "                         't5imdb50k', 'robertaxml8lang']\n",
        "\n",
        "model_trans_standardized_roll_ls = []\n",
        "for amodel in model_transformers_ls:\n",
        "  col_name = f'{amodel}_{roll_str}_stdscale'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_trans_standardized_roll_ls.append(col_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,amodel in enumerate(model_trans_standardized_roll_ls):\n",
        "  col_name_roll_stand = f'{col_name}_stand'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_transformer_df[amodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_transformer_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_transformer_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Transformer Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhnAu46O7aq_"
      },
      "source": [
        "#### **Explore Sentence Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpKbmbsCNFI7"
      },
      "source": [
        "**Search Corpus for Substring**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* In [Search_for_Substring] enter a Substring to search for in the Corpus\n",
        "\n",
        "* Enter a Substring long enough/unique enough so only a reasonable number of Sentences will be returned\n",
        "\n",
        "* Substring can contain spaces/punctuation, for example: 'in the garden'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b35u0YDONFI9"
      },
      "source": [
        "# Search Corpus Sentences for Substring\n",
        "\n",
        "Search_for_Substring = \"love\" #@param {type:\"string\"}\n",
        "\n",
        "sentno_matching_ls = corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Search_for_Substring, regex=False)]['sent_no']\n",
        "\n",
        "for i, asentno in enumerate(sentno_matching_ls):\n",
        "  # sentno, sentraw = asent\n",
        "  print(f\"\\n\\nMatch #{i}: Sentence #{asentno}\\n\\n\")\n",
        "  sent_highlight = re.sub(Search_for_Substring, Search_for_Substring.upper(), corpus_sents_df.iloc[asentno]['sent_raw'])\n",
        "  print(f'    {sent_highlight}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTFCZI667arB"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plR_75zVglhY"
      },
      "source": [
        "models_transformer_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xeZ5Qhb7arB"
      },
      "source": [
        "Crux_Window_Percent = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SentimentR_SMA_Model = \"RoBERTa Large 15 Databases\" #@param [\"RoBERTa Large 15 Databases\", \"NLPTown\", \"Yelp\", \"Hinglish\", \"IMDB 2 Sentiment\", \"Huggingface\", \"T5 IMDB 50k\", \"RoBERTa XML 8 Languages\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Vertical_Labels = False #@param {type:\"boolean\"}\n",
        "Vertical_Labels_Height = 5.4 #@param {type:\"slider\", min:-50, max:50, step:0.1}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SentimentR_SMA_Model == 'RoBERTa Large 15 Databases':\n",
        "  model_selected = f'roberta15lg'\n",
        "if SentimentR_SMA_Model == 'NLPTown':\n",
        "  model_selected = f'nlptown'\n",
        "if SentimentR_SMA_Model == 'Yelp':\n",
        "  model_selected = f'yelp'\n",
        "if SentimentR_SMA_Model == 'Hinglish':\n",
        "  model_selected = f'hinglish'\n",
        "if SentimentR_SMA_Model == 'IMDB 2 Sentiment':\n",
        "  model_selected = f'imdb2way'\n",
        "if SentimentR_SMA_Model == 'Huggingface':\n",
        "  model_selected = f'huggingface'\n",
        "if SentimentR_SMA_Model == 'T5 IMDB 50k':\n",
        "  model_selected = f't5imdb50k'\n",
        "if SentimentR_SMA_Model == 'RoBERTa XML 8 Languages':\n",
        "  model_selected = f'robertaxml8lang'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_{roll_str}_stdscale'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_selected_ls = [model_selected_fullname]\n",
        "print(f'corpus_models_selected_ls: {corpus_models_selected_ls}')\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "\n",
        "for amodel in corpus_models_selected_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_transformer_df, \n",
        "                                         col_series=corpus_models_selected_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_labels=Vertical_Labels,\n",
        "                                         sec_y_height=Vertical_Labels_Height, \n",
        "                                         subtitle_str='5% Crux - ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P34IC3Kh7arD"
      },
      "source": [
        "**Get Top-n Crux Peaks/Valleys with surrounding Context**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99RhVdHc7arE"
      },
      "source": [
        "Get_Peak_Cruxes = False #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "No_Paragraphs_on_Each_Side = 0 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "  \n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        ts_df = corpus_transformer_df,\n",
        "                        library_type='transformer',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "\n",
        "\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y90bCSB7arF"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFi5kyx-7arG"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  1047#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xv7Ah7zVpux"
      },
      "source": [
        "### **Stop Here (Paragraph Under Construction)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRjHsOIpXovL"
      },
      "source": [
        "##### **Paragraph Transformers SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBrSjSA4Qytu"
      },
      "source": [
        "# (Optional) Read Paragraph Sentiment Data generated by Transformers into DataFrame: corpus_parags_trans_df\n",
        "#            SKIP if no Transformer Paragraph Sentiment datafile to read in\n",
        "\n",
        "sum_sentiment_parags_transformers_filename = 'sum_sentiments_sents_transformers_ianmcewan_machineslikeme.csv'\n",
        "corpus_parags_trans_df = pd.read_csv(sum_sentiment_parags_transformers_filename)\n",
        "\n",
        "# Optional columns to drop\n",
        "corpus_parags_trans_df.drop(columns=['bertsst_pol', 'bertsst_prob'], axis=1, inplace=True)\n",
        "\n",
        "corpus_parags_trans_df.head(2)\n",
        "corpus_parags_trans_df.info()\n",
        "corpus_parags_trans_df.columns\n",
        "\n",
        "\"\"\"\n",
        "corpus_parags_trans_len = corpus_parags_trans_df.shape[0]\n",
        "\n",
        "if corpus_parags_trans_len != corpus_sents_df.shape[0]:\n",
        "  print('\\n\\n\\n======================================================================\\n')\n",
        "  print(f'ERROR: sentence sentiment values read into corpus_syuzhetr (len={corpus_transformers_len})')\n",
        "  print(f'       is not the same length as corpus_parags_trans_df (len={corpus_parags_trans_df.shape[0]}) ')\n",
        "  print(f'\\nRECOMMENDATION: Use the preprocessed corpus output created by this notebook ')\n",
        "  print(f'                as input to SyuzhetR in RStudio to generate sentiment series')\n",
        "  print(f'                and then retry importing')\n",
        "  print('\\n======================================================================\\n')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h1hIcSobYfQ"
      },
      "source": [
        "# Standardize all values with MedianIQR\n",
        "\n",
        "model_transformers_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "\n",
        "for model_transformer in model_transformers_ls:\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "  parags_len_ls = list(corpus_parags_trans_df['token_len'])\n",
        "  parags_sentiment_ls = list(corpus_parags_trans_df[model_transformer])\n",
        "  parags_sentiment_norm_ls = [parags_sentiment_ls[i]/parags_len_ls[i] for i in range(len(parags_len_ls))]\n",
        "\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_parags_trans_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  col_medianiqr = f'{model_transformer}_medianiqr'\n",
        "  corpus_parags_trans_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_parags_trans_df[model_transformer]).reshape(-1, 1))\n",
        "  col_lnorm_medianiqr = f'{model_transformer}_lnorm_medianiqr'\n",
        "  corpus_parags_trans_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2ee-U3HUbre"
      },
      "source": [
        "# Calculate Transformer Rolling Windows = win_per of Corpus (default 5%)\n",
        "\n",
        "\n",
        "\n",
        "win_per = 10           \n",
        "win_roll = int(win_per/100 * corpus_parags_trans_df.shape[0])\n",
        "\n",
        "# model_transformers_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "model_transformers_ls = ['nlptown_lnorm_medianiqr', 'robertalg15_lnorm_medianiqr', 'distillbertsst_lnorm_medianiqr', 'bertsst_lnorm_medianiqr']\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_transformers_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  col_name_roll = f'{amodel}_roll050'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  corpus_parags_trans_df[col_name_roll] = corpus_parags_trans_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "corpus_parags_trans_df['mean_all_roll050'] = corpus_parags_trans_df[col_name_roll_ls].mean(axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Kh--NklUbmp"
      },
      "source": [
        "NLPTown_Arc = True #@param {type:\"boolean\"}\n",
        "RoBERTaLg15_Arc = True #@param {type:\"boolean\"}\n",
        "DistillBERTSST_Arc = True #@param {type:\"boolean\"}\n",
        "BERTSST_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = True #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr4Wqoh36V29"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_transformers_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  if len(str(win_per)) == 1:\n",
        "    roll_str = 'roll0' + str(win_per) + '0'\n",
        "  else:\n",
        "    roll_str = 'roll' + str(win_per) + '0'\n",
        "  col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  corpus_sents_trans_df[col_name_roll] = corpus_sents_trans_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_' + roll_str\n",
        "model_transformers_lnorm_medianiqr_ls = []\n",
        "corpus_sents_trans_df[col_mean_roll] = corpus_sents_trans_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\n",
        "col_mean_lnorm_median_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "model_transformers_lnorm_medianiqr_ls = []\n",
        "for acol_name in model_transformers_ls:\n",
        "  model_transformers_lnorm_medianiqr_ls.append(acol_name+'_lnorm_medianiqr_'+roll_str)\n",
        "corpus_sents_trans_df[col_mean_lnorm_median_roll] = corpus_sents_trans_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IVvASZFUbdJ"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "win_per = SMA_Window_Percent\n",
        "win_roll = int(win_per/100 * corpus_parags_trans_df.shape[0])\n",
        "\n",
        "if len(str(win_per)) == 1:\n",
        "  roll_str = 'roll0' + str(win_per)\n",
        "else:\n",
        "  roll_str = 'roll' + str(win_per)\n",
        "\n",
        "# display(corpus_sents_df.head())\n",
        "\n",
        "model_transformers_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "\n",
        "# list of (scale, center) adjustments for each model so they can be compared on same graph\n",
        "model_transformers_scale_ls = [(0.3, -0.6), (1,0.1), (1,0.1), (1,0.1)]\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_transformers_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  print(f'creating: {col_name_roll}')\n",
        "  corpus_parags_trans_df[col_name_roll] = corpus_parags_trans_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "# model_transformers_lnorm_medianiqr_ls = []\n",
        "corpus_parags_trans_df[col_mean_roll] = corpus_parags_trans_df[col_name_roll_ls].mean(axis=1)\n",
        "\n",
        "\"\"\"\n",
        "col_mean_lnorm_median_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "# model_transformers_lnorm_medianiqr_ls = []\n",
        "for acol_name in model_transformers_ls:\n",
        "  model_transformers_lnorm_medianiqr_ls.append(acol_name)\n",
        "corpus_parags_trans_df[col_mean_lnorm_median_roll] = corpus_parags_trans_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\"\"\";\n",
        "\n",
        "\n",
        "model_transformers_subset_ls = []\n",
        "if NLPTown_Arc == True:\n",
        "  # model_transformers_subset_ls.append('nlptown_lnorm_medianiqr_roll050')\n",
        "  model_transformers_subset_ls.append('nlptown_lnorm_medianiqr' + '_' + roll_str)\n",
        "if RoBERTaLg15_Arc == True:\n",
        "  model_transformers_subset_ls.append('robertalg15_lnorm_medianiqr' +'_' + roll_str)\n",
        "if DistillBERTSST_Arc == True:\n",
        "  model_transformers_subset_ls.append('distillbertsst_lnorm_medianiqr' + '_' + roll_str)\n",
        "if BERTSST_Arc == True:\n",
        "  model_transformers_subset_ls.append('bertsst_lnorm_medianiqr' + '_' + roll_str)\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Safe)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "# add traces\n",
        "for i,amodel in enumerate(model_transformers_subset_ls):\n",
        "  fig.add_traces(go.Line(x = corpus_parags_trans_df['sent_no'],\n",
        "                        y = model_transformers_scale_ls[i][0]*corpus_parags_trans_df[amodel]+model_transformers_scale_ls[i][1],\n",
        "                        text = corpus_parags_trans_df['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model: <b>\"+amodel+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "if Mean_Subset_Arc == True:\n",
        "  mean_subset_col = 'mean_subset_roll050'\n",
        "  corpus_parags_trans_df[mean_subset_col] = corpus_parags_trans_df[model_transformers_subset_ls].mean(axis=1)\n",
        "  fig.add_traces(go.Line(x=corpus_parags_trans_df['sent_no'],\n",
        "                        y = corpus_parags_trans_df[mean_subset_col],\n",
        "                        line=dict(\n",
        "                              # color='#000000',\n",
        "                              width=5\n",
        "                              ),\n",
        "                        text = 'NA', # corpus_parags_trans_df['sent_raw'],\n",
        "                        name = mean_subset_col,\n",
        "                        hovertemplate = \"Model <b>%{mean_subset_col}</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Paragraph Transformer Sentiment Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "    xaxis_title=\"Paragraph Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89X8Sz2QdMn-"
      },
      "source": [
        "##### **(ABOVE) Plotly SMA Paragraph Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmbxdtyvQo3K"
      },
      "source": [
        "##### **Comparison of Paragraph Transformer Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIaxX1dEbvkE"
      },
      "source": [
        "# Compare Paragraph Transformer Standardized Sentiment Values\n",
        "\n",
        "model_trans_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "\n",
        "model_trans_standardized_roll_ls = []\n",
        "for amodel in model_trans_ls:\n",
        "  col_name = f'{amodel}_lnorm_medianiqr_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_trans_standardized_roll_ls.append(col_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,amodel in enumerate(model_trans_standardized_roll_ls):\n",
        "  col_name_roll_stand = f'{col_name}_stand'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')  # sent\n",
        "  model_roll_stand_np = np.array(corpus_parags_trans_df[amodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_parags_trans_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_parags_trans_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Paragraph Transformer Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtxzLN7CQo3R"
      },
      "source": [
        "# Create a comparison DataFrame of SentimentR Paragraph Models\n",
        "\n",
        "corr_transformers_models_ls = ['nlptown','robertalg15', 'distillbertsst','bertsst']\n",
        "\n",
        "corr_transformers_df = corpus_parags_trans_df[corr_transformers_models_ls].corr(method='spearman')\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dowF8jrw67W7"
      },
      "source": [
        "# Quick Sentence vs Paragraph Transformer Sentiment SMA Comparison\n",
        "\n",
        "plt.close()\n",
        "\n",
        "# Colab Jupyter wouldn't plot pd.Series from 2 different DataFrames on the same graph\n",
        "#   so combine into temporary DataFrame as a workaround\n",
        "\n",
        "temp_df = pd.DataFrame(columns = ['Sentences', 'Paragraphs'])\n",
        "\n",
        "y_col = f'mean_lnorm_medianiqr_{roll_str}'\n",
        "temp_df['Sentences'] = corpus_sents_trans_df[y_col]\n",
        "temp_df['Paragraphs'] = corpus_parags_trans_df[y_col]\n",
        "\n",
        "temp_df['Sentences'].plot(linewidth=10)\n",
        "temp_df['Paragraphs'].plot()\n",
        "\n",
        "\n",
        "# plt.plot(corpus_sents_trans_df[y_col], label='Sentences')\n",
        "\n",
        "# y_col = f'mean_lnorm_medianiqr_{roll_str}'\n",
        "# plt.plot(corpus_parags_trans_df[y_col], label='Paragraphs')\n",
        "\n",
        "# plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Paragraph Transformer Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJIK_j3wbx97"
      },
      "source": [
        "corpus_sents_trans_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZorifAHe7ye"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5CazR5rY_u2"
      },
      "source": [
        "##### **Explore Paragraph Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHihplfSY_u4"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvndYD2CY_u6"
      },
      "source": [
        "Crux_Window_Percent = 2 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SentimentR_SMA_Model = \"RoBERTa Large 15 Databases\" #@param [\"NLPTown\", \"RoBERTa Large 15 Databases\", \"Distilled BERT SST\", \"BERT SST\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SentimentR_SMA_Model == 'NLPTown':\n",
        "  model_selected = f'nlptown'\n",
        "if SentimentR_SMA_Model == 'RoBERTa Large 15 Databases':\n",
        "  model_selected = f'robertalg15'\n",
        "if SentimentR_SMA_Model == 'Distilled BERT SST':\n",
        "  model_selected = f'distillbertsst'\n",
        "if SentimentR_SMA_Model == 'BERT SST':\n",
        "  model_selected = f'bertsst'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_lnorm_medianiqr_{roll_str}'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_stand_ls = [model_selected_fullname]\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "for amodel in corpus_models_stand_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_trans_df, \n",
        "                                         col_series=corpus_models_stand_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_height=0, \n",
        "                                         subtitle_str=' ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR0h5f7bY_u8"
      },
      "source": [
        "**Get Top-n Crux Peaks/Valleys with surrounding Context**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beSU4keNY_u-"
      },
      "source": [
        "# Crux Details\n",
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 6 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        library_type='transformers',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG8t02zqY_vA"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Sm942CY_vB"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  4494#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISIf0aUPxe6y"
      },
      "source": [
        "# **Compare All Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAx6TUxysYoG"
      },
      "source": [
        "## **Review, Processes and Combine into Unified DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWDd_5inFgMq"
      },
      "source": [
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiG9p1S20Bu_"
      },
      "source": [
        "corpus_sentimentr_df.columns\n",
        "\n",
        "# jockers_rinker_roll100_stdscale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFAQOy4fr-vs"
      },
      "source": [
        "corpus_syuzhetr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYoIb-jVr-3k"
      },
      "source": [
        "corpus_transformer_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zURjifUsh3t"
      },
      "source": [
        "**Process**\n",
        "\n",
        "NOTE: Assume only base_model Raw Sentiment Series exist in each of the 4 Library DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCHRu2RwsW5l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml635x83yFUp"
      },
      "source": [
        "**Baseline Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk86ubNB_Hzn"
      },
      "source": [
        "# StandardScaler SMA for Baseline Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_sents_df.shape[0]*win_per/100)\n",
        "\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_baseline_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_sents_df[col_sma_winper] = corpus_sents_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{col_sma_winper}_stdscale'\n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_sents_df[col_sma_winper])\n",
        "  corpus_sents_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "\n",
        "  # Plot\n",
        "  # corpus_sents_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  corpus_sents_df[col_sma_winper_stdscale].plot()\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Baseline Sentiments\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\"\"\"\n",
        "for agroup in groups_ls:\n",
        "  for amodel in globals()[agroup]:\n",
        "    print(f'Processing Model: {amodel:>15} in Group: {agroup}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKyNxuUmyH4z"
      },
      "source": [
        "**SentimentR Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR_fg-dNLNKw"
      },
      "source": [
        "corpus_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPqb0SeVALi5"
      },
      "source": [
        "# StandardScaler SMA for SentimentR Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_sentimentr_df.shape[0]*win_per/100)\n",
        "\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_sentimentr_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_sentimentr_df[col_sma_winper] = corpus_sentimentr_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{amodel}_stdscaler_{roll_str}' \n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_sentimentr_df[col_sma_winper])\n",
        "  corpus_sentimentr_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "\n",
        "  # Plot\n",
        "  # corpus_sentimentr_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  corpus_sentimentr_df[col_sma_winper_stdscale].plot()\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Baseline Sentiments\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\"\"\"\n",
        "for agroup in groups_ls:\n",
        "  for amodel in globals()[agroup]:\n",
        "    print(f'Processing Model: {amodel:>15} in Group: {agroup}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iGbNzPGyK8-"
      },
      "source": [
        "**SyuzhetR Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnUuECApALc9"
      },
      "source": [
        "# StandardScaler SMA for SyuzhetR Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_syuzhetr_df.shape[0]*win_per/100)\n",
        "\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_syuzhetr_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_syuzhetr_df[col_sma_winper] = corpus_syuzhetr_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{amodel}_stdscaler_{roll_str}'\n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_syuzhetr_df[col_sma_winper])\n",
        "  corpus_syuzhetr_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "\n",
        "  # Plot\n",
        "  # corpus_syuzhetr_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  corpus_syuzhetr_df[col_sma_winper_stdscale].plot()\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Baseline Sentiments\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\"\"\"\n",
        "for agroup in groups_ls:\n",
        "  for amodel in globals()[agroup]:\n",
        "    print(f'Processing Model: {amodel:>15} in Group: {agroup}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3iWkOCuyNB3"
      },
      "source": [
        "**Transformer Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SN6gRLJALZP"
      },
      "source": [
        "# StandardScaler SMA for Transformer Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "models_transformer_ls = ['roberta15lg',\n",
        "                         'nlptown',\n",
        "                         'yelp',\n",
        "                         'hinglish',\n",
        "                         'imdb2way',\n",
        "                         'huggingface',\n",
        "                         't5imdb50k',\n",
        "                         'robertaxml8lang']\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_transformer_df.shape[0]*win_per/100)\n",
        "\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_transformer_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_transformer_df[col_sma_winper] = corpus_transformer_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{amodel}_stdscaler_{roll_str}'\n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_transformer_df[col_sma_winper])\n",
        "  corpus_transformer_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "\n",
        "  # Plot\n",
        "  # corpus_transformer_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  corpus_transformer_df[col_sma_winper_stdscale].plot()\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Transformer Sentiments\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\"\"\"\n",
        "for agroup in groups_ls:\n",
        "  for amodel in globals()[agroup]:\n",
        "    print(f'Processing Model: {amodel:>15} in Group: {agroup}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a1CN4OKL-R9"
      },
      "source": [
        "corpus_syuzhetr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06NwK2PrmIV9"
      },
      "source": [
        "# Vertically Concatenate ALL 4 Sentiment Groups StandardizedScaled SMA Sentence Sentiment Series into 1 Big DataFrame\n",
        "\n",
        "corpus_sents_all_df = pd.DataFrame()\n",
        "\n",
        "# Get Baseline Model StandardScaled SMA column names\n",
        "cols_baseline_stdscaler_ls = []\n",
        "for amodel in models_baseline_ls:\n",
        "  col_roll_stdscale = f'{amodel}_stdscaler_{roll_str}'\n",
        "  cols_baseline_stdscaler_ls.append(col_roll_stdscale)\n",
        "# print(f'\\nBaseline StdScaled SMA Columns:\\n    {cols_baseline_stdscaler_ls}')\n",
        "\n",
        "temp_baseline_df = corpus_sents_df[cols_baseline_stdscaler_ls].copy()\n",
        "temp_baseline_df = temp_baseline_df.add_prefix('baseline_')\n",
        "temp_baseline_df.columns\n",
        "\n",
        "# Get SentimentR Model StandardScaled SMA column names\n",
        "cols_sentimentr_stdscaler_ls = []\n",
        "for amodel in models_sentimentr_ls:\n",
        "  col_roll_stdscale = f'{amodel}_stdscaler_{roll_str}'\n",
        "  cols_sentimentr_stdscaler_ls.append(col_roll_stdscale)\n",
        "# print(f'\\nSentimentR StdScaled SMA Columns:\\n    {cols_sentimentr_stdscaler_ls}')\n",
        "\n",
        "temp_sentimentr_df = corpus_sentimentr_df[cols_sentimentr_stdscaler_ls].copy()\n",
        "temp_sentimentr_df = temp_sentimentr_df.add_prefix('sentimentr_')\n",
        "temp_sentimentr_df.columns\n",
        "\n",
        "# Get SyuzhetR Model StandardScaled SMA column names\n",
        "cols_syuzhetr_stdscaler_ls = []\n",
        "for amodel in models_syuzhetr_ls:\n",
        "  col_roll_stdscaler = f'{amodel}_stdscaler_{roll_str}'\n",
        "  cols_syuzhetr_stdscaler_ls.append(col_roll_stdscaler)\n",
        "# print(f'\\nSyuzhetR StdScaled SMA Columns:\\n    {cols_syuzhetr_stdscaler_ls}')\n",
        "\n",
        "temp_syuzhetr_df = corpus_syuzhetr_df[cols_syuzhetr_stdscaler_ls].copy()\n",
        "temp_syuzhetr_df = temp_syuzhetr_df.add_prefix('syuzhetr_')\n",
        "temp_syuzhetr_df.columns\n",
        "\n",
        "# Get Transformer Model StandardScaled SMA column names\n",
        "cols_transformer_stdscaler_ls = []\n",
        "for amodel in models_transformer_ls:\n",
        "  col_roll_stdscale = f'{amodel}_stdscaler_{roll_str}'\n",
        "  cols_transformer_stdscaler_ls.append(col_roll_stdscale)\n",
        "# print(f'\\nTransformer StdScaled SMA Columns:\\n    {cols_transformer_stdscalerls}')\n",
        "\n",
        "# If Transformers Sentiment DataFrame exists, add it to the Unified DataFrame\n",
        "var_name = 'corpus_transformer_df'\n",
        "if var_name in globals():\n",
        "  # print(f'{var_name} is declared globally')\n",
        "  # print(eval(f'{var_name}.shape[0]'))\n",
        "  corpus_transformer_df_len = eval(f'{var_name}.shape[0]')\n",
        "  print(f'{var_name} has {corpus_transformer_df_len} Sentences')\n",
        "\n",
        "  temp_transformer_df = corpus_transformer_df[cols_transformer_stdscaler_ls].copy()\n",
        "  temp_transformer_df = temp_transformer_df.add_prefix('transformer_')\n",
        "  temp_transformer_df.columns\n",
        "\n",
        "  print(f'\\n\\n{var_name} IS declared\\n    so adding to Unified DataFrame')\n",
        "  corpus_sents_all_df = pd.concat([temp_baseline_df,\n",
        "                                  temp_sentimentr_df,\n",
        "                                  temp_syuzhetr_df,\n",
        "                                  temp_transformer_df],\n",
        "                                  axis=1)\n",
        "\n",
        "  temp_transformer_df = pd.DataFrame()\n",
        "\n",
        "else:\n",
        "  print(f'\\n\\n{var_name} IS NOT declared\\n    so NOT adding to Unified DataFrame')\n",
        "\n",
        "  corpus_sents_all_df = pd.concat([temp_baseline_df,\n",
        "                                  temp_sentimentr_df,\n",
        "                                  temp_syuzhetr_df],\n",
        "                                  axis=1)\n",
        "\n",
        "temp_baseline_df = pd.DataFrame()\n",
        "temp_sentimentr_df = pd.DataFrame()\n",
        "temp_syuzhetr_df = pd.DataFrame()\n",
        "\n",
        "print(f'\\ncorpus_sents_all_df.shape: {corpus_sents_all_df.shape}')\n",
        "# corpus_sents_all_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r34lNnz0oeQN"
      },
      "source": [
        "# Create a Correlation Heatmap for All Sentence Models\n",
        "\n",
        "# Sentence Heatmap Correlation of StdScaler Roll100 Sentiments\n",
        "# Depends on 'col_stdscaler_rollwin_ls' defined in prior code cell\n",
        "\n",
        "Correlation_Algo = \"pearson\" #@param [\"pearson\", \"spearman\", \"kendall\"]\n",
        "# corr_methods_ls = ['pearson', 'spearman', 'kendall']\n",
        "\n",
        "# corr_df = corpus_sents_syuzhetr_df[syuzhetr_corr_models_ls].corr(method='spearman')\n",
        "corr_df = corpus_sents_all_df.filter(like='stdscale').corr(method=Correlation_Algo)\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "# plt.title(f'{CORPUS_FULL} Sentence Sentiment for All Model Sentiments\\n StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.title(f'{CORPUS_FULL} Sentence Sentiment for Transformer Model Sentiments\\n {Correlation_Algo.capitalize()} Correlation - StdScale Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nznsrhwwioqq"
      },
      "source": [
        "## **Compare any StandardizedScale Sentence Sentiment Models**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select any combination of Sentiment Models in order of the following four Groups: Baseline, SentimentR, SyuzhetR and Transformers\n",
        "\n",
        "* All Sentiment Time Series are StandardizedScaled version of SMA created in the notebook above this cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEaMXFw6hayU"
      },
      "source": [
        "Baseline_SentimentR = False #@param {type:\"boolean\"}\n",
        "Baseline_Syuzhet = False #@param {type:\"boolean\"}\n",
        "Baseline_Bing = False #@param {type:\"boolean\"}\n",
        "Baseline_SenticNet = False #@param {type:\"boolean\"}\n",
        "Baseline_SentiWord = True #@param {type:\"boolean\"}\n",
        "Baseline_NRC = False #@param {type:\"boolean\"}\n",
        "Baseline_AFINN = True #@param {type:\"boolean\"}\n",
        "Baseline_VADER = True #@param {type:\"boolean\"}\n",
        "Baseline_TextBlob = True #@param {type:\"boolean\"}\n",
        "Baseline_Flair = True #@param {type:\"boolean\"}\n",
        "Baseline_Pattern = True #@param {type:\"boolean\"}\n",
        "Baseline_Stanza = True #@param {type:\"boolean\"}\n",
        "# Baseline-Mean_All = False #@param {type:\"boolean\"}\n",
        "# Baseline-Mean_Subset = False #@param {type:\"boolean\"}\n",
        "# Baseline-MPQA = False #@param {type:\"boolean\"}\n",
        "# Baseline-SentiStrength = False #@param {type:\"boolean\"}\n",
        "\n",
        "SentimentR_JockersRinker = True #@param {type:\"boolean\"}\n",
        "SentimentR_Jockers = False #@param {type:\"boolean\"}\n",
        "SentimentR_HuLiu = False #@param {type:\"boolean\"}\n",
        "SentimentR_SenticNet = False #@param {type:\"boolean\"}\n",
        "SentimentR_SentiWord = False #@param {type:\"boolean\"}\n",
        "SentimentR_NRC = False #@param {type:\"boolean\"}\n",
        "SentimentR_LoughranMcDonald = True #@param {type:\"boolean\"}\n",
        "\n",
        "SyuzhetR_Syuzhet = True #@param {type:\"boolean\"}\n",
        "SyuzhetR_Bing = False #@param {type:\"boolean\"}\n",
        "SyuzhetR_AFINN = False #@param {type:\"boolean\"}\n",
        "SyuzhetR_NRC = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "Transformer_RoBERTaLg15 = True #@param {type:\"boolean\"}\n",
        "Transformer_T5IMDB50k = True #@param {type:\"boolean\"}\n",
        "Transformer_Huggingface = True #@param {type:\"boolean\"}\n",
        "Transformer_NLPTown = True #@param {type:\"boolean\"}\n",
        "Transformer_RoBERTaXML8lang = True #@param {type:\"boolean\"}\n",
        "Transformer_IMDB2way = False #@param {type:\"boolean\"}\n",
        "Transformer_Hinglish = True #@param {type:\"boolean\"}\n",
        "Transformer_Yelp = False #@param {type:\"boolean\"}\n",
        "# Mean_Subset_Arc = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PtG4UDBNsrn"
      },
      "source": [
        "corpus_sents_all_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkR7gXEehbbf"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "# SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# win_per = SMA_Window_Percent\n",
        "# win_roll = int(win_per/100 * corpus_sents_all_df.shape[0])\n",
        "\n",
        "# NOTE: all 4 Groups need to be run with the same SMA roll_per (usually 5 or 10%)\n",
        "#       to compare Models from the 4 different Groups\n",
        "\n",
        "\n",
        "model_all_subset_ls = []\n",
        "\n",
        "if Baseline_SentimentR == True:\n",
        "  model_all_subset_ls.append('baseline_sentimentr_stdscaler_roll10')\n",
        "if Baseline_Syuzhet == True:\n",
        "  model_all_subset_ls.append('baseline_syuzhet_stdscaler_roll10')\n",
        "if Baseline_Bing == True:\n",
        "  model_all_subset_ls.append('baseline_bing_stdscaler_roll10')\n",
        "if Baseline_SenticNet == True:\n",
        "  model_all_subset_ls.append('baseline_senticnet_stdscaler_roll10')\n",
        "if Baseline_SentiWord == True:\n",
        "  model_all_subset_ls.append('baseline_sentiword_stdscaler_roll10')\n",
        "if Baseline_NRC == True:\n",
        "  model_all_subset_ls.append('baseline_nrc_stdscaler_roll10')\n",
        "if Baseline_AFINN == True:\n",
        "  model_all_subset_ls.append('baseline_afinn_stdscaler_roll10')\n",
        "if Baseline_VADER == True:\n",
        "  model_all_subset_ls.append('baseline_vader_stdscaler_roll10')\n",
        "if Baseline_TextBlob == True:\n",
        "  model_all_subset_ls.append('baseline_stanza_stdscaler_roll10')\n",
        "if Baseline_Flair == True:\n",
        "  model_all_subset_ls.append('baseline_flair_stdscaler_roll10')\n",
        "if Baseline_Pattern == True:\n",
        "  model_all_subset_ls.append('baseline_pattern_stdscaler_roll10')\n",
        "if Baseline_Stanza == True:\n",
        "  model_all_subset_ls.append('baseline_stanza_stdscaler_roll10')\n",
        "\n",
        "if SentimentR_JockersRinker == True:\n",
        "  model_all_subset_ls.append('sentimentr_jockers_rinker_stdscaler_roll10')\n",
        "if SentimentR_Jockers == True:\n",
        "  model_all_subset_ls.append('sentimentr_jockers_stdscaler_roll10')\n",
        "if SentimentR_HuLiu == True:\n",
        "  model_all_subset_ls.append('sentimentr_huliu_stdscaler_roll10_')\n",
        "if SentimentR_SenticNet == True:\n",
        "  model_all_subset_ls.append('sentimentr_senticnet_stdscaler_roll10')\n",
        "if SentimentR_SentiWord == True:\n",
        "  model_all_subset_ls.append('sentimentr_sentiword_stdscaler_roll10le')\n",
        "if SentimentR_NRC == True:\n",
        "  model_all_subset_ls.append('sentimentr_nrc_stdscaler_roll10')\n",
        "if SentimentR_LoughranMcDonald == True:\n",
        "  model_all_subset_ls.append('sentimentr_lmcd_stdscaler_roll10')\n",
        "\n",
        "\n",
        "if SyuzhetR_Syuzhet == True:\n",
        "  model_all_subset_ls.append('syuzhetr_syuzhet_stdscaler_roll10')\n",
        "if SyuzhetR_Bing == True:\n",
        "  model_all_subset_ls.append('syuzhetr_bing_stdscaler_roll10')\n",
        "if SyuzhetR_AFINN == True:\n",
        "  model_all_subset_ls.append('syuzhetr_afinn_stdscaler_roll10')\n",
        "if SyuzhetR_NRC == True:\n",
        "  model_all_subset_ls.append('syuzhetr_nrc_stdscaler_roll10')\n",
        "\n",
        "# Exclude Transformer Models if not loaded/defined\n",
        "var_name = 'corpus_transformer_df'\n",
        "if var_name in globals():\n",
        "  # print(f'{var_name} is declared globally')\n",
        "  # print(eval(f'{var_name}.shape[0]'))\n",
        "  corpus_transformer_df_len = eval(f'{var_name}.shape[0]')\n",
        "  print(f'{var_name} has {corpus_transformer_df_len} Sentences')\n",
        "\n",
        "  if Transformer_RoBERTaLg15 == True:\n",
        "    model_all_subset_ls.append('transformer_roberta15lg_stdscaler_roll10')\n",
        "  if Transformer_T5IMDB50k == True:\n",
        "    model_all_subset_ls.append('transformer_t5imdb50k_stdscaler_roll10')\n",
        "  if Transformer_Huggingface == True:\n",
        "    model_all_subset_ls.append('transformer_huggingface_stdscaler_roll10')\n",
        "  if Transformer_NLPTown == True:\n",
        "    model_all_subset_ls.append('transformer_nlptown_stdscaler_roll10')\n",
        "  if Transformer_RoBERTaXML8lang == True:\n",
        "    model_all_subset_ls.append('transformer_robertaxml8lang_stdscaler_roll10')\n",
        "  if Transformer_IMDB2way == True:\n",
        "    model_all_subset_ls.append('transformer_imdb2way_stdscaler_roll10')\n",
        "  if Transformer_Hinglish == True:\n",
        "    model_all_subset_ls.append('transformer_hinglish_stdscaler_roll10')\n",
        "  if Transformer_Yelp == True:\n",
        "    model_all_subset_ls.append('transformer_yelp_stdscaler_roll10')\n",
        "\n",
        "\n",
        "else:\n",
        "  print(f'ERROR: {var_name} IS NOT declared\\n    Go back and load Transformer Sentiment Datafile\\n    and re-run this code cell')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Safe)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "\n",
        "# Add Sentiment Arc Plot Traces\n",
        "for i,amodel in enumerate(model_all_subset_ls):\n",
        "  print(f'Processing Model: {amodel}')\n",
        "  fig.add_traces(go.Line(x = corpus_sents_all_df.index.values,\n",
        "                        y = corpus_sents_all_df[amodel],\n",
        "                        text = corpus_sents_df['sent_raw'], # corpus_sents_df.iloc[x]['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model: <b>\"+amodel+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=CORPUS_FULL + \"<br>Compare Sentence StandardizedScaled SMA Sentiment Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "    xaxis_title=\"Sentence Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxQtrH196gl3"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def my_css():\n",
        "   display(HTML(\"\"\"<style>table.dataframe td{white-space: nowrap;}</style>\"\"\"))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', my_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIPzbt5Ikldp"
      },
      "source": [
        "with pd.option_context('display.max_colwidth', None):\n",
        "  display(corpus_transformer_df['sent_raw'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDR54Pbg5zqz"
      },
      "source": [
        "with pd.option_context('display.max_colwidth', None):\n",
        "  display(corpus_sentimentr_df.iloc[:10]['sent_raw'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7WUuj1j4XW7"
      },
      "source": [
        "## **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SIJZk6f2ccm"
      },
      "source": [
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "\n",
        "# Save the Sentences in the original Raw and Cleaned Corpus \n",
        "corpus_text_sentences_raw_filename = f'corpus_text_sentences_raw_{author_abbr_str}_{title_str}.txt' # _{datetime_now}.txt'\n",
        "print(f'Saving Corpus text raw sentences to file: {corpus_text_sentences_raw_filename}')\n",
        "corpus_sents_df['sent_raw'].to_csv(corpus_text_sentences_raw_filename)\n",
        "\n",
        "corpus_text_sentences_clean_filename = f'corpus_text_sentences_clean_{author_abbr_str}_{title_str}.txt' # _{datetime_now}.txt'\n",
        "print(f'Saving Corpus text clean sentences to file: {corpus_text_sentences_clean_filename}')\n",
        "corpus_sents_df['sent_clean'].to_csv(corpus_text_sentences_clean_filename)\n",
        "\n",
        "\n",
        "# Save the Paragraphs in the original Raw and Cleaned Corpus\n",
        "corpus_text_paragraphs_raw_filename = f'corpus_text_paragraphs_raw_{author_abbr_str}_{title_str}.txt' # _{datetime_now}.txt'\n",
        "print(f'Saving Corpus text raw paragraphs to file: {corpus_text_paragraphs_raw_filename}')\n",
        "corpus_parags_df['parag_raw'].to_csv(corpus_text_paragraphs_raw_filename)\n",
        "\n",
        "corpus_text_paragraphs_clean_filename = f'corpus_text_sentences_clean_{author_abbr_str}_{title_str}.txt' # _{datetime_now}.txt'\n",
        "print(f'Saving Corpus text clean sentences to file: {corpus_text_paragraphs_clean_filename}')\n",
        "corpus_parags_df['parag_clean'].to_csv(corpus_text_paragraphs_clean_filename)\n",
        "\n",
        "\n",
        "# Save the Sentences of each of the 4 Groups DataFrames NOTE: The Baseline role is fulfilled by the default corpus_sents_df DataFrame\n",
        "# corpus_sents_filename = f'sum_sentiments_sents_baseline_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "# print(f'Saving Sentence Baselines to file: {corpus_sents_filename}')\n",
        "# corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "corpus_sentimentr_filename = f'sum_sentiments_sents_sentimentr_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Sentence SentimentR to file: {corpus_sentimentr_filename}')\n",
        "corpus_sentimentr_df.to_csv(corpus_sentimentr_filename)\n",
        "\n",
        "corpus_syuzhetr_filename = f'sum_sentiments_sents_syuzhetr_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Sentence SyuzhetR to file: {corpus_syuzhetr_filename}')\n",
        "corpus_syuzhetr_df.to_csv(corpus_syuzhetr_filename)\n",
        "\n",
        "corpus_transformer_filename = f'sum_sentiments_sents_transformer_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Sentence Transformer to file: {corpus_transformer_filename}')\n",
        "corpus_transformer_df.to_csv(corpus_transformer_filename)\n",
        "\n",
        "\n",
        "# Save StandardizedScaled SMA Sentences of ALL Models from the Unified DataFrame\n",
        "corpus_sents_all_filename = f'sum_sentiments_sents_all_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Sentence ALL Models to file: {corpus_sents_all_filename}')\n",
        "# corpus_sents_all_df.to_csv(corpus_sents_all_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k80YU4ZoHCs2"
      },
      "source": [
        "## **Calculate Lexical Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CokfceWPtfD"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9cZmIkx7Too"
      },
      "source": [
        "# **EDA (Repeat for each Sentiment Model)** (Auto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJRV2n0M_xN6"
      },
      "source": [
        "#### **Histograms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJTcj9faMh61"
      },
      "source": [
        "**Sentiment Histogram Plots**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Histograms are provided for the (a) Length-Normed and (b) Scaled (Median Interquartile Range) Sentiment values for Sentences, Paragraphs and Sections. \n",
        "\n",
        "* There we used extensively early on to compare which Sentiment Time series preprocessing techniques worked best with our various Novel corpora according to two criteria: \n",
        "\n",
        "* (a) Vertical Scaling Method with the ability to transform histograms of sentiment values to well-behaved near-gaussian distributions and clipping outliers. After experimenting with various techniques including: mean/STD, median/MAD, and various two stage outlier/normalization methods median/IQR proved best (define).\n",
        "\n",
        "* (b) Horizontal Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPEv0DsBRhhQ"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU46yaCPPdSm"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYs2V2ILENaZ"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8b4CjTYwlgP"
      },
      "source": [
        "# Plot Histogram of Sentence lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuwVO3Q4Rmu4"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MhJ4T18PjTM"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SajUdBthwlgR"
      },
      "source": [
        "# Plot Histogram of Paragraph lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nuOQ7cZRrue"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfhjLMdjRwZX"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBFb-SLywlgS"
      },
      "source": [
        "# Plot Histogram of Section lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43YL_Iuf0JHZ"
      },
      "source": [
        "#### **Raw Sentiment Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AHHijpONvpt"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GorGKbFbR28W"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN27M4WlwlgT"
      },
      "source": [
        "# Plot Raw Sentence Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0QCx2OMN_jm"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxL5FryPR-ln"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d8uXvYJwlgU"
      },
      "source": [
        "# Plot Raw Paragraph Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_raw_sentiments(model_name=model_name, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp7xjR7GvxkT"
      },
      "source": [
        "# TODO: Add Section Crux Nos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2hFkRQHSLVE"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adhoCpB5SFQv"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "# col_meanstd = f'{model_name}_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='section', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_meanstd, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjZZqY6cwlgW"
      },
      "source": [
        "# Plot Raw Standardized Section Sentiments\n",
        "\n",
        "# NOTE: Compared with Length-Normed, the Raw Standardizations lose most SATS features\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "# col_lnorm_meanstd = f'{model_name}_lnorm_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='section', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_lnorm_meanstd, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDF_-tZhr44H"
      },
      "source": [
        "# Plot Raw Standardized Chapter Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "# col_lnorm_meanstd = f'{model_name}_lnorm_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='chapter', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_lnorm_meanstd, semantic_type='chapter', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1tIJZepmvLu"
      },
      "source": [
        "#### **Crux Points and Surrounding Contexts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FBWrNEJyit6"
      },
      "source": [
        "# Veify all the model sentiment variations\n",
        "\n",
        "[x for x in corpus_sects_df.columns if x.startswith(model_base)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKazAFV_qpCn"
      },
      "source": [
        "**Compare Chapters vs Sections Crux Points**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At the highest level, the Corpus is divided into Chapters which may then futher subdivided into Sections (e.g. extra spaces, punctuation like '* * *' or special printer glyph/fleuron).  Horizonal dark blue lines indicate Chapter divisions while Section boundries lie at both dark and light blue vertical lines.\n",
        "\n",
        "* Since each Chapter may contain multiple Sections, the Section Sentiment plot is more detailed/jagged than the Chapter Sentiment plots. By plotting both together, the smoother Chapter Sentiment plot gives a more general sense of the Corpus Sentiment Arc while the next, more-detailed Section Sentiment plot enables a more detailed investigation/localization of Crux Point neighborhoods.\n",
        "\n",
        "* At this early stage both the Chapter and Section Sentiment plots are too general to provide accurate/fixed Crux localization. As such, only aggregrate Sentiment values for each Chapter/Section are assigned to the mid-point of each Chapter/Section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sme-1HVRZOGs"
      },
      "source": [
        "# col_lnorm_medianiqr = 'pattern_lnorm_medianiqr'\n",
        "col_lnorm_medianiqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08xDu5WgkGAg"
      },
      "source": [
        "# col_lnorm_medianiqr = 'vader_lnorm_medianiqr'\n",
        "\n",
        "# corpus_chaps_df.drop(columns=['textblob_lnorm_medianiqr_lnorm_medianiqr', 'textblob_lnorm_medianiqr_medianiqr', 'textblob_lnorm_medianiqr_lnorm_meanstd'], inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06m1xDRzmWbq"
      },
      "source": [
        "corpus_chaps_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tojSdIGkPDz"
      },
      "source": [
        "corpus_chaps_df.columns\n",
        "# corpus_chaps_df.iloc[:20][['chap_no','sent_no_start','sent_no_mid','char_len','token_len','vader', 'vader_lnorm_medianiqr', 'textblob', 'textblob_lnorm_medianiqr']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu6F5kWerAPZ"
      },
      "source": [
        "corpus_chaps_df.vader_lnorm_medianiqr.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLgGy3HgO9--"
      },
      "source": [
        "col_lnorm_medianiqr = 'syuzhet_lnorm_medianiqr'\n",
        "model_name = 'syuzhet_lnorm_medianiqr'\n",
        "model_base = 'syuzhet'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M1bg8RcPH_2"
      },
      "source": [
        "# Plot Annotated Section Cruxes of Raw Sentiment Time Series\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[model_base], semantic_type='chapter', label_token_ct=3, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[model_base], semantic_type='section', label_token_ct=-1, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jku9ZQ9v9Wg"
      },
      "source": [
        "# Plot Annotated Section Cruxes of Standardized Sentiment Time Series\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='chapter', label_token_ct=3, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=-1, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxA0sDvlcdLx"
      },
      "source": [
        "**Sections Crux Points in Detail**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2tKXTD8v9RH"
      },
      "source": [
        "# Corpus Section Standardized Sentiment Time Series\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSw-wARBF2fW"
      },
      "source": [
        "**Verify Crux Point Sentence Number and Text Match**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At [Crux_Sentence_Text] enter the first few words that uniquely identify the Crux Sentence and confirm the Sentence No matches the information in the plot above. (NOTE: Search is for an exact match including case and puncutation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU6l4UaPdmAF"
      },
      "source": [
        "Crux_Sentence_Text = \"haiku\" #@param {type:\"string\"}\n",
        "\n",
        "# Verify individual Crux Sentence Numbers matches Content\n",
        "\n",
        "crux_sent_no = int(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Crux_Sentence_Text)]['sent_no'])\n",
        "\n",
        "print(f'The Sentence:\\n\\n    {Crux_Sentence_Text}\\n\\nMatches Sentence #{crux_sent_no}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Q_weR3H5Qu"
      },
      "source": [
        "**Review Context Around Any Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYAqkxi2FSw1"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  4494#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7-PoXAcq4S4"
      },
      "source": [
        "**Compare Paragraph vs Section Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzgfs-U9QZeq"
      },
      "source": [
        "# Verify the valid ranges for Sentences and Paragraphs\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "print(f'There are {corpus_sents_len} Sentences in the Corpus')\n",
        "corpus_parags_len = corpus_parags_df.shape[0]\n",
        "print(f'There are {corpus_parags_len} Paragraphs in the Corpus')\n",
        "\n",
        "# Create a new Corpus Paragraph DataFrame (corpus_parags_zoom_df) that is streteched out to have as many sample points as there are Sentences\n",
        "#   That is, go from an original corpus_parags_df of #Paragraph datapoints to an expanded corpus_parags_zoom_df of #Sentences datapoints using scipy.ndimage.interpolation.zoom\n",
        "\n",
        "corpus_parags_zoom_df = pd.DataFrame()\n",
        "\n",
        "resample_ratio = corpus_sents_df.shape[0]/corpus_parags_df.shape[0]   # ratio = no_sents/no_parags\n",
        "\n",
        "corpus_parags_df_numcols_ls = corpus_parags_df.select_dtypes(include=['number']).columns\n",
        "\n",
        "for acol in corpus_parags_df_numcols_ls:\n",
        "  parags_zoom_temp_np = zoom(np.array(corpus_parags_df[acol]), resample_ratio)\n",
        "  corpus_parags_zoom_df[acol] = pd.Series(parags_zoom_temp_np)\n",
        "\n",
        "print('\\n')\n",
        "print(f'New expanded corpus_parags_zoom_df.shape = {corpus_parags_zoom_df.shape}')\n",
        "print(f'           matches corpus_sents_df.shape = {corpus_sents_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wskHXVNSmjfQ"
      },
      "source": [
        "**Adjust the Paragraph Sentiment Plot to compare it with the Section Sentiment Plot for this Corpus**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Adjust [Scale_Vertical_Rolling_Paragraph] until the vertical min/max spans of the two plots are approximately equal\n",
        "\n",
        "* Adjust [Set_Paragraph_Rolling_Window_Percent] to set horizonal smoothness of Rolling Paragraph (typically 5,10 or 20%)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9P9k98ibh4n"
      },
      "source": [
        "col_lnorm_medianiqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgaBEAVCTMRt"
      },
      "source": [
        "Scale_Vertical_Rolling_Paragraph = 2.7 #@param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "Set_Paragraph_Rolling_Window_Percent = 3 #@param {type:\"slider\", min:0, max:30, step:1}\n",
        "\n",
        "# Compare Section Midpoints vs Sentence SMA Sentiment Values\n",
        "\n",
        "scale_sma_paragraph = Scale_Vertical_Rolling_Paragraph\n",
        "sentence_count = corpus_sents_df.shape[0]\n",
        "if Set_Paragraph_Rolling_Window_Percent == 0:\n",
        "  sma_parag_win = 1\n",
        "else:\n",
        "  sma_parag_win = int((Set_Paragraph_Rolling_Window_Percent/100)*sentence_count)\n",
        "\n",
        "\n",
        "# corpus_parags_zoom_df['vader_lnorm_medianiqr'].rolling(window=sma_parag_win, center=True).mean().apply(lambda x: x*scale_sma_paragraph).plot(label=f'{model_base} Paragraphs', alpha=0.3)\n",
        "corpus_parags_zoom_df[col_lnorm_medianiqr].rolling(window=sma_parag_win, center=True).mean().apply(lambda x: x*scale_sma_paragraph).plot(label=f'{model_base} Paragraphs', alpha=0.3)\n",
        "\n",
        "_ = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=0, title_xpos=0.5, title_ypos=1.0, sec_y_height=0, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} \\n SMA vs midpoint Paragraph MedianIQR Sentiment with Crux Points via SciPy.peaks')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUyZF6tE1rL0"
      },
      "source": [
        "#### **Zoom into a Section**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At [Select_Section_No] pick a Section of the Corpus to zoom into\n",
        "\n",
        "* Adjust [Scale_Vertical_Rolling_Sentences] until the vertical min/max spans of the two plots are approximately equal \n",
        "\n",
        "* Adjust [Set_Sentence_Rolling_Window_Percent] to set horizonal smoothness of Rolling Paragraph (typically 5,10 or 20%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu7DYRELQBoH"
      },
      "source": [
        "# Explore a Corpus Section Up-Close\n",
        "\n",
        "section_count = corpus_sects_df.shape[0]\n",
        "print(f'There are {section_count} Sections in this corpus,\\n  pick one numbered between 0 and {section_count-1}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Q2a854Zxl1"
      },
      "source": [
        "Select_Section_No =  24#@param {type:\"integer\"}\n",
        "Scale_Vertical_Rolling_Sentences = 0.2 #@param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "Set_Sentence_Rolling_Window_Percent = 3 #@param {type:\"slider\", min:0, max:30, step:1}\n",
        "\n",
        "# Make Copies instead of just using References / Only Reference, not copy()\n",
        "# section_sents_df = pd.DataFrame()\n",
        "# section_parags_df = pd.DataFrame()\n",
        "\n",
        "section_sents_df, section_parags_df = get_section_timeseries(Select_Section_No)\n",
        "\n",
        "# section_sents_df.head()\n",
        "\n",
        "print(f'section_sents_df.shape: {section_sents_df.shape}')\n",
        "print(f'section_parags_df.shape: {section_parags_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MbSPmp2dN9Z"
      },
      "source": [
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE-CDUmvFDOe"
      },
      "source": [
        "# Add expanded Paragraph sentiment to corpus_sents_df\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr_parag'] = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df')\n",
        "\n",
        "# NOTE: Define section_sents_df, MUST BE EXECUTED BEFORE ANY CRUX POINT DETECTION!!!\n",
        "\n",
        "parags_midpoint_ls = []\n",
        "col_name_parag = f'{model_name}_parag'\n",
        "section_sents_df[col_name_parag], parags_midpoint_ls = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df', model_name=model_name)\n",
        "\n",
        "# Verify Sentences and Expanded Paragraph lengths match\n",
        "print(f'\\nIn Section #{Select_Section_No}\\n')\n",
        "print(f'            Sentence Count: {section_sents_df.shape[0]}')\n",
        "print(f\"(expanded) Paragraph Count: {str(section_sents_df['parag_no'].unique().shape[0])}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOBqRxQz3gbh"
      },
      "source": [
        "##### **Section Histograms**\n",
        "\n",
        "EDA Unbalanced Paragraph and Sentence Features within selected Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MoMM3l8Px2R"
      },
      "source": [
        "# Verify the non-uniform distribution of Paragraph lengths within selected Section (thus necessity for noralizing Paragraph Sentiment by Paragraph length)\n",
        "\n",
        "section_sents_df['parag_no'].value_counts().hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Number of Sentences per Paragraph in Section #{Select_Section_No}')\n",
        "plt.xlabel('Number of Sentences per Paragraph')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5eC0i5KO9cm"
      },
      "source": [
        "section_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoVdCaReDJvl"
      },
      "source": [
        "section_sents_df[model_name].hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.xlabel('Sentence Sentiment')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSrXWnnEEm00"
      },
      "source": [
        "# Histogram of Paragraph Sentiments within selected Section\n",
        " \n",
        "# create a unified Section DataFrame with equal length Sentences and (expanded) Paragraphs Sentiment Series\n",
        "# section_sents_parags_df = section_sents_df.copy()\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr_parag_approx'] = parag_sentiment_expanded_ls\n",
        "\n",
        "\n",
        "section_sents_df[col_name_parag].hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Length-Normed Paragraph Sentiment in Section #{Select_Section_No}')\n",
        "plt.xlabel('Length-Normed Sentiment of Paragraph')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcao3RNuyCr_"
      },
      "source": [
        "**Naive Raw and LOWESS Smoothed Paragraph Sentiment plots within selected Section**\n",
        "\n",
        "NOTE: Horizonal x-axis narrative time axis not adjusted for variable paragraph lengths - simply used midpoints assuming equal length Paragraphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag47-b4EEO6k"
      },
      "source": [
        "**Raw and SMA Sentence Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkpQiVCoZTJ0"
      },
      "source": [
        "# section_crux_sents_dt\n",
        "\n",
        "# type(section_crux_sents_dt['vader_lnorm_medianiqr_roll50_frac14_win10'][0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB4fEZzJQPkg"
      },
      "source": [
        "##### **Section SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAIAX2OhW2k0"
      },
      "source": [
        "# SMA with Raw Sentiment values over entire Corpus\n",
        "\n",
        "sec_y_ht = -0.06\n",
        "\n",
        "plot_smas(section_view=False, model_name=model_base, text_unit='sentence', wins_ls=[5,10,15,20], alpha=0.5, y_height=sec_y_ht, subtitle_str=f'(Model: {model_base.capitalize()})', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8xWHqNAYV_p"
      },
      "source": [
        "# SMA with Length-Normed MedianIQR Sentiment values over entire Corpus\n",
        "\n",
        "sec_y_ht = -0.11\n",
        "\n",
        "plot_smas(section_view=False, model_name=model_name, text_unit='sentence', wins_ls=[5,10,15,20], alpha=0.5, y_height=sec_y_ht, subtitle_str=f'(Model: {model_base.capitalize()})', save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nA285mHvM4T"
      },
      "source": [
        "# Within the Selected_Section_No, plot Sentence SMA with vertical Paragraph boundries indicated\n",
        "\n",
        "sec_y_ht = 0.65\n",
        "\n",
        "# At Section boundries draw blue vertical lines \n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "  # 'BigNews1', xy=(sent_no, 0.5), xytext=(-10, 25), textcoords='offset points',                   rotation=90, va='bottom', ha='center', annotation_clip=True)\n",
        "\n",
        "  # plt.text(sent_no, -.5, 'goodbye',rotation=90, zorder=0)\n",
        "      \n",
        "for win_size in range(50,250,25):\n",
        "  section_sents_df[model_name].rolling(win_size, center=True).mean().plot(alpha=0.5, label=f'SMA win={win_size}')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base.capitalize()})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfZiEeDhWJJM"
      },
      "source": [
        "# Within the Selected_Section_No, compare Sentence SMA with vertical Paragraph boundries indicated\n",
        "#    3 Sentence SMAs: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "\n",
        "sec_y_ht = -0.5\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "awins_ls = [10]\n",
        "get_smas(section_sents_df, model_name=model_name, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=col_medianiqr, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=1.8, subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=model_base, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=8., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False);\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqlsQS-RYT7x"
      },
      "source": [
        "# Within the Selected_Section_No, compare Paragraph SMA: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "#     Plot Paragraph by Sentence Sentiment within selected Section\n",
        "\n",
        "get_smas(section_sents_df, model_name=model_name, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=col_medianiqr, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=1.8, subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=model_base, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=8., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTHlWyjzYf7C"
      },
      "source": [
        "# Within the Selected_Section_No, compare Paragraph SMA: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "#     Plot Paragraph by Paragraph Sentiment within selected Section\n",
        "\n",
        "\"\"\"\n",
        "# TODO: Fix ValueError: Image size of 1311x82731 pixels is too large. It must be less than 2^16 in each direction.\n",
        "\n",
        "\n",
        "sec_y_ht = -15\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(aparag_no, sec_y_ht, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(aparag_no, color='blue', alpha=0.1)\n",
        "\n",
        "awins_ls = [5]\n",
        "get_smas(section_parags_df, model_name=model_name, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_parags_df, model_name=col_medianiqr, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=8., subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_parags_df, model_name=model_base, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=10., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Paragraph Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhIM7zwuQUBq"
      },
      "source": [
        "##### **Raw Sentiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp0JvXFrV-B4"
      },
      "source": [
        "# Within the Selected_Section_No, compare Sentence Sentiment: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "#     Plot Raw vs MedianIQR Sentence Sentiment within selected Section\n",
        "\n",
        "sec_y_ht = -4.0\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_base, data=section_sents_df, alpha=0.3, label=f'Raw Sentence Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_sents_df, alpha=0.5, label=f'MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.plot(col_lnorm_medianiqr, data=section_sents_df, alpha=0.5, label=f'MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw vs MedianIQR Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzoXo8mVZMq_"
      },
      "source": [
        "# Within the Selected_Section_No, compare Sentence Sentiment: \n",
        "#     MedianIQR vs Length-Normed MedianIQR Sentence Sentiment within selected Section\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_name, data=section_sents_df, alpha=0.3, label=f'Length-Normed MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_sents_df, alpha=0.3, label=f'MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLength-Normed vs non-Normed Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tag3LC8Y9Is"
      },
      "source": [
        "# Within the Selected_Section_No, compare Paragraph Sentiment\n",
        "#    Standardized MedianIQR vs Length-Normed Standardized MedianIQR\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(aparag_no, sec_y_ht, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(aparag_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_name, data=section_parags_df, alpha=0.3, label=f'Length-Normed MedianIQR Paragraph Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_parags_df, alpha=0.3, label=f'MedianIQR Paragraph Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLength-Normed vs non-Normed Paragraph Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJmDFQeH6gI"
      },
      "source": [
        "##### **LOWESS Smoothed**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhYnx-MgH_G0"
      },
      "source": [
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0bylmqBf7Js"
      },
      "source": [
        "# Within the Selected_Section_No, use Lowess Smoothing and SciPy find_peaks()\n",
        "#    to get Sentence Crux Points \n",
        "\n",
        "win_lowess_start = 20\n",
        "win_lowess_end = 50\n",
        "win_lowess_step = 10\n",
        "\n",
        "win_lowess_no = 10\n",
        "sec_y_ht = 0\n",
        "\n",
        "for win_lowess_no in range(win_lowess_start, win_lowess_end, win_lowess_step):\n",
        "  section_crux_ls = get_lowess_cruxes(ts_df=section_sents_df, col_series=model_name, text_type='sentence', win_lowess=win_lowess_no, sec_y_height=sec_y_ht, subtitle_str=f'win={win_lowess_no}', do_plot=True, save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DQ4zGtRizlM"
      },
      "source": [
        "# TODO: Only printing sentiment for first crux point\n",
        "\n",
        "section_sents_df.iloc[62]['sent_raw']\n",
        "print('\\n')\n",
        "section_sents_df.iloc[62]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B0vm5RjgCLS"
      },
      "source": [
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "\n",
        "\n",
        "crux_sortsents_report(section_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa124qbuKzzH"
      },
      "source": [
        "# LOWESS Smoothed Sentences within chosen Selection No\n",
        "\n",
        "my_afrac = 1./12   # 1./12 ~ 0.08\n",
        "\n",
        "temp_df = get_lowess(section_sents_df, [model_name], plot_subtitle='LOWESS Smoothed MedianIRQ Sentence Sentiment', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                afrac=my_afrac, ait=7, alpha=0.8, do_plot=True, save2file=False)\n",
        "temp_df.columns\n",
        "col_lowess = f'{model_name}_{my_afrac:2.3f}lowess'\n",
        "col_lowess_clean = col_lowess.replace('_0.','_')\n",
        "section_sents_df[col_lowess_clean] = temp_df['median']\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Sentence Sentiments with selected Section #{Select_Section_No} (LOWESS frac={my_afrac:.2f})')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend('',frameon=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLeyWMtMaEqZ"
      },
      "source": [
        "# LOWESS Smoothed Paragraphs within chosen Selection No\n",
        "\n",
        "my_afrac = 1./8 # 1./8 ~ 0.125\n",
        "\n",
        "temp_df = get_lowess(section_parags_df, [model_name], plot_subtitle='LOWESS Smoothed Mean Rolling Sentence Sentiment', alabel=f'LOWESS Smoothed (afrac={my_afrac})', \n",
        "                afrac=my_afrac, ait=7, alpha=0.8, do_plot=True, save2file=False)\n",
        "\n",
        "section_parags_df[f'{model_name}_{my_afrac:.2f}_lowess'] = temp_df['median']\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Sentence Sentiments with selected Section #{Select_Section_No} (LOWESS frac={my_afrac:.2f})')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend('',frameon=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GfIuHNNmu-H"
      },
      "source": [
        "section_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEMWOxr7m2cl"
      },
      "source": [
        "section_crux_sents_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zouNSYD9Yloj"
      },
      "source": [
        "section_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vVdmcTcYb40"
      },
      "source": [
        "model_name = 'sentimentr_lnorm_medianiqr'\n",
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJa85Yx-rkkx"
      },
      "source": [
        "!pip install hdbscan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGA7kXGjrNET"
      },
      "source": [
        "from hdbscan import HDBSCAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZYo_LR8rF_J"
      },
      "source": [
        "\n",
        "\n",
        "y_ls = [1,2,4,7,9,5,4,7,9,56,57,54,60,200,297,275,243]\n",
        "y = np.reshape(y_ls, (-1, 1))\n",
        "type(y)\n",
        "y.shape\n",
        "\n",
        "clusterer = HDBSCAN(min_cluster_size=3)\n",
        "cluster_labels = clusterer.fit_predict(y)\n",
        "\n",
        "best_cluster = clusterer.exemplars_[cluster_labels[y.argmax()]].ravel()\n",
        "print(best_cluster)\n",
        "cluster_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsvTX-vtz8lq"
      },
      "source": [
        "type(y_ls)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo2zOaWVtUsm"
      },
      "source": [
        "y = np.reshape(crux_points_x_ls, (-1,1))\n",
        "\n",
        "clusterer = HDBSCAN(min_cluster_size=3)\n",
        "cluster_labels = clusterer.fit_predict(y)\n",
        "\n",
        "best_cluster = clusterer.exemplars_[cluster_labels[y.argmax()]].ravel()\n",
        "print(best_cluster)\n",
        "cluster_labels\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBXI8l6ywagn"
      },
      "source": [
        "len(color_palette)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB5VL_sV0tAG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRoIWeY1vx8M"
      },
      "source": [
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=3).fit(y)\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')\n",
        "\n",
        "color_palette = sns.color_palette('deep', 9)\n",
        "cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in cluster_labels]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, clusterer.probabilities_)]\n",
        "x = np.zeros_like(y) + 12.5\n",
        "plt.scatter(*y.T, x, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN7Rpw7B0xu-"
      },
      "source": [
        "print(crux_points_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLafViJ2zxrQ"
      },
      "source": [
        "len(crux_points_ls)\n",
        "crux_points_np.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irX4QSEbtTiy"
      },
      "source": [
        "crux_points_np = np.reshape(crux_points_ls, (-1,1))\n",
        "# np.reshape(crux_points_x_ls, (-1,1))\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=3).fit(crux_points_np)\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')\n",
        "\n",
        "color_palette = sns.color_palette('deep', 9)\n",
        "cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in cluster_labels]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, clusterer.probabilities_)]\n",
        "y_np = np.zeros_like(crux_points_np) + 12.5\n",
        "plt.scatter(*crux_points_np.T, y_np, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NRZ0Auka-uG"
      },
      "source": [
        "section_crux_sents_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGDccDTMfMT4"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10, 1./14, 1./16]\n",
        "# grid_fracs = [1./10, 1./14, 1./16]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.3]\n",
        "win_lowess=9\n",
        "\n",
        "\n",
        "# section_sents_df['vader_lnorm_medianiqr'].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot('vader_lnorm_medianiqr', data=section_sents_df)\n",
        "plt.title(f'LOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "section_crux_sents_dt = {}\n",
        "\n",
        "for afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  #   _ = get_lowess(section_sents_df, ['vader_lnorm_medianiqr'], plot_subtitle='Naive Raw + MedianIQR Midpoints', alabel=f'LOWESS Smoothed (afrac={my_afrac})', \n",
        "  #                afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # afrac = 1./7\n",
        "  # model_name = 'vader_lnorm_medianiqr' # model_name\n",
        "  sm_x, sm_y = sm_lowess(endog=section_sents_df[model_name].values, exog=section_sents_df.index.values, frac=afrac, it=3, return_sorted = True).T\n",
        "  col_lowess_frac = f'{model_name}_frac{int((afrac-int(afrac))*100)}_win{win_lowess}'\n",
        "  section_sents_df[col_lowess_frac] = sm_y\n",
        "  # _ = get_lowess(ts_df='section_sents_df', models_ls=[col_roll_str], text_unit='sentence', plot_subtitle='', alabel='', afrac=1./10, ait=5, alpha=0.5, do_plot=True, save2file=False)\n",
        "  section_crux_ls = list(get_lowess_cruxes(section_sents_df, col_series=col_lowess_frac, win_lowess=win_lowess, do_plot=False))\n",
        "  # col_lowess_frac = f'{model_name}_frac{int((afrac-int(afrac))*100)}_win{win_lowess}'\n",
        "  # print(f\"col_lowess_frac: {col_lowess_frac}\")\n",
        "  section_crux_sents_dt[col_lowess_frac] = section_crux_ls # list(zip(sm_x, sm_y))\n",
        "  # x, y = zip(*data)\n",
        "\n",
        "  # Set vertical y-axis magnification\n",
        "  y_mag = 30\n",
        "  plt.plot(sm_x, y_mag*sm_y)\n",
        "  plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nDifferent LOWESS Smoothed SMA Sentence Sentiments and Crux Points within selected Section #{Select_Section_No}')\n",
        "  plt.legend(loc='best')\n",
        "\n",
        "\n",
        "# Plot Crux Points for all LOWESS Curves on the x-axis\n",
        "crux_points_ls = []\n",
        "for key,value in section_crux_sents_dt.items():\n",
        "  model_lowess_name = key\n",
        "  crux_points_ls.extend(value)\n",
        "  plt.scatter(*zip(*value))\n",
        "\n",
        "# Plot Automatic HDBSCAN Clusters of Crux Points\n",
        "crux_points_np = np.reshape(crux_points_ls, (-1, 1))\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=3).fit(crux_points_np)\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')\n",
        "\n",
        "color_palette = sns.color_palette('deep', 9)\n",
        "cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in cluster_labels]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, clusterer.probabilities_)]\n",
        "y_np = np.zeros_like(crux_points_np) + 12.5\n",
        "plt.scatter(*crux_points_np.T, y_np, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} Sentence Crux Detection within selected Section #{Select_Section_No}\\nLOWESS Smoothed (Model: {model_lowess_name})')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\n",
        "# Plot the mean of all SMA MedianIQR Sentiment Time Series\n",
        "# col_meanroll = f'{model_name}_rollmean'\n",
        "# section_sents_df[col_meanroll] = section_sents_df[col_rolls_ls].mean(axis=1)\n",
        "# section_sents_df[col_meanroll].plot(label='mean', color='black', linewidth=3)\n",
        "\n",
        "# Plot corresponding Crux Points\n",
        "# model_name = 'vader_lnorm_medianiqr'\n",
        "section_crux_ls = get_lowess_cruxes(section_sents_df, col_series=model_name, win_lowess=10, do_plot=False) # 'vader_lnorm_medianiqr_0.07_lowess')\n",
        "section_sents_df.shape[0]\n",
        "print('\\n');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDx21JbaeL9s"
      },
      "source": [
        "!pip install kmeans1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFPbi6WqQVUs"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPcTifCyeUJh"
      },
      "source": [
        "import kmeans1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN4M4LEAeoCa"
      },
      "source": [
        "crux_points_x_ls = [x[0] for x in crux_points_ls]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htu7N02odHaU"
      },
      "source": [
        "**Enter how many Clusters of potential Crux Points you see in the Plot above**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBSVG9LXa2FD"
      },
      "source": [
        "Cluster_Count = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "clusters, centroids = kmeans1d.cluster(crux_points_x_ls, Cluster_Count)\n",
        "\n",
        "print(clusters)  \n",
        "print(centroids)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVCq0YUXBpX8"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2]\n",
        "\n",
        "# section_sents_df['vader_lnorm_medianiqr'].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot('vader_lnorm_medianiqr', data=section_sents_df)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "for my_afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  _ = get_lowess(section_sents_df, [model_name], plot_subtitle='Naive Raw + MedianIQR Midpoints', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                 afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # corpus_cruxes_dt['vader_lnorm_medianiqr'] = plot_crux_sections(model_names_ls=['vader_lnorm_medianiqr'], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL}\\n LOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpzOnRqZS3jZ"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.12, 0.14, 0.16, 0.18, 0.2]\n",
        "\n",
        "# section_sents_df[model_name].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot(model_name, data=section_sents_df)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "for my_afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  _ = get_lowess(section_parags_df, [model_name], plot_subtitle='MedianIQR Midpoints', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                 afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # corpus_cruxes_dt[model_name] = plot_crux_sections(model_names_ls=[model_name], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL}\\n LOWESS Smoothed Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O85J51__Ka-z"
      },
      "source": [
        "section_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48u_zr6-KkH-"
      },
      "source": [
        "# SMA Sentences\n",
        "\n",
        "# grid_afracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.3]\n",
        "# grid_fracs = [1./6, 1./7, 1./8, 1./9, 1./10, 1./15, 1./20]\n",
        "scale_roll = 1.\n",
        "win_lowess_per = 30\n",
        "win_lowess = int(win_lowess/100 * section_sents_df.shape[0])\n",
        "\n",
        "col_meanroll_lowess_ls = []\n",
        "\n",
        "col_meanroll = f'{model_name}_mean_roll050'\n",
        "for afrac in grid_fracs:\n",
        "  lowess_smooth_df = get_lowess(section_sents_df, [col_meanroll], plot_subtitle='SMA Mean of MedianIQR ', alabel=f'LOWESS afrac={afrac:.3f}', \n",
        "                afrac=afrac, ait=7, alpha=0.3, do_plot=True, save2file=False)\n",
        "  # print(f'type: {lowess_smooth_df.columns}')\n",
        "\n",
        "# col_lowess_str = f'{col_mean_roll}_lowess_frac{10*win_lowess}'\n",
        "col_meanroll_lowess_str = f'{col_meanroll}_frac{int((afrac-int(afrac))*100)}_win{win_lowess}'\n",
        "col_meanroll_lowess_ls.append(col_meanroll_lowess_str)\n",
        "section_sents_df[col_meanroll_lowess_str] = section_sents_df[model_name].apply(lambda x: x*scale_roll).rolling(win_lowess, center=True).mean()\n",
        "section_sents_df[col_meanroll_lowess_str].plot(alpha=0.7)\n",
        "get_lowess(section_sents_df, [col_meanroll_lowess_str], plot_subtitle='SMA Mean of MedianIQR ', alabel=f'LOWESS (afrac={my_afrac:.3f})', \n",
        "                afrac=afrac, ait=7, alpha=1.0, do_plot=True, save2file=False);\n",
        "\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KlWvNkcMI-M"
      },
      "source": [
        "**Raw Paragraph Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhRcTp4ALYUQ"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "win_sents_ls = [5,10,15,20,25]\n",
        "scale_roll = 6\n",
        "\n",
        "plt.plot(model_name, data=section_parags_df, alpha=0.3, label=f'Raw Paragraph Sentiment within selected Segment #{Select_Section_No}')\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5TcmNEsMMiE"
      },
      "source": [
        "**Length-Normed Paragraph Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BemEIw0Tknk"
      },
      "source": [
        "corpus_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyRTfn6HI50X"
      },
      "source": [
        "# Plot and Compare Naive Raw and LOWESS Smoothed Paragraph Sentiment Time Series within selected Section\n",
        "\n",
        "section_parag_lowess_df = pd.DataFrame()\n",
        "section_parag_lowess_df['parag_no'] = section_parags_df['parag_no'].copy()\n",
        "\n",
        "parags_midpoint_sentiment_ls = []\n",
        "for parags_midpoint_idx in parags_midpoint_ls:\n",
        "  parags_midpoint_sentiment_ls.append(float(corpus_parags_df[corpus_parags_df['parag_no'] == parags_midpoint_idx][model_name]))\n",
        "\n",
        "col_midapprox = f'{model_name}_midapprox'\n",
        "section_parag_lowess_df[col_midapprox] = parags_midpoint_sentiment_ls\n",
        "\n",
        "\n",
        "section_parag_lowess_df[col_midapprox].plot(label='Raw Midpoints')\n",
        "plt.xlabel(f'Niave Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "\n",
        "_ = get_lowess(section_parag_lowess_df, [col_midapprox], plot_subtitle=f'{model_base.capitalize()} Naive Raw + MedianIQR Midpoints', alabel='LOWESS Midpoints', afrac=1./4, ait=7, do_plot=True, save2file=False);\n",
        "\n",
        "# section_lowess_parags_df = get_lowess(section_sents_parags_df, ['vader_lnorm_medianiqr'], plot_subtitle='Approximate Paragraph MedianIQR', afrac=1./4, ait=7, do_plot=True, save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24M7uauXygFc"
      },
      "source": [
        "**Length-Noramlized Raw and LOWESS Smoothed Paragraph Sentiment plots within selected Section**\n",
        "\n",
        "NOTE: Horizonal x-axis narrative time axis adjusted for variable paragraph lengths - used midpoints of unequal length Paragraphs to more accurately visualize Sentiment Arc and precisely localize Crux Points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOsXfELh-tt_"
      },
      "source": [
        "# Verify details on currently selected Section\n",
        "print(f'Details on Section #{Select_Section_No}')\n",
        "print('------------------------')\n",
        "print(f' Paragraph Count: {section_parags_df.shape[0]}')\n",
        "print(f' Sentence Count:  {section_sents_df.shape[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfQS_qAo-Qdh"
      },
      "source": [
        "# %load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq2JjzKIHEdO"
      },
      "source": [
        "# Plot Raw and Rolling Sentence Sentiments within selected Section\n",
        "\n",
        "win_per = 5  # Rolling Window size in percentage of total Corpus length\n",
        "\n",
        "section_sents_parags_df.plot(x='sent_no', y='vader_lnorm_medianiqr')\n",
        "\n",
        "plt.title(f'Raw and Rolling Sentence Sentiments within selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No} (Length-Normalized in terms of Paragraphs)')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "\n",
        "section_sents_parags_df['vader_lnorm_medianiqr'].rolling(int((win_per/100)*section_sents_parags_df.shape[0])).mean().plot(label=\"Approx Paragraph VADER SMA (win=5%)\");\n",
        "\n",
        "# section_sents_parags_df.plot(x='sent_no', y='vader_lnorm_medianiqr', label='Sentence VADER MedianIQR')\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr'].rolling(int(0.05*section_sents_parags_df.shape[0])).mean().plot(label=\"Sentence VADER SMA (win=5%)\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laYb3dm101Qa"
      },
      "source": [
        "**Get Crux Points within selected Section**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peaks] to search for Peaks (unselect to search for Valley)\n",
        "\n",
        "* Pick [Crux_Rank] (1-5) to get the 1st to 5th biggest Peak or Valley Crux Paragraph\n",
        "\n",
        "* Pick [Context_Paragraphs_Each_Side] (0-5) to get n paragraphs before and n paragraphs after the selected Crux Paragraph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyekwnvX4wkj"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# ARCHIVED\n",
        "\n",
        "def get_sentnocontext(ts_df, model_name='vader', get_peaks=True, crux_rank=1, n_sideparags=1):\n",
        "  # get_cruxparags_section()\n",
        "  '''\n",
        "  Given a Section DataFrame with model_name sentiment column and crux peak/valley, rank and side paragraphs context\n",
        "  Return a list with the appropriate Crux Paragraph within this Section, and context\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "  Given a sentence number in the Corpus\n",
        "  Return the containing paragraph and n-paragraphs on either side\n",
        "  (e.g. if n=2, return 2+1+2=5 paragraphs)\n",
        "  '''\n",
        "\n",
        "  crux_parags_context_ls = []\n",
        "\n",
        "  if get_peaks == True:\n",
        "    sort_asc_flag=False\n",
        "  else:\n",
        "    sort_asc_flag=True\n",
        "\n",
        "  crux_parag_no = ts_df.sort_values(by=[model_name], ascending=sort_asc_flag).iloc[crux_rank-1]['parag_no']\n",
        "\n",
        "  print(f'crux_parag_no: {crux_parag_no}')\n",
        "\n",
        "  if n_sideparags == 0:\n",
        "    crux_parags_context_ls = list(corpus_parags_df[corpus_parags_df['parag_no'] == crux_parag_no]['parag_raw'])\n",
        "\n",
        "  else:\n",
        "    parag_start = crux_parag_no - n_sideparags\n",
        "    parag_end = crux_parag_no + n_sideparags + 1\n",
        "    crux_parags_context_ls = list(corpus_parags_df.iloc[parag_start:parag_end]['parag_raw'])\n",
        "\n",
        "  return crux_parags_context_ls\n",
        "\n",
        "# Test\n",
        "parags_context_ls = get_sentnocontext(ts_df=section_parags_df, model_name='vader_lnorm_medianiqr', get_peaks=True, crux_rank=1, n_sideparags=1)\n",
        "parags_context_ls\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxeabYXf2iqB"
      },
      "source": [
        "# def get_crux_parags_report(ts_df, model_name='vader', get_peaks=True, crux_rank=1, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence):\n",
        "\n",
        "# def get_sentnocontext_report(ts_df, model_name='vader', get_peaks=True, crux_rank=1, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence):\n",
        "'''\n",
        "Given a DataFrame with model_name sentiment column and crux peak/valley, rank and side paragraphs context\n",
        "Return a list with the appropriate Crux Paragraph, and context\n",
        "'''\n",
        "\"\"\"\n",
        "\n",
        "def get_sentnocontext_report(the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n",
        "  '''\n",
        "  Wrapper function around  get_sentnocontext()\n",
        "  Prints a nicely formatted context report\n",
        "  '''\n",
        "\n",
        "  context_noparags = the_n_sideparags*2+1\n",
        "\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f'The {context_noparags} Paragraph(s) Context around the Sentence #{Crux_Sentence_No} Crux Point:')\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f\"\\nCrux Sentence Raw Text: -------------------------------\\n\\n    {corpus_sents_df[corpus_sents_df['sent_no'] == the_sent_no]['sent_raw']}\") # iloc[the_sent_no]['sent_raw']}\")\n",
        "\n",
        "  print(f\"\\n{context_noparags} Paragraph(s) Context: ------------------------------\")\n",
        "  # context_parags_ls = get_sentnocontext(sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_parags_ls = get_sentnocontext(sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_len = len(context_parags_ls)\n",
        "  context_mid = context_len//2\n",
        "  for i, aparag in enumerate(context_parags_ls):\n",
        "    if i==context_mid:\n",
        "      # print(f'\\n>>> Paragraph #{i}: <<< Crux Point Sentence CAPITALIZED within this Paragraph\\n\\n    {aparag}')\n",
        "      print(f'\\n<*> {aparag}')\n",
        "    else:\n",
        "      # print(f'\\n    Paragraph #{i}:\\n\\n    {aparag}')\n",
        "      print(f'\\n    {aparag}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# get_sentnocontext_report(sent_no=1051, n_sideparags=1, sent_highlight=True)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxQGzYhgHEUD"
      },
      "source": [
        "# Get_Peaks = True #@param {type:\"boolean\"}\n",
        "Crux_Rank = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "\n",
        "# try:\n",
        "  \n",
        "# get_sentnocontext_report(ts_df=section_sents_df, model_name=model_name, get_peaks=Get_Peaks, crux_rank=Crux_Rank, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "get_sentnocontext_report(corpus_sents_df, the_sent_no=Crux_Rank, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "\n",
        "# except:\n",
        "#   print('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cPnJ7-QAHBp"
      },
      "source": [
        "section_parags_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga65SGcg5sM9"
      },
      "source": [
        "print(section_parags_df.sort_values(by=['vader_lnorm_medianiqr'], ascending=False).iloc[0]) # ['parag_no'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGuFadpM8QoB"
      },
      "source": [
        "parags_context_ls = get_cruxparag_context(ts_df=section_sents_parags_df, model_name='vader_lnorm_medianiqr', get_peaks=True, crux_rank=1, n_sideparags=2, sent_highlight=True)\n",
        "parags_context_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPE_ODwK2imT"
      },
      "source": [
        "section_sents_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gz8UImlq5Uy"
      },
      "source": [
        "# **Save Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NH8ltVX9SvD"
      },
      "source": [
        "## **Section, Chapter Crux DataFrames (summary stats/sentiments only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovjd7uvr2hS8"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLb7mQgV7_JK"
      },
      "source": [
        "# Create a Chapter Summary DataFrame extracting only key information (no text)\n",
        "\n",
        "corpus_chaps_summary_df = corpus_chaps_df[['chap_no','sent_no_start','sent_no_mid','char_len','token_len',\n",
        "                 'sentimentr','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'syuzhet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'bing','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'sentiword','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'senticnet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'nrc','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'afinn','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'vader','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'textblob','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'pattern','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'stanza','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 ]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSvXSwp-2ePp"
      },
      "source": [
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5s8R38P81M7"
      },
      "source": [
        "# Create a Section Summary DataFrame extracting only key information (no text)\n",
        "\n",
        "corpus_sects_summary_df = corpus_sects_df[['sect_no','sent_no_start','sent_no_mid','char_len','token_len',\n",
        "                 'sentimentr','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'syuzhet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'bing','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'sentiword','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'senticnet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'nrc','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'afinn','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'vader','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'textblob','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'pattern','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'stanza','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 ]]\n",
        "\n",
        "corpus_sects_summary_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Op-IgH9iP2"
      },
      "source": [
        "# Save the original Corpus text at 4 levels of semantic grouping: sentences, paragraphs, sections and chapters\n",
        "\n",
        "# Save Section and Chapter DataFrames Metainformation (e.g. sent_no_start) and Sentiment Values\n",
        "corpus_sects_summary_filename = f'corpus_section_summary_lexrules_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Section Summary to file: {corpus_sects_summary_filename}')\n",
        "corpus_sects_summary_df.to_csv(corpus_sects_summary_filename)\n",
        "\n",
        "corpus_chaps_summary_filename = f'corpus_chapter_summary_lexrules_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Chapters Summary to file: {corpus_chaps_summary_filename}')\n",
        "corpus_chaps_summary_df.to_csv(corpus_chaps_summary_filename)\n",
        "\n",
        "# Save Corpus Cruxes Dictionary is saved to a JSON file\n",
        "corpus_cruxes_summary_filename = f'corpus_cruxes_summary_lexrules_{author_abbr_str}_{title_str}.json' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes Summary to file: {corpus_cruxes_summary_filename}')\n",
        "with open(corpus_cruxes_summary_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSO0vT7oXOXO"
      },
      "source": [
        "# Verify exported Section Summary file\n",
        "\n",
        "!head -n 5 $corpus_sects_summary_filename\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3RIJOdsI_Ud"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0A0M6TPu_Ml"
      },
      "source": [
        "## **Compare Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpX60Cld_B7z"
      },
      "source": [
        "def drop_dupcols(df):\n",
        "  '''\n",
        "  Given a DataFrame\n",
        "  Drop repeatitive columns\n",
        "  '''\n",
        "\n",
        "  col_drop_ls = []\n",
        "\n",
        "  col_ls = list(df.columns)\n",
        "  print(f'BEFORE: Columns #{len(df.columns)}')\n",
        "\n",
        "  for i, acol in enumerate(col_ls):\n",
        "    acol_word_ls = acol.split('_')\n",
        "    # print(f'acol_word_ls: {acol_word_ls}')\n",
        "    if (len(acol_word_ls)) == len(set(acol_word_ls)):\n",
        "      continue\n",
        "    else:\n",
        "      col_drop_ls.append(acol)\n",
        "\n",
        "  df.drop(columns=col_drop_ls, inplace=True, axis=1)\n",
        "\n",
        "  print(f'AFTER: Columns #{len(df.columns)}')\n",
        "\n",
        "  return col_drop_ls\n",
        "\n",
        "dropped_cols_ls = drop_dupcols(corpus_parags_df)\n",
        "print(f'dropped: {dropped_cols_ls}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIOFkFzYinfd"
      },
      "source": [
        "corpus_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lpvN8PR9VRk"
      },
      "source": [
        "# List of Tuples (Model, Scaling Factor) to plot together with same size for comparison\n",
        "\n",
        "models_sma_ls = [('vader_lnorm_medianiqr',1),\n",
        "                 ('textblob_lnorm_medianiqr',20),\n",
        "                 ('afinn_lnorm_medianiqr',4),\n",
        "                 ('sentimentr_lnorm_medianiqr',1),\n",
        "                 ('syuzhet_lnorm_medianiqr',1),\n",
        "                 ('bing_lnorm_medianiqr',0.1),\n",
        "                 ('sentiword_lnorm_medianiqr',10),\n",
        "                 ('senticnet_lnorm_medianiqr',.5),\n",
        "                 ('nrc_lnorm_medianiqr',0.2),\n",
        "                 ('pattern_lnorm_medianiqr',20),\n",
        "                 ('stanza_lnorm_medianiqr',0.5),\n",
        "                 ('hfbert_lnorm_medianiqr',5),\n",
        "                 ('nlptown_lnorm_medianiqr',5),\n",
        "                 ('robertalg15_lnorm_medianiqr',5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1pjqF9nTQg-"
      },
      "source": [
        "# List of Tuples (Model, Scaling Factor) to plot together with same size for comparison\n",
        "\n",
        "models_sma_ls = [('vader',1),\n",
        "                 ('textblob',20),\n",
        "                 ('afinn',4),\n",
        "                 ('sentimentr',1),\n",
        "                 ('syuzhet',1),\n",
        "                 ('bing',0.1),\n",
        "                 ('sentiword',10),\n",
        "                 ('senticnet',.5),\n",
        "                 ('nrc',0.2),\n",
        "                 ('pattern',20),\n",
        "                 ('stanza',0.5),\n",
        "                 ('hfbert',5),\n",
        "                 ('nlptown',5),\n",
        "                 ('robertalg15',5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RihyT4wZFP5k"
      },
      "source": [
        "corpus_sents_df[['stanza_lnorm_medianiqr','stanza']].rolling(500,center=True).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaxzZkVlBcNf"
      },
      "source": [
        "def plot_autoscaled_ts(ts_df=corpus_sents_df, ts_ls=['vader_lnorm_medianiqr', 'textblob_lnorm_medianiqr', \n",
        "                                                     'syuzhet_lnorm_medianiqr', 'sentimentr_lnorm_medianiqr',\n",
        "                                                     'bing_lnorm_medianiqr', 'afinn_lnorm_medianiqr',\n",
        "                                                     'pattern_lnorm_medianiqr', 'stanza_lnorm_medianiqr']):\n",
        "  '''\n",
        "  Given a DataFrame and list of Columns/Time Series\n",
        "  Automatically scale all to the same range and plot together\n",
        "  '''\n",
        "\n",
        "  ts_spans_ls = []\n",
        "\n",
        "  current_min = ts_df[ts_ls[0]].min()\n",
        "  current_max = ts_df[ts_ls[0]].max()\n",
        "  ts_spans_ls.append(float(current_max - current_min))\n",
        "\n",
        "  for ats in ts_ls[1:]:\n",
        "    current_min = ts_df[ats].min()\n",
        "    current_max = ts_df[ats].max()\n",
        "    ts_spans_ls.append(float(current_max - current_min))\n",
        "\n",
        "  # find index of maximum span\n",
        "  max_index = ts_spans_ls.index(max(ts_spans_ls))\n",
        "  max_span_value = ts_spans_ls[max_index]\n",
        "  max_span_model = ts_ls[max_index]\n",
        "  print(f'max span is: {max_span_value} from {max_span_model}')\n",
        "\n",
        "  for i, ats in enumerate(ts_ls):\n",
        "    y_scaling_factor = max_span_value/ts_spans_ls[i]\n",
        "    print(f'ats={ats} with scaling={y_scaling_factor}')\n",
        "    ts_y_scaled_ser = ts_df[ats].apply(lambda x: x*y_scaling_factor)\n",
        "    # plt.plot()\n",
        "    plot_df = pd.DataFrame()\n",
        "    plot_df['x_value'] = ts_df.index\n",
        "    plot_df['y_scaled'] = ts_y_scaled_ser\n",
        "    plot_df['y_scaled_roll050'] = plot_df['y_scaled'].rolling(350, center=True).mean()\n",
        "    plot_df['y_scaled_roll050'].plot(label=f'{ats}')\n",
        "    plt.legend(loc='best')\n",
        "    # sns.lineplot(data=plot_df, x='x_value', y='y_scaled', alpha=0.5, label=f'{ats}')\n",
        "\n",
        "  return ts_spans_ls, ts_ls\n",
        "\n",
        "# Test\n",
        "ts_spans_ls, ts_ls = plot_autoscaled_ts()\n",
        "zip_ls = zip(ts_spans_ls, ts_ls)\n",
        "for aspan, amodel in zip_ls:\n",
        "  print(f'model: {amodel} with span: {aspan}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emkaDWvDILDc"
      },
      "source": [
        "ts_df=corpus_sents_df, ts_ls=['vader_lnorm_medianiqr', 'textblob_lnorm_medianiqr', \n",
        "                                                     'syuzhet_lnorm_medianiqr', 'sentimentr_lnorm_medianiqr',\n",
        "                                                     'bing_lnorm_medianiqr', 'afinn_lnorm_medianiqr',\n",
        "                                                     'pattern_lnorm_medianiqr', 'stanza_lnorm_medianiqr']):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T8pf6DsIGmq"
      },
      "source": [
        "# Test\n",
        "ts_spans_ls, ts_ls = plot_autoscaled_ts(ts_df=corpus_parags_df)\n",
        "zip_ls = zip(ts_spans_ls, ts_ls)\n",
        "for aspan, amodel in zip_ls:\n",
        "  print(f'model: {amodel} with span: {aspan}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWGdzLS4Ibr-"
      },
      "source": [
        "corpus_sents_df['pattern_lnorm_medianiqr'].hist(bins=100) # rolling(100, center=True).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42RrUksBOxfo"
      },
      "source": [
        "models_sma_ls = [('vader_lnorm_medianiqr',1),\n",
        "                 ('textblob_lnorm_medianiqr',20),\n",
        "                 ('sentimentr_lnorm_medianiqr',1),\n",
        "                 ('syuzhet_lnorm_medianiqr',1),\n",
        "                 ('stanza_lnorm_medianiqr',0.2)]\n",
        "\n",
        "win_per = 5  # 5=5% of full corpus length\n",
        "win_roll = int(corpus_sents_df.shape[0]* win_per/100)\n",
        "\n",
        "for amodel, amag in models_sma_ls:\n",
        "  corpus_parags_df[amodel].rolling(win_roll, center=True).mean().apply(lambda x: amag*x).plot(linewidth=4, label=amodel)\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQgdwsJGZN_"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpOCp1-88rrF"
      },
      "source": [
        "# **Standardize and Remove Outliers (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mul5MSZrgKsw"
      },
      "source": [
        "## **Remove Outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07hyJuT1c5rJ"
      },
      "source": [
        "### **Before Removing Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKX8f0Q1I53M"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  print(f'Plotting {model_name}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_name, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzUoTpEWeyCy"
      },
      "source": [
        "# sns.lineplot(data=corpus_sents_df, x='sent_no', y='y_scaled', legend='brief', label='y_scaled')\n",
        "      \n",
        "# plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nSMA Smoothed Sentence Sentiment Plot (windows={win_ls})')\n",
        "# plt.legend(loc='best')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr1k-3Rsc5XL"
      },
      "source": [
        "# Plot all Raw Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_LS:\n",
        "  print(f'Plotting {model_name}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_name, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FrNFec7tNij"
      },
      "source": [
        "### **Remove Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYXa7qoevyuo"
      },
      "source": [
        "# Trim outliers to max of 3*Median Abs Variance in Standardized Sentiment Time Series\n",
        "#   and overwrite results in model_name column\n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\n",
        "# Sentence\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Sentence: {col_noouts_str} --------------------')\n",
        "  corpus_sents_df[col_noouts_str] = clip_outliers(corpus_sents_df[amodel_str])\n",
        "  \n",
        "  print(f'  old Standardized max: {corpus_sents_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sents_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_sents_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_sents_df[col_noouts_str].min()}')\n",
        "  \n",
        "# col_rename_dt = rename_cols(corpus_sents_df, models_ls) # ERROR: created 1 new col with col_rename_dt dictionary name instead of mapping correctly\n",
        "# col_rename_dt\n",
        "# _ = corpus_sents_df.rename(columns=col_rename_dt, inplace=True, errors='raise');\n",
        "\n",
        "# Paragraph\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Paragraph: {col_noouts_str} --------------------')\n",
        "  corpus_parags_df[col_noouts_str] = clip_outliers(corpus_parags_df[amodel_str])\n",
        "\n",
        "  print(f'  old Standardized max: {corpus_parags_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_parags_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_parags_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_parags_df[col_noouts_str].min()}')\n",
        "\n",
        "# Section\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Section: {col_noouts_str} --------------------')\n",
        "  corpus_sects_df[col_noouts_str] = clip_outliers(corpus_sects_df[amodel_str])\n",
        "\n",
        "  print(f'  old Standardized max: {corpus_sects_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sects_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_sects_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_sects_df[col_noouts_str].min()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyPsLI9E4tbB"
      },
      "source": [
        "# Trim outliers to max of 3*Median Abs Variance in Standardized Sentiment Time Series\n",
        "#   and overwrite results in model_name column\n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\"\"\"\n",
        "# Sentences\n",
        "for amodel in MODELS_LS:\n",
        "  col_stand = amodel + '_stand'\n",
        "  col_standout = amodel + '_standout'\n",
        "  print(f'Sentences: {col_stand} --------------------')\n",
        "  # corpus_sents_df[amodel] = corpus_sents_df[col_stand]\n",
        "  corpus_sents_df[col_standout] = clip_outliers(corpus_sents_df[col_stand])\n",
        "  \n",
        "  print(f'  old Standardized max: {corpus_sents_df[col_stand].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sents_df[col_stand].min()}')\n",
        "  print(f'  new max: {corpus_sents_df[col_standout].max()}')\n",
        "  print(f'  new min: {corpus_sents_df[col_standout].min()}')\n",
        "  \n",
        "# col_rename_dt = rename_cols(corpus_sents_df, models_ls) # ERROR: created 1 new col with col_rename_dt dictionary name instead of mapping correctly\n",
        "# col_rename_dt\n",
        "# _ = corpus_sents_df.rename(columns=col_rename_dt, inplace=True, errors='raise');\n",
        "\n",
        "# Paragraphs\n",
        "for amodel in MODELS_LS:\n",
        "  col_stand = amodel + '_stand'\n",
        "  col_standout = amodel + '_standout'\n",
        "  print(f'Paragraphs: {col_stand} --------------------')\n",
        "  # corpus_parags_df[amodel] = corpus_parags_df[col_stand]\n",
        "  corpus_parags_df[col_standout] = clip_outliers(corpus_parags_df[col_stand])\n",
        "  print(f'  old Standardized max: {corpus_parags_df[col_stand].max()}')\n",
        "  print(f'  old Standardized min: {corpus_parags_df[col_stand].min()}')\n",
        "  print(f'  new max: {corpus_parags_df[col_standout].max()}')\n",
        "  print(f'  new min: {corpus_parags_df[col_standout].min()}')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPKABrvstSIo"
      },
      "source": [
        "### **After Removing Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ccxOjqghbgP"
      },
      "source": [
        "# Plot all Raw Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "# Exlopre to find which Sentiment Time Series still have outliers after initial 2.5*Median Abs Dev Clipping\n",
        "MODELS_SENTS_EXCLUDE_LS = ['nrc','bing','afinn','stanza']  # Likely these TS are not normal or heavy tailed so 2.5*MedAbsDev did not clip well\n",
        "MODELS_SENTS_CUSTOM_LS = [x for x in MODELS_LS if x not in MODELS_SENTS_EXCLUDE_LS] \n",
        "\n",
        "for model_name in MODELS_SENTS_CUSTOM_LS:\n",
        "  model_noouts = f'{model_name}_noouts'\n",
        "  print(f'Plotting {model_noouts}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_noouts, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment w/Trimmed Outliers Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJHXLvayIhOC"
      },
      "source": [
        "# Plot all Raw Paragraph Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_LS:\n",
        "  model_standout = f'{model_name}_standout'\n",
        "  print(f'Plotting {model_standout}')\n",
        "  sns.lineplot(data=corpus_parags_df, x='parag_no', y=model_standout, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Paragraph Sentiment Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNp9zwBfmgpI"
      },
      "source": [
        "## **Standardize Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ_wcgiyx-NJ"
      },
      "source": [
        "### **Before Standardizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEUbDF4hx-NN"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  model_noouts_str = f'{model_name}_noouts'\n",
        "  print(f'Plotting {model_noouts_str}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_noouts_str, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_noouts_str}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IrnbsMByTmX"
      },
      "source": [
        "### **Standardized the NoOutliers Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhkW7faSmPWh"
      },
      "source": [
        "# Standardize Sentence and Paragraphs Sentiment Time Series (Section TS was Standardized above)\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "# orig_cols_ls = list(set(corpus_all_df.columns) - set(['sent_no','parag_no','sent_raw','sent_clean']))\n",
        "# cols_ls = []\n",
        "\n",
        "# Sentences\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_sents_df[acol].values.reshape(-1,1)))\n",
        "    corpus_sents_df[acol_new] = pd.Series(temp_np.squeeze())\n",
        "\n",
        "# Paragraphs\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_parags_df[acol].values.reshape(-1,1)))\n",
        "    corpus_parags_df[acol_new] = pd.Series(temp_np.squeeze())\n",
        "\n",
        "# Paragraphs\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_sects_df[acol].values.reshape(-1,1)))\n",
        "    corpus_sects_df[acol_new] = pd.Series(temp_np.squeeze())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn6EWXYaybhj"
      },
      "source": [
        "### **After Standardizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcGzZNZJO7fd"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  model_standouts_str = f'{model_name}_standouts'\n",
        "  print(f'Plotting {model_standouts_str}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_standouts_str, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_standouts_str}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37_ztlEpzYHx"
      },
      "source": [
        "MODELS_LS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Be4bSszK-A"
      },
      "source": [
        "## **Deselect Poorly Behaved Sentiment Time Series Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY_HdLarzK-D"
      },
      "source": [
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "NCR_Arc = True #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6z5qYX3zm9n"
      },
      "source": [
        "# Create and Verify custom list of Models to include\n",
        "\n",
        "MODELS_CUSTOM_LS = []\n",
        "\n",
        "if VADER_Arc:\n",
        "  MODELS_CUSTOM_LS.append('vader')\n",
        "if TextBlob_Arc:\n",
        "  MODELS_CUSTOM_LS.append('textblob')\n",
        "if Stanza_Arc:\n",
        "  MODELS_CUSTOM_LS.append('stanza')\n",
        "if SentimentR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentimentr')\n",
        "if Syuzhet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('syuzhet')\n",
        "if AFINN_Arc:\n",
        "  MODELS_CUSTOM_LS.append('afinn')\n",
        "if Bing_Arc:\n",
        "  MODELS_CUSTOM_LS.append('bing')\n",
        "if Pattern_Arc:\n",
        "  MODELS_CUSTOM_LS.append('pattern')\n",
        "if SentiWord_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentiword')\n",
        "if SenticNet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('senticnet')\n",
        "if NCR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('nrc')\n",
        "\n",
        "print(f'Here are the Models we are using to ensemble and save:\\n   {MODELS_CUSTOM_LS}')\n",
        "\n",
        "models_incl_ls = []\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  models_incl_ls.append(amodel[:2])\n",
        "models_incl_str = ''.join(models_incl_ls)\n",
        "\n",
        "print(f'Here is a custom name abbr: {models_incl_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfiyOjMBBraW"
      },
      "source": [
        "## **Calculate Median of All (Trimmed Outliers then Standardized) Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1tX6BmT4U5h"
      },
      "source": [
        "# Create a list of models with Outliers trimmed and Standardized Time Series \n",
        "\n",
        "MODELS_CUSTOM_STANDOUTS_LS = []\n",
        "\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  model_standout_str = f'{amodel}_standouts'\n",
        "  MODELS_CUSTOM_STANDOUTS_LS.append(model_standout_str)\n",
        "\n",
        "print(f'List of NoOutliers/Standardized Models to compute Median on: {MODELS_CUSTOM_STANDOUTS_LS}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx3-Qug9Y6h0"
      },
      "source": [
        "# Disregard poorly behaved time series identified and stored in MODELS_SENTS_EXCLUDE_LS \n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\n",
        "corpus_sents_df['median_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_sents_df.head(2)\n",
        "\n",
        "corpus_sents_df['std_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_sents_df.head(2)\n",
        "\n",
        "corpus_sents_df['mean_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_sents_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2qunU4VfJ4J"
      },
      "source": [
        "# (For now) Paragraph Sentiment TS are better behaved and don't require excluding any Model\n",
        "\n",
        "corpus_parags_df['median_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_parags_df['std_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_parags_df['mean_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_parags_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtTY4t4k1QBc"
      },
      "source": [
        "# (For now) Paragraph Sentiment TS are better behaved and don't require excluding any Model\n",
        "\n",
        "corpus_sects_df['median_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_sects_df['std_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_sects_df['mean_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_parags_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp_gYLYbkjxd"
      },
      "source": [
        "## **Save Processed Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmyasWR1kjxh"
      },
      "source": [
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "corpus_sents_filename = f'corpus_sentences_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sentences to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "corpus_parags_filename = f'corpus_paragraphs_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Paragraphs to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "corpus_sects_filename = f'corpus_sections_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sections to file: {corpus_sects_filename}')\n",
        "corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "corpus_cruxes_filename = f'corpus_cruxes_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes to file: {corpus_cruxes_filename}')\n",
        "with open(corpus_cruxes_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUnbmOFckt2t"
      },
      "source": [
        "# **EDA Visualizations and Comparisons**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww1a9l8Lkjxk"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiHul8A1kjxn"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbY7D82jkjxp"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_sects_df.head(2)\n",
        "corpus_sects_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvvfzj7xkjxp"
      },
      "source": [
        "corpus_cruxes_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDiTp97_zlUm"
      },
      "source": [
        "# norm_cols_ls = ['distbertsst_norm', 'nlptown_norm','xlnet_sst5_norm','bert_imdb_norm', 'bertuc_googapps_norm', 'roberta_lg15_norm']\n",
        "\"\"\"\n",
        "\n",
        "# ARCHIVE\n",
        "\n",
        "cols_norm_ls = []\n",
        "cols_stand_ls = []\n",
        "\n",
        "for acol in corpus_all_df.columns:\n",
        "  if acol.endswith('_norm'):\n",
        "    print(f'Adding {acol} to norm_cols_ls')\n",
        "    cols_norm_ls.append(acol)\n",
        "  elif acol.endswith('_stand'):\n",
        "    print(f'Adding {acol} to stand_cols_ls')\n",
        "    cols_stand_ls.append(acol)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "print(f'\\nNormalized Columns: {cols_norm_ls}')\n",
        "\n",
        "print(f'\\nStandardized Columns: {cols_stand_ls}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O3l61be6_GQ"
      },
      "source": [
        "### **LOWESS Smoothed Single Plot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUjsXyRG81zT"
      },
      "source": [
        "**Normalized Sentiment Smoothed with LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5MYEwGuZ2vh"
      },
      "source": [
        "MODELS_STAND_LS = []\n",
        "for amodel in MODELS_LS:\n",
        "  MODELS_STAND_LS.append(f'{amodel}_stand')\n",
        "\n",
        "print(f'MODELS_STAND_LS: {MODELS_STAND_LS}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBT-Qd4o8919"
      },
      "source": [
        "**Standardized Sentiment Smoothed with LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ-Oe8icVpHn"
      },
      "source": [
        "# Plot and Compare all LOWESS Smoothed *Standardized* Sentiment Time Series\n",
        "\n",
        "corpus_lowess_stand_df = get_lowess(corpus_all_df, cols_stand_ls, plot_subtitle='Standardized', afrac=1./10, ait=5, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JizqEQE6X3oQ"
      },
      "source": [
        "### **High Level: Section View**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIwEIY4cd2PB"
      },
      "source": [
        "##### **Raw Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryWst950gpYx"
      },
      "source": [
        "# Plot Raw Section Sentiments\n",
        "\"\"\"\n",
        "\n",
        "# ARCHIVED\n",
        "\n",
        "for amodel in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  amodel_stand_str = f'{amodel}'\n",
        "  plot_raw_sentiments(model_name=amodel_stand_str, semantic_type='section', save2file=False)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx4mryQwVz7h"
      },
      "source": [
        "# Raw Standardized Section Sentiment Time Series\n",
        "\n",
        "_ = plot_stand_crux_sections(ts_df=corpus_sects_df, model_names_ls=MODELS_CUSTOM_STANDOUTS_LS, semantic_type='section', label_token_ct=5, title_xpos=0.5, title_ypos=1, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0HK0lbzjXXF"
      },
      "source": [
        "# Plot the Median of the Customized Set of NoOutliers/Standardized Section Sentiment Time Series\n",
        "\n",
        "corpus_sects_df['median_standouts_custom_lex'].plot()\n",
        "plt.title(f'{CORPUS_FULL}\\n Median of NoOutliers/Standardized Section Sentiment Time Series');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKz34IzZdxIg"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeOWF6frEm0S"
      },
      "source": [
        "Window_Width = 4 #@param {type:\"slider\", min:2, max:20, step:1}\n",
        "\n",
        "# DEFAULT 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1lngzR7XM5l"
      },
      "source": [
        "# SMA of Custom Set of NoOutliers/Standardized Section Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/Adjust the win_ls to explore different window_size by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  # print(f'Plotting {acol_name}')\n",
        "  get_smas(corpus_sects_df, model_name, text_unit='section', subtitle_str='(NOTE: different x-scale)', win_ls=[Window_Width])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5Qw6frQdz1C"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAuz19B5ntxk"
      },
      "source": [
        "# Standardized Section LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sects_df, models_ls=MODELS_STAND_LS, text_unit='section', afrac=1./4, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-u2ScQZD8Lv"
      },
      "source": [
        "LOWESS_fraction = 0.15 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 (approx 0.17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L82SbpnkO_3"
      },
      "source": [
        "# Standardized Median Section LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sects_df, models_ls=['median'], text_unit='section', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6Ipaa-lWVwP"
      },
      "source": [
        "### **Middle Level: Paragraph Views**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDYYpJYRfrZ-"
      },
      "source": [
        "##### **Raw Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxyJD-fjf4ev"
      },
      "source": [
        "# Plot Custom Set of NoOutliers/Standardized Paragraph Sentiments Time Series \n",
        "\n",
        "for amodel in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  plot_raw_sentiments(model_name=amodel, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwD9lNuJkCfo"
      },
      "source": [
        "# Plot the Median for the Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series\n",
        "\n",
        "corpus_parags_df['median_standouts_custom_lex'].plot()\n",
        "plt.title(f'{CORPUS_FULL}\\n Median of Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jii7hmhleyHo"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJn79SwJDKHt"
      },
      "source": [
        "Window_Width = 10 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "490cQGkgIBK2"
      },
      "source": [
        "# SMA Standardized Paragraph Sentiment Arcs\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots (5-20 defaults)\n",
        "\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  get_smas(corpus_parags_df, model_name, text_unit='paragraph', win_ls=[Window_Width])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDo7OHyHCtUJ"
      },
      "source": [
        "Window_Percentage = 5 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT: 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mahzCm5W9-QL"
      },
      "source": [
        "# SMA Plot the Median for the Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots\n",
        "\n",
        "win_percentage = Window_Percentage  # 5 means rolling window size is 5% of corpus length (5-20 default)\n",
        "\n",
        "win_size = int(corpus_parags_df.shape[0]*(win_percentage*0.01))\n",
        "\n",
        "plot_title = f'{CORPUS_FULL}\\n Median of Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series (win={win_percentage}%)'\n",
        "corpus_parags_df['median_standouts_custom_lex'].rolling(win_size).mean().plot(title=plot_title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2CIR3U9e4WC"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIzHfrCdCC9d"
      },
      "source": [
        "LOWESS_fraction = 0.17 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 (approx 0.17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2337wydGe4WC"
      },
      "source": [
        "# Standardized Paragraph LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots (0.10-0.20 default)\n",
        "\n",
        "_ = get_lowess(corpus_parags_df, models_ls=MODELS_CUSTOM_STANDOUTS_LS, text_unit='paragraph', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0emSLGZCh6-Y"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vq8s_aShoEJ"
      },
      "source": [
        "# Plot it\n",
        "\n",
        "_ = get_lowess(corpus_parags_df, models_ls=['median_standouts_custom_lex'], text_unit='paragraph', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzcDX02pZGbr"
      },
      "source": [
        "### **Low Level: Sentence Views**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV6spOW1fSKf"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE_-_JJ9FeVj"
      },
      "source": [
        "Window_Percentage = 10 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT: 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gYKbdptopin"
      },
      "source": [
        "# Plot all Standardized Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "models_stand_ls = []\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  get_smas(corpus_sents_df, model_name, text_unit='sentence', alpha=0.3, win_ls=[Window_Percentage])\n",
        "\n",
        "# print(f'models_stand_ls: {models_stand_ls}')\n",
        "corpus_sents_df['median_stand'] = corpus_sents_df[models_stand_ls].median()\n",
        "plt.plot(corpus_sents_df.sent_no, corpus_sents_df.median_stand, color='black', label='Median')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0fL5RXKfZG6"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-NGw8yEFnJD"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDlGY9CMfUj9"
      },
      "source": [
        "# Standardized Paragraph LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sents_df, models_ls=MODELS_CUSTOM_STANDOUTS_LS, text_unit='sentence', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9SQeAvgGFb6"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-GtGa5mlEhZ"
      },
      "source": [
        "# Median of Custom Set of NoOutlier/Standardized Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sents_df, models_ls=['median_standouts_custom_lex'], text_unit='paragraph', afrac=1./8, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1VhYgLai3RK"
      },
      "source": [
        "## **Save Standardized-NoOutliers Sentiment Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3zmDWROOEvM"
      },
      "source": [
        "# Save all Processed DataFrames\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "corpus_sents_filename = f'corpus_sentences_only_lexrules_standouts_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Paragraphs DataFrame\n",
        "corpus_parags_filename = f'corpus_paragraphs_only_lexrules_standouts_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Section DataFrame\n",
        "corpus_sects_filename = f'corpus_sections_only_lexrules_standouts_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_sects_filename}')\n",
        "corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "\n",
        "# Save Cruxes\n",
        "corpus_cruxes_filename = f'corpus_cruxes_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes to file: {corpus_cruxes_filename}')\n",
        "with open(corpus_cruxes_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vUzh39HHJXz"
      },
      "source": [
        "# **Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJI_13B0HMnU"
      },
      "source": [
        "## **Sentiment Stability**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R6SMWtSHJLK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9sEH8_IG6Qq"
      },
      "source": [
        "## **Crux Point Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRjKHnQaOTu2"
      },
      "source": [
        "### **Gather (n) Highest/Lowest Sentiment Values for Each Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG6Y0vPFOkzn"
      },
      "source": [
        "def getn_cruxes(crux_dt, model_name='vader', get_n=6):\n",
        "  '''\n",
        "  Given a Crux Dictionary, a Model item/dict within, and and integer n\n",
        "  Return the n highest and n lowest sentiment values\n",
        "  NOTE: if get_n == 0, return all Crux Points for all Models\n",
        "  '''\n",
        "\n",
        "  cruxes_all_df = pd.DataFrame\n",
        "  cruxes_n_top_df = pd.DataFrame()\n",
        "\n",
        "  cruxes_all_df = pd.DataFrame.from_dict(crux_dt[model_name])\n",
        "\n",
        "  cruxes_all_df = cruxes_all_df.transpose().reset_index().rename(columns={'index':'var'})\n",
        "\n",
        "  cruxes_all_df.rename(columns={'var':'sent_no',0:model_name,1:'sent_raw'}, inplace=True)\n",
        "  cruxes_all_df.drop(columns=['sent_raw'], inplace=True)\n",
        "  cruxes_all_df.rename(columns={model_name:'sentiment'}, inplace=True)\n",
        "  cruxes_all_df['sentiment'] = cruxes_all_df['sentiment'].astype('float')\n",
        "  cruxes_all_df['model_name'] = model_name\n",
        "  cruxes_all_df = cruxes_all_df[['sent_no','model_name','sentiment']]\n",
        "\n",
        "  if get_n > 0:\n",
        "    cruxes_n_top_df = cruxes_all_df.nlargest(get_n, 'sentiment')\n",
        "    cruxes_n_top_df = cruxes_n_top_df.append(cruxes_all_df.nsmallest(get_n, 'sentiment'))\n",
        "  elif get_n ==0:\n",
        "    cruxes_n_top_df = cruxes_all_df\n",
        "  else:\n",
        "    print(f'ERROR: argument get_n must be either 0 (return all Cruxes) or greater than 0')\n",
        "    \n",
        "  return cruxes_n_top_df\n",
        "\n",
        "# Test\n",
        "\n",
        "cruxes_n_top_df = getn_cruxes(corpus_cruxes_dt, model_name='vader', get_n=3)\n",
        "cruxes_n_top_df.head(6)\n",
        "cruxes_n_top_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H67tOfiaSaHB"
      },
      "source": [
        "# Acculumate all the Crux Points from All Models into one cruxes_n_top_all_df DataFrame\n",
        "\n",
        "cruxes_n_top_all_df = pd.DataFrame()\n",
        "\n",
        "for amodel in MODELS_LS:\n",
        "  print(f'Appending Cruxes from {amodel}')\n",
        "  cruxes_n_top_df = getn_cruxes(corpus_cruxes_dt, model_name=amodel, get_n=12)\n",
        "  cruxes_n_top_all_df = cruxes_n_top_all_df.append(cruxes_n_top_df, ignore_index=True)\n",
        "\n",
        "for amodel in MODELS_LS:\n",
        "  crux_ct = len(cruxes_n_top_all_df[cruxes_n_top_all_df['model_name'] == amodel])\n",
        "  print(f'{amodel.capitalize()} has {crux_ct} Cruxes')\n",
        "\n",
        "print(f'There are a total of {cruxes_n_top_all_df.shape[0]} in all Models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQDWi6YLYVn0"
      },
      "source": [
        "# Plot Crux Points in 2D Space: Scatterplot\n",
        "\n",
        "sns.lmplot('sent_no', 'sentiment', data=cruxes_n_top_all_df, fit_reg=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab2oHmU1Yqcl"
      },
      "source": [
        "# Plot Crux Points in 1D Space: Histogram\n",
        "\n",
        "sns.distplot(cruxes_n_top_all_df.sent_no, bins=2000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtdL2OjAJm55"
      },
      "source": [
        "sns.clustermap(cruxes_n_top_all_df.sent_no)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiTZC_X4G6EC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE8EJC1wHCEn"
      },
      "source": [
        "## **Sentiment Arc Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFKeAFHlG-vm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TfBnxjYG5_u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wjBEXGyvU4x"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRo_Tt35VW8"
      },
      "source": [
        "**Bi/Tri-Polarity Lexicons**\n",
        "\n",
        "NRC: \n",
        "* https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
        "\n",
        "MPQA (Upitt):\n",
        "* https://mpqa.cs.pitt.edu/lexicons/effect_lexicon/\n",
        "* https://github.com/nlpcl-lab/mpqa2.0-preprocessing\n",
        "* https://github.com/kvangundy/basic-sentiment-analyzer/blob/master/sentimentDict.csv\n",
        "\n",
        "SentimentAnalysis.R (20210217 124s):\n",
        "* QDAP \n",
        "* DictionaryGI: Harvard-IV dictionary asused in the General Inquirer software (2005-/1637+)\n",
        "* DictionaryLM: Loughran-McDonald Financial dictionary (2355-/354+/297?)\n",
        "* DictionaryHE: Henry's Financial Dictionary (85-/105+)\n",
        "\n",
        "Custom Lexicons:\n",
        "* https://nealcaren.org/lessons/wordlists/ \n",
        "\n",
        "Lexicons\n",
        "* https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsOKzM-RYHf5"
      },
      "source": [
        "# Smooth Raw Sentiment Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWKU0OzT_9pe"
      },
      "source": [
        "**Simple Moving Average (SMA) by Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGluyBsFkQ7C"
      },
      "source": [
        "# Line Plots of Sentiment Values\n",
        "\n",
        "# set a grey background (use sns.set_theme() if seaborn version 0.11.0 or above) \n",
        "sns.set(style=\"darkgrid\")\n",
        "# df = sns.load_dataset(\"iris\")\n",
        "\n",
        "fig, axs = plt.subplots(4, 2, figsize=(12, 18))\n",
        "\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"median\", color=\"skyblue\", ax=axs[0, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"vader_mean_roll050\", color=\"skyblue\", ax=axs[0, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"sentimentr_mean_roll050\", color=\"olive\", ax=axs[1, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"syuzhet_mean_roll050\", color=\"olive\", ax=axs[1, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"bing_mean_roll050\", color=\"gold\", ax=axs[2, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"textblob_mean_roll050\", color=\"gold\", ax=axs[2, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"sentiword_mean_roll050\", color=\"teal\", ax=axs[3, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"senticnet_mean_roll050\", color=\"teal\", ax=axs[3, 1])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySuYAv2YxCO0"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iX1nCK6ljY8"
      },
      "source": [
        "%whos dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC3r9eWywyXB"
      },
      "source": [
        "\n",
        "# g=sns.pointplot(x=0, y=1, data=df, dodge=True,plot_kws=dict(alpha=0.3))\n",
        "# plt.setp(g.collections, alpha=.3) #for the markers\n",
        "# plt.setp(g.lines, alpha=.3)       #for the lines\n",
        "\n",
        "for i, sa_model in enumerate(corpus_sents_df.columns):\n",
        "  if (sa_model.endswith('_roll050')):\n",
        "    # if (sa_model != 'sent_no'):\n",
        "    if (sa_model == 'median'):\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, color='black')\n",
        "    else:\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, alpha=0.3)\n",
        "\n",
        "# print(f'{i}: {sa_model}')\n",
        "\n",
        "'''\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='median')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='textb')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTKI4Fu2ACKP"
      },
      "source": [
        "**Exponential Moving Average**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvVMrD58AL97"
      },
      "source": [
        "# Not Necessary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wEab5F8_3Wl"
      },
      "source": [
        "**LOWESS and LOESS Smoothing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPOo6VAMmWh_"
      },
      "source": [
        "sa_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlPwfjS8mKpY"
      },
      "source": [
        "def plot_lowess(df, df_cols_ls, aplot=True, afrac=1./10, ait=5):\n",
        "  '''\n",
        "  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n",
        "  Return a DataFrame with LOWESS values\n",
        "  If 'plot=True', also output plot\n",
        "  '''\n",
        "\n",
        "  # global corpus_sents_norm_df\n",
        "\n",
        "  lowess_df = pd.DataFrame()\n",
        "\n",
        "  for i,acol in enumerate(df_cols_ls):\n",
        "    sm_x, sm_y = sm_lowess(endog=df[acol].values, exog=df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n",
        "    col_new = f'{acol}_lowess'\n",
        "    lowess_df[col_new] = pd.Series(sm_x)\n",
        "    if aplot:\n",
        "      plt.plot(sm_x, sm_y, label=acol, alpha=0.5, linewidth=2)\n",
        "\n",
        "      frac_str = str(round(100*afrac))\n",
        "      plt.title(f'{CORPUS_FULL} \\n LOWESS (frac={frac_str} Sentence Sentiment (Model: {sa_model})')\n",
        "      plt.legend(title='Sentiment Series')\n",
        "\n",
        "  return lowess_df\n",
        "\n",
        "# Test\n",
        "new_lowess_col = f'{sa_model}_lowess'\n",
        "my_frac = 1./10\n",
        "my_frac_per = round(100*my_frac)\n",
        "new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n",
        "corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], afrac=my_frac)\n",
        "corpus_sents_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qRUhRfmmyUA"
      },
      "source": [
        "corpus_sents_norm_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxbZdPVbnL6w"
      },
      "source": [
        "corpus_sents_norm_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25CDCoDJmTcI"
      },
      "source": [
        "norm_cols_ls = []\n",
        "for acol in corpus_sents_norm_df.columns:\n",
        "  if acol.endswith('_2norm'):\n",
        "    norm_cols_ls.append(acol)\n",
        "\n",
        "print(f'All norm_cols_ls')\n",
        "\n",
        "temp_cols_ls = list(set(norm_cols_ls) - set(['stanza_2norm','afinn_2norm']))\n",
        "\n",
        "print(f'Trimmed temp_cols_ls:')\n",
        "print(temp_cols_ls)\n",
        "\n",
        "\n",
        "plot_lowess(corpus_sents_norm_df, temp_cols_ls, 'Normed')\n",
        "\n",
        "'''\n",
        "for i, sa_model in enumerate(corpus_sents_df.columns):\n",
        "  if (sa_model.endswith('_roll050')):\n",
        "    # if (sa_model != 'sent_no'):\n",
        "    if (sa_model == 'median'):\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, color='black')\n",
        "    else:\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, alpha=0.3)\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7hs9FIxpugn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq8fWCKCpuaW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvKW23iwAGnk"
      },
      "source": [
        "%time\n",
        "\n",
        "plt.title(f'{BOOK_TITLE_FULL} \\n {sa_model} with statsmodels LOWESS (frac=0.1/iter=5)')\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['median'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "plt.plot(sm_x, sm_y, label='Median', color='black', linewidth=3)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['vader_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='VADER', color='royalblue', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['jockers_rinker_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='Jockers-Rinker', color='red', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['syuzhet_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='Syuzhet', color='tomato', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['huliu_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='HuLiu', color='teal', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['textblob_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='TextBlob', color='lime', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['sentiword_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='SentiWord', color='goldenrod', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['senticnet_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='SenticNet', color='forestgreen', alpha=0.5)\n",
        "\n",
        "# y_upper = corpus_sentiments_df['norm_score'].max() + 0.01\n",
        "# y_lower = corpus_sentiments_df['norm_score'].min() - 0.01\n",
        "# y_range = y_upper - y_lower + 0.02\n",
        "# plt.ylim([0.71, 0.74])\n",
        "# plt.ylim([y_lower, y_upper])\n",
        "# plt.plot(x, y, 'k.');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iORvQhmNFrw"
      },
      "source": [
        "**Discrete Cosine Transform (DCT)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917t5E9t_Pez"
      },
      "source": [
        "Libraries:\n",
        "\n",
        "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html#scipy.fft.dct\n",
        "* https://github.com/search?q=discrete+cosine \n",
        "\n",
        "Tutorial\n",
        "\n",
        "* https://realpython.com/python-scipy-fft/#the-discrete-cosine-and-sine-transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI-UtGpXwsJ6"
      },
      "source": [
        "from sktime.transformations.series.cos import CosineTransformer\n",
        "from sktime.datasets import load_airline\n",
        "y = load_airline()\n",
        "transformer = CosineTransformer()\n",
        "y_hat = transformer.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HUsqY_c3wsGA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YfHqTtxwsC4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKYGvaNxwr_m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noayFmY5v6Wc"
      },
      "source": [
        "!pip install pyts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-DLnLsjVWB-"
      },
      "source": [
        "**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n",
        "\n",
        "* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n",
        "* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n",
        "* http://futuredata.stanford.edu/asap/ \n",
        "* https://www.datadoghq.com/blog/auto-smoother-asap/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsz35wSyWlvA"
      },
      "source": [
        "import scipy.stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLFb9ZitVV3E"
      },
      "source": [
        "# ASAP Simple (Brute Force)\n",
        "def moving_average(data, _range):\n",
        "    ret = np.cumsum(data, dtype=float)\n",
        "    ret[_range:] = ret[_range:] - ret[:-_range]\n",
        "    return ret[_range - 1:] / _range\n",
        "\n",
        "def SMA(data, _range, slide):\n",
        "    ret = moving_average(data, _range)[::slide]\n",
        "    return list(ret)\n",
        "\n",
        "def kurtosis(values):\n",
        "    return scipy.stats.kurtosis(values)\n",
        "\n",
        "def roughness(vals):\n",
        "    return np.std(np.diff(vals))\n",
        "\n",
        "def smooth_simple(data, max_window=5, resolution=None):\n",
        "    data = np.array(data)\n",
        "    # Preaggregate according to resolution\n",
        "    window_size = 1\n",
        "    slide_size = 1\n",
        "    if resolution:\n",
        "        slide_size = int(len(data) / resolution)\n",
        "        if slide_size > 1:\n",
        "            data = SMA(data, slide_size, slide_size)\n",
        "    orig_kurt   = kurtosis(data)\n",
        "    min_obj     = roughness(data)\n",
        "    for w in range(2, int(len(data) / max_window + 1)):\n",
        "        smoothed = SMA(data, w, 1)\n",
        "        if kurtosis(smoothed) >= orig_kurt:\n",
        "            r = roughness(smoothed)\n",
        "            if r < min_obj:\n",
        "                min_obj = r\n",
        "                window_size = w\n",
        "    return window_size, slide_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHdCbShgVVrR"
      },
      "source": [
        "# Plot time series before and after smoothing\n",
        "def plot(data, window_size, slide_size, plot_title):\n",
        "    plt.clf()\n",
        "    plt.figure()\n",
        "    data = SMA(data, slide_size, slide_size)\n",
        "    method_names = [\"Original\", \"ASAP Smoothed\"]\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
        "    smoothed = SMA(data, window_size, 1)\n",
        "    smoothed_range = range(int(window_size/2), int(window_size/2) + len(smoothed))\n",
        "    ax1.set_xlim(0, len(data))\n",
        "    ax1.plot(data, linestyle='-', linewidth=1.5)\n",
        "    ax2.plot(smoothed_range, smoothed, linestyle='-', linewidth=1.5)\n",
        "    axes = [ax1, ax2]\n",
        "    for i in range(2):\n",
        "        axes[i].get_xaxis().set_visible(False)\n",
        "        axes[i].text(0.02, 0.8, \"%s\" %(method_names[i]),\n",
        "            verticalalignment='center', horizontalalignment='left',\n",
        "            transform=axes[i].transAxes, fontsize=25)\n",
        "\n",
        "    fig.set_size_inches(16, 12)\n",
        "    plt.tight_layout(w_pad=1)\n",
        "    plt.title(plot_title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpSktA4gWV3J"
      },
      "source": [
        "corpus_sentiments_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "710ghW5qVVoF"
      },
      "source": [
        "# Taxi\n",
        "# raw_data = load_csv('Taxi.csv')\n",
        "# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n",
        "# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n",
        "window_size, slide_size = smooth_simple(list(corpus_sentiments_df['median']), resolution=1000)\n",
        "print(\"Window Size: \", window_size)\n",
        "plot_title = f'{BOOK_TITLE_FULL} \\n Median Sentiment Smoothed with ASAP from Stanford (res=1000)'\n",
        "plot(list(corpus_sentiments_df['median']), window_size, slide_size, plot_title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wkkx80YYLCg"
      },
      "source": [
        "# Calculate Error Metrics based on Distance from Median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0K9P4jA_bDj"
      },
      "source": [
        "Libraries:\n",
        "\n",
        "* https://github.com/wannesm/dtaidistance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmSSemdcLQ9x"
      },
      "source": [
        "# sentiment_lowess_df['median']\n",
        "sentiment_lowess_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tvyv-LtJiqJ"
      },
      "source": [
        "# Rank each Sentiment Model by Error/Distance Metrics from Median\n",
        "\n",
        "sentiment_lowess_df['min'] = sentiment_lowess_df[['vader','jockers-rinker','syuzhet','huliu','textblob','sentiword','senticnet']].min(axis=1)\n",
        "sentiment_lowess_df['max'] = sentiment_lowess_df[['vader','jockers-rinker','syuzhet','huliu','textblob','sentiword','senticnet']].max(axis=1)\n",
        "sentiment_lowess_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18y52tS9JTeO"
      },
      "source": [
        "# LOWESS Smoothed Median Curve with Min/Max Confidence Intervals\n",
        "\n",
        "plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Min/Max Confidence Intervals')\n",
        "\n",
        "sns.lineplot(data=sentiment_lowess_df, x='x_value', y='median', linewidth=3, color='black')\n",
        "plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cQqCyABTkzT"
      },
      "source": [
        "# Error Metrics of each Model relative to the Median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BBTjp7nTkhK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHVeb7Jy9zaa"
      },
      "source": [
        "# Group and Classify Sentiment Arcs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xeV-709-_M2"
      },
      "source": [
        "Libraries\n",
        "\n",
        "* https://github.com/alan-turing-institute/sktime\n",
        "\n",
        "* https://github.com/johannfaouzi/pyts \n",
        "\n",
        "Code\n",
        "\n",
        "* https://colab.research.google.com/drive/1oEFfK5KTJyFQGs2xunc1cDW2OcbkZKCY\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFEtVAt29zMP"
      },
      "source": [
        "# https://colab.research.google.com/drive/1oEFfK5KTJyFQGs2xunc1cDW2OcbkZKCY\n",
        "\n",
        "# https://github.com/johannfaouzi/pyts \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "804w9PYAYP_p"
      },
      "source": [
        "# Export Manual and Automatic Sentiment Polarities and Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wnHGCQnO9aA"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwmMgQILMYba"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}