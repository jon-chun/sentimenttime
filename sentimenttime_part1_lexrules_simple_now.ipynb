{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentimenttime_part1_lexrules_simple_now.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8owpM75RILKn",
        "XUvKJEybUIeP",
        "yXwKR4gA8Ouk",
        "jHscLkclSYqN",
        "yL8R_ANtfYG6",
        "j2tIua7tTSRz",
        "B0BukxX9ZKYA",
        "TRjHsOIpXovL",
        "89X8Sz2QdMn-",
        "WmbxdtyvQo3K",
        "C5CazR5rY_u2",
        "7tAxuAxU7ueg",
        "QZjqwTvU76AR",
        "NA3dWsnF78mi",
        "SkNZVk128jV9",
        "dUcANLM_8mtT",
        "Cn4KQYpH3glK",
        "jRTjCPLb8cbB",
        "G2blGfVlKb_s",
        "E9cZmIkx7Too",
        "ZJRV2n0M_xN6",
        "43YL_Iuf0JHZ",
        "r1tIJZepmvLu",
        "BUyZF6tE1rL0",
        "cOBqRxQz3gbh",
        "mB4fEZzJQPkg",
        "KhIM7zwuQUBq",
        "RKJmDFQeH6gI",
        "BuQgdwsJGZN_",
        "5FrNFec7tNij",
        "pPKABrvstSIo",
        "zNp9zwBfmgpI",
        "oQ_wcgiyx-NJ",
        "6IrnbsMByTmX",
        "nn6EWXYaybhj",
        "34Be4bSszK-A",
        "gfiyOjMBBraW",
        "4O3l61be6_GQ",
        "JizqEQE6X3oQ",
        "EIwEIY4cd2PB",
        "XKz34IzZdxIg",
        "H5Qw6frQdz1C",
        "B6Ipaa-lWVwP",
        "XDYYpJYRfrZ-",
        "Jii7hmhleyHo",
        "f2CIR3U9e4WC",
        "XzcDX02pZGbr",
        "OV6spOW1fSKf",
        "s0fL5RXKfZG6",
        "7vUzh39HHJXz",
        "VJI_13B0HMnU",
        "B9sEH8_IG6Qq",
        "yRjKHnQaOTu2",
        "EE8EJC1wHCEn",
        "3wjBEXGyvU4x",
        "vsOKzM-RYHf5",
        "-wkkx80YYLCg",
        "wHVeb7Jy9zaa",
        "804w9PYAYP_p"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jon-chun/sentimenttime/blob/main/sentimenttime_part1_lexrules_simple_now.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibHFmIWoU3Vx"
      },
      "source": [
        "# **An Analytic Methodology to Extract Narratives from Text: Using Sentiment Analysis to find the Arcs and Crux Points in Novels, Social Media and Chat Transcripts**\n",
        "\n",
        "By: Jon Chun\n",
        "12 Jun 2021\n",
        "\n",
        "References:\n",
        "\n",
        "* Coming...\n",
        "\n",
        "TODO:\n",
        "* Clearly document workflow and partition across notebooks/libraries\n",
        "* Code review and extraction to libraries\n",
        "* Corpus ingestion for any format\n",
        "* XAI (mlm false peak 1717SyuzhetR/1732SentimentR/1797robertalg15 adam watches war argument at dinner) \n",
        "* Centralize and Standardize Model name lists\n",
        "* Normalize model SA Series lengths\n",
        "* Standardize all SA Series with the same method\n",
        "* Seamless report generation/file saving\n",
        "* Get raw text from SentimentR\n",
        "* Filter out non-printable characters\n",
        "* Roll-over Crux-Points (SentNo+Sent/Parag) (plotly)\n",
        "* Label/Roll-over Chapter/Sect No at Boundries\n",
        "* Generate Report PDF/csv\n",
        "* Option to select raw or discrete2continous transformation (Bing)\n",
        "* Annotation functionality + Share/Collaboration of findings/reseearch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u932nJxdh0Ac"
      },
      "source": [
        "# Configuration (Auto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_a9eQyBiiTG"
      },
      "source": [
        "**Global Configuration Constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT2MyyjpihFj"
      },
      "source": [
        "# Hardcoded Sentiment Analysis Models\n",
        "\n",
        "CORPUS_ENCODING = 'utf-8' # Default character/text encoding scheme (others: 'utf-8', but often 'iso-8859-1', 'windows-1252', 'cp1252', or 'ascii')\n",
        "\n",
        "MODELS_LS = ['vader','textblob','stanza','afinn','bing','sentimentr','syuzhet','pattern','sentiword','senticnet','nrc']\n",
        "            \n",
        "# Minimum lengths for Sentences and Paragraphs\n",
        "#   (Shorter Sents/Parags will be deleted)\n",
        "\n",
        "MIN_CHAP_LEN = 5000\n",
        "MIN_SECT_LEN = 25  # Minimum char length to be included in section DataFrame\n",
        "MIN_PARAG_LEN = 2\n",
        "MIN_SENT_LEN = 2\n",
        "\n",
        "# Min/Max statistics on each lexicon's sentiment values applied to corpus\n",
        "corpus_lexicons_stats_dt = {}\n",
        "corpus_cruxes_dt = {}\n",
        "\n",
        "# Crux Points Dict key:model, value:list of crux point tuples (x,y)\n",
        "corpus_cruxes_all_dt = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOPa6HH-OjZp"
      },
      "source": [
        "**Install Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drpZJilASHUN"
      },
      "source": [
        "# fast detection of character set encoding for text/files\n",
        "\n",
        "!pip install cchardet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEjSzsusOOJ-"
      },
      "source": [
        "# common ML code\n",
        "\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ0UVdasuTTS"
      },
      "source": [
        "%pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmtqmvu6OlR9"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7bf4lfgwMEz"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import io\n",
        "import glob\n",
        "import json\n",
        "import contextlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOmyq4h7OOFi"
      },
      "source": [
        "# IMPORT LIBRARIES\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OslLdEsvOuFU"
      },
      "source": [
        "import re\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YelenXz5BcmE"
      },
      "source": [
        "from itertools import cycle  # For plotly\n",
        "\n",
        "import collections\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Suximbjnw8D"
      },
      "source": [
        "# Import libraries for logging\n",
        "\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import time                     # (TODO: check no dependencies and delete)\n",
        "from time import gmtime, strftime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPZmScjVDYyw"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Download for sentence tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download for nltk/VADER sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMl2mfF8Haw8"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler   # To normalize time series\n",
        "from sklearn.preprocessing import StandardScaler # To Standardize time series: center(sub mean) and rescale within 1 SD (only for well-behaved guassian distributions)\n",
        "from sklearn.preprocessing import RobustScaler   # To Standardize time series: center(sub median) and rescale within 25%-75% (1st-3rd) IQR (better for noisy, outliers distributions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nckwluDXwa1c"
      },
      "source": [
        "mean_std_scaler = StandardScaler()\n",
        "median_iqr_scaler = RobustScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U589lvKXmFV-"
      },
      "source": [
        "# Zoom interpolates new datapoints between existing datapoints to expand a time series \n",
        "\n",
        "from scipy.ndimage.interpolation import zoom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wcZfSOuBlW7"
      },
      "source": [
        "from scipy import interpolate\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy import signal\n",
        "from scipy.signal import argrelextrema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY3UyvYjAvDN"
      },
      "source": [
        "from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess\n",
        "from statsmodels import robust"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcSc4jsggSy2"
      },
      "source": [
        "**Define Library-Dependent Objects**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjGN2sN3uRpN"
      },
      "source": [
        "import contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02LJQlYpgQGs"
      },
      "source": [
        "corpus_sects_df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwl0MBDyOwtX"
      },
      "source": [
        "**Configure Jupyter Notebook**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD1cyqWsfjxA"
      },
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzfybE5kfmE-"
      },
      "source": [
        "# Configure matplotlib and seaborn\n",
        "\n",
        "# Plotting pretty figures and avoid blurry images\n",
        "# %config InlineBackend.figure_format = 'retina'\n",
        "# Larger scale for plots in notebooks\n",
        "# sns.set_context('talk')\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [16, 8]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rc('figure', facecolor='white')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIIjSbyeP2fg"
      },
      "source": [
        "# Configure Jupyter\n",
        "\n",
        "# Enable multiple outputs from one code cell\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets, interactive\n",
        "\n",
        "# Configure Google Colab\n",
        "\n",
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS_El2PiQlyP"
      },
      "source": [
        "# Text wrap\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuM_qnOHUil5"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dLkfn4KFmDf"
      },
      "source": [
        "**Configuration Details Snapshot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FNPovQBFZky"
      },
      "source": [
        "# Snap Shot of Time, Machine, Data and Library/Version Blueprint\n",
        "# TODO:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wiSBHxoOGZz"
      },
      "source": [
        "# Pick ONE Method (a) or (b) to Get Corpus Textfile\n",
        "\n",
        "**Choose either (a) OR (b), not both**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KRfiXXQOZcq"
      },
      "source": [
        "## **Option (a): Connect to Google gDrive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G64etjAUOOSm"
      },
      "source": [
        "# Connect to Google gDrive\n",
        "\n",
        "# Flag to indicate first run through code \n",
        "flag_first_run = True\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0fvFZq-eFaw"
      },
      "source": [
        "# Select the Corpus subdirectory on your Google gDrive\n",
        "\n",
        "\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\" #@param {type:\"string\"}\n",
        "gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n",
        "# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/ddefoe_robinsoncrusoe\" #@param {type:\"string\"}\n",
        "\n",
        "CORPUS_SUBDIR = gdrive_subdir\n",
        "corpus_filename = CORPUS_SUBDIR\n",
        "\n",
        "# Change to working subdirectory\n",
        "if flag_first_run == True:\n",
        "  full_path_str = gdrive_subdir\n",
        "  flag_first_run = False\n",
        "else:\n",
        "  full_path_str = f'/gdrive/MyDrive{gdrive_subdir[1:]}'\n",
        "\n",
        "%cd $full_path_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKV1uMBEO8TR"
      },
      "source": [
        "## **Option (b): Upload Corpus Textfile**\n",
        "\n",
        "***Only do this if your Google subdirectory doesn't already contain a plain text file of your Corpus or you wish to overwrite it and use a newer version***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH6dlB2fO6Ln"
      },
      "source": [
        "# Execute this code cell to upload plain text file of corpus\n",
        "#   Should be *.txt format with paragraphs separated by at least 2 newlines\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3LORQ4fRGBW"
      },
      "source": [
        "# Verify file was uploaded\n",
        "\n",
        "# Get uploaded filename\n",
        "corpus_filename = list(uploaded.keys())[0]\n",
        "print(f'Uploaded Corpus filename is: {corpus_filename}')\n",
        "CORPUS_FILENAME = corpus_filename\n",
        "\n",
        "!ls -al $corpus_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsm8awD4AZ8O"
      },
      "source": [
        "# **Configuration (Manual)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfilGg6Mkxnd"
      },
      "source": [
        "# Verify subdirectory change\n",
        "\n",
        "!pwd\n",
        "!ls -altr *.txt\n",
        "\n",
        "# TODO: Intelligently automate the filling of form based upon directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP3WLEv_g5aq"
      },
      "source": [
        "# CORPUS_TITLE = 'Robinson Crusoe' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Daniel Defoe\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"ddefoe_robinsoncrusoe_final_hand.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/ddefoe_robinsoncrusoe\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "CORPUS_TITLE = 'To The Lighthouse' #@param {type:\"string\"}\n",
        "CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n",
        "CORPUS_FILENAME = \"ttl_final_hand.txt\" #@param {type:\"string\"}\n",
        "CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n",
        "\n",
        "# CORPUS_TITLE = 'To The Lighthouse' #@param {type:\"string\"}\n",
        "# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n",
        "# CORPUS_FILENAME = \"ttl_final_hand.txt\" #@param {type:\"string\"}\n",
        "# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n",
        "\n",
        "CHAPTER_HEADINGS = \"CHAPTER (ArabicNo)\" #@param [\"CHAPTER (ArabicNo)\", \"CHAPTER (RomanNo)\"]\n",
        "SECTION_HEADINGS = \"SECTION (ArabicNo)\" #@param [\"SECTION (ArabicNo)\", \"SECTION (RomanNo)\", \"----- (Hyphens)\", \"None\"]\n",
        "\n",
        "LEXICONS_SUBDIR = \"./research/2021/sa_book_code/books_sa/lexicons\" #@param {type:\"string\"}\n",
        "\n",
        "CORPUS_FULL = f'{CORPUS_TITLE} by: {CORPUS_AUTHOR}'\n",
        "\n",
        "PLOT_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n",
        "\n",
        "FILE_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n",
        "\n",
        "\n",
        "gdrive_subdir = CORPUS_SUBDIR\n",
        "corpus_filename = CORPUS_FILENAME\n",
        "author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "author_abbr_str = (CORPUS_AUTHOR.split(' ')[0][0]+CORPUS_AUTHOR.split(' ')[1]).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "\n",
        "print(f'\\nWorking Corpus Datafile: ------------------------------ \\n\\n    {CORPUS_SUBDIR}')\n",
        "print(f'\\nFull Corpus Title/Author: ------------------------------ \\n\\n    {CORPUS_FULL}')\n",
        "\n",
        "\n",
        "if CHAPTER_HEADINGS == 'CHAPTER (ArabicNo)':\n",
        "  # pattern_chap = r'CHAPTER [0-9]{1,2}[.]? [A-Z \\.-:—;-’\\'\"]*[^\\n]*'\n",
        "  pattern_chap = r'CHAPTER [0123456789]{1,2}[^\\n]*'\n",
        "elif CHAPTER_HEADINGS == 'CHAPTER (RomanNo)':\n",
        "  # pattern_chap = r'CHAPTER [IVX]{,6}[.]?[\\n]*'\n",
        "  pattern_chap = r'CHAPTER [IVX]{,6}[.]?[\\n]*'\n",
        "else:\n",
        "  print(f'ERROR: Illegal CHAPTER_HEADINGS value = {CHAPTER_HEADINGS}')\n",
        "\n",
        "\n",
        "if SECTION_HEADINGS == 'SECTION (ArabicNo)':\n",
        "  # pattern_sect = r'SECTION [0-9]{1,2} [^\\n]*'\n",
        "  # TODO: [^\\n] gets parsed into [^\\\\n] causing problems, so simplify\n",
        "  pattern_sect = r'SECTION [0123456789]{1,2}[^\\n]*'\n",
        "elif SECTION_HEADINGS == 'SECTION (RomanNo)':\n",
        "  pattern_sect = r'SECTION [IVX]{,6}[.]* [A-Z \\.-:—;-’\\'\"]*[\\n]*'\n",
        "elif SECTION_HEADINGS == '----- (Hyphens)':\n",
        "  pattern_sect = r'^[- ]*\\n'\n",
        "elif SECTION_HEADINGS == 'None':\n",
        "  pass\n",
        "else:\n",
        "  print(f'ERROR: Illegal SECTION_HEADING value = {SECTION_HEADINGS}')\n",
        "\n",
        "print(f'\\nCHAPTER Headings: ------------------------------ \\n\\n    {CHAPTER_HEADINGS}')\n",
        "\n",
        "print(f'\\nSECTION Headings: ------------------------------ \\n\\n    {SECTION_HEADINGS}')\n",
        "\n",
        "\n",
        "print(f'\\nCorpus file information: ------------------------------ \\n')\n",
        "!ls -al $CORPUS_FILENAME\n",
        "\n",
        "# Verify contents of Corpus File is Correctly Formatted\n",
        "#   \n",
        "# TODO: ./utils/verify_format.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8owpM75RILKn"
      },
      "source": [
        "# **Utility Functions (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMLbyx6gIPqj"
      },
      "source": [
        "## **Files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBOvvP-BSIiC"
      },
      "source": [
        "# https://dev.to/bowmanjd/character-encodings-and-detection-with-python-chardet-and-cchardet-4hj7\n",
        "\n",
        "import cchardet as chardet\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "def get_file_encoding(filename):\n",
        "    \"\"\"Detect encoding and return decoded text, encoding, and confidence level.\"\"\"\n",
        "    filepath = Path(filename)\n",
        "\n",
        "    # We must read as binary (bytes) because we don't yet know encoding\n",
        "    blob = filepath.read_bytes()\n",
        "\n",
        "    detection = chardet.detect(blob)\n",
        "    encoding = detection[\"encoding\"]\n",
        "    confidence = detection[\"confidence\"]\n",
        "    text = blob.decode(encoding)\n",
        "\n",
        "    return text, encoding, confidence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMA-040nIypX"
      },
      "source": [
        "\n",
        "def corpus2lines(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of every line defined by puncutation/NLTK.sent_tokenize or newlines [\\n]{2,}\n",
        "  '''\n",
        "\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  print(f'BEFORE stripping out headings len: {len(corpus_raw_str)}')\n",
        "\n",
        "  corpus_parags_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n",
        "  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n",
        "\n",
        "  # Strip off whitespace from Paragraphs\n",
        "  corpus_parags_ls = [x.strip() for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out empty lines Paragraphs\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n",
        "\n",
        "  corpus_sents_raw_ls = []\n",
        "  print(f'   Parag count before processing sents: {len(corpus_parags_ls)}')\n",
        "  corpus_sents_raw_ls = []\n",
        "  for i, aparag in enumerate(corpus_parags_ls):\n",
        "    aparag_sents_ls = (sent_tokenize(aparag))\n",
        "\n",
        "    # Strip off whitespace from Sentences\n",
        "    aparag_sents_ls = [x.strip() for x in aparag_sents_ls]\n",
        "\n",
        "    # Filter out empty line Sentences\n",
        "    aparag_sents_ls = [x for x in aparag_sents_ls if (len(x.strip()) > MIN_SENT_LEN)]\n",
        "\n",
        "    corpus_sents_raw_ls.extend(aparag_sents_ls)\n",
        "\n",
        "  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n",
        "  # parag_before_punctstrip_ct = len(corpus_parags_ls)\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  # print(f'Punctuation only Paragraph Count: {len(corpus_parags_ls) - parag_before_punctstrip_ct}')\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Section separator 'SECTION ' lines\n",
        "  # for i,temp_str in enumerate(corpus_parags_ls):\n",
        "  #   if temp_str.startswith('SECTION '):\n",
        "  #     print(f'Parag #{i}: {temp_str}')\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('SECTION '))]\n",
        "\n",
        "  # Filter out any possible embedded 'SECTION ' lines\n",
        "  # for i,temp_str in enumerate(corpus_parags_ls):\n",
        "  #   if 'SECTION' in temp_str:   # .contains('SECTION '):\n",
        "  #     print(f'Parag #{i}: {temp_str}')\n",
        "  # corpus_parags_ls = del_substrs_list(corpus_parags_ls, pattern_sect) # [re.sub(rf'{pattern_sect}', '', x) for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out the Chapter separator 'CHAPTER ' lines\n",
        "  # for i,temp_str in enumerate(corpus_parags_ls):\n",
        "  #   if temp_str.startswith('CHAPTER '):\n",
        "  #     print(f'Parag #{i}: {temp_str}')\n",
        "  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('CHAPTER '))]\n",
        "\n",
        "  print(f'About to return corpus_sents_raw_ls with len = {len(corpus_sents_raw_ls)}')\n",
        "  return corpus_sents_raw_ls, corpus_raw_str\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ7WX8L5IyiX"
      },
      "source": [
        "temp_ls, temp_str = corpus2lines(corpus_filename)\n",
        "len(temp_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW9GotRqIycE"
      },
      "source": [
        "temp_ls[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl08rkO3TrqK"
      },
      "source": [
        "def del_subword_list(list_ls, pattern_regex):\n",
        "  '''\n",
        "  Given a list of strings and a regex pattern\n",
        "  Return a filtered list with the regex pattern removed from every string element\n",
        "  '''\n",
        "\n",
        "  # Verfiy no 'SECTION' headers were not filtered out \n",
        "\n",
        "  list_clean_ls = []\n",
        "  pattern_seek = re.compile(rf'{pattern_regex}')\n",
        "\n",
        "  parags_matching_ls = [line for line in list_ls if pattern_seek.search(str(line))]\n",
        "  match_ct = len(parags_matching_ls)\n",
        "  print(f'Found {match_ct} Element matching regex pattern')\n",
        "  if len(parags_matching_ls) > 0:\n",
        "    for aparag in parags_matching_ls:\n",
        "      parag_orig_len = len(aparag)\n",
        "      # aparag_clean = re.sub(rf'{pattern_sect}', '', aparag)\n",
        "      pattern_str = f'{pattern_regex}'\n",
        "      aparag_clean = re.sub(pattern_str, \"-\", aparag)\n",
        "      print(f'Converted old: {aparag}\\n          new: {aparag_clean}\\n\\n          using pattern:{pattern_sect}')\n",
        "      parag_clean_len = len(aparag_clean)\n",
        "      # aparag_clean = re.sub(r'SECTION', '', aparag)\n",
        "      if parag_orig_len != parag_clean_len:\n",
        "        print(f'\\nOriginal:\\n\\n    {aparag}\\nClean:\\n\\n    {aparag_clean}')\n",
        "      list_clean_ls.append(aparag_clean)\n",
        "  else:\n",
        "    print(f'pattern_regex {pattern_regex} not found in any paragraph')\n",
        "\n",
        "  print(f'BEFORE: list_ls length={len(list_ls)}')\n",
        "  print(f'AFTER: list_clean_ls length={len(list_clean_ls)}')\n",
        "\n",
        "  return list_clean_ls \n",
        "\n",
        "# Test\n",
        "\n",
        "# test_ls = del_substrs_list(corpus_parags_ls, pattern_sect)\n",
        "# print(f'test_ls: {test_ls}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9b9n8IkX4s6"
      },
      "source": [
        "\"\"\"\n",
        "tempb_ls = ['apples are good', 'oranges are bad', 'bananas are ok']\n",
        "\n",
        "# pattern_a = re.compile(r'are')\n",
        "pattern_a = ' bad'\n",
        "tempc_ls = del_substrs_list(tempb_ls, f'{pattern_a}')\n",
        "\" \".join(tempc_ls)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIaG5dFeTpiP"
      },
      "source": [
        "def strip_section_sentences(text_str, pattern_regex):\n",
        "  '''\n",
        "  Given a regex pattern and a long text string of possibly multiple sentences and paragraphs\n",
        "  Return a cleaned version of text string with any matching pattern regex removed\n",
        "  '''\n",
        "\n",
        "  text_clean_str = re.sub(rf'{pattern_regex}', '', text_str)\n",
        "\n",
        "  return text_clean_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2ZKrOg_8Pcz"
      },
      "source": [
        "def corpus2chaps(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of min preprocessed raw CHAPTERs (corpus_parags_raw_temp_ls)\n",
        "  '''\n",
        "\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  print(f'BEFORE stripping out headings len: {len(corpus_raw_str)}')\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  # TODO: Finish, https://realpython.com/python-eval-function/\n",
        "\n",
        "  # Filter out SECTION [\\d]{1,2}[^\\n]* patterns from raw text corpus\n",
        "  # pattern = r'SECTION [\\d]{1,2}[^\\n]*'\n",
        "  # Replace all occurrences of character s with an empty string\n",
        "  print(f'pattern_chap: {pattern_chap}')\n",
        "  print(f'BEFORE stripping out headings len: {len(corpus_raw_str)}')\n",
        "  pattern_str = fr\"{pattern_chap}\" \n",
        "  print(f'pattern_str: {pattern_str}')\n",
        "  pattern = eval(\"re.compile(\"+pattern_str+\")\")\n",
        "  print(f'pattern={pattern}')\n",
        "  print(f'type(pattern): {type(pattern)}')\n",
        "  \n",
        "  #   pattern = exec('re.compile('+pattern_str+')')\n",
        "\n",
        "  # res = pattern.findall('fdasjkfdl fdjasklf fjdksla CHAPTER 12 dfjdks fdsjklf ds\\nfjdsk fjdsk fdsjk fds\\n fds jfdksl fds jk \\n fjdks fdjs')\n",
        "  # print(f'res matches: {res}')\n",
        "  corpus_raw_str = pattern.sub(r' ',corpus_raw_str)\n",
        "  # corpus_raw_str = re.sub(pattern, '', corpus_raw_str)\n",
        "  print(f'AFTER stripping out headings len: {len(corpus_raw_str)}')\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # print(f'len(corpus_raw_str) = {len(corpus_raw_str)}')\n",
        "  # corpus_chaps_ls = re.split(rf'{pattern_chap}', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  # corpus_chaps_ls = re.split(r'CHAPTER [IVX]{,6}[.]* [A-Z \\.-:—;-’\\'\"]*[\\n]*', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  # corpus_chaps_ls = re.split(r'CHAPTER [\\d]{1,2}[.]* [A-Z \\.-:—;-’\\'\"]*[\\n]*', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  corpus_chaps_ls = re.split(rf'{pattern_chap}', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  print(f'len(corpus_chaps_ls): {len(corpus_chaps_ls)}')\n",
        "\n",
        "  # Strip off whitespace\n",
        "  corpus_chaps_ls = [x.strip() for x in corpus_chaps_ls]\n",
        "\n",
        "  # Filter out empty lines\n",
        "  corpus_chaps_ls = [x for x in corpus_chaps_ls if not (len(x.strip()) <= MIN_PARAG_LEN)]\n",
        "\n",
        "  # Filter out CHAPTER lines\n",
        "  corpus_chaps_ls = [x for x in corpus_chaps_ls if not (x.strip().startswith('CHAPTER '))]\n",
        "\n",
        "  # Filter out SECTION lines (could be embedded or leading)\n",
        "  corpus_chaps_ls = [re.sub(rf'{pattern_sect}', '', x) for x in corpus_chaps_ls]\n",
        "\n",
        "  print(f'AFTER stripping out headings len: {len(\" \".join(corpus_chaps_ls))}')\n",
        "\n",
        "  return corpus_chaps_ls, corpus_raw_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-6PAO7y-MdJ"
      },
      "source": [
        "corpus_chaps_ls, corpus_raw_str = corpus2chaps(corpus_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE12va0o4B5K"
      },
      "source": [
        "ch_no=2\n",
        "search_term = 'CHAPTER'\n",
        "print(f'Chapter #{ch_no} Beginning -----')\n",
        "corpus_chaps_ls[ch_no][:500]\n",
        "print('\\n')\n",
        "print(f'Chapter #{ch_no} Ending -----')\n",
        "corpus_chaps_ls[ch_no][-500:]\n",
        "print('\\n')\n",
        "print(f'Any {search_term} found?')\n",
        "search_term in corpus_chaps_ls[ch_no]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA-RZpX9UWhB"
      },
      "source": [
        "pattern_sect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV3Lps-IO8oM"
      },
      "source": [
        "import re\n",
        "\n",
        "s = \"123(45)678\"\n",
        "\n",
        "print(re.split(r'([()])', s))\n",
        "# ['123', '(', '45', ')', '678']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Tz1-WO-RJ-"
      },
      "source": [
        "corpus_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRArC4bqP1Yw"
      },
      "source": [
        "temp_str = 'the rain in Spain. CHAPTER 2 It was a dark and stormy night.'\n",
        "temp_str.split(r'CHAPTER ') # [\\d]{1,2}')\n",
        "print('\\n')\n",
        "re.split(r'CHAPTER [\\d]{1,2}', temp_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SpeAqU1nzjz"
      },
      "source": [
        "pattern_sect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5nrHLut8AEl"
      },
      "source": [
        "# Detection \n",
        "\n",
        "def corpus2sects(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of min preprocessed raw sections/CHAPTERs (corpus_parags_raw_temp_ls)\n",
        "  '''\n",
        "\n",
        "  corpus_sects_ls = []\n",
        "\n",
        "\n",
        "  # encoding = CORPUS_ENCODING,  'windows-1252', 'utf-8', 'cp1252', 'iso-8859-1'\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  pattern_sect = 'SECTION [\\d]{1,2}[.]?[^\\n]*'\n",
        "  # corpus_sects_ls = re.split(r'SECTION [\\d]{1,2}[.]* [A-Z \\.-:—;-’\\'\"]*[\\n]*', corpus_raw_str flags=re.I) # , flags=re.I)\n",
        "  corpus_sects_ls = re.split(rf'{pattern_sect}', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  # corpus_sects_ls = re.split(rf'{pattern_sect}', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "  print(f'len(corpus_raw_str: {len(corpus_raw_str)}')\n",
        "  print(f'len(corpus_sects_ls): {len(corpus_sects_ls)}')\n",
        "  print(f'    First section: Length={len(corpus_sects_ls[0])}\\n    {corpus_sects_ls[0][:500]}')\n",
        "  print(f'    Second section: {corpus_sects_ls[1][:500]}')\n",
        "  print('\\n')\n",
        "  print(f'    Second-Last section: {corpus_sects_ls[-2][:500]}')\n",
        "  print(f'    Last section: {corpus_sects_ls[-1][:500]}')\n",
        "\n",
        "\n",
        "\n",
        "  # Strip off whitespace\n",
        "  corpus_sects_ls = [x.strip() for x in corpus_sects_ls]\n",
        "\n",
        "  # Filter out empty lines\n",
        "  corpus_sects_ls = [x for x in corpus_sects_ls if not (len(x.strip()) <= MIN_PARAG_LEN)]\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Section separator 'SECTION ' lines\n",
        "  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('SECTION '))]\n",
        "\n",
        "  # Filter out any possible embedded 'CHAPTER ' lines\n",
        "  # TODO: Zeroing out corpus_sects_ls\n",
        "  # corpus_sects_ls = del_substrs_list(corpus_sects_ls, pattern_chap) # corpus_sects_ls = [re.sub(rf'{pattern_sect}', '', x) for x in corpus_sects_ls]\n",
        "\n",
        "  # Filter out the Chapter separator 'CHAPTER ' lines\n",
        "  # Keep for now, messy but enables proper SECTION assignments to appropraite CHAPTERs\n",
        "  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('CHAPTER '))]\n",
        "\n",
        "  print(f'About to process {len(corpus_sects_ls)} Sections')\n",
        "  # Filter out Sentences in Section that don't have a corresponding Sentence in corpus_sents_df \n",
        "  # Old Strategy: Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \n",
        "  corpus_sects_filtered_ls = []\n",
        "  for i, asection in enumerate(corpus_sects_ls):\n",
        "    sect_sentences_ls = []\n",
        "    sect_sentences_raw_ls = sent_tokenize(asection)\n",
        "    for j, asentence in enumerate(sect_sentences_raw_ls):\n",
        "      if (asentence in corpus_sents_ls):\n",
        "        sect_sentences_ls.append(asentence)\n",
        "      else:\n",
        "        print(f'Not found Sect Sent: {asentence} in corpus_sents_df')\n",
        "    section_str = ' '.join(sect_sentences_ls)\n",
        "    corpus_sects_filtered_ls.append(section_str)\n",
        "\n",
        "    # if re.search(rf'{pattern_chap}', asect):\n",
        "    #   print(f'In Section #{i} removing embedded CHAPTER:\\n\\n    {asect}')\n",
        "    #   asect = re.sub(rf'{pattern_chap}', ' ', asect)\n",
        "\n",
        "\n",
        "  return corpus_sects_filtered_ls, corpus_raw_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrpteKrMg9vZ"
      },
      "source": [
        "corpus_sects_ls, test_raw_str = corpus2sects(corpus_filename)\n",
        "len(corpus_sects_ls[18])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCusfM1F5QSF"
      },
      "source": [
        "section_no = 2\n",
        "search_term = 'SECTION'\n",
        "print(f'Section #{ch_no} Beginning -----')\n",
        "corpus_sects_ls[section_no][:500]\n",
        "print('\\n')\n",
        "print(f'section #{section_no} Ending -----')\n",
        "corpus_sects_ls[section_no][-500:]\n",
        "print('\\n')\n",
        "print(f'Any {search_term} found?')\n",
        "search_term in corpus_sects_ls[section_no]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8kTgFmHhyR8"
      },
      "source": [
        "pattern_sect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-bhCkEWgrhv"
      },
      "source": [
        "len(test_sects_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBi9Ar-4L_0d"
      },
      "source": [
        "# corpus_sects_ls[18][-500:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIWf9zyK_9n0"
      },
      "source": [
        "# pattern_chap = 'CHAPTER [0-9]{1,2}[.]? [A-Z \\\\.-:—;-’\\\\\\'\"]*[^\\\\n]*'\n",
        "\n",
        "# pattern_sect = 'SECTION [0-9]{1,2} [^\\\\n]*'\n",
        "# pattern_sect = 'SECTION [\\d]{1,2}[.]?'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaLa3884CvNT"
      },
      "source": [
        "pattern_sect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxuFL4_VW4yt"
      },
      "source": [
        "pattern_regex = re.compile(r'CHAPTER [0-9]{1,2} ')\n",
        "temp_ls = del_substrs_list(test_sects_ls, pattern_regex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NdFA5ALXykX"
      },
      "source": [
        "temp_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjmRx8PUT-Tf"
      },
      "source": [
        "for i, asect in enumerate(test_sects_ls):\n",
        "  if 'CHAPTER ' in asect:\n",
        "    print(f'Section #{i}: {asect[-500:]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fDHqC44AIww"
      },
      "source": [
        "\n",
        "# Debug\n",
        "\n",
        "test_sects_ls, test_raw_str = corpus2sects(corpus_filename)\n",
        "print(f'Section Count: {len(test_sects_ls)}')\n",
        "print('\\n')\n",
        "print(f'First Section: -----\\n\\n    {test_sects_ls[0][:500]}')\n",
        "print('\\n')\n",
        "print(f'Last Section: -----\\n\\n    {test_sects_ls[-1][:500]}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epg8wRPBAIq_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8z8ZY5n8BRs"
      },
      "source": [
        "def corpus2parags(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of min preprocessed raw paragraphs (corpus_parags_ls)\n",
        "  '''\n",
        "\n",
        "  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  corpus_parags_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n",
        "  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n",
        "\n",
        "  # Strip off whitespace\n",
        "  corpus_parags_ls = [x.strip() for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out empty lines\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n",
        "\n",
        "  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n",
        "  parag_before_punctstrip_ct = len(corpus_parags_ls)\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  print(f'Punctuation only Paragraph Count: {len(corpus_parags_ls) - parag_before_punctstrip_ct}')\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.strip().startswith('----- '))]\n",
        "\n",
        "  # Filter out the Section separator 'SECTION ' lines\n",
        "  # for i,temp_str in enumerate(corpus_parags_ls):\n",
        "  #   if temp_str.startswith('SECTION '):\n",
        "  #     print(f'Parag #{i}: {temp_str}')\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('SECTION '))]\n",
        "\n",
        "  # Filter out any possible embedded 'SECTION ' lines\n",
        "  # for i,temp_str in enumerate(corpus_parags_ls):\n",
        "  #   if 'SECTION' in temp_str:   # .contains('SECTION '):\n",
        "  #     print(f'Parag #{i}: {temp_str}')\n",
        "  # corpus_parags_ls = del_substrs_list(corpus_parags_ls, pattern_sect) # [re.sub(rf'{pattern_sect}', '', x) for x in corpus_parags_ls]\n",
        "\n",
        "  # Filter out the Chapter separator 'CHAPTER ' lines\n",
        "  for i,temp_str in enumerate(corpus_parags_ls):\n",
        "    if temp_str.startswith('CHAPTER '):\n",
        "      print(f'Parag #{i}: {temp_str}')\n",
        "  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('CHAPTER '))]\n",
        "\n",
        "  return corpus_parags_ls, corpus_raw_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvaLm3aFecCu"
      },
      "source": [
        "# corpus_parags_ls, corpus_raw_str = corpus2parags(CORPUS_FILENAME)\n",
        "# len(corpus_parags_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b175_N9YXbB"
      },
      "source": [
        "pattern_sect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF05sFo9Yw9p"
      },
      "source": [
        "# Verfiy no 'SECTION' headers were not filtered out \n",
        "\"\"\"\n",
        "pattern_test = re.compile(rf'{pattern_sect}')\n",
        "parags_sectionheader_ls = [line for line in corpus_parags_ls if pattern_test.search(str(line))]\n",
        "print(f'Found {len(parags_sectionheader_ls)} Paragraphs with SECTION pattern')\n",
        "if len(parags_sectionheader_ls) > 0:\n",
        "  corpus_parags_clean_ls = []\n",
        "  for aparag in corpus_parags_ls:\n",
        "    aparag_clean = re.sub(rf'{pattern_sect}', '', aparag)\n",
        "    # aparag_clean = re.sub(r'SECTION', '', aparag)\n",
        "    if len(aparag_clean) > 0:\n",
        "      corpus_parags_clean_ls.append(aparag_clean)\n",
        "else:\n",
        "  print('pattern_sect [SECTION] not found in any paragraph')\n",
        "\n",
        "print(f'BEFORE: corpus_parags_ls length={len(corpus_parags_ls)}')\n",
        "print(f'AFTER: corpus_parags_clean_ls length={len(corpus_parags_clean_ls)}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFJV1E1rdVMQ"
      },
      "source": [
        "# parags_sectionheader_ls = [line for line in corpus_parags_ls if pattern_test.search(str(line))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX1gjU7S8esY"
      },
      "source": [
        "def parag2sents(corpus_parags_ls):\n",
        "  '''\n",
        "  Given a list of paragraphs,\n",
        "  Return a list of lists of Sentences [sent_no, parag_no, asent(text)]\n",
        "  '''\n",
        "\n",
        "  sent_no = 0\n",
        "  # sent_base = 0\n",
        "  corpus_sents_ls = []\n",
        "  for parag_no,aparag in enumerate(corpus_parags_ls):\n",
        "    sents_ls = sent_tokenize(aparag)\n",
        "    # Delete (whitespace only) sentences\n",
        "    sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n",
        "    # Delete (punctuation only) sentences\n",
        "    sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n",
        "    # Delete numbers (int or float) sentences\n",
        "    sents_ls = [x for x in sents_ls if not (x.strip().isnumeric())]\n",
        "\n",
        "    # Filter out leading SECTION separator 'SECTION ' lines\n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      if temp_str.startswith('SECTION '):\n",
        "        print(f'Sentence #{i}: {temp_str}')\n",
        "    sents_ls = [x for x in sents_ls if not (x.startswith('SECTION '))]\n",
        "\n",
        "    # Filter out leading Chapter separator 'CHAPTER ' lines\n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      if temp_str.startswith('CHAPTER '):\n",
        "        print(f'Sentence #{i}: {temp_str}')\n",
        "    sents_ls = [x for x in sents_ls if not (x.startswith('CHAPTER '))]\n",
        "    \n",
        "    # Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \n",
        "    for i,temp_str in enumerate(sents_ls):\n",
        "      # TODO: More specific, robust filtering mechnism \n",
        "      if (re.search(rf'{pattern_sect}', temp_str)):\n",
        "        pass\n",
        "      if (re.search(rf'{pattern_chap}', temp_str)):\n",
        "        pass\n",
        "      else:\n",
        "        corpus_sents_ls.append([sent_no, parag_no, temp_str])\n",
        "        sent_no += 1\n",
        "\n",
        "    # print(f'Returning with corpus_sents_ls length = {len(corpus_sents_ls)}')\n",
        "  \n",
        "  return corpus_sents_ls\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnbzWr0Oxeo3"
      },
      "source": [
        "pattern_chap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0K2HCpSwdA_"
      },
      "source": [
        "pattern_test = pattern_sect\n",
        "\n",
        "if (re.search(rf'{pattern_test}', 'this is SECTION 123 one section that is ok')):\n",
        "  print('GOT IT')\n",
        "else:\n",
        "  print('NOPE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-adBsfM38eo0"
      },
      "source": [
        "# corpus_sents_ls = parag2sents(corpus_parags_ls)\n",
        "# len(corpus_sents_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MZXXNCe8ell"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWOSMNGy8BN9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaRD7RL9IOeg"
      },
      "source": [
        "# Generate full path and timestamp for new filepath/filename\n",
        "\n",
        "def gen_pathfiletime(file_str, subdir_str=''):\n",
        "\n",
        "  # Geenreate compressed author and title substrings\n",
        "  author_raw_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "  title_raw_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "\n",
        "  # Generate current/unique datetime string\n",
        "  datetime_str = str(datetime.now().strftime('%Y%m%d%H%M%S'))\n",
        "\n",
        "  # Built fullpath+filename string\n",
        "  file_base, file_ext = file_str.split('.')\n",
        "\n",
        "  author_str = re.sub('[^A-Za-z0-9]+', '', author_raw_str)\n",
        "  title_str = re.sub('[^A-Za-z0-9]+', '', title_raw_str)\n",
        "\n",
        "  full_filepath_str = f'{subdir_str}{file_base}_{author_str}_{title_str}_{datetime_str}.{file_ext}'\n",
        "\n",
        "  # print(f'Returning from gen_savepath() with full_filepath={full_filepath}')\n",
        "\n",
        "  return full_filepath_str\n",
        "\n",
        "# Test\n",
        "# pathfilename_str = gen_pathfiletime('hist_paraglen.png')\n",
        "# print(pathfilename_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9FDtpQ6QaEJ"
      },
      "source": [
        "#This function converts to lower-case, removes square bracket, removes numbers and punctuation\n",
        " \n",
        "def text_clean(text):\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)  # Expand contrations\n",
        "    text = re.sub(\"\\\\'s\", \" own\", text)  # After expanding normal apostrophes, expand possessive apostrophes \"Mary's car\" -> \"Mary own car\"\n",
        "\n",
        "    # TODO: More formally\n",
        "    # https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0\n",
        "    text = re.sub(\"-\\n\", \"\", text)       # Join end of line words split by continuation hyphens\n",
        "    text = re.sub(\"-\\n\\r\", \"\", text)\n",
        "    text = re.sub(\"-\\r\", \"\", text)\n",
        "    text = re.sub(\"\\[.*?\\]\", \" \", text)\n",
        "\n",
        "    text = re.sub(\"-\", \" \", text)  # Special care for hypenated words well-known: choose option (a)\n",
        "                                   # (a) 'well known', (b) 'wellknown' (c) 'well known' and 'wellknown' cf: https://datascience.stackexchange.com/questions/81072/how-to-process-the-hyphenated-english-words-for-any-nlp-problem\n",
        "    text = re.sub(\"/\", \" \", text)  # sociability/conversation/interesting -> sociability conversation interesting                             \n",
        "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
        "    text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n",
        "    text = re.sub(\"[\\n]\", \" \", text)  # Replace newline with space\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afWoWCZ7R_U9"
      },
      "source": [
        "# Verify saved under newest filename\n",
        "\n",
        "def get_recentfile(file_type='csv'):\n",
        "  '''\n",
        "  Given a file extension type,\n",
        "  Return the most recently created file of that type \n",
        "  in the current directory\n",
        "  '''\n",
        "  file_pattern = \"./*.\" + file_type\n",
        "  print(f'file_pattern: {file_pattern}')\n",
        "  list_of_files = glob.glob(file_pattern) # * means all if need specific format then *.csv\n",
        "  latest_file = max(list_of_files, key=os.path.getmtime)\n",
        "\n",
        "  return latest_file\n",
        "\n",
        "# Test\n",
        "\n",
        "# get_recentfile('txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUvKJEybUIeP"
      },
      "source": [
        "## **Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8TwYCrwwUco"
      },
      "source": [
        "# Test to find longest String in Corpus (in terms of #tokens) \n",
        "#      must be <510 tokens for Transformers\n",
        "\n",
        "# corpus_sents_df['sent_raw'].astype(str).apply(lambda x: len(x.split())).max() # split().len()).max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnbQ_2Xmj1uW"
      },
      "source": [
        "# get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentimentr, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtxqQjU0Odos"
      },
      "source": [
        "# corpus_sents_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLASKwzZF-Lh"
      },
      "source": [
        "def get_sentiments(model_base, sentiment_fn, sentiment_type='lexicon'):\n",
        "  '''\n",
        "  Given a model_base name and sentiment evaluation function\n",
        "  Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "  '''\n",
        "\n",
        "  # Calculate Sentiment Polarities\n",
        "\n",
        "  if sentiment_type == 'lexicon':\n",
        "    print(f'Processing Lexicon/Sentences...')\n",
        "    corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon/Paragraphs...')\n",
        "    corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon/Sections...')\n",
        "    corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon/Chapters...')\n",
        "    corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "  \n",
        "  elif sentiment_type == 'compound':\n",
        "    # VADER\n",
        "\n",
        "    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n",
        "    print(f'Processing Lexicon/Sentences...')\n",
        "    corpus_sents_df['scores'] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon/Paragraphs...')\n",
        "    corpus_parags_df['scores'] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon/Sections...')\n",
        "    corpus_sects_df['scores'] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon/Chapters...')\n",
        "    corpus_chaps_df['scores'] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "\n",
        "    # Extract Compound Sentiment\n",
        "    corpus_sents_df[model_base]  = corpus_sents_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_parags_df[model_base]  = corpus_parags_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_sects_df[model_base]  = corpus_sects_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_chaps_df[model_base]  = corpus_chaps_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "\n",
        "  elif sentiment_type == 'function':\n",
        "    # TextBlob\n",
        "\n",
        "    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n",
        "    print(f'Processing Lexicon/Sentences...')\n",
        "    corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon/Paragraphs...')\n",
        "    corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon/Sections...')\n",
        "    corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    print(f'Processing Lexicon/Chapters...')\n",
        "    corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: sentiment_type={sentiment_type} but must be one of (lexicon, compound, function)')\n",
        "    return\n",
        "\n",
        "  # Create new column names\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "\n",
        "\n",
        "  # Get Chapter Standardization with MeanSTD and RobustStandardization with MedianIQRScaling\n",
        "  corpus_chaps_df[col_meanstd]  = mean_std_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Chapter Sentiment by dividing by Chapter Length\n",
        "  chaps_len_ls = list(corpus_chaps_df['token_len'])\n",
        "  chaps_sentiment_ls = list(corpus_chaps_df[model_base])\n",
        "  chaps_sentiment_norm_ls = [chaps_sentiment_ls[i]/chaps_len_ls[i] for i in range(len(chaps_len_ls))]\n",
        "  # RobustStandardize Chapter sentiment values\n",
        "  corpus_chaps_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  # Get Section Standardization with MeanSTD and RobustStandardization with MedianIQRScaling\n",
        "  corpus_sects_df[col_meanstd]  = mean_std_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sects_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Section Sentiment by dividing by Section Length\n",
        "  sects_len_ls = list(corpus_sects_df['token_len'])\n",
        "  sects_sentiment_ls = list(corpus_sects_df[model_base])\n",
        "  sects_sentiment_norm_ls = [sects_sentiment_ls[i]/sects_len_ls[i] for i in range(len(sects_len_ls))]\n",
        "  # RobustStandardize Section sentiment values\n",
        "  corpus_sects_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_sects_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sects_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "\n",
        "  # Normalize the Paragraph Sentiment by dividing by Chapter Length\n",
        "  parags_len_ls = list(corpus_parags_df['token_len'])\n",
        "  parags_sentiment_ls = list(corpus_parags_df[model_base])\n",
        "  parags_sentiment_norm_ls = [parags_sentiment_ls[i]/parags_len_ls[i] for i in range(len(parags_len_ls))]\n",
        "  # RobustStandardize Paragraph sentiment values\n",
        "  corpus_parags_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_parags_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_parags_df[model_base]).reshape(-1, 1))\n",
        "  corpus_parags_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "  sents_len_ls = list(corpus_sents_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_sents_df[model_base])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/sents_len_ls[i] for i in range(len(sents_len_ls))]\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_sents_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sents_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJNepWBIkVjm"
      },
      "source": [
        "# Read in lexicon at given path into Dict[word]=polarity\n",
        "\n",
        "def get_lexicon(lexicon_name, lexicon_format=2):\n",
        "    \"\"\"\n",
        "    Read sentiment lexicon.csv file at lexicon_path\n",
        "    into appropriate Dict[word]=polarity\n",
        "\n",
        "    1. lexicon_dt[word] = <polarity value>\n",
        "\n",
        "    Args:\n",
        "        sa_lib (str, optional): [description]. Defaults to 'syuzhet'.\n",
        "    \"\"\"\n",
        "    \n",
        "    # global lexicon_df\n",
        "\n",
        "    lexicon_df = pd.DataFrame()\n",
        "    \n",
        "    # print(os.getcwd())\n",
        "    \"\"\"\n",
        "    lexicons_ls = os.listdir('../sa_lexicons/')\n",
        "    if (lexicon_name in lexicons_ls):\n",
        "      print(f'Found {lexicon_name} in lexicon_directory)')\n",
        "    # print(glob.glob('*.csv'))\n",
        "    cp_cmd = f'copy ../sa_lexicons/{lexicon_name} ./'\n",
        "    print(f'cp_cmd = {cp_cmd}')\n",
        "    os.system(cp_cmd)\n",
        "    os.system('cp ../sa_lexicons/' + lexicon_name.strip() + ' ./')\n",
        "    os.listdir('.')  \n",
        "    \"\"\";\n",
        "    \n",
        "    try:\n",
        "      lexicon_df = pd.read_csv(lexicon_name)\n",
        "      lexicon_df.info()\n",
        "      # lexicon_df = lexicon_tmp_df.copy()\n",
        "      # print(lexicon_df.head())\n",
        "      return lexicon_df\n",
        "    except:\n",
        "      print(f'ERROR: Cannot read lexicon.csv at {lexicon_name}')\n",
        "      return -1\n",
        "\n",
        "'''\n",
        "    print\n",
        "    if (sa_lexicon == 'default'):\n",
        "        lexicon_df = pd.read_csv(LEXICON_PATH)\n",
        "        lexicon_df.columns = ['index_no', 'word', 'polarity']\n",
        "        lexicon_df.drop(['index_no'], axis=1, inplace=True)\n",
        "        lexicon_df.dropna(inplace=True)\n",
        "        lexicon_dt = lexicon_df.set_index('word').T.to_dict('list')\n",
        "        # unlist the polarity to type: float\n",
        "        for key in lexicon_dt:\n",
        "            lexicon_dt[key] = float(lexicon_dt[key][0])\n",
        "        \n",
        "    ### print(f\"Exit get_sa_lex() with {len(lexicon_dt.keys())} entries in syuzhet_dt\")\n",
        "    return lexicon_dt\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWdzLF0jwI-u"
      },
      "source": [
        "# Sentence to Sentiment Polarity according to passed in Lexicon Dictionary\n",
        "\n",
        "def text2sentiment(text_str, lexicon_dt):\n",
        "  '''\n",
        "  Given a text_str and lexicon_dt, calculate \n",
        "  the sentimety polarity.\n",
        "  '''\n",
        "\n",
        "  # Remove all not alphanumeric and whitespace characters\n",
        "  text_str = re.sub(r'[^\\w\\s]', '', text_str) \n",
        "\n",
        "  text_str = text_str.strip().lower()\n",
        "  if (len(text_str) < 1):\n",
        "      print(f\"ERROR: text2sentiment() given empty/null/invalid string: {text_str}\")\n",
        "\n",
        "  text_ls = text_str.split()\n",
        "  # print(f'text_ls: {text_ls}')\n",
        "\n",
        "  # Accumulated Total Sentiment Polarity for entire Sentence\n",
        "  text_sa_tot = 0.0\n",
        "\n",
        "  for aword in text_ls:\n",
        "      # print(f'getting sa for word: {aword}')\n",
        "      try:\n",
        "          word_sa_fl = float(lexicon_dt[aword])\n",
        "          text_sa_tot += word_sa_fl\n",
        "          # print(f\">>{aword} has a sentiment value of {word_sa_fl}\")\n",
        "      except TypeError: # KeyError:\n",
        "          # aword is not in lexicon so it adds 0 to the sentence sa sum\n",
        "          # print(f\"TypeError: cannot convert {lexicon_dt[aword]} to float\")\n",
        "          continue\n",
        "      except KeyError:\n",
        "          # print(f\"KeyError: missing key {aword} in defaultdict syuzhet_dt\")\n",
        "          continue\n",
        "      except:\n",
        "          e = sys.exc_info()[0]\n",
        "          # print(f\"ERROR {e}: sent2lex_sa() cannot catch aword indexing into syuzhet_dt error\")\n",
        "  \n",
        "  # print(f\"Leaving sent2lex_sa() with sentence sa value = {str(text_sa_tot)}\")\n",
        "  \n",
        "  return text_sa_tot\n",
        "\n",
        "\n",
        "# Test\n",
        "\n",
        "# sent2sentiment('I hate and despise and abhor and dislike and am disgusted by Mondays.', lexicon_jockersrinker_dt)\n",
        "# sent2sentiment('hate Mondays.', lexicon_jockersrinker_dt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6AEBZzvEz4a"
      },
      "source": [
        "def plot_smas(section_view=True, model_name='vader', text_unit='sentence', wins_ls=[20], alpha=0.5, subtitle_str='', y_height=0, save2file=False):\n",
        "  '''\n",
        "  Given a model, text_unit\n",
        "  Plot a SMA using default values and wrapping the function get_smas()\n",
        "  '''\n",
        "\n",
        "  if (section_view == True) and not any(x == text_unit for x in ['sentence', 'paragraph']):\n",
        "    print(f'ERROR: You can only plot SMA within a Section with Sentence or Paragraph text units')\n",
        "    return -99\n",
        "\n",
        "  if text_unit == 'sentence':\n",
        "    if section_view == False:\n",
        "      ts_df = corpus_sents_df\n",
        "    else:\n",
        "      ts_df = section_sents_df\n",
        "    wins_ls = [5,10,20]\n",
        "  elif text_unit == 'paragraph':\n",
        "    if section_view == False:\n",
        "      ts_df = corpus_parags_df\n",
        "    else:\n",
        "      ts_df = section_parags_df\n",
        "    wins_ls = [5,10,20]\n",
        "  elif text_unit == 'section':\n",
        "    ts_df = corpus_sects_df\n",
        "    wins_ls=[20]\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n",
        "\n",
        "  sectno_loc = ts_df[model_name].min()\n",
        "\n",
        "  if section_view ==False:\n",
        "    # At Section boundries draw blue vertical lines \n",
        "    section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "    for i, sent_no in enumerate(section_boundries_ls):\n",
        "      plt.text(sent_no, y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n",
        "      plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "      # 'BigNews1', xy=(sent_no, 0.5), xytext=(-10, 25), textcoords='offset points',                   rotation=90, va='bottom', ha='center', annotation_clip=True)\n",
        "\n",
        "      # plt.text(sent_no, -.5, 'goodbye',rotation=90, zorder=0)\n",
        "\n",
        "    # At Chapter boundaries draw red vertical lines\n",
        "    chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n",
        "    for i, sent_no in enumerate(chapter_boundries_ls):\n",
        "      plt.axvline(sent_no, color='navy', alpha=0.1)\n",
        "      # plt.text(sent_no, .5, 'hello', rotation=90, zorder=0)\n",
        "\n",
        "  get_smas(ts_df, model_name=model_name, text_unit=text_unit, wins_ls=wins_ls, alpha=alpha, subtitle_str=subtitle_str, save2file=save2file)\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_sma_sents_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcAMyjyn8ugj"
      },
      "source": [
        "# SMA 5% Sentiment of Sentence Sentiment\n",
        "\n",
        "def get_smas(ts_df, model_name, text_unit='sentence', wins_ls=[5,10], alpha=0.5, scale_factor=1., subtitle_str='', mean_adj=0., do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a model_name and time series DataFrame and list of win_rolls in percentages\n",
        "  Return the rolling means of the time series using the window sizes in win_rolls\n",
        "  '''\n",
        "\n",
        "  temp_roll_df = pd.DataFrame() # TODO: save sma rolling values into temp_df and return this value\n",
        "\n",
        "  win_1per = int(ts_df.shape[0]*0.01)\n",
        "  if text_unit ==  'sentence':\n",
        "    # win_1per = win_s1per\n",
        "    x_idx = 'sent_no'\n",
        "    fname_abbr = 'sents'\n",
        "  elif text_unit == 'paragraph':\n",
        "    # win_1per = win_p1per\n",
        "    x_idx = 'parag_no'\n",
        "    fname_abbr = 'parags'\n",
        "  elif text_unit == 'section':\n",
        "    win_1per = 1\n",
        "    wins_ls = [int(0.1 * corpus_sects_df.shape[0])]  # Edge case to deal with very few Section data points\n",
        "    x_idx = 'sect_no'\n",
        "    fname_abbr = 'sects'\n",
        "  else:\n",
        "    print(f'ERROR: text_unit={text_unit} but must be either sentence, paragraph or section')\n",
        "  \n",
        "  for i, awin_size in enumerate(wins_ls):\n",
        "    if len(str(awin_size)) == 1:\n",
        "      awin_str = '0'+str(awin_size)+'0'\n",
        "    else:\n",
        "      awin_str = str(awin_size)+ '0'\n",
        "    col_roll_str = f'{model_name}_mean_roll{awin_str}'\n",
        "    win_size = awin_size*win_1per\n",
        "    ts_df[col_roll_str] = ts_df[model_name].rolling(window=win_size, center=True).mean()\n",
        "  \n",
        "    if do_plot == True:\n",
        "      alabel = f'{model_name} (win={awin_size})'\n",
        "      ts_df['y_scaled'] = ts_df[col_roll_str]*scale_factor + mean_adj \n",
        "      sns.lineplot(data=ts_df, x=x_idx, y='y_scaled', legend='brief', label=alabel, alpha=alpha)\n",
        "      \n",
        "  plt.title(f'{CORPUS_FULL} (Model: {model_name}: {subtitle_str}) \\nSMA Smoothed {text_unit} Sentiment Plot (windows={wins_ls})')\n",
        "  # plt.legend(loc='best')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_{fname_abbr}_sa_mean_050100sma.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return temp_roll_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULzfHeDK8udN"
      },
      "source": [
        "def get_lexstats(ts_df, model_name, text_unit='sentence'):\n",
        "  '''\n",
        "  Given a model name\n",
        "  calculate, store and return time series stats\n",
        "  '''\n",
        "  \n",
        "  global corpus_lexicons_stats_dt\n",
        "\n",
        "  temp_dt = {}\n",
        "  \n",
        "  if text_unit == 'sentence':\n",
        "    stat_idx = f'{model_name}_sents'\n",
        "  elif text_unit == 'paragraph':\n",
        "    stat_idx = f'{model_name}_parags'\n",
        "  elif text_unit == 'section':\n",
        "    stat_idx = f'{model_name}_sects'\n",
        "  elif text_unit == 'chapter':\n",
        "    stat_idx = f'{model_name}_chaps'\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must either be sentence, paragraph, or section')\n",
        "\n",
        "  sentiment_min = ts_df[model_name].min()\n",
        "  sentiment_max = ts_df[model_name].max()\n",
        "\n",
        "  temp_dt = {'sentiment_min' : sentiment_min,\n",
        "             'sentiment_max' : sentiment_max}\n",
        "\n",
        "  corpus_lexicons_stats_dt[stat_idx] = temp_dt\n",
        "                                     \n",
        "  return \n",
        "\n",
        "# Test\n",
        "# get_lexstats('afinn')\n",
        "# corpus_lexicons_stats_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltdJn-7ePNM9"
      },
      "source": [
        "def lex_discrete2continous_sentiment(text, lexicon):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_tot = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    word_sentiment = text2sentiment(str(aword), lexicon)\n",
        "    text_sentiment_tot += word_sentiment\n",
        "  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.01)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6xMI98l8uaH"
      },
      "source": [
        "def clip_outliers(floats_ser):\n",
        "  '''\n",
        "  Given a pd.Series of float values\n",
        "  Return a list with outliers removed, values limited within 3 median absolute deviations from median\n",
        "  '''\n",
        "  # https://www.statsmodels.org/stable/generated/statsmodels.robust.scale.mad.html#statsmodels.robust.scale.mad\n",
        "\n",
        "  # Old mean/std, less robust\n",
        "  # ser_std = floats_ser.std()\n",
        "  # ser_median = floats_ser.mean() # TODO: more robust: asym/outliers -> median/IQR or median/median abs deviation\n",
        "\n",
        "  floats_np = np.array(floats_ser)\n",
        "  ser_median = floats_ser.median()\n",
        "  ser_mad = robust.mad(floats_np)\n",
        "  print(f'ser_median = {ser_median}')\n",
        "  print(f'ser_mad = {ser_mad}')\n",
        "\n",
        "  if ser_mad == 0:\n",
        "    # for TS with small ranges (e.g. -1.0 to +1.0) Median Abs Deviation = 0\n",
        "    #   so pass back the original time series\n",
        "    floats_clip_ls = list(floats_ser)\n",
        "\n",
        "  else:\n",
        "    ser_oldmax = floats_ser.max()\n",
        "    ser_oldmin = floats_ser.min()\n",
        "    print(f'ser_max = {ser_oldmax}')\n",
        "    print(f'ser_min = {ser_oldmin}')\n",
        "\n",
        "    ser_upperlim = ser_median + 2.5*ser_mad\n",
        "    ser_lowerlim = ser_median - 2.5*ser_mad\n",
        "    print(f'ser_upperlim = {ser_upperlim}')\n",
        "    print(f'ser_lowerlim = {ser_lowerlim}')\n",
        "\n",
        "    # Clip outliers to max or min values\n",
        "    floats_clip_ls = np.clip(floats_np, ser_lowerlim, ser_upperlim)\n",
        "    # print(f'max floast_ls {floats_ls.max()}')\n",
        "\n",
        "    # def map2range(value, low, high, new_low, new_high):\n",
        "    #   '''map a value from one range to another'''\n",
        "    #   return value * 1.0 / (high - low + 1) * (new_high - new_low + 1)\n",
        "\n",
        "    # Map all float values to range [-1.0 to 1.0]\n",
        "    # floats_clip_sig_ls = [map2range(i, ser_oldmin, ser_oldmax, ser_upperlim, ser_lowerlim) for i in floats_clip_ls]\n",
        "\n",
        "    # listmax_fl = float(max(floats_ls))\n",
        "    # floats_ls = [i/listmax_fl for i in floats_ls]\n",
        "    #floats_ls = [1/(1+math.exp(-i)) for i in floats_ls]\n",
        "\n",
        "  return floats_clip_ls  # floats_clip_sig_ls\n",
        "\n",
        "# Test\n",
        "# Will not work on first run as corpus_sents_df is not defined yet\n",
        "'''\n",
        "data = np.array([1, 4, 4, 7, 12, 13, 16, 19, 22, 24])\n",
        "test_ls = clip_outliers(corpus_sents_df['vader'])\n",
        "print(f'new min is {min(test_ls)}')\n",
        "print(f'new max is {max(test_ls)}')\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXwKR4gA8Ouk"
      },
      "source": [
        "## **Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8Hf8nU98uXI"
      },
      "source": [
        "def rename_cols(ts_df, col_old_ls, suffix_str='_raw'):\n",
        "  '''\n",
        "  Given a DataFrame, list of columns in DataFrame and a suffix,\n",
        "  Return a Dictionary mapping old col names to new col name (orig+suffix)\n",
        "  '''\n",
        "\n",
        "  col_new_ls = []\n",
        "  for acol in col_old_ls:\n",
        "    acol_new = acol + suffix_str\n",
        "    col_new_ls.append(acol_new)\n",
        "\n",
        "  # Create dict for col mapping: keys=old col names, value=new col names\n",
        "  col_rename_dt = dict(zip(col_old_ls, col_new_ls))\n",
        "\n",
        "  # ts_df.rename(columns=col_rename_dt, errors=\"raise\")\n",
        "\n",
        "  return col_rename_dt\n",
        "\n",
        "# test_ls = [col for col in corpus_sents_df.columns if not(renaming_fun(col) is None)]\n",
        "# print(f'test_ls: {test_ls}')\n",
        "\n",
        "# Test\n",
        "# col_rename_dt = rename_cols(corpus_sents_df, sentiment_only_cols_ls)\n",
        "# col_rename_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YJJcvDVnUuT"
      },
      "source": [
        "## **Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIo6-zGKnZps"
      },
      "source": [
        "def norm2negpos1(data_ser):\n",
        "  '''\n",
        "  Given a series of floating number\n",
        "  Return a a list of same values normed between -1.0 and +1.0\n",
        "  '''\n",
        "  # data_np = np.matrix(data_ser)\n",
        "\n",
        "  scaler=MinMaxScaler(feature_range=(-1.0, 1.0))\n",
        "  temp_ser = scaler.fit_transform(np.matrix(data_ser))\n",
        "  \n",
        "  return temp_ser\n",
        "\n",
        "# Test\n",
        "'''\n",
        "temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n",
        "print(type(temp_np))\n",
        "temp_np.shape\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpVHeYKYnUhU"
      },
      "source": [
        "def standardize_ts(data_ser):\n",
        "  '''\n",
        "  Given a series of floating number\n",
        "  Return a a list of same values normed between -1.0 and +1.0\n",
        "  '''\n",
        "  # data_np = np.matrix(data_ser)\n",
        "\n",
        "  std_scaler = StandardScaler()\n",
        "  df_std = std_scaler.fit_transform(np.array(data_ser))\n",
        "  \n",
        "  return df_std\n",
        "\n",
        "# Test\n",
        "'''\n",
        "temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n",
        "print(type(temp_np))\n",
        "temp_np.shape\n",
        "temp_np\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylt_kLuEFDrj"
      },
      "source": [
        "# This must be defined AFTER the corpus_sects_df DataFrame is created in the Preprocessing Step below\n",
        "\n",
        "# Raw Plot of Section Sentiments (Adjusted for (x-axis) mid-Section Sentence No and (y-axis) Sentiment weighted by Section length )\n",
        "\n",
        "# corpus_sects_df = pd.DataFrame()  # Create empty early as required by some utility functions\n",
        "\n",
        "def plot_crux_sections(model_names_ls, semantic_type='section', subtitle_str='', label_token_ct=0, title_xpos = 0.8, title_ypos=0.2, sec_y_height=0, save2file=False):\n",
        "  '''\n",
        "  Given a Sections DataFrame, model_name and semantic type,\n",
        "  Return a Plot of the Cruxes\n",
        "  '''\n",
        "\n",
        "  crux_points_dt = {}\n",
        "  model_stand_names_ls = []\n",
        "  section_boundries_ls = []\n",
        "\n",
        "\n",
        "  # print(f'Using model_names: {model_names_ls}')\n",
        "\n",
        "  # sns.lineplot(data=ts_df, x='sent_no_mid', y=amodel_stand, markers=['o'], alpha=0.5, label=amodel_stand); # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment (Bing Lexicon)')\n",
        "\n",
        "\n",
        "  # At Section boundries draw blue vertical lines \n",
        "  section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "  for i, sent_no in enumerate(section_boundries_ls):\n",
        "    plt.text(sent_no, sec_y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1);\n",
        "\n",
        "  # At Chapter boundaries draw red vertical lines\n",
        "  chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n",
        "  for i, sent_no in enumerate(chapter_boundries_ls):\n",
        "    plt.axvline(sent_no, color='navy', alpha=0.1);\n",
        "\n",
        "  # Error check and assign DataFrame associated with each semantic_type\n",
        "  if semantic_type == 'section':\n",
        "    # Get midpoints of each Section\n",
        "    ts_df=corpus_sects_df\n",
        "    midpoints_ls = list(corpus_sects_df['sent_no_mid'])\n",
        "  elif semantic_type == 'chapter':\n",
        "    # Get midpoints of each Chapter\n",
        "    ts_df=corpus_chaps_df\n",
        "    midpoints_ls = list(corpus_chaps_df['sent_no_mid'])\n",
        "  else:\n",
        "    print(f\"ERROR: semantic_type={semantic_type} must be either 'section' or 'chapter'\")\n",
        "    return -1\n",
        "\n",
        "  # How many sentiment time series are we plotting?\n",
        "  if len(model_names_ls) == 1:\n",
        "    \n",
        "    # Plotting only one model\n",
        "    model_name_full = str(model_names_ls[0])\n",
        "    model_name_root = model_name_full.split('_')[0]\n",
        "    print(f'model_name_full: {model_name_full} and model_name_root: {model_name_root}')\n",
        "    if model_name_root in MODELS_LS:\n",
        "      # Plot\n",
        "      print(f'about to sns.lineplot model: ') # {ts_df}')\n",
        "      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "      # g._legend.remove()\n",
        "      # print(f'model_name_full={model_name_full}')\n",
        "      # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], markers=\"o\", alpha=0.5, label=model_name_full)\n",
        "    else:\n",
        "      print(f'ERROR: model_names_ls[0]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n",
        "      return -1\n",
        "\n",
        "    # If plotting only one model, add labels\n",
        "    midpoints_sentiment_ls = list(ts_df[model_name_full])\n",
        "    sect_ct = 0\n",
        "    for x,y in zip(midpoints_ls, midpoints_sentiment_ls): \n",
        "      label_token_int = int(label_token_ct)\n",
        "      if label_token_int < 0:\n",
        "        label = ''\n",
        "      elif label_token_int == 0:\n",
        "        # if arg label_token_ct == 0, just print sent_no\n",
        "        label = f\"#{x}({sect_ct})\"\n",
        "      else:\n",
        "        # if arg label_token_ct > 0, print the first label_token_ct words of sentence at crux point\n",
        "        label = f\"#{x}({sect_ct}) {' '.join(corpus_sents_df.iloc[x-1]['sent_raw'].split()[:label_token_int])}\"; # \\nPolarity: {y:.2f}'\n",
        "\n",
        "      # Save Crux point in crux_points_dt Dictionary if plotting Cruxes for a single/specific Model\n",
        "      crux_full_str = ' '.join(corpus_sents_df.iloc[x]['sent_raw'].split())\n",
        "      crux_points_dt[x] = [y, crux_full_str]\n",
        "\n",
        "      plt.annotate(label,\n",
        "                   (x,y),\n",
        "                   textcoords='offset points',\n",
        "                   xytext=(0,10),\n",
        "                   ha='center',\n",
        "                   rotation=90)\n",
        "      sect_ct += 1\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment ({model_name_full.capitalize()})\\n{subtitle_str}', x=title_xpos, y=title_ypos);\n",
        "    # Plot\n",
        "    plt.plot(midpoints_ls, midpoints_sentiment_ls, marker=\"o\", ms=6) # , markevery=[0,1])\n",
        "\n",
        "  else:\n",
        "    # If plotting multiple models\n",
        "    model_names_str = 'Multiple Models'\n",
        "    for i, model_name_full in enumerate(model_names_ls):\n",
        "      # Error check and assign correct model names\n",
        "      model_name_root = model_name_full.split('_')[0]\n",
        "      if model_name_root in MODELS_LS:\n",
        "        # Plot\n",
        "        g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "        # g._legend.remove()\n",
        "        # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], marker=\"o\", alpha=0.5, label=model_name_full)\n",
        "      else:\n",
        "        print(f'ERROR: model_names_ls[]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n",
        "        return -1\n",
        "\n",
        "      # Plot\n",
        "      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "      # g._legend.remove()\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment (Standardized Models)\\n{subtitle_str}', x=title_xpos, y=title_ypos)\n",
        "\n",
        "  # plt.legend(loc='best');\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    models_names_ls = [x[:2] for x in model_names_ls]\n",
        "    models_names_str = ''.join(models_names_ls)\n",
        "    plot_filename = f'plot_cruxes_{semantic_type}_{models_names_str}_{models_names_str}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return crux_points_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUNMIlJKHyz3"
      },
      "source": [
        "def plot_histogram(model_name='vader', text_unit='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a model, text_unit\n",
        "  Plot a Histogram using the default DataFrame\n",
        "  '''\n",
        "\n",
        "  if text_unit == 'sentence':\n",
        "    ts_df = corpus_sents_df\n",
        "\n",
        "  elif text_unit == 'paragraph':\n",
        "    ts_df = corpus_parags_df\n",
        "\n",
        "  elif text_unit == 'section':\n",
        "    ts_df = corpus_sects_df\n",
        "\n",
        "  elif text_unit == 'chapter':\n",
        "    ts_df = corpus_chaps_df\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n",
        "\n",
        "  sns.histplot(ts_df[model_name], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram {text_unit.capitalize()} Sentiment (Model {model_name.capitalize()})')\n",
        "  # get_smas(ts_df, model_name=model_name, text_unit=text_unit, win_ls=wins_def_ls)\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_hist_{text_unit}_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGTkfsSWFCeO"
      },
      "source": [
        "# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n",
        "\n",
        "def plot_raw_sections(ts_df='corpus_sents_df', model_name='vader', semantic_type='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, model_name column, semantic_type \n",
        "  Plot the raw sentiment types\n",
        "  Options to save2file\n",
        "  ''' \n",
        "  \n",
        "  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n",
        "  sns.lineplot(data=ts_df, x='sect_no', y=model_name, alpha=0.5).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_nostand_sects_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmrtifYoIjOT"
      },
      "source": [
        "# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n",
        "\n",
        "def plot_raw_sentiments(model_name='vader', semantic_type='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, model_name column, semantic_type \n",
        "  Plot the raw sentiment types\n",
        "  Options to save2file\n",
        "  ''' \n",
        "  \n",
        "  if semantic_type == 'sentence':\n",
        "    ts_df = corpus_sents_df\n",
        "    x_units = 'sent_no'\n",
        "  elif semantic_type == 'paragraph':\n",
        "    ts_df = corpus_parags_df\n",
        "    x_units = 'parag_no'\n",
        "  elif (semantic_type == 'section') | (semantic_type == 'section_stand'):\n",
        "    ts_df = corpus_sects_df\n",
        "    x_units = 'sect_no'\n",
        "  elif (semantic_type == 'chapter') | (semantic_type == 'chapter_stand'):\n",
        "    ts_df = corpus_chaps_df\n",
        "    x_units = 'chap_no'\n",
        "    \n",
        "  else:\n",
        "    print(f'ERROR: {semantic_type} must be sentence, paragraph or section')\n",
        "\n",
        "\n",
        "  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n",
        "  sns.lineplot(data=ts_df, x=x_units, y=model_name, alpha=0.5, label=model_name).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n",
        "  \n",
        "  plt.legend(loc='best')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_raw_sentiments_{semantic_type}_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB8ibstaeiVd"
      },
      "source": [
        "# TODO: must plot in order to save, cannot save without first plotting\n",
        "\n",
        "def get_lowess(ts_df='corpus_parags_df', models_ls=MODELS_LS, text_unit='paragraph', plot_subtitle='', alabel='', afrac=1./10, ait=5, alpha=0.5, do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n",
        "  Return a DataFrame with LOWESS values\n",
        "  If 'plot=True', also output plot\n",
        "  '''\n",
        "\n",
        "  # global corpus_all_df\n",
        "\n",
        "  lowess_df = pd.DataFrame()\n",
        "\n",
        "  # Step 1: Calculate LOWESS smoothed values\n",
        "  for i,acol in enumerate(models_ls):\n",
        "    sm_x, sm_y = sm_lowess(endog=ts_df[acol].values, exog=ts_df.index.values, frac=afrac, it=ait, return_sorted = True).T\n",
        "    col_new = f'{acol}_lowess'\n",
        "    lowess_df[col_new] = pd.Series(sm_y)\n",
        "    # Optionally plot LOWESS for all models\n",
        "    if do_plot:\n",
        "      if alabel == '':\n",
        "        alabel == acol\n",
        "      plt.plot(sm_x, sm_y, label=alabel, alpha=alpha, linewidth=2)\n",
        "\n",
        "  lowess_df['median'] = lowess_df.median(axis=1) # sm_y # corpus_all_df[df_cols_ls].median(axis=1)\n",
        "  \n",
        "  # Step 2: Optionally plot LOWESS for median\n",
        "  if do_plot:\n",
        "    # sm_x, sm_y = sm_lowess(endog=lowess_df.median, exog=lowess_df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n",
        "    # plt.plot(sm_x, sm_y, label='median', alpha=0.9, linewidth=2, color='black')\n",
        "    \n",
        "    frac_str = str(round(100*afrac))\n",
        "    plt.title(f'{CORPUS_FULL} \\n {plot_subtitle} {text_unit} Standardized Sentiment Smoothed with LOWESS (frac={frac_str})')\n",
        "    plt.legend(title='Sentiment Model')\n",
        "\n",
        "  # Step 3: Optionally save to file\n",
        "  if save2file:\n",
        "    # Save Plot to file.\n",
        "    plot_filename = f'plot_{text_unit}_lowess_{plot_subtitle.split()[0].lower()}_{author_abbr_str}_{title_str}.png'\n",
        "    # plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plot_filename, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "\n",
        "  return lowess_df\n",
        "\n",
        "# Test\n",
        "'''\n",
        "new_lowess_col = f'{sa_model}_lowess'\n",
        "my_frac = 1./10\n",
        "my_frac_per = round(100*my_frac)\n",
        "new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n",
        "corpus_all_df[new_lowess_col] = plot_lowess(corpus_all_df, [sa_model], afrac=my_frac)\n",
        "corpus_all_df.head()\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cANCC2iz6nwo"
      },
      "source": [
        "def get_sent2dets(sent_no):\n",
        "  '''\n",
        "  Given a Sentence Number\n",
        "  Return the corresponding Paragraph, Section and Chapter Numbers that contain it\n",
        "  '''\n",
        "\n",
        "  # Get Paragraph No containing given Sentence No\n",
        "  sent_parag_no = int(corpus_sents_df[corpus_sents_df['sent_no']==sent_no]['parag_no'])\n",
        "\n",
        "  # Get Section No containing given Sentence No.\n",
        "  corpus_sects_ls = list(corpus_sects_df['sect_no'])\n",
        "  for asect_no in corpus_sects_ls:\n",
        "    if (int(corpus_sects_df[corpus_sects_df['sect_no'] == asect_no]['sent_no_start']) > sent_no):\n",
        "      break\n",
        "    sent_sect_no = asect_no\n",
        "    # print(f'asect={asect_no}')\n",
        "\n",
        "  # Get Chapter No containing given Sentence No.\n",
        "  corpus_chaps_ls = list(corpus_chaps_df['chap_no'])\n",
        "  for achap_no in corpus_chaps_ls:\n",
        "    if (int(corpus_chaps_df[corpus_chaps_df['chap_no'] == achap_no]['sent_no_start']) > sent_no):\n",
        "      break\n",
        "    sent_chap_no = achap_no\n",
        "    # print(f'achap={achap_no}')\n",
        "\n",
        "\n",
        "  return sent_parag_no, sent_sect_no, sent_chap_no\n",
        "\n",
        "# Test\n",
        "# sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(1408)\n",
        "# print(f'sent_parag_no={sent_parag_no}\\nsent_sect_no={sent_sect_no}\\nsent_chap_no={sent_chap_no}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6liffwhYtSw"
      },
      "source": [
        "# get_sentnocontext_report(the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sijR4OknJive"
      },
      "source": [
        "def get_sentnocontext(sent_no=1, n_sideparags=1, sent_highlight=True):\n",
        "  '''\n",
        "  Given a sentence number in the Corpus\n",
        "  Return the containing paragraph and n-paragraphs on either side\n",
        "  (e.g. if n=2, return 2+1+2=5 paragraphs)\n",
        "  '''\n",
        "\n",
        "  parag_target_no = int(corpus_sents_df[corpus_sents_df['sent_no'] == sent_no]['parag_no'])\n",
        "  # print(f'parag_target_no = {parag_target_no} and type: {type(parag_target_no)}')\n",
        "\n",
        "  if n_sideparags == 0:\n",
        "    parags_context_ls = list(corpus_parags_df[corpus_parags_df['parag_no'] == parag_target_no]['parag_raw'])\n",
        "\n",
        "  else:\n",
        "    parag_start = parag_target_no - n_sideparags\n",
        "    parag_end = parag_target_no + n_sideparags + 1\n",
        "    parags_context_ls = list(corpus_parags_df.iloc[parag_start:parag_end]['parag_raw'])\n",
        "\n",
        "\n",
        "  if sent_highlight == True:\n",
        "    parag_match_str = str(parags_context_ls[n_sideparags])\n",
        "    # print(f'parag_match_str:\\n  {parag_match_str}')\n",
        "    sent_idx = sent_no\n",
        "    sent_str = (corpus_sents_df[corpus_sents_df['sent_no']==sent_idx]['sent_raw'].values)[0]\n",
        "    sent_str_up = sent_str.upper()\n",
        "    # print(f'sent_str:\\n  {sent_str}')\n",
        "    # parags_context_ls[n_sideparags] \n",
        "    parags_context_ls[n_sideparags] = parag_match_str.replace(sent_str, sent_str_up)\n",
        "\n",
        "  return parags_context_ls\n",
        "\n",
        "# Te\n",
        "# context_highlighted = get_sentnoparags(sent_no=1051, n_sideparags=1)\n",
        "# print(context_highlighted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM_I8uDfJztH"
      },
      "source": [
        "def get_sentnocontext_report(the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n",
        "  '''\n",
        "  Wrapper function around  get_sentnocontext()\n",
        "  Prints a nicely formatted context report\n",
        "  '''\n",
        "\n",
        "  context_noparags = the_n_sideparags*2+1\n",
        "\n",
        "  # print('-------------------------------------------------------------')\n",
        "  print(f'The {context_noparags} Paragraph(s) Context around the Sentence #{the_sent_no} Crux Point:')\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f\"\\nCrux Sentence #{the_sent_no} Raw Text: -------------------------------\\n\\n    {str(corpus_sents_df[corpus_sents_df['sent_no'] == the_sent_no]['sent_raw'].values[0])}\\n\") # iloc[the_sent_no]['sent_raw']}\")\n",
        "\n",
        "  sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(the_sent_no)\n",
        "  print(f\"\\nCrux Sentence #{the_sent_no} is Contained in: ---------------------------\\n\\n    Paragraph #{sent_parag_no}\\n      Section #{sent_sect_no}\\n      Chapter #{sent_chap_no}\\n\")\n",
        "\n",
        "  print(f\"\\n{context_noparags} Paragraph(s) Context: ------------------------------\")\n",
        "  context_parags_ls = get_sentnocontext(sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_len = len(context_parags_ls)\n",
        "  context_mid = context_len//2\n",
        "  for i, aparag in enumerate(context_parags_ls):\n",
        "    if i==context_mid:\n",
        "      # print(f'\\n>>> Paragraph #{i}: <<< Crux Point Sentence CAPITALIZED within this Paragraph\\n\\n    {aparag}')\n",
        "      print(f'\\n<*> {aparag}')\n",
        "    else:\n",
        "      # print(f'\\n    Paragraph #{i}:\\n\\n    {aparag}')\n",
        "      print(f'\\n    {aparag}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# get_sentnocontext_report(sent_no=1051, n_sideparags=1, sent_highlight=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y04GiohGypNX"
      },
      "source": [
        "def get_section_timeseries(sect_no):\n",
        "  '''\n",
        "  Given a Section No in the current Corpus\n",
        "  Return the start,mid and ending Sent No for this Section as well as the Sentiment Time Series between the start/end Sentence for this Section\n",
        "  '''\n",
        "  \n",
        "  section_count = corpus_sects_df.shape[0]\n",
        "\n",
        "  # Compute the start, mid and end Sentence numbers for the selected Section\n",
        "  if Select_Section_No >= section_count:\n",
        "    print(f'ERROR: You picked Section #{Select_Section_No}.\\n  Section for this Corpus must be between 0 and {section_count-1}')\n",
        "    return -1\n",
        "\n",
        "  else:\n",
        "\n",
        "    # Get the starting and middle Sentence No of this Section\n",
        "    sect_sent_start = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_start'].values)\n",
        "    # sect_sent_mid = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_mid'].values)\n",
        "\n",
        "    # Calculate last Sentence No of this Section\n",
        "    if Select_Section_No == (section_count-1):   \n",
        "      print(f'You selected the last Section of this Corpus')\n",
        "      sect_sent_end = corpus_sents_df.shape[0] - 1\n",
        "    else:\n",
        "      sect_sent_end = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No+1]['sent_no_start'].values) # - 1\n",
        "      \n",
        "    print(f'Section #{sect_no}:----------')\n",
        "    print(f'\\nsect_sent_start: {sect_sent_start}')\n",
        "    # print(f'sect_sent_mid: {sect_sent_mid}')\n",
        "    print(f'sect_sent_end: {sect_sent_end}')\n",
        "\n",
        "\n",
        "  # Comput the start, and end Paragraph numbers for the selected Section\n",
        "  sect_parag_start = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_start]['parag_no'].values)\n",
        "  sect_parag_end = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_end]['parag_no'].values)\n",
        "\n",
        "  print(f'\\nsect_parag_start: {sect_parag_start}')\n",
        "  print(f'sect_parag_end: {sect_parag_end}')\n",
        "\n",
        "\n",
        "  # Extract and Return both a Sentence and Paragraph DataFrame for this Section \n",
        "\n",
        "  section_sents_df = corpus_sents_df.iloc[sect_sent_start:sect_sent_end]\n",
        "\n",
        "  section_parags_df = corpus_parags_df.iloc[sect_parag_start:sect_parag_end]\n",
        "\n",
        "\n",
        "  return section_sents_df, section_parags_df\n",
        "\n",
        "# Test\n",
        "\n",
        "# section_sents_df, section_parags_df = get_section_timeseries(Select_Section_No)\n",
        "\n",
        "# section_sents_df.head()\n",
        "\n",
        "# print(f'\\nsection_sents_df.shape: {section_sents_df.shape}')\n",
        "# print(f'section_parags_df.shape: {section_parags_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Et56z3-Jr1E"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_crux_points(col_series, semantic_type='sentence', win_lowess=5, do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  crux_ls = []\n",
        "\n",
        "  if semantic_type == 'sentence':\n",
        "    ts_df = corpus_sents_df\n",
        "    x_units = 'sent_no'\n",
        "  elif semantic_type == 'paragraph':\n",
        "    ts_df = corpus_parags_df\n",
        "    x_units = 'parag_no'\n",
        "  elif (semantic_type == 'section') | (semantic_type == 'section_stand'):\n",
        "    ts_df = corpus_sects_df\n",
        "    x_units = 'sect_no'\n",
        "  elif (semantic_type == 'chapter') | (semantic_type == 'chapter_stand'):\n",
        "    ts_df = corpus_chaps_df\n",
        "    x_units = 'chap_no'\n",
        "    \n",
        "  else:\n",
        "    print(f'ERROR: {semantic_type} must be sentence, paragraph or section')\n",
        "\n",
        "\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "\n",
        "  series_no_min = ts_df[x_units].min()\n",
        "  seires_no_max = ts_df[x_units].max()\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values\n",
        "\n",
        "  half_win = int((win_lowess/100)*series_len)\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win) # np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL}\\nRaw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig('argrelextrema.png')\n",
        "\n",
        "  return crux_coord_ls\n",
        "\n",
        "\n",
        "  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n",
        "  sns.lineplot(data=ts_df, x=x_units, y=model_name, alpha=0.5, label=model_name).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n",
        "  \n",
        "  plt.legend(loc='best')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_raw_sentiments_{semantic_type}_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgFMgdQ3X33F"
      },
      "source": [
        "\"\"\"\n",
        "def get_lowess_cruxes(ts_df, col_series, text_type='sentence', win_lowess=5, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "\n",
        "  sent_no_min = ts_df.sent_no.min()\n",
        "  sent_no_max = ts_df.sent_no.max()\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values\n",
        "\n",
        "  half_win = int((win_lowess/100)*series_len)\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    if text_type == 'sentence':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(sent_no, sec_y_height, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "    elif text_type == 'paragraph':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(aparag_no, sec_y_height, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(aparag_no, color='blue', alpha=0.1)    \n",
        "    else:\n",
        "      print(f\"ERROR: text_type is {text_type} but must be either 'sentence' or 'paragarph'\")\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} Raw Sentence Crux Detection in Section #{Select_Section_No}\\nLOWESS Smoothed {subtitle_str} and SciPy find_peaks')\n",
        "    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig('argrelextrema.png')\n",
        "\n",
        "  return crux_coord_ls\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAPVrHeyeX7T"
      },
      "source": [
        "\"\"\"\n",
        "def get_crux_points(ts_df, col_series, text_type='sentence', win_per=5, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "\n",
        "  sent_no_min = ts_df.sent_no.min()\n",
        "  sent_no_max = ts_df.sent_no.max()\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values\n",
        "\n",
        "  half_win = int((win_per/100)*series_len)\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    if text_type == 'sentence':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(sent_no, sec_y_height, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "    elif text_type == 'paragraph':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(aparag_no, sec_y_height, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(aparag_no, color='blue', alpha=0.1)    \n",
        "    else:\n",
        "      print(f\"ERROR: text_type is {text_type} but must be either 'sentence' or 'paragarph'\")\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} Raw Sentence Crux Detection in Section #{Select_Section_No}\\nLOWESS Smoothed {subtitle_str} and SciPy find_peaks')\n",
        "    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig('argrelextrema.png')\n",
        "\n",
        "  return crux_coord_ls\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv376c5_bfrg"
      },
      "source": [
        "def crux_sortsents(crux_ls, atop_n=3, get_peaks=True, sort_key='sent_no'):\n",
        "  '''\n",
        "  Given a list of tuples (sent_no, sentiment value), atop_n cruxes to retrieve and bool flag get_peaks\n",
        "  Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n",
        "  '''\n",
        "  # print(f'Entered crux_sortsents with crux_ls={crux_ls}\\natop_n={atop_n}')\n",
        "\n",
        "  crux_old_ls = []\n",
        "  crux_new_ls = []\n",
        "\n",
        "  if sort_key == 'sent_no':\n",
        "    crux_old_ls = sorted(crux_ls, key=lambda tup: (tup[0]))\n",
        "  else:\n",
        "    crux_old_ls = sorted(crux_ls, key=lambda tup: (tup[1]), reverse=get_peaks)\n",
        "\n",
        "  \"\"\"\n",
        "  if get_peaks == True:\n",
        "    crux_old_ls = [x for x in crux_old_ls if x[1] > 0]\n",
        "  else:\n",
        "    crux_old_ls = [x for x in crux_old_ls if x[1] < 0]\n",
        "  \"\"\";\n",
        "\n",
        "  # Return only the n_top cruxes if more cruxes than n_top else return all cruxes\n",
        "  if (sort_key != 'sent_no') & (len(crux_old_ls) >= atop_n):\n",
        "    # trim crux list if user asked for less than total number\n",
        "    crux_old_ls = crux_old_ls[:atop_n]\n",
        "\n",
        "  # Retrieve the Sentence raw text for each Crux and add as Tuple(sent_no, sentiment_val, raw_text) to return List\n",
        "  for asent_no, asentiment_val in crux_old_ls:\n",
        "    asent_raw = str(corpus_sents_df[corpus_sents_df['sent_no'] == asent_no]['sent_raw'].values[0])\n",
        "    crux_new_ls.append((int(asent_no), float(f'{asentiment_val:.3f}'), str(asent_raw),)) # Append a Tuple to return List\n",
        "\n",
        "  return crux_new_ls\n",
        "\n",
        "# Test\n",
        "# crux_n_top_ls = crux_sortsents(section_crux_ls, atop_n=3, get_peaks=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFDjK1o8gnQ0"
      },
      "source": [
        "def crux_sortsents_report(crux_ls, library_type='baseline', top_n=3, get_peaks=True, sort_by='sent_no', n_sideparags=1, sentence_highlight=True):\n",
        "  '''\n",
        "  Wrapper function to produce report based upon 'crux_sortsents() described as:\n",
        "    Given a list of tuples (sent_no, sentiment value), top_n cruxes to retrieve and bool flag get_peaks\n",
        "    Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n",
        "\n",
        "    # get_sentnocontext_report\n",
        "  '''\n",
        "\n",
        "  if get_peaks == True:\n",
        "    crux_label = 'Peak'\n",
        "  else:\n",
        "    crux_label = 'Valley'\n",
        "\n",
        "  # Filter and keep only the desired crux type in List crux_subset_ls\n",
        "  crux_subset_ls = []\n",
        "  for acrux_tup in crux_ls:\n",
        "    crux_type, crux_x_coord, crux_y_coord = acrux_tup\n",
        "    if crux_type.lower() == crux_label.lower():\n",
        "      crux_subset_ls.append((crux_x_coord,crux_y_coord)) # Append a Tuple to List\n",
        "\n",
        "  flag_2few_cruxes = False\n",
        "\n",
        "  # Check to see if asked for more Cruxes than were found \n",
        "  top_n_found = len(crux_subset_ls)\n",
        "  if top_n_found < top_n:\n",
        "    flag_2few_cruxes = True\n",
        "    print(f'\\n\\nWARNING: You asked for {top_n} {crux_label}s\\n         but there only {top_n_found} were found above.\\n')\n",
        "    print(f'             Displaying as many {crux_label}s as possible,')\n",
        "    print(f'             to retrieve more, go back to the previous code cells and re-run with wider Crux Window.\\n\\n')\n",
        "\n",
        "\n",
        "  # Get Sentence no and raw text for appropriate Crux subset\n",
        "  # print(f'Calling crux_n_top_ls with crux_subset_ls={crux_subset_ls}\\ntop_n={top_n}\\nget_peaks={get_peaks}')\n",
        "  crux_n_top_ls = crux_sortsents(crux_ls=crux_subset_ls, atop_n=top_n, get_peaks=get_peaks, sort_key=sort_by)\n",
        "  # print(f'Returning crux_n_top_ls = {crux_n_top_ls}')\n",
        "\n",
        "  # Print appropriate header\n",
        "  print('------------------------------')\n",
        "  # print(f'library_type: {library_type}')\n",
        "  if library_type in ['baselines','sentimentr','syuzhetr','transformers']:\n",
        "    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n",
        "      print(f'Library: {library_type.capitalize()} ALL Top {top_n} {crux_label}s Found\\n')\n",
        "    else:\n",
        "      print(f'Library #{library_type.capitalize()} ONLY Top {top_n_found} {crux_label}s Found\\n')\n",
        "  else:\n",
        "    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n",
        "      print(f'Section #{Select_Section_No} ALL Top {top_n} {crux_label}s Found\\n')\n",
        "    else:\n",
        "      print(f'Section #{Select_Section_No} ONLY Top {top_n_found} {crux_label}s Found\\n')\n",
        "\n",
        "  # Print summary of subset Cruxes\n",
        "  for i,crux_sent_tup in enumerate(crux_n_top_ls):\n",
        "    # crux_type, crux_x_coord, crux_y_coord = crux_sent_tup\n",
        "    crux_x_coord, crux_y_coord, crux_sent_raw = crux_sent_tup\n",
        "    print(f'   {crux_label} #{i} at Sentence #{crux_x_coord} with Sentiment Value {crux_y_coord}')\n",
        "  # print('------------------------------\\n')\n",
        "  # print('Sent_No  Sentiment   Sentence (Raw Text)\\n')\n",
        "  \n",
        "  # Print details of each Crux in subset\n",
        "  for sent_no, sent_pol, sent_raw in crux_n_top_ls: \n",
        "    sent_no = int(sent_no)\n",
        "    print('\\n\\n-------------------------------------------------------------')\n",
        "    print(f'Sentence #{sent_no}   Sentiment: {sent_pol:.3f}\\n') #     {sent_raw}\\n')\n",
        "    # print('------------------------------')\n",
        "    get_sentnocontext_report(the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n",
        "    # get_sentnocontext(sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVQGritEnYSy"
      },
      "source": [
        "library_type='syuzhetr'\n",
        "if library_type in ['baseline','sentimentr','syuzhetr','transformers']:\n",
        "  print(\"It is IN\")\n",
        "else:\n",
        "  print(\"BOO\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJw2WDlwHH5y"
      },
      "source": [
        "# For the selected Section, create an expanded Paragraph DataFrame to match the number of Sentences in the Section\n",
        "\n",
        "def expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df', model_name='vader_lnorm_medianiqr'):\n",
        "  '''\n",
        "  Given a Corpus Paragraph DataFrame and a longer Sentence DataFrame that cover the same Section of a Corpus\n",
        "  Return an expanded version of the Paragraph DataFrame of equal length to the Sentence DataFrame so they can be plotted/compared along the same x-axis\n",
        "  '''\n",
        "\n",
        "  parag_sentiment_expanded_ls = []\n",
        "  parags_midpoint_ls = []\n",
        "  sent_sum = 0\n",
        "  parag_start = section_parags_df.parag_no.min()\n",
        "  print(f'parag_start: {parag_start}')\n",
        "  parag_end = section_parags_df.parag_no.max() + 1 # shape[0] + 3\n",
        "  print(f'parag_end: {parag_end}')\n",
        "  parags_range_ls = list(range(parag_start, parag_end, 1))\n",
        "  print(f'parags_range_ls: {parags_range_ls}')\n",
        "  for i, aparag_no in enumerate(parags_range_ls):\n",
        "    aparag_sentiment_fl = float(corpus_parags_df[corpus_parags_df['parag_no']==aparag_no][model_name])\n",
        "    sent_ct = len(corpus_sents_df[corpus_sents_df.parag_no == aparag_no])\n",
        "    parag_midpoint_int = int(sent_ct//2 + sent_sum)\n",
        "    parags_midpoint_ls.append(parag_midpoint_int)\n",
        "    for asent in range(sent_ct):\n",
        "      parag_sentiment_expanded_ls.append(aparag_sentiment_fl)\n",
        "    sent_sum += sent_ct\n",
        "    print(f'#{i}: Paragraph #{aparag_no} has {sent_ct} Sentences and Avg Sentiment: {aparag_sentiment_fl:.3f}')\n",
        "\n",
        "  print(f'\\nSentence Total: {sent_sum} vs Original section_sents_df: {section_sents_df.shape[0]}')\n",
        "  print(f'  Paragraph Sentiment length: {len(parag_sentiment_expanded_ls)}')\n",
        "\n",
        "  # section_sents_parags_df = section_sents_df.copy()\n",
        "  \n",
        "  # section_sents_parags_df.head(1);\n",
        "\n",
        "  # corpus_sents_df['']\n",
        "\n",
        "  return parag_sentiment_expanded_ls, parags_midpoint_ls\n",
        "\n",
        "# Test\n",
        "# section_sents_df['vader_lnorm_medianiqr_parag'] = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXRq54NQwI7D"
      },
      "source": [
        "def get_crux_points(ts_df, col_series, text_type='sentence', win_per=5, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  # print('entered get_crux_points')\n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "  # print(f'series_len = {series_len}')\n",
        "\n",
        "  sent_no_min = ts_df.sent_no.min()\n",
        "  sent_no_max = ts_df.sent_no.max()\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values.flatten()\n",
        "\n",
        "  half_win = int((win_per/100)*series_len)\n",
        "  # print(f'half_win = {half_win}')\n",
        "  # print(f'sm_y type = {type(sm_y)}')\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  # print(f'peak_indexes type = {type(peak_indexes)}')\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_x_adj_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  peak_label_ls = ['peak'] * len(peak_y_ls)\n",
        "  peak_coord_ls = tuple(zip(peak_label_ls, peak_x_adj_ls, peak_y_ls))\n",
        "\n",
        "  # peak_y_all_ls = peak_y_ls + valley_y_ls\n",
        "  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_x_adj_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  valley_label_ls = ['valley'] * len(valley_y_ls)\n",
        "  valley_coord_ls = tuple(zip(valley_label_ls, valley_x_adj_ls, valley_y_ls))\n",
        "\n",
        "  # Combine Peaks and Valley Coordinates into List of Tuples(label, x_coord, y_coord)\n",
        "  crux_coord_ls = peak_coord_ls + valley_coord_ls\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  #  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  #  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  #  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    section_sent_no_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "    section_no_ls = list(corpus_sects_df['sect_no'])\n",
        "    for i, asect_no in enumerate(section_sent_no_boundries_ls):\n",
        "      # Plot vertical lines for section boundries\n",
        "      plt.text(asect_no, sec_y_height, f'Section #{section_no_ls[i]}', alpha=0.2, rotation=90)\n",
        "      plt.axvline(asect_no, color='blue', alpha=0.1)    \n",
        "\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, annotation_clip=True) # xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} SMA Smoothed Sentence Sentiment Arcs Crux Detection\\n{subtitle_str} Models: {col_series}')\n",
        "    plt.xlabel(f'Sentence No') # within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n SMA Smoothed Sentence Sentiment Arcs Crux Points')\n",
        "    # plt.legend(loc='best')\n",
        "    plt.savefig(f\"{CORPUS_FILENAME.split('.')[0]}_find_peaks.png\")\n",
        "\n",
        "  return crux_coord_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNdJQGtghS_X"
      },
      "source": [
        "def get_standardscaler(series_name, values_ser):\n",
        "  '''\n",
        "  Given a Series of values\n",
        "  Return a list of StandardSclar transformations on that input Series\n",
        "  '''\n",
        "\n",
        "  scaler = StandardScaler()  \n",
        "\n",
        "  # Convert to np.array\n",
        "  values_np = np.array(values_ser)\n",
        "  \n",
        "  values_flat_np = values_np.reshape((len(values_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(values_flat_np)\n",
        "  print(f'Model: {series_name}\\n       Mean: {scaler.mean_}, StandardDeviation: {np.sqrt(scaler.var_)}') # % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  values_flat_xform_np = scaler.transform(values_flat_np)\n",
        "\n",
        "  return values_flat_xform_np.flatten().tolist()\n",
        "\n",
        "# Test\n",
        "# stdscaler_series_ls = get_standardscaler('vader_lnorm_medianiqr_roll100', corpus_sents_df['vader_lnorm_medianiqr_roll100'])\n",
        "# corpus_sents_df['vader_roll100_stdscaler'] = pd.Series(stdscaler_series_ls)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU3aHagiRjqR"
      },
      "source": [
        "# **Preprocess and Review Corpus Text (Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnWJCkqDhA5L"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "len(sections_ls)\n",
        "min(sections_ls, key=len) \n",
        "\n",
        "# TODO: Spell check and correct common OCR errors\n",
        "\n",
        "# SymSpellPy\n",
        "# JamSpell\n",
        "# OCR - https://github.com/Alvant/MIL-OCR\n",
        "\n",
        "# !pip install -U symspellpy\n",
        "\n",
        "# Did not need these\n",
        "# dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "\n",
        "\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# term_index is the column of the term and count_index is the\n",
        "# column of the term frequency\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# lookup suggestions for single-word input strings\n",
        "input_term = \"memebers\"  # misspelling of \"members\"\n",
        "input_term = \"summermorning\"\n",
        "# max edit distance per lookup\n",
        "# (max_edit_distance_lookup <= max_dictionary_edit_distance)\n",
        "suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST,\n",
        "                               max_edit_distance=2)\n",
        "# display suggestion term, term frequency, and edit distance\n",
        "for suggestion in suggestions:\n",
        "    print(suggestion)\n",
        "\n",
        "\n",
        "\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell\n",
        "\n",
        "# Set max_dictionary_edit_distance to avoid spelling correction\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# term_index is the column of the term and count_index is the\n",
        "# column of the term frequency\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# a sentence without any spaces\n",
        "input_term = \"thequickbrownfoxjumpsoverthelazydog\"\n",
        "input_term = \"summermorning\"\n",
        "result = sym_spell.word_segmentation(input_term)\n",
        "print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n",
        "                          result.log_prob_sum))\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQYjye-v9XMY"
      },
      "source": [
        "### **Get Corpus by Sections, Chapters, Paragraphs and Sentences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6qZH_qAVPQD"
      },
      "source": [
        "#### **Try to Automatically Detected File/Text Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4TH13cFUErD"
      },
      "source": [
        "# Try to automatically discover Corpus text Encoding scheme (default to 'utf-8', but often 'iso-8859-1', 'windows-1252', 'cp1252', or 'ascii')\n",
        "\n",
        "CORPUS_ENCODING = 'utf-8' # Python3 default encoding\n",
        "\n",
        "corpus_str, corpus_encode, encoding_confidence = get_file_encoding(CORPUS_FILENAME)\n",
        "CORPUS_ENCODING = str(corpus_encode).lower()\n",
        "\n",
        "if encoding_confidence > 0.6:\n",
        "  print(f'Setting file/text encoding to {CORPUS_ENCODING}\\n')\n",
        "  print(f\"    {encoding_confidence*100:.2f}% confidence Encoding = '{CORPUS_ENCODING}' for '{CORPUS_FILENAME}'\")\n",
        "else:\n",
        "  print(f\"ERROR: Less than 60% confidence estimating Encoding scheme for '{CORPUS_FILENAME}'\\n\")\n",
        "  print(f\"       Only {encoding_confidence*100:.2f}% confidence Encoding = '{CORPUS_ENCODING}'\")\n",
        "  print(f\"       Manually verify corpus file '{CORPUS_FILENAME}' encoding, set as GLOBAL_CONSTATANT and rerun\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVCkjat0vffd"
      },
      "source": [
        "#### **Get All (non-null) Raw Lines**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an7SzlAuvXA0"
      },
      "source": [
        "corpus_lines_ls, lines_raw_str = corpus2lines(corpus_filename)\n",
        "\n",
        "print(f'\\n\\nRead raw corpus lines: character count: {len(lines_raw_str)}')\n",
        "print(f'                        raw line count:  {len(corpus_lines_ls)}')\n",
        "\n",
        "line_ct = 10\n",
        "print(f'First {line_ct} raw lines: --------------------\\n')\n",
        "for i,aline in enumerate(corpus_lines_ls[:line_ct]):\n",
        "  print(f'Line #{i}:\\n    {aline}')\n",
        "print(f'\\n\\nLast {line_ct} raw lines: -------------------\\n')\n",
        "for i,aline in enumerate(corpus_lines_ls[-line_ct:]):\n",
        "  print(f'Line #{i}: {aline}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgL9-4Z59gND"
      },
      "source": [
        "#### **Get Chapters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtV6MuLK9Osr"
      },
      "source": [
        "# Verify Corpus filename/contents\n",
        "\n",
        "corpus_filename\n",
        "print('\\n')\n",
        "!ls ddefoe*.txt\n",
        "print('\\n')\n",
        "!head -n 5 $corpus_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EORwizvE3ghU"
      },
      "source": [
        "corpus_chaps_ls[1][:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qbbunLuzJWH"
      },
      "source": [
        "corpus_chaps_ls, corpus_raw_str = corpus2chaps(corpus_filename)\n",
        "\n",
        "corpus_chaps_ls[0][:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDhm8w9g9Op2"
      },
      "source": [
        "corpus_chaps_ls, corpus_raw_str = corpus2chaps(corpus_filename)\n",
        "\n",
        "corpus_chaps_ls[0][:500]\n",
        "\n",
        "ch_no=1\n",
        "# search_term = 'CHAPTER'\n",
        "\n",
        "print(f'Chapter #{ch_no} Beginning -----')\n",
        "# corpus_chaps_ls[ch_no][:500]\n",
        "corpus_chaps_ls[0][:500]\n",
        "print('\\n')\n",
        "\n",
        "print(f'Chapter #{ch_no} Ending -----')\n",
        "corpus_chaps_ls[ch_no][-500:]\n",
        "print('\\n')\n",
        "\n",
        "print(f'Any {search_term} found?')\n",
        "search_term in corpus_chaps_ls[ch_no]\n",
        "\n",
        "\"\"\"\n",
        "print('\\n\\nAFTER ----------')\n",
        "print(f'len(corpus_raw_str): {len(corpus_raw_str)}')\n",
        "print(\"\\n\\n-----\")\n",
        "print(f'len(corpus_chaps_ls): {len(corpus_chaps_ls)}')\n",
        "print(\"\\n\\n-----\")\n",
        "\n",
        "\n",
        "print(f'corpus_chaps_ls[0]:\\n\\n    {corpus_chaps_ls[0]}')\n",
        "print(\"\\n\\n-----\")\n",
        "print(f'corpus_chaps_ls[1]:\\n\\n    {corpus_chaps_ls[1]}')\n",
        "print(\"\\n\\n-----\")\n",
        "print(f'corpus_chaps_ls[2]:\\n\\n    {corpus_chaps_ls[2]}')\n",
        "\n",
        "print(\"\\n\\n-----\")\n",
        "print(f'corpus_chaps_ls[-2]:\\n\\n    {corpus_chaps_ls[-2]}')\n",
        "print(\"\\n\\n-----\")\n",
        "print(f'corpus_chaps_ls[-1]:\\n\\n    {corpus_chaps_ls[-1]}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vllUrlrdhpVw"
      },
      "source": [
        "print(corpus_chaps_ls[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll0GJs3u9dEq"
      },
      "source": [
        "#### **Get Sections**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4xEj611WvH-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJjRq60kQpa7"
      },
      "source": [
        "# Verify Corpus filename/contents\n",
        "\n",
        "corpus_filename\n",
        "print('\\n')\n",
        "!ls ddefoe*.txt\n",
        "print('\\n')\n",
        "!head -n 5 $corpus_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwiZTA-r6i-y"
      },
      "source": [
        "if SECTION_HEADINGS != 'None':\n",
        "  # encoding = 'windows-1252', 'utf-8', 'cp1252', 'iso-8859-1'\n",
        "  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n",
        "  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n",
        "  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n",
        "  with open(corpus_filename, \"r\", encoding='utf-8') as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  len(corpus_raw_str)\n",
        "\n",
        "  # Extract and process Sections from Corpus\n",
        "  corpus_sects_ls, corpus_str_raw = corpus2sects(corpus_filename)\n",
        "\n",
        "  print('\\n\\nAFTER ----------')\n",
        "  print(f'len(corpus_raw_str): {len(corpus_raw_str)}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'len(corpus_sects_ls): {len(corpus_sects_ls)}\\n\\n')\n",
        "  \"\"\"\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[0]:\\n\\n    {corpus_sects_ls[0]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[1]:\\n\\n    {corpus_sects_ls[1]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[2]:\\n\\n    {corpus_sects_ls[2]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[-2]:\\n\\n    {corpus_sects_ls[-2]}')\n",
        "  print(\"\\n\\n-----\")\n",
        "  print(f'corpus_sects_ls[-1]:\\n\\n    {corpus_sects_ls[-1]}')\n",
        "\"\"\";\n",
        "\n",
        "else:\n",
        "  print(f'No Sections within Chapters for {CORPUS_FULL}\\n\\n    Using Chapters as Sections.')\n",
        "  # If no Sections, then just copy Chapters to Sections as filler\n",
        "\n",
        "  if SECTION_HEADINGS == 'None':\n",
        "    corpus_sects_ls = [x for x in corpus_chaps_ls];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2l6eIDV6i8O"
      },
      "source": [
        "# Verify no CHAPTER headings remain\n",
        "if SECTION_HEADINGS != 'None':\n",
        "  for i,aline in enumerate(corpus_sects_ls):\n",
        "    if aline.strip().startswith('CHAPTER '):\n",
        "      print(f'CHAPTER aline: {aline}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPfffj2i6i5y"
      },
      "source": [
        "# Verify\n",
        "if SECTION_HEADINGS != 'None':\n",
        "  print(f'len(corpus_sects_ls): {len(corpus_sects_ls)}')\n",
        "  print(corpus_sects_ls[0][:500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0JYdtsuUlb3"
      },
      "source": [
        "# Verify Chapter and Section Counts\n",
        "\n",
        "print(f'CHAPTER Count: len(corpus_chaps_ls): {len(corpus_chaps_ls)}')\n",
        "print(f'SECTION Count: len(corpus_sects_ls): {len(corpus_sects_ls)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N9PUl0V9orH"
      },
      "source": [
        "#### **Get Paragraphs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz8tvxAc9OnA"
      },
      "source": [
        "# Read corpus into a single string then split into paragraphs\n",
        "\n",
        "corpus_parags_ls, corpus_raw_str = corpus2parags(CORPUS_FILENAME)\n",
        "print(f'Found #{len(corpus_parags_ls)} paragraphs\\n')\n",
        "\n",
        "print('\\nThe first 10 Paragraphs of the Corpus:')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_ls[:10]\n",
        "\n",
        "print('\\n\\nThe last 10 Paragraphs of the Corpus:')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_ls[-20:]\n",
        "print('\\n')\n",
        "\n",
        "n_shortest = 10\n",
        "print(f'The {n_shortest} shortest Paragraphs in the Corpus are:')\n",
        "print('--------------------------------------------')\n",
        "temp_parags_ls = sorted(corpus_parags_ls, key=lambda x: (len(x), x))\n",
        "for i, asent in enumerate(temp_parags_ls[:n_shortest]):\n",
        "  print(f'Shortest Paragraph #{i}: {asent}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1oOpMwK9qqj"
      },
      "source": [
        "#### **Get Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBosd46UOGFC"
      },
      "source": [
        "len(corpus_parags_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdbvVHsp9OiG"
      },
      "source": [
        "corpus_sents_ls = parag2sents(corpus_parags_ls)\n",
        "\n",
        "print(f'Found {len(corpus_sents_ls)} Sentences in Corpus\\n')\n",
        "\n",
        "print(f'    First List Object in Sentence List {corpus_sents_ls[0]}\\n')\n",
        "\n",
        "print(f'    Last List Object in Sentence List {corpus_sents_ls[-1]}\\n');\n",
        "\n",
        "print(f\"List Object format: ['sent_no', 'parag_no', 'sent_raw']\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJiMNdPNXxEu"
      },
      "source": [
        "len(corpus_sents_ls)\n",
        "print('\\n')\n",
        "corpus_sents_ls[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaJn88wwXCkH"
      },
      "source": [
        "# junk_str = pattern_test.match('fjdskldf SECTION djfdksl')\n",
        "# junk_str\n",
        "# pattern_test = re.compile('SECTION')\n",
        "# print([line for line in corpus_sents_ls if pattern_test.search(str(line))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JdQ3aZu_OGm"
      },
      "source": [
        "### **Create DataFrames**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCGQN51f_qjB"
      },
      "source": [
        "#### **Create Sentence DataFrame: [corpus_sents_df]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg0oqhsBKKu-"
      },
      "source": [
        "# TODO: Encoding wrong?\n",
        "#   '...in her hair. He had hold of her bag' (In Section 0, around Sentence 133)\n",
        "#   encoding read and/or nltk.sent_tokenizer() turns into\n",
        "#   '...in her hair-He had hold of her bag.'\n",
        "\n",
        "# print(corpus_sents_df.iloc[133]['sent_raw'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR8pxTKs9Oe3"
      },
      "source": [
        "# Create Corpus Sentence DataFrame\n",
        "\n",
        "corpus_sents_df = pd.DataFrame(corpus_sents_ls)\n",
        "corpus_sents_df.columns = ['sent_no', 'parag_no', 'sent_raw']\n",
        "corpus_sents_df['sent_raw'] = corpus_sents_df['sent_raw'].astype('string')\n",
        "# Double check to drop any rows where raw Sentence is NaN or empty string ''\n",
        "corpus_sents_df.dropna(subset=['sent_raw'], inplace=True)\n",
        "\n",
        "\n",
        "print(f'First 10 Sentences of {CORPUS_FULL}')\n",
        "corpus_sents_df.head(10)\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n26-rdgzSfHw"
      },
      "source": [
        "corpus_sents_df['sent_raw'].str.contains('SECTION').sum()\n",
        "print('\\n')\n",
        "corpus_sents_df[corpus_sents_df['sent_raw'].str.contains('SECTION')] # ['sent_no']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgezNV5b_hNJ"
      },
      "source": [
        "#### **Create Paragraph DataFrame: [corpus_parags_df]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dSDvcM1_Tky"
      },
      "source": [
        "# Create Corpus Paragraph DataFrame\n",
        "\n",
        "parag_no_ls = []\n",
        "parag_raw_ls = []\n",
        "\n",
        "corpus_parags_df = pd.DataFrame()\n",
        "\n",
        "for i, aparag in enumerate(corpus_parags_ls):\n",
        "  parag_no_ls.append(i)\n",
        "  parag_raw_ls.append(aparag)\n",
        "\n",
        "corpus_parags_df = pd.DataFrame(\n",
        "    {'parag_no': parag_no_ls,\n",
        "     'parag_raw': parag_raw_ls,\n",
        "    })\n",
        "\n",
        "# Double check to drop any rows where raw Paragraph is NaN or empty string ''\n",
        "corpus_parags_df.dropna(subset=['parag_raw'], inplace=True)\n",
        "corpus_parags_df['parag_raw'] = corpus_parags_df['parag_raw'].astype('string')\n",
        "\n",
        "# Test \n",
        "print(f'First 10 Paragraphs of {CORPUS_FULL}')\n",
        "corpus_parags_df.head(10)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2vVN-Nu_tcZ"
      },
      "source": [
        "#### **Create Section DataFrame: [corpus_sects_df]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia4l5kj6j0aV"
      },
      "source": [
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng3MyvFmgk21"
      },
      "source": [
        "corpus_sects_df.shape # ['sect_raw']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGWa3mtCFWxy"
      },
      "source": [
        "\"\"\"\n",
        "corpus_sects_df = pd.DataFrame(corpus_sects_ls)\n",
        "corpus_sects_df.columns = ['sect_raw']\n",
        "corpus_sects_df['sect_raw'] = corpus_sects_df['sect_raw'].astype('string')\n",
        "# Double check to drop any rows where raw Sentence is NaN or empty string ''\n",
        "# corpus_sents_df.dropna(subset=['sent_raw'], inplace=True)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28dEnH3Zr3PO"
      },
      "source": [
        "# corpus_sects_df.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW6Wdq3PJXM2"
      },
      "source": [
        "corpus_sents_df.iloc[133]['sent_raw']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SadITC15sBB5"
      },
      "source": [
        "# corpus_sects_df.iloc[0]['sect_raw'][-20:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo3ry1cjQ6kM"
      },
      "source": [
        "len(corpus_sects_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcizVPogRM4D"
      },
      "source": [
        "# print(corpus_sects_ls[19][:500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2j6r-8n_Tev"
      },
      "source": [
        "# Create Corpus Section DataFrame\n",
        "\n",
        "sect_no_ls = []\n",
        "sect_raw_ls = []\n",
        "\n",
        "# corpus_sects_df = pd.DataFrame()\n",
        "\n",
        "# Filter out all the CHAPTER [\\d]{1,2} lines\n",
        "corpus_sects_noheaders_ls = []\n",
        "pattern = r'CHAPTER [IVX]{,6}[.]*[\\\\n]*'\n",
        "\n",
        "\"\"\"\n",
        "for i,asect in enumerate(corpus_sects_ls):\n",
        "  if re.search(pattern, asect) == None:\n",
        "    corpus_sects_noheaders_ls.append(asect)\n",
        "corpus_sects_ls = corpus_sects_noheaders_ls\n",
        "\"\"\";\n",
        "\n",
        "for i, asect in enumerate(corpus_sects_ls):\n",
        "  sect_no_ls.append(i)\n",
        "  sect_raw_ls.append(asect)\n",
        "\n",
        "\n",
        "corpus_sects_df = pd.DataFrame(\n",
        "    {'sect_no': sect_no_ls,\n",
        "     'sect_raw': sect_raw_ls,\n",
        "    })\n",
        "\n",
        "\n",
        "# Calculate the sentence number at the mid-point of each Section\n",
        "\n",
        "sect_mid_sentno_ls = []\n",
        "sect_start_sentno_ls = []\n",
        "sect_end_sentno_ls = []\n",
        "sect_sentno_base = 0\n",
        "for i, sect_text in enumerate(corpus_sects_df.sect_raw):\n",
        "  if len(sect_text) > MIN_SECT_LEN:\n",
        "    # Create list of Sentences by sent_tokenizing Section raw text string\n",
        "    sect_sents_ls = sent_tokenize(sect_text)\n",
        "    print(f'\\n    Section len(sect_sents_ls): {len(sect_sents_ls)}')\n",
        "\n",
        "    # Calc and save the sent_no that begins each Section\n",
        "    sect_start_sent = sect_sents_ls[0].strip()[:30]  # Match on the first 20 chars\n",
        "    print(f'For Section #{i} starting sentence: {sect_start_sent}')\n",
        "\n",
        "    # Fix to remove leading/trailing parenthesis that are being interpreted by Python\n",
        "    # sect_start_sent = sect_start_sent.replace('(','').replace(')','')\n",
        "    sect_start_sent = sect_start_sent.strip('()[] ')\n",
        "\n",
        "    sect_start_sentno = int(list(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(sect_start_sent, regex=False, )]['sent_no'])[0]) # Problems with Sentences beginning/endings with parenthenses\n",
        "    print(f'                 starting sent_no: {sect_start_sentno}')\n",
        "    sect_start_sentno_ls.append(int(sect_start_sentno))\n",
        "\n",
        "\n",
        "\n",
        "    # Calc and save the sent_no that ends each Section\n",
        "    sect_end_sent = sect_sents_ls[-1].strip()[:30]  # Match on the first 20 chars\n",
        "    print(f'For Section #{i} ending sentence: {sect_end_sent}')\n",
        "\n",
        "    # Fix to remove leading/trailing parenthesis that are being interpreted by Python\n",
        "    # sect_first_sent = sect_first_sent.replace('(','').replace(')','')\n",
        "    sect_end_sent = sect_end_sent.strip('()[]\" ')\n",
        "\n",
        "    print(f'(cleaned) sect_end_sent: {sect_end_sent}')\n",
        "    # Problems with Sentences beginning/endings with parenthenses\n",
        "    sect_end_sentno = int(list(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(sect_end_sent, regex=False)]['sent_no'])[0]) \n",
        "\n",
        "    print(f'                 ending sent_no: {sect_end_sentno}')\n",
        "    sect_end_sentno_ls.append(int(sect_end_sentno))\n",
        "\n",
        "\n",
        "\n",
        "    # Find Sentence No for the starting Sentence of each Section\n",
        "\n",
        "    # Find the Sentence No for the middle Sentence of each Section\n",
        "    # sect_sents_len = len(sect_sents_ls)\n",
        "\n",
        "    print(f'type(sect_start_sentno): {type(sect_start_sentno)}')\n",
        "    print(f'type(sect_end_sentno): {type(sect_end_sentno)}')\n",
        "\n",
        "    sect_mid_sentno = int((sect_end_sentno-sect_start_sentno)//2 + sect_start_sentno)\n",
        "    # print(f'Section #{i}: {len(sect_sents_ls)} Sentences, midpoint: {sect_mid_sentno}, cumulative midpoint: {sect_mid_sentno}')\n",
        "    sect_mid_sentno_ls.append(sect_mid_sentno)\n",
        "\n",
        "corpus_sects_df['sent_no_start'] = pd.Series(sect_start_sentno_ls)\n",
        "corpus_sects_df['sent_no_mid'] = pd.Series(sect_mid_sentno_ls)\n",
        "corpus_sects_df['sent_no_end'] = pd.Series(sect_end_sentno_ls)\n",
        "corpus_sects_df['sect_raw'] = corpus_sects_df['sect_raw'].astype('string')\n",
        "\n",
        "# Test \n",
        "print(f'First 2 Sections of {CORPUS_FULL}')\n",
        "# corpus_sects_df.head(2)\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPkn1d9ZDqqf"
      },
      "source": [
        "# Test boundaries\n",
        "\n",
        "print(f\"Corpus last sentence:\\n    {corpus_sents_df.iloc[0]['sent_raw']}\")\n",
        "print(f\"\\nCorpus last sentence:\\n    {corpus_sents_df.iloc[2228]['sent_raw']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-CkRGqhBesW"
      },
      "source": [
        "# Test search\n",
        "\n",
        "corpus_sents_df[corpus_sents_df['sent_raw'].str.contains('love')][['sent_no','sent_raw']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg6-jzBG_wKV"
      },
      "source": [
        "#### **Create Chapter DataFrame: [corpus_chaps_df]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDHOoTryNQ8d"
      },
      "source": [
        "# Create Corpus Section DataFrame\n",
        "\n",
        "# TODO: Consider turning this into a function to enable deeper nested hierarchies (Gen/Per/Auth/)\n",
        "\n",
        "# def get_chap_sents_boundaries(chap_str):\n",
        "#   '''\n",
        "#   Given a single text string for a Chapter\n",
        "#   Return the corpus_sents_df['sent_no'] for it's first and last Sentences\n",
        " #  '''\n",
        "\n",
        "chap_no_ls = []\n",
        "chap_raw_ls = []\n",
        "\n",
        "\n",
        "\n",
        "# Filter out all the CHAPTER and SECTION header lines\n",
        "corpus_sects_noheaders_ls = []\n",
        "pattern = r'CHAPTER [IVX]{,6}[.]*[\\\\n]*'\n",
        "\n",
        "\n",
        "for i, achap in enumerate(corpus_chaps_ls):\n",
        "  chap_no_ls.append(i)\n",
        "  chap_raw_ls.append(achap)\n",
        "\n",
        "\n",
        "corpus_chaps_df = pd.DataFrame(\n",
        "    {'chap_no': chap_no_ls,\n",
        "     'chap_raw': chap_raw_ls,\n",
        "    })\n",
        "\n",
        "\n",
        "# Calculate the sentence number at the mid-point of each Section\n",
        "\n",
        "chap_mid_sentno_ls = []\n",
        "chap_start_sentno_ls = []\n",
        "chap_end_sentno_ls = []\n",
        "chap_sentno_base = 0\n",
        "for i, chap_text in enumerate(corpus_chaps_df.chap_raw):\n",
        "  if len(chap_text) > MIN_CHAP_LEN:\n",
        "    # Create list of Sentences by sent_tokenizing Chapter raw text string\n",
        "    chap_sents_ls = sent_tokenize(chap_text)\n",
        "    print(f'\\n    Chapter len(chap_sents_ls): {len(chap_sents_ls)}')\n",
        "\n",
        "    # Calc and save the sent_no that begins each Chapter\n",
        "    chap_start_sent = chap_sents_ls[0].strip()[:30]  # Match on the first 20 chars\n",
        "    print(f'For Chapter #{i} starting sentence: {chap_start_sent}')\n",
        "\n",
        "    # Fix to remove leading/trailing parenthesis that are being interpreted by Python\n",
        "    # chap_start_sent = chap_start_sent.replace('(','').replace(')','')\n",
        "    chap_start_sent = chap_start_sent.strip('()[] ')\n",
        "\n",
        "    chap_start_sentno = int(list(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(chap_start_sent, regex=False, )]['sent_no'])[0]) # Problems with Sentences beginning/endings with parenthenses\n",
        "    print(f'                 starting sent_no: {chap_start_sentno}')\n",
        "    chap_start_sentno_ls.append(int(chap_start_sentno))\n",
        "\n",
        "\n",
        "\n",
        "    # Calc and save the sent_no that ends each Chapter\n",
        "    chap_end_sent = chap_sents_ls[-1].strip()[:30]  # Match on the first 20 chars\n",
        "    print(f'For Chapter #{i} ending sentence: {chap_end_sent}')\n",
        "\n",
        "    # Fix to remove leading/trailing parenthesis that are being interpreted by Python\n",
        "    # chap_first_sent = chap_first_sent.replace('(','').replace(')','')\n",
        "    chap_end_sent = chap_end_sent.strip('()[] ')\n",
        "\n",
        "    chap_end_sentno = int(list(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(chap_end_sent, regex=False, )]['sent_no'])[0]) # Problems with Sentences beginning/endings with parenthenses\n",
        "    print(f'                 ending sent_no: {chap_end_sentno}')\n",
        "    chap_end_sentno_ls.append(int(chap_end_sentno))\n",
        "\n",
        "\n",
        "\n",
        "    # Find Sentence No for the starting Sentence of each Chapter\n",
        "\n",
        "    # Find the Sentence No for the middle Sentence of each Chapter\n",
        "    # chap_sents_len = len(chap_sents_ls)\n",
        "\n",
        "    # print(f'type(chap_start_sentno): {type(chap_start_sentno)}')\n",
        "    # print(f'type(chap_end_sentno): {type(chap_end_sentno)}')\n",
        "\n",
        "    chap_mid_sentno = int((chap_end_sentno-chap_start_sentno)//2 + chap_start_sentno)\n",
        "    # print(f'Chapter #{i}: {len(chap_sents_ls)} Sentences, midpoint: {chap_mid_sentno}, cumulative midpoint: {chap_mid_sentno}')\n",
        "    chap_mid_sentno_ls.append(chap_mid_sentno)\n",
        "\n",
        "corpus_chaps_df['sent_no_start'] = pd.Series(chap_start_sentno_ls)\n",
        "corpus_chaps_df['sent_no_mid'] = pd.Series(chap_mid_sentno_ls)\n",
        "corpus_chaps_df['sent_no_end'] = pd.Series(chap_end_sentno_ls)\n",
        "corpus_chaps_df['chap_raw'] = corpus_chaps_df['chap_raw'].astype('string')\n",
        "\n",
        "# Test \n",
        "print(f'First 2 Chapters of {CORPUS_FULL}')\n",
        "# corpus_chaps_df.head(2)\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU9OyGgN-ebU"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_chaps_df.iloc[0]\n",
        "print('\\n')\n",
        "corpus_chaps_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZDgxR4-5OLC"
      },
      "source": [
        "corpus_chaps_df[['chap_no','sent_no_start','sent_no_mid','sent_no_end']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5Q2wLFgVr-3"
      },
      "source": [
        "# Search by sent_no\n",
        "\n",
        "print(corpus_sents_df.iloc[2176]['sent_raw'])\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(corpus_sents_df.iloc[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNKoxe3kRV71"
      },
      "source": [
        "# TODO: More General Cleanup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRUmgyLvRM1v"
      },
      "source": [
        "# TODO: Normalize Paragraphs by Lengths (Smart Aggregate/Split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SdZ4e3ALYdD"
      },
      "source": [
        "### **Add Descriptive Statistics and Clean Raw Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWu8_C6PQ4iE"
      },
      "source": [
        "corpus_sects_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ZxHMuRe2wF"
      },
      "source": [
        "# Calculate some char/token metrics and do some EDA on them\n",
        "\n",
        "# This code must be run AFTER corpus_sects_df is created below\n",
        "\n",
        "corpus_sents_df['char_len'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x))\n",
        "corpus_sents_df['token_len'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x.split())) \n",
        "\n",
        "corpus_parags_df['char_len'] = corpus_parags_df['parag_raw'].apply(lambda x: len(x))\n",
        "corpus_parags_df['token_len'] = corpus_parags_df['parag_raw'].apply(lambda x: len(x.split())) \n",
        "\n",
        "corpus_sects_df['char_len'] = corpus_sects_df['sect_raw'].apply(lambda x: len(x))\n",
        "corpus_sects_df['token_len'] = corpus_sects_df['sect_raw'].apply(lambda x: len(x.split())) \n",
        "\n",
        "corpus_chaps_df['char_len'] = corpus_chaps_df['chap_raw'].apply(lambda x: len(x))\n",
        "corpus_chaps_df['token_len'] = corpus_chaps_df['chap_raw'].apply(lambda x: len(x.split())) \n",
        "\n",
        "# corpus_sents_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7au9Zy2kR0NK"
      },
      "source": [
        "# Default cleaned raw text\n",
        "\n",
        "# Sentences\n",
        "# Let's take a look at the updated text\n",
        "corpus_sents_df['sent_clean'] = corpus_sents_df['sent_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Text\n",
        "corpus_sents_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_sents_df.dropna(how='any', axis=0, subset=['sent_raw'], inplace=True)\n",
        "corpus_sents_df.dropna(how='any', axis=0, subset=['sent_clean'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Sentences:')\n",
        "print('--------------------------------------')\n",
        "corpus_sents_df.head(2)\n",
        "\n",
        "\n",
        "# Paragraphs\n",
        "# Let's take a look at the updated text\n",
        "corpus_parags_df['parag_clean'] = corpus_parags_df['parag_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Text\n",
        "corpus_parags_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_parags_df.dropna(how='any', axis=0, subset=['parag_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Paragraphs:')\n",
        "print('--------------------------------------')\n",
        "corpus_parags_df.head(2)\n",
        "\n",
        "\n",
        "# Sections\n",
        "# Let's take a look at the updated text\n",
        "corpus_sects_df['sect_clean'] = corpus_sects_df['sect_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Text\n",
        "corpus_sects_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_sects_df.dropna(how='any', axis=0, subset=['sect_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Sections:')\n",
        "print('--------------------------------------')\n",
        "# corpus_sects_df.head(2)\n",
        "\n",
        "\n",
        "# Chapters\n",
        "# Let's take a look at the updated text\n",
        "corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Chapters with NaN or '' Raw Text\n",
        "corpus_chaps_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_chaps_df.dropna(how='any', axis=0, subset=['chap_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Chapters:')\n",
        "print('--------------------------------------')\n",
        "# corpus_sects_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf99apfwKPAO"
      },
      "source": [
        "# Verify \n",
        "\n",
        "print(f'corpus_sents_df.shape: {corpus_sents_df.shape}')\n",
        "print(f'corpus_parags_df.shape: {corpus_parags_df.shape}')\n",
        "print(f'corpus_sects_df.shape: {corpus_sects_df.shape}')\n",
        "print(f'corpus_chaps_df.shape: {corpus_chaps_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHscLkclSYqN"
      },
      "source": [
        "##**Save Preprocess Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEzo8eltSvWS"
      },
      "source": [
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "# Sentences Raw\n",
        "corpus_sents_filename = f'corpus_text_raw_{author_abbr_str}_{title_str}.csv'\n",
        "print(f'Saving to file: {corpus_sents_filename}')\n",
        "corpus_sents_df['sent_raw'].to_csv(corpus_sents_filename)\n",
        "\n",
        "# Sentences Clean\n",
        "corpus_sents_filename = f'corpus_text_clean_{author_abbr_str}_{title_str}.csv'\n",
        "print(f'Saving to file: {corpus_sents_filename}')\n",
        "corpus_sents_df['sent_clean'].to_csv(corpus_sents_filename)\n",
        "\n",
        "\n",
        "# Sentence DataFrame\n",
        "corpus_sents_filename = f'corpus_sents_{author_abbr_str}_{title_str}.csv'\n",
        "print(f'Saving to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "# Paragraph DataFrame\n",
        "corpus_parags_filename = f'corpus_parags_{author_abbr_str}_{title_str}.csv'\n",
        "print(f'Saving to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "if SECTION_HEADINGS != 'None':\n",
        "  # Section DataFrame\n",
        "  corpus_sects_filename = f'corpus_sects_{author_abbr_str}_{title_str}.csv'\n",
        "  print(f'Saving to file: {corpus_sects_filename}')\n",
        "  corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "# Chapter DataFrame\n",
        "corpus_chaps_filename = f'corpus_chaps_{author_abbr_str}_{title_str}.csv'\n",
        "print(f'Saving to file: {corpus_chaps_filename}')\n",
        "corpus_chaps_df.to_csv(corpus_chaps_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8R_ANtfYG6"
      },
      "source": [
        "# (Optional) EDA Raw Text Features: Interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1njcRD-jgGJh"
      },
      "source": [
        "**(Optional) Can Skip Ahead to: 'EDA of Raw Text and Extracted Features'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ti9jQK7grxO"
      },
      "source": [
        "# Review Cleaned Up Sentences\n",
        "\n",
        "corpus_sents_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TN5ksy55kXy"
      },
      "source": [
        "# Summary Statistics\n",
        "\n",
        "corpus_sents_df.describe()\n",
        "corpus_sents_df['token_len'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxvxkbUGzkfy"
      },
      "source": [
        "# Create histogram of Paragraph lengths\n",
        "\n",
        "sns.histplot(data=corpus_sents_df['char_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Lengths');\n",
        "\n",
        "if (PLOT_OUTPUT == 'All'):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'hist_paraglen.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqUtGz8UjY2b"
      },
      "source": [
        "# Plot histogram of Sentence lengths\n",
        "\n",
        "sns.histplot(data=corpus_sents_df['token_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Lengths')\n",
        "\n",
        "if (PLOT_OUTPUT == 'All'):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'hist_sentlen.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OoBrucYR9Xc"
      },
      "source": [
        "# SELECT CORPUS TYPE\n",
        "# TODO: Customized Preprocessing (e.g. Tweets) by Corpus Type\n",
        "\n",
        "# Novel, Tweets, Chat Transcript\n",
        "\n",
        "# Processing Options\n",
        "\n",
        "# Apply first level cleaning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2tIua7tTSRz"
      },
      "source": [
        "# (Optional) Manually Create Sentiment Arc Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU11XOZIPalR"
      },
      "source": [
        "***Can skip to Section [Load Sentiment Polarities...] or [Calculate VADER...]***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cke3OowdWk6I"
      },
      "source": [
        "**Interactively Enter Cruxes and Edge Cases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv7Foe3oTMmz"
      },
      "source": [
        "# Setup data structures for endpoints of Sentiment Time Series\n",
        "\n",
        "#   [-1.0 to +1.0] = [v.neg, neg, neutral, pos, v.pos]\n",
        "\n",
        "corpus_man_crux_ols = []  # working datastructure to dynamically build ordered list of manually selected Crux Points\n",
        "corpus_man_cruxes_odt = OrderedDict() # Once all manual Crux points selected, this will be working data structure\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0] - 1\n",
        "\n",
        "corpus_parags_len = corpus_sents_df.parag_no.max() # make sure no omissions/repeats/skips\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-_ylgiAc6Aw"
      },
      "source": [
        "# <INPUT> Set the Begining and Ending Sentiment Values (Manual Versions)\n",
        "\n",
        "# Start of Corpus Sentiment Analysis Time Series\n",
        "Corpus_Starting_Sentiment = -0.1 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "# corpus_sa_begin = Corpus_Starting_Sentiment\n",
        "\n",
        "# End of Corpus Sentiment Analysis Time Series\n",
        "Corpus_Ending_Sentiment = -1 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "# corpus_sa_end = Corpus_Ending_Sentiment\n",
        "\n",
        "corpus_man_crux_ols = [tuple((0, Corpus_Starting_Sentiment)), tuple((corpus_sents_len, Corpus_Ending_Sentiment))]\n",
        "# corpus_man_cruxes_dt[0.] = corpus_sa_begin\n",
        "# corpus_man_cruxes_dt[float(corpus_sents_len)] = corpus_sa_end\n",
        "\n",
        "print(f'Manual Cruxes with Start/End: {corpus_man_crux_ols}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmcLrzyUw93d"
      },
      "source": [
        "**Seach for Key Words that suggest Min/Max Sentiment Crux**\n",
        "* Specific to the Novel: Introduction of Pivotal Character, Scene, Factual Reveal, McGuffin, etc...\n",
        "* General to Events/Themes: Death, Birth, Fight, Accident, Money, Sex, etc... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UESgwwIAY28T"
      },
      "source": [
        "# <INPUT> Search Corpus for Line No of Peaks/Valleys\n",
        "# TODO: Better Vis\n",
        "Search_String = \"Death\" #@param {type:\"string\"}\n",
        "if (Search_String == \"\"):\n",
        "  search_str = \"accident\"\n",
        "else:\n",
        "  search_str = Search_String.lower()\n",
        "\n",
        "# search the list of cleaned paragraphs\n",
        "# results_ls = [x for x in search_match_ls if re.search(subs, x)]\n",
        "\n",
        "# creating and passsing series to new column\n",
        "match_sents_ser = corpus_sents_df[\"sent_clean\"].str.find(search_str)\n",
        "\n",
        "# print(f'Found #{len(match_index>0)} Matches')\n",
        "match_sents_df = corpus_sents_df.loc[match_sents_ser > 0]\n",
        "print(f'Found #{match_sents_df.shape[0]} Matching Sentences')\n",
        "print('------------------------------------')\n",
        "# print(f'  {match_sents_df}')\n",
        "match_sents_df[['sent_no', 'parag_no', 'sent_raw', 'token_len']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oVYBfz6-EVo"
      },
      "source": [
        "**Get Context for Matched Sentence by Retrieving Surrounding Paragraph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AvYQqT5TsdV"
      },
      "source": [
        "# Extract Surrounding Paragraphs for context on matching Sentences\n",
        "\n",
        "def get_parag4sentno(asent_no):\n",
        "  '''\n",
        "  Return the original raw paragraph containing a \n",
        "  given sentence number.\n",
        "  '''\n",
        "  # parag_df = pd.DataFrame()\n",
        "  # print(f'Passed in sent_no: {asent_no}')\n",
        "  aparag_no = int(corpus_sents_df.loc[corpus_sents_df['sent_no'] == asent_no]['parag_no'])\n",
        "  # print(f'  This sent_no {asent_no} is in parag_no: {aparag_no}')\n",
        "  aparag_str = corpus_sents_df.loc[corpus_sents_df['parag_no'] == aparag_no]['sent_raw'].str.cat() # ['sent_clean']\n",
        "  # sentno_parag_df = corpus_sents_df[corpus_sents_df['sent_no']==asent_no]\n",
        "  # print(f'Sent #{asent_no} is in the paragraph: ')\n",
        "  # print(aparag)\n",
        "  # print(f'returning aparag_no: [{aparag_no}]: {aparag}')\n",
        "  return aparag_no, aparag_str\n",
        "\n",
        "'''\n",
        "# Testing\n",
        "asent_no = 7\n",
        "print(f'Searching for paragraph containing Sentence #{asent_no}')\n",
        "\n",
        "aparag_no, aparag_str = get_parag4sentno(asent_no)\n",
        "print(f'\\n  Found in Paragraph #{aparag_no} \\n\\n{aparag_str}')\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnHz08NyDroK"
      },
      "source": [
        "# Extract Surrounding Paragraphs for context on matching Sentences\n",
        "\n",
        "def get_parag_str(aparag_no):\n",
        "  '''\n",
        "  Return the original raw paragraph containing a \n",
        "  given sentence number.\n",
        "  '''\n",
        "  # parag_df = pd.DataFrame()\n",
        "  # print(f'Passed in sent_no: {asent_no}')\n",
        "  # aparag_no = int(corpus_sents_df.loc[corpus_sents_df['sent_no'] == asent_no]['parag_no'])\n",
        "  # print(f'  This sent_no {asent_no} is in parag_no: {aparag_no}')\n",
        "  aparag_str = corpus_sents_df.loc[corpus_sents_df['parag_no'] == aparag_no]['sent_raw'].str.cat() # ['sent_clean']\n",
        "  # sentno_parag_df = corpus_sents_df[corpus_sents_df['sent_no']==asent_no]\n",
        "  # print(f'Sent #{asent_no} is in the paragraph: ')\n",
        "  # print(aparag)\n",
        "  # print(f'returning aparag_no: [{aparag_no}]: {aparag}')\n",
        "  return aparag_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByKLYZDsK123"
      },
      "source": [
        "# Summarize current status of manually selected Crux Points\n",
        "# TODO:\n",
        "\n",
        "def crux_sum_short():\n",
        "  print(f'\\nOrdered list of all manually selected Crux Points')\n",
        "  print('---------------------------------------')\n",
        "  for i, acrux_tp in enumerate(corpus_man_crux_ols):\n",
        "    asent_no, asent_pol = acrux_tp\n",
        "    asent_str = corpus_sents_df[corpus_sents_df.sent_no==asent_no].sent_raw.str.cat()\n",
        "    # print(f'Type: {type(asent_str)}')\n",
        "    print(f'Sent No {asent_no:4d}: Polarity: {asent_pol}\\nText: {asent_str}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz0FLzt2UeSe"
      },
      "source": [
        "# Summarize current manually selected Crux Points\n",
        "\n",
        "def crux_summary():\n",
        "  print(f'\\nOrdered list of all manually selected Crux Points')\n",
        "  print('---------------------------------------\\n\\n')\n",
        "  for i, acrux_tp in enumerate(corpus_man_crux_ols):\n",
        "    asent_no, asent_pol = acrux_tp\n",
        "    asent_str = corpus_sents_df[corpus_sents_df.sent_no==asent_no].sent_raw.str.cat()\n",
        "    # print(f'Type: {type(asent_str)}')\n",
        "    print(f'Sent No {asent_no:4d}: Polarity: {asent_pol}')\n",
        "    print('------------------------------')\n",
        "    print(f'Text: {asent_str}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiUmseNw3CkN"
      },
      "source": [
        "# View the Paragraph containing your Matching Sentence:\n",
        "\n",
        "def get_nparags_context(crux_sent_no, parag_ct):\n",
        "\n",
        "  parag_win = int(parag_ct)\n",
        "  parag_crux_str = ''\n",
        "\n",
        "  parag_crux_no = 0\n",
        "\n",
        "  if (crux_sent_no < 0) | (crux_sent_no > corpus_sents_len):\n",
        "    print(f'ERROR: Pick a Sentence No between 0-{corpus_sents_len-1}')\n",
        "  else:\n",
        "    # get_sent_no = crux_sent_no\n",
        "    # print(f'Retrieving Sentence No: {get_sent_no}')\n",
        "    # print('----------')\n",
        "\n",
        "    parag_crux_no, aparag_str = get_parag4sentno(crux_sent_no)\n",
        "    if parag_win == 1:\n",
        "      print(f'Match #{i}: Sentence No. {asent_no} found in Paragraph No. {parag_crux_no}')\n",
        "      print('----------------------------')\n",
        "      print(f'Sentence:\\n')\n",
        "      # print(f'     {corpus_sents_df[corpus_sents_df.sent_no == crux_sent_no]}\\n\\n')\n",
        "      corpus_sents_df[corpus_sents_df.sent_no == crux_sent_no]\n",
        "      print('----------------------------')\n",
        "      print(f'Paragraph Context:\\n')\n",
        "      print(f'     {aparag_str}\\n\\n')\n",
        "    else:\n",
        "      parag_half_win = int((parag_win-1)/2)\n",
        "      parag_start = parag_crux_no - parag_half_win\n",
        "      parag_end = parag_crux_no + parag_half_win\n",
        "      print(f'Retrieving {parag_ct} Contextual Paragraphs Nos {parag_start} to {parag_end}')\n",
        "      print(f'  for Crux Point centered on Sentence No {crux_sent_no}')\n",
        "      for i in range(parag_start, parag_end + 1, 1):\n",
        "        if i == parag_crux_no:\n",
        "          print(f'\\n   ---------------------------------------------------------')\n",
        "          print(f'** Crux Point Paragraph #{i} with Sentence No. {crux_sent_no} **')\n",
        "          print(f'   ---------------------------------------------------------')\n",
        "          parag_crux_str = get_parag_str(i)\n",
        "          print(parag_crux_str)\n",
        "        else:\n",
        "          print(f'\\n   ----------------------')\n",
        "          print(f'   Regular Paragraph #{i}')\n",
        "          print(f'   ----------------------')\n",
        "          print(get_parag_str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjoXC8Fg3lBP"
      },
      "source": [
        "# Insert new crux point into ordered list: corpus_man_crux_ls\n",
        "\n",
        "# NOTE: For very long lists, use Python simple bisect library (at cost of additional dependency)\n",
        "\n",
        "\n",
        "def insert_ord_tp_list(crux_ord_ols, crux_tp):\n",
        "  '''\n",
        "  Insert new crux tuple: crux_tp = (sent_no, sentiment_polarity)\n",
        "  into ordered list of tuples while maintaining sent_no order\n",
        "  '''\n",
        "  sent_no, senti_pol = crux_tp\n",
        "\n",
        "  # Searching for the position\n",
        "  for i in range(len(crux_ord_ols)):\n",
        "    if crux_ord_ols[i][0] == sent_no:\n",
        "      # Attempting to insert duplicate\n",
        "      return crux_ord_ols\n",
        "    elif crux_ord_ols[i][0] < sent_no:\n",
        "      insert_idx = i\n",
        "    else:\n",
        "      break\n",
        "      \n",
        "  # Inserting n in the list\n",
        "  list = crux_ord_ols[:i] + [crux_tp] + crux_ord_ols[i:]\n",
        "  return list\n",
        "\n",
        "'''\n",
        "# Test\n",
        "crux_test_ls = [(1,0), (5,1), (10,-1)]\n",
        "crux_test_tp = (3,10)\n",
        "  \n",
        "print(insert_ord_tp_list(crux_test_ls, crux_test_tp))\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PPOLJDZ1wHc"
      },
      "source": [
        "**Start of Human in the Loop Manual Crux Point Identification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1MLexX57Guc"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "print('Enter a Sentence number based upon your search above to see the ')\n",
        "print('  surrounding Paragraph context.')\n",
        "print('----------------------------------------')\n",
        "print(f'(Enter an integer between 0 and {corpus_sents_len-1})\\n\\n')\n",
        "\n",
        "print('\\n')\n",
        "print('Enter an ODD NUMBER for the Number of surrounding Paragraphs ')\n",
        "print('  around the Sentence No to give Context.')\n",
        "print('----------------------------------------')\n",
        "print(f'(Enter an integer: 3, 5, 7\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exxWtaUPC30l"
      },
      "source": [
        "# Input your Context Retrieval Parameters\n",
        "\n",
        "Sentence_No =  2692#@param {type:\"integer\"}\n",
        "No_Paragraphs_Context = \"3\" #@param [\"1\", \"3\", \"5\"]\n",
        "\n",
        "get_nparags_context(Sentence_No, No_Paragraphs_Context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30fBikIrrgMe"
      },
      "source": [
        "**Add Crux to Manually Generated Sentiment Arc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgljeT6ypNbo"
      },
      "source": [
        "# Instructions to add current sentence as a Crux Point\n",
        "\n",
        "crux_summary()\n",
        "\n",
        "print('--------------------------------------------------------')\n",
        "print(f'INSTRUCTIONS To current Sentence No: {Sentence_No} as a Crux Point')\n",
        "print('--------------------------------------------------------')\n",
        "\n",
        "print(\"\\nCheck this box if you want to add the Sentence/Paragraph above \")\n",
        "print(\"  as a new Min/Max Crux Point with your approximation \")\n",
        "print(\"  for a Sentiment Polarity value between -1.0 to +1.0\\n\\n\")\n",
        "\n",
        "print(f\"Crux Sentence No: {Sentence_No} in Paragraph No: {parag_crux_no}\\n\")\n",
        "print(parag_crux_str)\n",
        "\n",
        "print(\"\\n\\nLeave Add_Sentence_Crux 'unchecked' to not add\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia1IXChRmyr3"
      },
      "source": [
        "# <INPUT> Option to add this Sentence/Paragraph as a Min/Max Crux Point\n",
        "\n",
        "Sentiment_Polarity = -0.4 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "Add_Sentence_Crux = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "# Add Crux if selected and give current summary status\n",
        "\n",
        "if Add_Sentence_Crux == True:\n",
        "  crux_new_tp = tuple((Sentence_No, Sentiment_Polarity))\n",
        "  corpus_man_crux_ols = insert_ord_tp_list(corpus_man_crux_ols, crux_new_tp)\n",
        "  if (corpus_man_crux_ols):\n",
        "    print(f'Successfully inserted new Crux = {crux_new_tp}')\n",
        "    print(f'Added Crux at Sentence No={Sentence_No} with Polarity={Sentiment_Polarity}')\n",
        "    # corpus_man_cruxes_dt[Sentence_No] = Sentiment_Polarity\n",
        "  else:\n",
        "    print(f'ERROR: Could not insert new Crux = {crux_new_tp}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lps67T-lUTi2"
      },
      "source": [
        "# Summary of current Crux Points after addition\n",
        "\n",
        "print('\\n------------------------------------------------------------')\n",
        "print(f'After addition of new Crux Point (Sentence No {Sentence_No})')\n",
        "print('------------------------------------------------------------\\n')\n",
        "\n",
        "crux_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjZT9_0CrzFk"
      },
      "source": [
        "**Delete Manually Selected Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTX30nokNPBp"
      },
      "source": [
        "len(corpus_man_crux_ols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxS-J_hPOfkI"
      },
      "source": [
        "crux_tp = (1, 2)\n",
        "a, b = crux_tp\n",
        "print(f'a is {a} and b is {b}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi7Cu1iErvzc"
      },
      "source": [
        "# Insert new crux point into ordered list: corpus_man_crux_ls\n",
        "\n",
        "# FIX: 20210616 and move to utility functions\n",
        "\n",
        "# NOTE: For very long lists, use Python simple bisect library (at cost of additional dependency)\n",
        "\n",
        "\n",
        "def del_ord_tp_list(acorpus_man_crux_ols, crux_tp):\n",
        "  '''\n",
        "  Insert new crux tuple: crux_tp = (sent_no, sentiment_polarity)\n",
        "  into ordered list of tuples while maintaining sent_no order\n",
        "  '''\n",
        "  crux_ct = len(acorpus_man_crux_ols)\n",
        "  sent_no = crux_tp[0]\n",
        "  print(f'Deleting sent_no: {sent_no} over crux_ls len={len(acorpus_man_crux_ols)}')\n",
        "\n",
        "  # Searching for the positionk\n",
        "  del_idx = -1\n",
        "  for i in range(len(acorpus_man_crux_ols)):\n",
        "    acrux_sent_no = acorpus_man_crux_ols[i][0]\n",
        "    print(f'Crux #{i} is sent_no={acrux_sent_no}')\n",
        "    if acrux_sent_no == sent_no:\n",
        "      print(f'Matching index at {i}')\n",
        "      del_idx = i\n",
        "      \n",
        "  # Delete n in the list\n",
        "  print(f'Deletion index = {del_idx}')\n",
        "  if del_idx == 0:\n",
        "    # Delete the first Crux\n",
        "    list = acorpus_man_crux_ols[1:]\n",
        "    return list\n",
        "  elif del_idx == crux_ct -1:\n",
        "    # Delete the last Crux\n",
        "    list = acorpus_man_crux_ols[:-1]\n",
        "    return list    \n",
        "  elif (del_idx > 0) & (del_idx < crux_ct):\n",
        "    # Delete an interior Crux\n",
        "    before_idx = i - 1\n",
        "    after_idx = i\n",
        "    list = acorpus_man_crux_ols[:before_idx] + acorpus_man_crux_ols[after_idx:]\n",
        "    print(f'Returning list: {list}')\n",
        "    return list\n",
        "  else:\n",
        "    print('No matching Crux tuple found')\n",
        "    return acorpus_man_crux_ols\n",
        "  \n",
        "\n",
        "# Test\n",
        "crux_test_ls = [(1,0), (5,1), (10,-1)]\n",
        "crux_test_tp = (5,5)\n",
        "  \n",
        "print(del_ord_tp_list(crux_test_ls, crux_test_tp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI5hZqmSLg5R"
      },
      "source": [
        "# Instructions to Delete a Crux Point\n",
        "\n",
        "crux_summary()\n",
        "\n",
        "print('--------------------------------------------------------')\n",
        "print('INSTRUCTIONS To Delete a Crux Point')\n",
        "print('--------------------------------------------------------')\n",
        "\n",
        "print(\"\\nEnter the Sentence No of a Crux you want to delete.\\n\")\n",
        "print(f'     Current Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n",
        "print(\" Skip this if you want to keep all manually selected Crux Points.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzowFY-I8U1_"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "print(\"\\nEnter the Sentence No of a Crux you want to delete.\\n\")\n",
        "print(f'     Current Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n",
        "print(\" Skip this if you want to keep all manually selected Crux Points.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ShFCfOsrvsv"
      },
      "source": [
        "Delete_Sent_No =  777#@param {type:\"integer\"}\n",
        "# Select a Crux to Delete\n",
        "# TODO: Drop down list\n",
        "\n",
        "# corpus_man_crux_ols\n",
        "corpus_man_crux_temp_ols = []\n",
        "\n",
        "crux_sent_set = set([x[0] for x in corpus_man_crux_ols])\n",
        "if not(Delete_Sent_No in crux_sent_set):\n",
        "  print(f'ERROR: {Delete_Sent_No} is not a Crux Point Sentence No')\n",
        "else:\n",
        "  # Keep the same tuple format for uniformity and future features\n",
        "  crux_del_tp = tuple((Delete_Sent_No, 'dummy_sentence'))\n",
        "  print(f'Selected {(crux_del_tp)} to delete')\n",
        "  # corpus_man_crux_temp_ols = \n",
        "  print(f'WTF: {del_ord_tp_list(corpus_man_crux_ols, crux_del_tp)}')\n",
        "  corpus_man_crux_old = del_ord_tp_list(corpus_man_crux_ols, crux_del_tp)\n",
        "  print(f\"corpus_man_crux_ols: {corpus_man_crux_ols}\")\n",
        "  # get_sent_no = Sentence_No\n",
        "  # print(f'Retrieving Sentence No: {get_sent_no}')\n",
        "  # print('----------')\n",
        "  print(f'Updated Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJdNu-oYrwHv"
      },
      "source": [
        "**Review Summary of all Manually Selected Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmpggAWuvMb1"
      },
      "source": [
        "# Generate Report Summary of All Manually Selected Cruxes\n",
        "\n",
        "f = io.StringIO()\n",
        "with contextlib.redirect_stdout(f):\n",
        "    crux_summary()\n",
        "crux_summary = f.getvalue()\n",
        "\n",
        "# Print Manual Crux Report Summary to Screen\n",
        "\n",
        "# print(crux_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ULFfHbxwDfu"
      },
      "source": [
        "# Save Manual Crux Summary Report\n",
        "\n",
        "plot_filename = 'man_cruxes.txt'\n",
        "plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "\n",
        "with open(plotpathfilename_str, 'a+') as outfp:\n",
        "  outfp.write(crux_summary)\n",
        "\n",
        "# Verify \n",
        "\n",
        "!ls -alt $plotpathfilename_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG_hGPgjxY7X"
      },
      "source": [
        "# Verify Report Content\n",
        "\n",
        "!cat man_cruxes_fscottfitzgerald_thegreatgatsby_20210616214050.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zPYRKCex8-U"
      },
      "source": [
        "**Clean and Organize Manual Crux Points into new Data Structures**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YICJNt3AXI2d"
      },
      "source": [
        "print(corpus_sents_df[corpus_sents_df['sent_no']==5]['sent_raw'].squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2bLtpweWQt4"
      },
      "source": [
        "# Convert and assemble all the Crux values in lists to save in a new Crux DataFrame\n",
        "\n",
        "\n",
        "pol_val_ls = [x[1] for x in corpus_man_crux_ols]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "parag_no_ls = [(get_parag4sentno(x))[0] for x in sent_no_ls]\n",
        "parag_str_ls = [get_parag_str(x) for x in parag_no_ls]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "sent_raw_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_raw'].squeeze() for x in sent_no_ls]\n",
        "sent_raw_ls\n",
        "sent_clean_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_clean'].squeeze() for x in sent_no_ls]\n",
        "sent_clean_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFEYg7Gfbj9i"
      },
      "source": [
        "# Create a Dict of Crux Points to Tuples (Polarity, Raw Sentence)\n",
        "\n",
        "# First Create the Tuples for each Sentence No (Sentiment Polarity, Raw Text)\n",
        "def merge(list1, list2):\n",
        "    merged_list = tuple(zip(list1, list2)) \n",
        "    return merged_list\n",
        "      \n",
        "crux_tp_ls = merge(pol_val_ls, sent_raw_ls)\n",
        "\n",
        "# Second, Create the Dictionary C\n",
        "corpus_man_cruxes_dt = {sent_no_ls[i]: crux_tp_ls[i] for i in range(len(crux_tp_ls))}\n",
        "\n",
        "# Verify\n",
        "corpus_man_cruxes_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtIHecpMdbXa"
      },
      "source": [
        "corpus_sents_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8qmpFRaTfu2"
      },
      "source": [
        "**Plot Interpolated Manual Sentiment Arc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ASg7uUZWf3"
      },
      "source": [
        "corpus_man_sa_df = pd.DataFrame({'sent_no':xn, 'sentiment':yn, 'sent_raw':corpus_sents_df.sent_raw.values})\n",
        "corpus_man_sa_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36VfGi4a47Yl"
      },
      "source": [
        "# Hermite Interpolation with SciPy\n",
        "\n",
        "pol_val_ls = [x[1] for x in corpus_man_crux_ols]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "parag_no_ls = [(get_parag4sentno(x))[0] for x in sent_no_ls]\n",
        "parag_str_ls = [get_parag_str(x) for x in parag_no_ls]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "sent_raw_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_raw'].squeeze() for x in sent_no_ls]\n",
        "sent_raw_ls\n",
        "sent_clean_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_clean'].squeeze() for x in sent_no_ls]\n",
        "sent_clean_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElJKawWfZWbv"
      },
      "source": [
        "sent_no_ls\n",
        "pol_val_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xufA403s3lbF"
      },
      "source": [
        "corpus_man_crux_np = np.asarray(corpus_man_crux_ols)\n",
        "corpus_man_crux_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdgKbjpi63a9"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMkgU-JhwudG"
      },
      "source": [
        "x2 = np.array(sent_no_ls)\n",
        "y2 = np.array(pol_val_ls)\n",
        "\n",
        "xn = np.linspace(0, corpus_sents_len, corpus_sents_len)\n",
        "yn = interpolate.pchip_interpolate(x2, y2, xn)\n",
        "\n",
        "crux_man_df = pd.DataFrame(\n",
        "    {'sent_no': sent_no_ls,\n",
        "     'pol_val': pol_val_ls\n",
        "     }\n",
        ")\n",
        "\n",
        "# plt.plot(x2, y2, 'ok', label='True values')\n",
        "# plt.plot(xn, yn, label='Hermite Interpolation')\n",
        "\n",
        "# plt.plot(xn, yn4, label='Spline order 4')\n",
        "# plt.plot(xn, yn5, label='Spline order 5')\n",
        "# plt.plot(xn, yn6, label='Spline order 6')\n",
        "# plt.plot(xn, yn7, label='Spline order 7')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# sns.histplot(data=corpus_sents_df['char_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Lengths')\n",
        "sns.histplot(data=crux_man_df, x='sent_no', y='pol_val', kde=True).set_title(f'{CORPUS_FULL} \\n Manual Cruxes with Hermite Smoothing')\n",
        "\n",
        "\n",
        "# Save graph to file.\n",
        "plot_filename = 'man_crux_plot.png'\n",
        "plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "plt.savefig(plotpathfilename_str, format='png', dpi=300)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW-VkS2s_ogy"
      },
      "source": [
        "**Gaussian Process Regression**\n",
        "\n",
        "* https://blog.dominodatalab.com/fitting-gaussian-process-models-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLFFYDfYpPjn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0BukxX9ZKYA"
      },
      "source": [
        "# (Optional) Load Sentiment Polarities: Interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhPJ9e-V9NMu"
      },
      "source": [
        "***If you upload a file of Sentiment Values you don't have to Calculate them in the following sections***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpx7viqf9AAJ"
      },
      "source": [
        "!ls -altr *.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw8_mUHI9sdr"
      },
      "source": [
        "# Test\n",
        "\n",
        "files.download('sentiments_raw_all_virginiawoolf_tothelighthouse_20210618161224.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90axHClm9cGZ"
      },
      "source": [
        "# Upload your precomputed Sentiment Values\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02NH5nnto2AC"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL9JCyCI-Sic"
      },
      "source": [
        "# Verify the file was uploaded correctly\n",
        "\n",
        "newest_csvfile = get_recentfile().split('/')[-1]\n",
        "print(f'The most recently updated *.csv file is: {newest_csvfile}')\n",
        "\n",
        "!head -n 10 $newest_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTfBj8qdi_vZ"
      },
      "source": [
        "%whos DataFrame\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgjz-E-mYgzI"
      },
      "source": [
        "# Upload file into DataFrame\n",
        "\n",
        "corpus_test_df = pd.read_csv(newest_csvfile)\n",
        "corpus_test_df['sent_clean'] = corpus_test_df['sent_clean'].astype('string')\n",
        "corpus_test_df['sent_raw'] = corpus_test_df['sent_raw'].astype('string')\n",
        "corpus_test_df.head()\n",
        "corpus_test_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ajyb8Jpmzw"
      },
      "source": [
        "***Skip to Section <Calculate Median of All...> if SA Loaded***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGA6sQy6RPDC"
      },
      "source": [
        "corpus_lexicons_stats_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7CfH00OFkQV"
      },
      "source": [
        "# **Either (Load) and/or (Calculate) Sentiment Values**\n",
        "\n",
        "Sentiment Models\n",
        "\n",
        "* VADER [-1.0 to 1.0] zero peak\n",
        "* TextBlob [-1.0 to 1.0] zero peak\n",
        "* Stanza outliers [-1.0 to 199.0] pos, outliers(+peak)\n",
        "* AFINN [-14 (-8 to 8) 20] discrete\n",
        "* SentimentR 11,710 [-5.4 to 8.8] norm\n",
        "* Syuzhet [-5.4 to 8.8] norm\n",
        "* Bing [-100.0 (-20.0 to 20.0) 100] discrete, outliers\n",
        "* Pattern [-1.0 to 1.0] norm\n",
        "* SentiWord [-3.8 to 4.4] norm\n",
        "* SenticNet [-3.8 to 10] norm\n",
        "* NRC [-100.0 (-5.0 to 5.0) 100] zero, outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ueo8fTSqqaak"
      },
      "source": [
        "## **Read Verification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys6WgCgCDOQN"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d-AM_WvbqFu"
      },
      "source": [
        "# Verfiy there are no NaN or Empty strings that passed the cleaning process\n",
        "\n",
        "# Only execute if previously datafile/corpus_sents_df\n",
        "\n",
        "\n",
        "# corpus_sents_df[corpus_sents_df['sent_clean'].isnull()]\n",
        "\n",
        "# corpus_sents_df[corpus_sents_df['sent_clean'].apply(lambda x: len(str(x)) <= 0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAAQ0SwfZynO"
      },
      "source": [
        "# Verify that hyphenated words are correctly handled (e.g. 'summer-mroning' -> 'summer morning')\n",
        "\n",
        "# corpus_sents_df[corpus_sents_df['sent_clean'].str.contains('summer', na=False)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnI6dd-kFe5E"
      },
      "source": [
        "## **(Optional) Load Raw Sentence Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiLWzvjFGRLy"
      },
      "source": [
        "### **Select Datafile to Read**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90gk7UOSwbHf"
      },
      "source": [
        "# By convention, Summary Sentiment files are named with the RegEx template 'sum_sentiments_*.csv'\n",
        "\n",
        "!ls -altr sum_sentiments_*.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SfWfj8xA8KX"
      },
      "source": [
        "#### **(Required) Import Baselines (Lexicons, Rules, Embeddings, OpenNLP)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI_j9yNqCtWu"
      },
      "source": [
        "# Read in previous Corpus DataFrames with Baseline Model Sentiment Series\n",
        "\n",
        "# These files are just raw text and metadata, no Sentiment Series\n",
        "# !ls -altr corpus_*_clean_ianmcewan_machineslikeme_20210708_2125.csv\n",
        "\n",
        "!ls -altr corpus_*_lexrules_ianmcewan_machineslikeme.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ROaod40B4px"
      },
      "source": [
        "# (Optional) Read Corpus Sentence DataFrame \n",
        "\n",
        "# corpus_sents_df_filename = 'corpus_sentences_lexrules_ianmcewan_machineslikeme.csv'\n",
        "corpus_sents_df_filename = 'sum_sentiments_sents_baseline_ddefoe_robinsoncrusoe.csv'\n",
        "corpus_sents_df = pd.read_csv(corpus_sents_df_filename)\n",
        "\n",
        "# corpus_sents_df.head(2)\n",
        "# corpus_sents_df.info()\n",
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdIHTvUWB4jg"
      },
      "source": [
        "# (Optional) Read Corpus Paragraph DataFrame \n",
        "\n",
        "# corpus_parags_df_filename = 'corpus_paragraphs_lexrules_ianmcewan_machineslikeme.csv'\n",
        "corpus_parags_df_filename = 'corpus_paragraphs_lexrules_ianmcewan_machineslikeme.csv'\n",
        "corpus_parags_df = pd.read_csv(corpus_parags_df_filename)\n",
        "\n",
        "# corpus_parags_df.head(2)\n",
        "# corpus_parags_df.info()\n",
        "corpus_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NWvzfpxB4bd"
      },
      "source": [
        "# (Optional) Read Corpus Section DataFrame \n",
        "\n",
        "# corpus_sects_df_filename = 'corpus_section_summary_lexrules_ianmcewan_machineslikeme.csv'\n",
        "corpus_sects_df_filename = 'corpus_section_summary_lexrules_ianmcewan_machineslikeme.csv'\n",
        "corpus_sects_df = pd.read_csv(corpus_sects_df_filename)\n",
        "\n",
        "# corpus_sects_df.head(2)\n",
        "# corpus_sects_df.info()\n",
        "corpus_sects_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzpXCdHBBTz3"
      },
      "source": [
        "# (Optional) Read Corpus Chapter DataFrame\n",
        "\n",
        "# corpus_chaps_df_filename = 'corpus_chapter_summary_lexrules_ianmcewan_machineslikeme.csv'\n",
        "corpus_chaps_df_filename = 'corpus_chapter_summary_lexrules_ianmcewan_machineslikeme.csv'\n",
        "corpus_chaps_df = pd.read_csv(corpus_chaps_df_filename)\n",
        "\n",
        "# corpus_chaps_df.head(2)\n",
        "# corpus_chaps_df.info()\n",
        "corpus_chaps_df.columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJJNNaxR0iJ0"
      },
      "source": [
        "##### **Sentence Baseline SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqmU4QRLOClI"
      },
      "source": [
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "NRC_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = False #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ux_0MXCTTnK"
      },
      "source": [
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XhK7VYKKLg6"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "win_per = SMA_Window_Percent\n",
        "win_roll = int(win_per/100 * corpus_sents_df.shape[0])\n",
        "\n",
        "if len(str(win_per)) == 1:\n",
        "  roll_str = 'roll0' + str(win_per) + '0'\n",
        "else:\n",
        "  roll_str = 'roll' + str(win_per) + '0'\n",
        "\n",
        "# display(corpus_sents_df.head())\n",
        "\n",
        "model_baselines_ls = ['sentimentr','syuzhet','bing',\n",
        "                      'sentiword','senticnet','nrc',\n",
        "                      'afinn','vader','textblob',\n",
        "                      'pattern','stanza']\n",
        "\n",
        "# list of (scale, center) adjustments for each model so they can be compared on same graph\n",
        "model_baselines_scale_ls = [(3, -0.6), (2000,0.1), (100,0.1), \n",
        "                            (100,0.1), (0.2,0.1), (200,0.1),\n",
        "                            (10,0.1), (5000,0.1), (4000,0.1),\n",
        "                            (1000,0.1), (200,-1000)]\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_baselines_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  # print(f'creating: {col_name_roll}')\n",
        "  corpus_sents_df[col_name_roll] = corpus_sents_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "# model_transformers_lnorm_medianiqr_ls = []\n",
        "corpus_sents_df[col_mean_roll] = corpus_sents_df[col_name_roll_ls].mean(axis=1)\n",
        "\n",
        "\n",
        "# display(corpus_sents_df.head())\n",
        "\"\"\"\n",
        "model_baseline_ls = ['sentimentr_lnorm_medianiqr', 'syuzhet_lnorm_medianiqr', \n",
        "                     'bing_lnorm_medianiqr', 'senticnet_lnorm_medianiqr',\n",
        "                     'sentiword_lnorm_medianiqr', 'nrc_lnorm_medianiqr',\n",
        "                     'afinn_lnorm_medianiqr', 'vader_lnorm_medianiqr',\n",
        "                     'textblob_lnorm_medianiqr', 'pattern_lnorm_medianiqr',\n",
        "                     'stanza_lnorm_medianiqr'] # , 'mean_roll050']\n",
        "\"\"\";\n",
        "\n",
        "model_baseline_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  model_baseline_subset_ls.append('sentimentr_lnorm_medianiqr_' + roll_str)\n",
        "if Syuzhet_Arc == True:\n",
        "  model_baseline_subset_ls.append('syuzhet_lnorm_medianiqr_' + roll_str)\n",
        "if Bing_Arc == True:\n",
        "  model_baseline_subset_ls.append('bing_lnorm_medianiqr_' + roll_str)\n",
        "if SenticNet_Arc == True:\n",
        "  model_baseline_subset_ls.append('senticnet_lnorm_medianiqr_' + roll_str)\n",
        "if SentiWord_Arc == True:\n",
        "  model_baseline_subset_ls.append('sentiword_lnorm_medianiqr_' + roll_str)\n",
        "if NRC_Arc == True:\n",
        "  model_baseline_subset_ls.append('nrc_lnorm_medianiqr_' + roll_str)\n",
        "if AFINN_Arc == True:\n",
        "  model_baseline_subset_ls.append('afinn_lnorm_medianiqr_' + roll_str)\n",
        "if VADER_Arc == True:\n",
        "  model_baseline_subset_ls.append('vader_lnorm_medianiqr_' + roll_str)\n",
        "if TextBlob_Arc == True:\n",
        "  model_baseline_subset_ls.append('textblob_lnorm_medianiqr_' + roll_str)\n",
        "if Pattern_Arc == True:\n",
        "  model_baseline_subset_ls.append('pattern_lnorm_medianiqr_' + roll_str)\n",
        "if Stanza_Arc == True:\n",
        "  model_baseline_subset_ls.append('stanza_lnorm_medianiqr_' + roll_str)\n",
        "if Mean_All_Arc == True:\n",
        "  model_baseline_subset_ls.append('mean_all_' + roll_str)\n",
        "\n",
        "col_mean_subset_roll = f'mean_subset_{roll_str}'\n",
        "corpus_sents_df[col_mean_subset_roll] = corpus_sents_df[model_baseline_subset_ls].mean(axis=1)\n",
        "\n",
        "if Mean_Subset_Arc == True:\n",
        "  model_baseline_subset_ls.append('mean_subset_' + roll_str)\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Safe)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "# add traces\n",
        "for i,amodel in enumerate(model_baseline_subset_ls):\n",
        "  fig.add_traces(go.Line(x = corpus_sents_df['sent_no'],\n",
        "                        y = model_baselines_scale_ls[i][0]*corpus_sents_df[amodel]+model_baselines_scale_ls[i][1],\n",
        "                        text = corpus_sents_df['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model: <b>\"+amodel+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "if Mean_Subset_Arc == True:\n",
        "  mean_subset_col = 'mean_subset_'+roll_str\n",
        "  corpus_sents_df[mean_subset_col] = corpus_sents_df[model_baseline_subset_ls].mean(axis=1)\n",
        "  fig.add_traces(go.Line(x=corpus_sents_df['sent_no'],\n",
        "                        y = 0.1*corpus_sents_df[mean_subset_col],\n",
        "                        line=dict(\n",
        "                              # color='#000000',\n",
        "                              width=5\n",
        "                              ),\n",
        "                        text = 'NA', # corpus_sents_df['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model <b>%{mean_subset_col}</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f\"Sentence Baseline Sentiment Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "    xaxis_title=\"Sentence Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bafsX4zKd-wT"
      },
      "source": [
        "##### **(ABOVE) Plotly SMA Sentence Baselines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuvAAR2x0x1B"
      },
      "source": [
        "##### **Paragraph Baseline SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhh0wT5yGaa0"
      },
      "source": [
        "SentimentR_Arc = False #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = False #@param {type:\"boolean\"}\n",
        "Bing_Arc = False #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = False #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = False #@param {type:\"boolean\"}\n",
        "NRC_Arc = False #@param {type:\"boolean\"}\n",
        "AFINN_Arc = False #@param {type:\"boolean\"}\n",
        "VADER_Arc = False #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = False #@param {type:\"boolean\"}\n",
        "Pattern_Arc = False #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_All_Arc = False #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXbrKIlBFG2s"
      },
      "source": [
        "# Paragraph SMA Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "win_per = SMA_Window_Percent\n",
        "win_roll = int(win_per/100 * corpus_parags_df.shape[0])\n",
        "\n",
        "if len(str(win_per)) == 1:\n",
        "  roll_str = 'roll0' + str(win_per) + '0'\n",
        "else:\n",
        "  roll_str = 'roll' + str(win_per) + '0'\n",
        "\n",
        "# display(corpus_parags_df.head())\n",
        "\n",
        "model_baselines_ls = ['sentimentr','syuzhet','bing',\n",
        "                      'sentiword','senticnet','nrc',\n",
        "                      'afinn','vader','textblob',\n",
        "                      'pattern','stanza']\n",
        "\n",
        "# list of (scale, center) adjustments for each model so they can be compared on same graph\n",
        "model_baselines_scale_ls = [(3, -6), (2000,1), (200,1), \n",
        "                            (10000,1), (0.001,-1000), (200,1),\n",
        "                            (1000,1), (15000,1), (20000,1),\n",
        "                            (5000,1), (150,-50000)]\n",
        "\n",
        "model_baselines_scale_ls = [(1,1),(1,1),(1,1),\n",
        "                            (1,1),(0.01,1),(1,1),\n",
        "                            (1,1),(1,1),(30,1),\n",
        "                            (1,1),(0.0001,1)]\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_baselines_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  # print(f'creating: {col_name_roll}')\n",
        "  corpus_parags_df[col_name_roll] = corpus_parags_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "# model_transformers_lnorm_medianiqr_ls = []\n",
        "corpus_parags_df[col_mean_roll] = corpus_parags_df[col_name_roll_ls].mean(axis=1)\n",
        "\n",
        "\n",
        "# display(corpus_parags_df.head())\n",
        "\"\"\"\n",
        "model_baseline_ls = ['sentimentr_lnorm_medianiqr', 'syuzhet_lnorm_medianiqr', \n",
        "                     'bing_lnorm_medianiqr', 'senticnet_lnorm_medianiqr',\n",
        "                     'sentiword_lnorm_medianiqr', 'nrc_lnorm_medianiqr',\n",
        "                     'afinn_lnorm_medianiqr', 'vader_lnorm_medianiqr',\n",
        "                     'textblob_lnorm_medianiqr', 'pattern_lnorm_medianiqr',\n",
        "                     'stanza_lnorm_medianiqr'] # , 'mean_roll050']\n",
        "\"\"\";\n",
        "\n",
        "model_baseline_subset_ls = []\n",
        "if SentimentR_Arc == True:\n",
        "  model_baseline_subset_ls.append('sentimentr_lnorm_medianiqr_' + roll_str)\n",
        "if Syuzhet_Arc == True:\n",
        "  model_baseline_subset_ls.append('syuzhet_lnorm_medianiqr_' + roll_str)\n",
        "if Bing_Arc == True:\n",
        "  model_baseline_subset_ls.append('bing_lnorm_medianiqr_' + roll_str)\n",
        "if SenticNet_Arc == True:\n",
        "  model_baseline_subset_ls.append('senticnet_lnorm_medianiqr_' + roll_str)\n",
        "if SentiWord_Arc == True:\n",
        "  model_baseline_subset_ls.append('sentiword_lnorm_medianiqr_' + roll_str)\n",
        "if NRC_Arc == True:\n",
        "  model_baseline_subset_ls.append('nrc_lnorm_medianiqr_' + roll_str)\n",
        "if AFINN_Arc == True:\n",
        "  model_baseline_subset_ls.append('afinn_lnorm_medianiqr_' + roll_str)\n",
        "if VADER_Arc == True:\n",
        "  model_baseline_subset_ls.append('vader_lnorm_medianiqr_' + roll_str)\n",
        "if TextBlob_Arc == True:\n",
        "  model_baseline_subset_ls.append('textblob_lnorm_medianiqr_' + roll_str)\n",
        "if Pattern_Arc == True:\n",
        "  model_baseline_subset_ls.append('pattern_lnorm_medianiqr_' + roll_str)\n",
        "if Stanza_Arc == True:\n",
        "  model_baseline_subset_ls.append('stanza_lnorm_medianiqr_' + roll_str)\n",
        "if Mean_All_Arc == True:\n",
        "  model_baseline_subset_ls.append('mean_all_' + roll_str)\n",
        "\n",
        "col_mean_subset_roll = f'mean_subset_{roll_str}'\n",
        "corpus_parags_df[col_mean_subset_roll] = corpus_parags_df[model_baseline_subset_ls].mean(axis=1)\n",
        "\n",
        "if Mean_Subset_Arc == True:\n",
        "  model_baseline_subset_ls.append('mean_subset_' + roll_str)\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Safe)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "model_base_standardized_roll_ls = []\n",
        "for amodel in model_baselines_ls:\n",
        "  col_name = f'{amodel}_lnorm_medianiqr_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_base_standardized_roll_ls.append(col_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,amodel in enumerate(model_base_standardized_roll_ls):\n",
        "  col_name_roll_stand = f'{col_name}_stand'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_parags_df[amodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_sents_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_sents_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "# add traces\n",
        "for i,amodel in enumerate(model_baseline_subset_ls):\n",
        "  fig.add_traces(go.Line(x = corpus_parags_df['parag_no'],\n",
        "                        y = model_baselines_scale_ls[i][0]*corpus_parags_df[amodel]+model_baselines_scale_ls[i][1],\n",
        "                        text = corpus_parags_df['parag_raw'][:5],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model: <b>\"+amodel+\"</b><br>Paragraph #<b>%{x}</b><br>Polarity <b>%{y}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "if Mean_Subset_Arc == True:\n",
        "  mean_subset_col = 'mean_subset_'+roll_str\n",
        "  corpus_parags_df[mean_subset_col] = corpus_parags_df[model_baseline_subset_ls].mean(axis=1)\n",
        "  fig.add_traces(go.Line(x=corpus_parags_df['parag_no'],\n",
        "                        y = 0.1*corpus_parags_df[mean_subset_col],\n",
        "                        line=dict(\n",
        "                              # color='#000000',\n",
        "                              width=5\n",
        "                              ),\n",
        "                        text = 'NA', # corpus_parags_df['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model <b>%{mean_subset_col}</b><br>Paragraph #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Paragraph Baseline Sentiment Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "    xaxis_title=\"Paragraph Number\",\n",
        "    showlegend=True,\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "plt.grid()\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaemiyQMFCqe"
      },
      "source": [
        "##### **(ABOVE) Plotly SMA Paragraph Baselines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TszUw6HFQ6lU"
      },
      "source": [
        "##### **Comparison of Sentence Baseline Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSNxgw5TjU8L"
      },
      "source": [
        "# Compare Sentence Baseline Standardized Sentiment Values\n",
        "\n",
        "model_baselines_ls = ['sentimentr', 'syuzhet', 'bing',\n",
        "                  'sentiword', 'senticnet', 'nrc',\n",
        "                  'afinn', 'vader', 'textblob',\n",
        "                  'pattern', 'stanza']\n",
        "\n",
        "model_base_standardized_roll_ls = []\n",
        "for amodel in model_baselines_ls:\n",
        "  col_name = f'{amodel}_lnorm_medianiqr_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_base_standardized_roll_ls.append(col_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,amodel in enumerate(model_base_standardized_roll_ls):\n",
        "  col_name_roll_stand = f'{col_name}_stand'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_sents_df[amodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_sents_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_sents_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentences for Baseline Model Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dTdyXzyQ6lc"
      },
      "source": [
        "# Create a comparison DataFrame of SentimentR Sentence Models\n",
        "\n",
        "baseline_corr_models_ls = ['sentimentr','syuzhet','bing','sentiword','senticnet','nrc','afinn','vader','textblob','pattern','stanza']\n",
        "\n",
        "corr_df = corpus_sents_df[baseline_corr_models_ls].corr(method='spearman')\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wycvgBTtvJYY"
      },
      "source": [
        "##### **Explore Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJYjOu9Ks_pL"
      },
      "source": [
        "# Search Corpus Sentences for substring\n",
        "\n",
        "# Test search of Sentences\n",
        "\n",
        "corpus_sents_df[corpus_sents_df['sent_raw'].str.contains('summer')]\n",
        "\n",
        "sents_matching_ls = corpus_sents_df[corpus_sents_df['sent_raw'].str.contains('rape', regex=False)]['sent_raw'] # ,'sent_raw']\n",
        "# sents_matching_ls\n",
        "\n",
        "for i, asent in enumerate(sents_matching_ls):\n",
        "  # sentno, sentraw = asent\n",
        "  print(f'Match #{i}: {asent}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqLEmlijsiJv"
      },
      "source": [
        "# corpus_sents_df[corpus_sents_df['sent_raw'].apply(lambda x: x.contains('He wrestled her')]['sent_no'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap_K_gpH0FTm"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHPx5a0xvJYb"
      },
      "source": [
        "Crux_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SentimentR_SMA_Model = \"Stanza\" #@param [\"SentimentR\", \"SyuzhetR\", \"Bing\", \"SenticNet\", \"SentiWord\", \"NRC\", \"AFINN\", \"VADER\", \"TextBlob\", \"Pattern\", \"Stanza\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SentimentR_SMA_Model == 'SentimentR':\n",
        "  model_selected = f'sentimentr'\n",
        "if SentimentR_SMA_Model == 'SyuzhetR':\n",
        "  model_selected = f'syuzhet'\n",
        "if SentimentR_SMA_Model == 'Bing':\n",
        "  model_selected = f'bing'\n",
        "if SentimentR_SMA_Model == 'SenticNet':\n",
        "  model_selected = f'senticnet'\n",
        "if SentimentR_SMA_Model == 'SentiWord':\n",
        "  model_selected = f'sentiword'\n",
        "if SentimentR_SMA_Model == 'NRC':\n",
        "  model_selected = f'nrc'\n",
        "if SentimentR_SMA_Model == 'AFINN':\n",
        "  model_selected = f'afinn'\n",
        "if SentimentR_SMA_Model == 'VADER':\n",
        "  model_selected = f'vader'\n",
        "if SentimentR_SMA_Model == 'TextBlob':\n",
        "  model_selected = f'textblob'\n",
        "if SentimentR_SMA_Model == 'Pattern':\n",
        "  model_selected = f'pattern'\n",
        "if SentimentR_SMA_Model == 'Stanza':\n",
        "  model_selected = f'stanza'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_lnorm_medianiqr_{roll_str}'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_stand_ls = [model_selected_fullname]\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "for amodel in corpus_models_stand_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_df, \n",
        "                                         col_series=corpus_models_stand_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_height=12, \n",
        "                                         subtitle_str=' ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False);\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsixcTu8cveR"
      },
      "source": [
        "model_crux_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6PU1zR8vJYf"
      },
      "source": [
        "**Get Top-n Crux Peaks/Valleys with surrounding Context**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUoJz_nyvJYh"
      },
      "source": [
        "# Crux Point Details\n",
        "Get_Peak_Cruxes = False #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 10 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        library_type='baselines', \n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes,\n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "else:\n",
        "  # import sys\n",
        "  # with open('filename.txt', 'w') as f:\n",
        "  #   print('This message will be written to a file.', file=f)\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-XtE7xovJYj"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1By1LTGvJYk"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  2620#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 4 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id_VKOPiR7Mg"
      },
      "source": [
        "#### **Import SentimentR Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X05awke6vYdu"
      },
      "source": [
        "# Test\n",
        "\n",
        "# !pip install kmeans1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBWhXS9bxrtd"
      },
      "source": [
        "\"\"\"\n",
        "import kmeans1d\n",
        "\n",
        "k = corpus_sentimentr_df.shape[0]//500  \n",
        "\n",
        "clusters, centroids = kmeans1d.cluster(np.array(corpus_sentimentr_df['jockers_rinker']), k)\n",
        "type(clusters)\n",
        "\n",
        "[[x,clusters.count(x)] for x in set(clusters)]\n",
        "centroids\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTPbrXDM0Gm1"
      },
      "source": [
        "# Verify SentimentR Sentiment Files exported from RStudio\n",
        "!pwd\n",
        "!ls -altr sum_sentiments_*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlONG-ZwuuVb"
      },
      "source": [
        "# Verify file head layout\n",
        "\n",
        "!head -n 3 sum_sentiments_vwoolf_tothelighthouse_sentimentR_7models.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJp4DgcIwa7A"
      },
      "source": [
        "# (Optional) Read Sentiment Series generated in RStudio by SentimentR into DataFrame: corpus_sents_sentimentr_df\n",
        "#            SKIP if no SyuzhetR sentiment datafile to read in\n",
        "\n",
        "# MANUALLY: copy and paste the filename above into the quotes below for sum_sentiment_sentimentR_filename\n",
        "sum_sentiment_sentimentR_filename = 'sum_sentiments_vwoolf_tothelighthouse_sentimentR_7models.csv'\n",
        "\n",
        "corpus_sents_sentimentr_df = pd.read_csv(sum_sentiment_sentimentR_filename, encoding = 'unicode_escape', engine ='python')\n",
        "\n",
        "# Rename columns if necessary\n",
        "col_rename_map = {'Unnamed: 0' : 'sent_no'}\n",
        "\"\"\"\n",
        "                  'ttl_sents_syuzhet_vec' : 'syuzhet',\n",
        "                  'ttl_sents_bing_vec' : 'bing',\n",
        "                  'ttl_sents_afinn_vec' : 'afinn',\n",
        "                  'ttl_sents_nrc_vec' : 'nrc'}\n",
        "\"\"\";\n",
        "corpus_sents_sentimentr_df.rename(columns=col_rename_map,inplace=True)\n",
        "\n",
        "corpus_sents_sentimentr_df.head(2)\n",
        "corpus_sents_sentimentr_df.info()\n",
        "corpus_sents_sentimentr_df.columns\n",
        "\n",
        "corpus_sents_sentimentr_len = corpus_sents_sentimentr_df.shape[0]\n",
        "\n",
        "# BUG FIX: SentimentR can create many additional rows that must be deleted \n",
        "#          to enable it to be merged with other Sentiment Models on the same Corpus\n",
        "#          OR just leave SentimentR unmerged and analyze separately (preferred)\n",
        "#\n",
        "# POSSIBLE SOLUTIONS (from worst/easiest to better)\n",
        "#     \n",
        "#   1) Trim extra n rows from head/tail\n",
        "#   2) Naive Downsampling of Series\n",
        "#   3) Clustering and distribute deletes of near-medians from longest runs, avoiding outliers, start/end\n",
        "#   4) DTW character-preserving Compression\n",
        "# \n",
        "#          Simplification, 1D cluster jockers_rinker column as proxy for full interrow distance features\n",
        "#                          and delete rows near the median from the largest cluster (vs taking into account\n",
        "#                          all features in Euclidian or other distance metric)\n",
        "\n",
        "# import kmeans1d\n",
        "\n",
        "# Approximate k cluster number as 1 cluster for every 500 sentences in Corpus\n",
        "# k = corpus_sents_sentimentr_df.shape[0]//500  \n",
        "# clusters, centroids = kmeans1d.cluster(np.array(corpus_sents_sentimentr_df['jockers_rinker']), k)\n",
        "\n",
        "def del_oneincluster(df, cluster_per=1):\n",
        "  '''\n",
        "  TODO: Skip for now and use kmeans1d instead\n",
        "  Given a DataFrame and a Cluster Percent to calculate a sliding window\n",
        "  Return DataFrame with one row removed within a sliding window cluster with most self-similiar rows\n",
        "  '''\n",
        "\n",
        "  # Compute sliding window for cluster size\n",
        "  win_cluster_len = int(cluster_per/100 * df.shape[0])\n",
        "  win_start = 0\n",
        "  win_stop = df.shape[0] - win_cluster_len\n",
        "\n",
        "  # Get numeric columns\n",
        "  numeric_df = df.select_dtypes(include=numerics)\n",
        "\n",
        "  most_selfsimilar_value = 0\n",
        "  most_selfsimilar_index = 0\n",
        "  for i in range(win_start, win_stop, 1):\n",
        "    selfsim_score = selfsim_metric(numeric_df.iloc[i:win_cluster_len+1])\n",
        "    if selfsim_score > most_selfsimilar_value:\n",
        "      most_selfsimilar_index = i\n",
        "\n",
        "  oneless_df = del_onerow(most_selfsimilar_index)\n",
        "\n",
        "  return oneless_df\n",
        "\n",
        "# BAD SOLUTION, just trim the last n rows of corpus_sents_sentimentr_df to make lengths match for merging\n",
        "# corpus_sents_sentimentr_df = corpus_sents_sentimentr_df.iloc[:-n,:]\n",
        "\n",
        "if corpus_sents_sentimentr_len != corpus_sents_df.shape[0]:\n",
        "  print('\\n\\n\\n======================================================================\\n')\n",
        "  print(f'ERROR: sentence sentiment values read into corpus_syuzhetr (len={corpus_sents_sentimentr_len})')\n",
        "  print(f'       is not the same length as corpus_sents_df (len={corpus_sents_df.shape[0]}) ')\n",
        "  print(f'\\nRECOMMENDATION: Use the preprocessed corpus output created by this notebook ')\n",
        "  print(f'                as input to SentimentR in RStudio to generate sentiment series')\n",
        "  print(f'                and then retry importing')\n",
        "  print('\\n======================================================================\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqrVGplu3txO"
      },
      "source": [
        "##### **Sentence SentimentR SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWnIVHzc4I27"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# display(corpus_sentimentr_df.head())\n",
        "\n",
        "win_per = SMA_Window_Percent             \n",
        "win_roll = int(win_per/100 * corpus_sents_sentimentr_df.shape[0])\n",
        "\n",
        "model_sentimentr_ls = ['jockers_rinker', 'jockers', 'huliu', \n",
        "                       'senticnet', 'sentiword', 'nrc', 'lmcd']\n",
        "\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_sentimentr_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  if len(str(win_per)) == 1:\n",
        "    roll_str = 'roll0' + str(win_per) + '0'\n",
        "  else:\n",
        "    roll_str = 'roll' + str(win_per) + '0'\n",
        "  col_name_roll = f'{amodel}_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  corpus_sents_sentimentr_df[col_name_roll] = corpus_sents_sentimentr_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_' + roll_str\n",
        "corpus_sents_sentimentr_df[col_mean_roll] = corpus_sents_sentimentr_df[col_name_roll_ls].mean(axis=1)\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Bold)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "# add traces\n",
        "model = 'mean_' + roll_str\n",
        "fig.add_traces(go.Line(x=corpus_sents_sentimentr_df['sent_no'],\n",
        "                       y = corpus_sents_sentimentr_df[model],\n",
        "                       line=dict(\n",
        "                            color='#000000',\n",
        "                            width=5\n",
        "                            ),\n",
        "                       text = corpus_sents_sentimentr_df.index.values,\n",
        "                       name = model,\n",
        "                       hovertemplate = \"Model <b>Mean: \"+str(win_per)+\"%</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                       marker_color=next(palette)))\n",
        "\n",
        "for amodel in model_sentimentr_ls:\n",
        "  model_roll = f'{amodel}_' + roll_str\n",
        "  fig.add_traces(go.Line(x=corpus_sents_sentimentr_df['sent_no'],\n",
        "                        y = corpus_sents_sentimentr_df[model_roll],\n",
        "                        text = corpus_sents_sentimentr_df['sent_raw'],\n",
        "                        name = model_roll,\n",
        "                        hovertemplate = \"Model <b>\"+model_roll+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b><br>Index: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"SentimentR Sentence Sentiment Models <b><i>\" + roll_str.upper() + '</i></b>',\n",
        "    xaxis_title=\"Sentence Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP-Moy3DdrRr"
      },
      "source": [
        "##### **(ABOVE) Plotly SMA Sentence SentimentR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yaxyt-MqOxdV"
      },
      "source": [
        "##### **Comparison of Sentence SentimentR Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0vjwYkU2ChY"
      },
      "source": [
        "corpus_sents_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa2RSwSBnqxd"
      },
      "source": [
        "# Compare Sentence SentimentR Standardized Sentiment Values\n",
        "\n",
        "model_sentimentr_ls = ['jockers_rinker', 'jockers', 'huliu', 'senticnet', 'sentiword', 'nrc', 'lmcd']\n",
        "\n",
        "model_sentimentr_standardized_roll_ls = []\n",
        "for amodel in model_sentimentr_ls:\n",
        "  col_name = f'{amodel}_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_sentimentr_standardized_roll_ls.append(col_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,amodel in enumerate(model_sentimentr_standardized_roll_ls):\n",
        "  col_name_roll_stand = f'{amodel}_stand'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_sents_sentimentr_df[amodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_sents_sentimentr_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_sents_sentimentr_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence SentimentR Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5K19LgkOung"
      },
      "source": [
        "# Create a comparison DataFrame of SentimentR Sentence Models\n",
        "\n",
        "sentimentr_corr_models_ls = ['jockers_rinker', 'jockers', 'huliu', 'senticnet', 'sentiword', 'nrc', 'lmcd']\n",
        "\n",
        "corr_df = corpus_sents_sentimentr_df[sentimentr_corr_models_ls].corr(method='spearman')\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Q1TnnflcZU"
      },
      "source": [
        "corpus_sects_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S83JA_esuSxc"
      },
      "source": [
        "##### **Explore Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7TRTSm34mnl"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDdUBweW1YiB"
      },
      "source": [
        "Crux_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SentimentR_SMA_Model = \"Jockers-Rinker\" #@param [\"Jockers-Rinker\", \"Jockers\", \"Hu-Liu\", \"SenticNet\", \"SentiWord\", \"NRC\", \"Loughan-McDonald\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SentimentR_SMA_Model == 'Jockers-Rinker':\n",
        "  model_selected = f'jockers_rinker'\n",
        "if SentimentR_SMA_Model == 'Jockers':\n",
        "  model_selected = f'jockers_rinker'\n",
        "if SentimentR_SMA_Model == 'Hu-Liu':\n",
        "  model_selected = f'huliu'\n",
        "if SentimentR_SMA_Model == 'SenticNet':\n",
        "  model_selected = f'senticnet'\n",
        "if SentimentR_SMA_Model == 'SentiWord':\n",
        "  model_selected = f'sentiword'\n",
        "if SentimentR_SMA_Model == 'NRC':\n",
        "  model_selected = f'nrc'\n",
        "if SentimentR_SMA_Model == 'Loughran-McDonald':\n",
        "  model_selected = f'lmcd'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_{roll_str}_stand'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_stand_ls = [model_selected_fullname]\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "for amodel in corpus_models_stand_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_sentimentr_df, \n",
        "                                         col_series=corpus_models_stand_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_height=0, \n",
        "                                         subtitle_str=' ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_FRnxmUtRIY"
      },
      "source": [
        "**Get Top-n Crux Peaks/Valleys with surrounding Context**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyeQ7Jpfszhq"
      },
      "source": [
        "# Crux Details\n",
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Sort_by_SentenceNo = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = 'sentiment_val'\n",
        "  \n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        library_type='sentimentr',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "for amodel in corpus_models_stand_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_syuzhetr_df, \n",
        "                                         col_series=corpus_models_stand_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_height=0, \n",
        "                                         subtitle_str=' ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "\"\"\";  \n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fHyY7YBsbcn"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W7iWQErsbco"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  1864#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 1 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, \n",
        "                         the_n_sideparags=No_Paragraphs_on_Each_Side, \n",
        "                         the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1d2Q3zqSES1"
      },
      "source": [
        "#### **Import SyuzhetR Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npwz74KeliXE"
      },
      "source": [
        "# Verify SentimentR Sentiment Files exported from RStudio\n",
        "!pwd\n",
        "!ls -altr sum_sentiments*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfdxWHlFvnKk"
      },
      "source": [
        "# Verify file head layout\n",
        "\n",
        "!head -n 3 sum_sentiments_vwoolf_tothelighthouse_syuzhetR_4models.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrxrUOf0w4o7"
      },
      "source": [
        "# (Optional) Read Sentiment Series generated in RStudio by SyuzhetR into DataFrame: corpus_sents_syuzhetr_df\n",
        "#            SKIP if no SyuzhetR sentiment datafile to read in\n",
        "\n",
        "sum_sentiment_syuzhetR_filename = 'sum_sentiments_vwoolf_tothelighthouse_syuzhetR_4models.csv'\n",
        "corpus_sents_syuzhetr_df = pd.read_csv(sum_sentiment_syuzhetR_filename, encoding = 'unicode_escape', engine ='python')\n",
        "\n",
        "# Rename columns if necessary\n",
        "col_rename_map = {'Unnamed: 0' : 'sent_no'}\n",
        "\"\"\"\n",
        "                  'ttl_sents_syuzhet_vec' : 'syuzhet',\n",
        "                  'ttl_sents_bing_vec' : 'bing',\n",
        "                  'ttl_sents_afinn_vec' : 'afinn',\n",
        "                  'ttl_sents_nrc_vec' : 'nrc'}\n",
        "\"\"\";\n",
        "corpus_sents_syuzhetr_df.rename(columns=col_rename_map,inplace=True)\n",
        "\n",
        "corpus_sents_syuzhetr_df.head(2)\n",
        "corpus_sents_syuzhetr_df.info()\n",
        "corpus_sents_syuzhetr_df.columns\n",
        "\n",
        "corpus_sents_syuzhetr_len = corpus_sents_syuzhetr_df.shape[0]\n",
        "\n",
        "if corpus_sents_syuzhetr_len != corpus_sents_df.shape[0]:\n",
        "  print('\\n\\n\\n======================================================================\\n')\n",
        "  print(f'ERROR: sentence sentiment values read into corpus_syuzhetr (len={corpus_sents_syuzhetr_len})')\n",
        "  print(f'       is not the same length as corpus_sents_df (len={corpus_sents_df.shape[0]}) ')\n",
        "  print(f'\\nRECOMMENDATION: Use the preprocessed corpus output created by this notebook ')\n",
        "  print(f'                as input to SyuzhetR in RStudio to generate sentiment series')\n",
        "  print(f'                and then retry importing')\n",
        "  print('\\n======================================================================\\n');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IDpH20z3lhu"
      },
      "source": [
        "##### **Sentence Syuzhet SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oStnaOSJ4-GT"
      },
      "source": [
        "# BUG: Causes multiple plots rather than one plot with multiple lines\n",
        "\n",
        "# !pip install -U kaleido\n",
        "\n",
        "# import kaleido"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMVbqEA0WDka"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "# SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# Save_to_File = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "# display(corpus_syuzhetr_df.head())\n",
        "\n",
        "model_syuzhetr_ls = ['syuzhet', 'bing', 'afinn', 'nrc'] # , 'mean_roll050']\n",
        "\n",
        "win_per = 10 # SMA_Window_Percent\n",
        "win_roll = int(win_per/100 * corpus_sents_syuzhetr_df.shape[0])\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_syuzhetr_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  if len(str(win_per)) == 1:\n",
        "    roll_str = 'roll0' + str(win_per) + '0'\n",
        "  else:\n",
        "    roll_str = 'roll' + str(win_per) + '0'\n",
        "  col_name_roll = f'{amodel}_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  corpus_sents_syuzhetr_df[col_name_roll] = corpus_sents_syuzhetr_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_' + roll_str\n",
        "corpus_sents_syuzhetr_df[col_mean_roll] = corpus_sents_syuzhetr_df[col_name_roll_ls].mean(axis=1)\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Bold)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "# add traces\n",
        "model = 'mean_' + roll_str # 'mean_lnorm_medianiqr_' + roll_str\n",
        "fig.add_traces(go.Line(x=corpus_sents_syuzhetr_df['sent_no'],\n",
        "                       y = corpus_sents_syuzhetr_df[model],\n",
        "                       line=dict(\n",
        "                            color='#000000',\n",
        "                            width=5\n",
        "                            ),\n",
        "                       text = corpus_sents_syuzhetr_df['sent_raw'],\n",
        "                       name = model,\n",
        "                       hovertemplate = \"Model <b>Mean \"+ str(SMA_Window_Percent) + \"% Roll</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n",
        "                       marker_color=next(palette)))\n",
        "\n",
        "for amodel in model_syuzhetr_ls:\n",
        "  model_roll = f'{amodel}_' + roll_str # lnorm_medianiqr_' + roll_str\n",
        "  fig.add_traces(go.Line(x=corpus_sents_syuzhetr_df['sent_no'],\n",
        "                        y = corpus_sents_syuzhetr_df[model_roll],\n",
        "                        text = corpus_sents_syuzhetr_df['sent_raw'],\n",
        "                        name = model_roll,\n",
        "                        hovertemplate = \"Model <b>\"+model_roll+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"SyuzhetR Sentence Sentiment Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "    xaxis_title=\"Sentence Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aUtGZvXd406"
      },
      "source": [
        "##### **(ABOVE) Plotly SMA Sentence Syuzhet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMIE4UF_PlxL"
      },
      "source": [
        "##### **Comparison of Sentence SyuzhetR Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0iHrpqpmnuE"
      },
      "source": [
        "# Compare Sentence SyuzhetR Standardized Sentiment Values\n",
        "\n",
        "model_syuzhetr_ls = ['syuzhet', 'bing', 'afinn', 'nrc']\n",
        "\n",
        "model_syuzhetr_standardized_roll_ls = []\n",
        "for amodel in model_syuzhetr_ls:\n",
        "  col_name = f'{amodel}_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_syuzhetr_standardized_roll_ls.append(col_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,amodel in enumerate(model_syuzhetr_standardized_roll_ls):\n",
        "  col_name_roll_stand = f'{col_name}_stand'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_sents_syuzhetr_df[amodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_sents_syuzhetr_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_sents_syuzhetr_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence SyuzhetR Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onf1ah-vPlAT"
      },
      "source": [
        "# Create a comparison DataFrame of SentimentR Sentence Models\n",
        "\n",
        "syuzhetr_corr_models_ls = ['syuzhet', 'bing', 'afinn', 'nrc']\n",
        "\n",
        "corr_df = corpus_sents_syuzhetr_df[syuzhetr_corr_models_ls].corr(method='spearman')\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OemcAFXd6JD5"
      },
      "source": [
        "##### **Explore Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "262TERB06JD8"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhmPgogw6JEA"
      },
      "source": [
        "Crux_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SyuzhetR_SMA_Model = \"Syuzhet\" #@param [\"Syuzhet\", \"Bing\", \"AFINN\", \"NRC\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SyuzhetR_SMA_Model == 'Syuzhet':\n",
        "  model_selected = f'syuzhet'\n",
        "if SyuzhetR_SMA_Model == 'Bing':\n",
        "  model_selected = f'bing'\n",
        "if SyuzhetR_SMA_Model == 'AFINN':\n",
        "  model_selected = f'afinn'\n",
        "if SyuzhetR_SMA_Model == 'NRC':\n",
        "  model_selected = f'nrc'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_{roll_str}'\n",
        "  print(f'model_selected_fullname: {model_selected_fullname}')\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_stand_ls = [model_selected_fullname]\n",
        "print(f'corpus_models_stand_ls: {corpus_models_stand_ls}')\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "for amodel in corpus_models_stand_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_syuzhetr_df, \n",
        "                                         col_series=corpus_models_stand_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_height=0, \n",
        "                                         subtitle_str=' ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "print(f'model_crux_ls: {model_crux_ls}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhx-E08q6JEF"
      },
      "source": [
        "**Get Top-n Crux Peaks/Valleys with surrounding Context**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxq3Irr36JEK"
      },
      "source": [
        "# Crux Details\n",
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 10 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 1 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        library_type='syuzhetr',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FE5BtcJ6JEN"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsfMfACJ6JES"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  3787#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 5 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, \n",
        "                         the_n_sideparags=No_Paragraphs_on_Each_Side, \n",
        "                         the_sent_highlight=Highlight_Crux_Sentence)\n",
        "\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, \n",
        "                         the_n_sideparags=No_Paragraphs_on_Each_Side, \n",
        "                         the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpHVFidu7whr"
      },
      "source": [
        "#### **Compare Sentence SentimentR vs Syuzhet Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7vZsZT-W0nw"
      },
      "source": [
        "# transformer_mean_roll_norm_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXY05cztTMl_"
      },
      "source": [
        "# Compare Sentence SentimentR vs SyuzhetR SMA smoothed series\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Standardize SentimentR Mean Rolling\n",
        "sentimentr_mean_roll_np = np.array(corpus_sents_sentimentr_df[f'mean_{roll_str}'])\n",
        "sentimentr_mean_roll_np = sentimentr_mean_roll_np.reshape((len(sentimentr_mean_roll_np), 1))\n",
        "\n",
        "scaler = scaler.fit(sentimentr_mean_roll_np)\n",
        "print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "# standardization the dataset and print the first 5 rows\n",
        "sentimentr_mean_roll_norm_np = scaler.transform(sentimentr_mean_roll_np)\n",
        "\n",
        "# Standardize SyuzhetR Mean Rolling\n",
        "syuzhetr_mean_roll_np = np.array(corpus_sents_syuzhetr_df[f'mean_{roll_str}'])\n",
        "syuzhetr_mean_roll_np = syuzhetr_mean_roll_np.reshape((len(syuzhetr_mean_roll_np), 1))\n",
        "\n",
        "scaler = scaler.fit(syuzhetr_mean_roll_np)\n",
        "print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "# standardization the dataset and print the first 5 rows\n",
        "syuzhetr_mean_roll_norm_np = scaler.transform(syuzhetr_mean_roll_np)\n",
        "\n",
        "\n",
        "# Plot normalized Series\n",
        "plt.plot(sentimentr_mean_roll_norm_np, label=\"SentimentR\")\n",
        "plt.plot(syuzhetr_mean_roll_norm_np, label=\"SyuzhetR\")\n",
        "# plt.plot(transformer_mean_roll_norm_np, label=\"SyuzhetR\")\n",
        "\n",
        "# corpus_sents_syuzhetr_df[f'mean_{roll_str}'].apply(lambda x: Scale_SyuzhetR*x).plot(label='SyuzhetR')\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Sentence SentimentR vs Syuzhet Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twq2IuGz7vlR"
      },
      "source": [
        "# Compare Sentence SentimentR vs SyuzhetR SMA smoothed series\n",
        "\n",
        "# Create a unified DataFrame of Mean Roll_{win_per} from\n",
        "#     SentimentR and SyuzhetR\n",
        "\n",
        "col_mean_roll = f'mean_{roll_str}'\n",
        "\n",
        "compare_sentimentr_syuzhetr_df = pd.concat([\n",
        "    corpus_sents_sentimentr_df[col_mean_roll],\n",
        "    corpus_sents_syuzhetr_df[col_mean_roll]],\n",
        "    axis=1)\n",
        "\n",
        "col_sentimentr_mean_roll = f'sentimentr_{col_mean_roll}'\n",
        "col_syuzhetr_mean_roll = f'syuzhet_{col_mean_roll}'\n",
        "\n",
        "col_mapping = {\n",
        "    compare_sentimentr_syuzhetr_df.columns[0]:'sentimentr_mean_roll', \n",
        "    compare_sentimentr_syuzhetr_df.columns[1]:'syuzhet_mean_roll'\n",
        "}\n",
        "\n",
        "compare_sentimentr_syuzhetr_df.rename(columns=col_mapping,\n",
        "                                      inplace=True)\n",
        "\n",
        "# compare_sentimentr_syuzhetr_df.iloc[1000:1005]\n",
        "\n",
        "\n",
        "# Get correlation matrix of the comparison DataFrame\n",
        "corr_df = compare_sentimentr_syuzhetr_df.corr(method='spearman')\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8wOUSyOSIVs"
      },
      "source": [
        "#### **Import Transformer Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUb7DPElSVxc"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8EO6mxZRZrN"
      },
      "source": [
        "# By convention, Summary Sentiment files are named with the RegEx template 'sum_sentiments_*.csv'\n",
        "\n",
        "!ls -altr sum_sentiments_sents_trans_*.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltuVywQyXt1J"
      },
      "source": [
        "**Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYZ7xFFDQrzD"
      },
      "source": [
        "# (Optional) Read Sentence Sentiment Data generated by Transformers into DataFrame: corpus_sents_trans_df\n",
        "#            SKIP if no Transformer Sentence Sentiment datafile to read in\n",
        "\n",
        "# sum_sentiment_sents_transformers_filename = 'sum_sentiments_sents_transformers_ianmcewan_machineslikeme.csv'\n",
        "sum_sentiment_sents_transformers_filename = 'sum_sentiments_sents_trans_ddefoe_robinsoncrusoe.csv'\n",
        "corpus_sents_trans_df = pd.read_csv(sum_sentiment_sents_transformers_filename)\n",
        "\n",
        "# Optional columns to drop\n",
        "corpus_sents_trans_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "# Convert to string type\n",
        "corpus_sents_trans_df['sent_raw'] = corpus_sents_trans_df['sent_raw'].astype('string')\n",
        "\n",
        "# Add summary statistics\n",
        "corpus_sents_trans_df['char_len'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: len(x))\n",
        "corpus_sents_trans_df['token_len'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: len(x.split())) \n",
        "\n",
        "corpus_sents_trans_df.head(2)\n",
        "corpus_sents_trans_df.info()\n",
        "corpus_sents_trans_df.columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_iGanTXZSGB"
      },
      "source": [
        "# Standardize all values with MedianIQR\n",
        "\n",
        "model_transformers_ls = ['nlptown', 'robertalg15', 'yelp', 'hinglish', 'imdb2way', 'huggingface', 't5imdb50k', 'robertaxml8lang']\n",
        "\n",
        "for model_transformer in model_transformers_ls:\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing Sentiment by Sentence Length\n",
        "  sents_len_ls = list(corpus_sents_trans_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_sents_trans_df[model_transformer])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/sents_len_ls[i] for i in range(len(sents_len_ls))]\n",
        "\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_sents_trans_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  col_medianiqr = f'{model_transformer}_medianiqr'\n",
        "  corpus_sents_trans_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sents_trans_df[model_transformer]).reshape(-1, 1))\n",
        "  col_lnorm_medianiqr = f'{model_transformer}_lnorm_medianiqr'\n",
        "  corpus_sents_trans_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNeuepF33Zyu"
      },
      "source": [
        "##### **Sentence Transformers SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkyav7kjw5wp"
      },
      "source": [
        "corpus_sents_trans_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfwwefDOXqq8"
      },
      "source": [
        "RoBERTaLg15_Arc = True #@param {type:\"boolean\"}\n",
        "T5IMDB50k_Arc = False #@param {type:\"boolean\"}\n",
        "Huggingface_Arc = False #@param {type:\"boolean\"}\n",
        "NLPTown_Arc = False #@param {type:\"boolean\"}\n",
        "RoBERTaXML8lang_Arc = False #@param {type:\"boolean\"}\n",
        "IMDB2way_Arc = False #@param {type:\"boolean\"}\n",
        "Hinglish_Arc = False #@param {type:\"boolean\"}\n",
        "Yelp_Arc = False #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFE8fSS-Xqq_"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "# display(corpus_sents_df.head())\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# display(corpus_syuzhetr_df.head())\n",
        "\n",
        "model_transformers_ls = ['nlptown', 'robertalg15',\n",
        "                         'yelp', 'hinglish',\n",
        "                         'imdb2way', 'huggingface',\n",
        "                         't5imdb50k', 'robertaxml8lang']\n",
        "\n",
        "# Scaling Dictionary for each plot in form of tuple (scale, center) \n",
        "#     so they can be compared on same graph\n",
        "model_transformers_scale_dt = {'nlptown' : (0.5, -1),\n",
        "                               'robertalg15' : (1,0),\n",
        "                               'yelp' : (1.0, 0.5),\n",
        "                               'hinglish' : (1.0, 0.5),\n",
        "                               'imdb2way' : (1.0, 0.5),\n",
        "                               'huggingface' : (1.0, 0.5),\n",
        "                               't5imdb50k' : (1.0, 0.5),\n",
        "                               'robertxml8lang' : (1.0, 0.5)}\n",
        "\n",
        "win_per = SMA_Window_Percent\n",
        "win_roll = int(win_per/100 * corpus_sents_trans_df.shape[0])\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_transformers_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  if len(str(win_per)) == 1:\n",
        "    roll_str = 'roll0' + str(win_per) + '0'\n",
        "  else:\n",
        "    roll_str = 'roll' + str(win_per) + '0'\n",
        "  col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  corpus_sents_trans_df[col_name_roll] = corpus_sents_trans_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_' + roll_str\n",
        "model_transformers_lnorm_medianiqr_ls = []\n",
        "corpus_sents_trans_df[col_mean_roll] = corpus_sents_trans_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\n",
        "col_mean_lnorm_median_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "model_transformers_lnorm_medianiqr_ls = []\n",
        "for acol_name in model_transformers_ls:\n",
        "  model_transformers_lnorm_medianiqr_ls.append(acol_name+'_lnorm_medianiqr_'+roll_str)\n",
        "corpus_sents_trans_df[col_mean_lnorm_median_roll] = corpus_sents_trans_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\n",
        "\n",
        "model_transformers_subset_ls = []\n",
        "\"\"\"\n",
        "if NLPTown_Arc == True:\n",
        "  model_transformers_subset_ls.append('nlptown_lnorm_medianiqr'+'_'+roll_str)\n",
        "if T5IMDB50k_Arc == True:\n",
        "  model_transformers_subset_ls.append('t5imdb50k_lnorm_medianiqr'+'_'+roll_str)\n",
        "if Huggingface_Arc == True:\n",
        "  model_transformers_subset_ls.append('huggingface_lnorm_medianiqr'+'_'+roll_str)\n",
        "if RoBERTaLg15_Arc == True:\n",
        "  model_transformers_subset_ls.append('robertalg15_lnorm_medianiqr'+'_'+roll_str)\n",
        "if RoBERTaXML8lang_Arc == True:\n",
        "  model_transformers_subset_ls.append('robertaxml8lang_lnorm_medianiqr'+'_'+roll_str)\n",
        "if IMDB2way_Arc == True:\n",
        "  model_transformers_subset_ls.append('imdb2way_lnorm_medianiqr'+'_'+roll_str)\n",
        "if Hinglish_Arc == True:\n",
        "  model_transformers_subset_ls.append('hinglish_lnorm_medianiqr'+'_'+roll_str)\n",
        "if Yelp_Arc == True:\n",
        "  model_transformers_subset_ls.append('yelp_lnorm_medianiqr'+'_'+roll_str)\n",
        "\"\"\";\n",
        "\n",
        "if NLPTown_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'nlptown_{roll_str}_stdscale')\n",
        "if T5IMDB50k_Arc == True:\n",
        "  model_transformers_subset_ls.append(f't5imdb50k_{roll_str}_stdscale')\n",
        "if Huggingface_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'huggingface_{roll_str}_stdscale')\n",
        "if RoBERTaLg15_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'robertalg15_{roll_str}_stdscale')\n",
        "if RoBERTaXML8lang_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'robertaxml8lang_{roll_str}_stdscale')\n",
        "  #                                     robertaxml8lang_roll100_stdscale\n",
        "if IMDB2way_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'imdb2way_{roll_str}_stdscale')\n",
        "if Hinglish_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'hinglish_{roll_str}_stdscale')\n",
        "if Yelp_Arc == True:\n",
        "  model_transformers_subset_ls.append(f'yelp_{roll_str}_stdscale')\n",
        "\n",
        "print(f'model_transformers_subset_ls: {model_transformers_subset_ls}')\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Safe)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "\n",
        "# add traces\n",
        "for i, amodel in enumerate(model_transformers_subset_ls):\n",
        "  model_root = amodel.split('_')[0]\n",
        "  print(f'In loop, model={amodel}\\n         model_root={model_root}')\n",
        "  fig.add_traces(go.Line(x = corpus_sents_trans_df['sent_no'],\n",
        "                        y = model_transformers_scale_dt[model_root][0]*corpus_sents_trans_df[amodel] + model_transformers_scale_dt[model_root][1],\n",
        "                        text = corpus_sents_trans_df['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model <b>\"+amodel+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "if Mean_Subset_Arc == True:\n",
        "  mean_subset_col = col_mean_roll\n",
        "  corpus_sents_trans_df[mean_subset_col] = corpus_sents_trans_df[model_transformers_subset_ls].mean(axis=1)\n",
        "  fig.add_traces(go.Line(x=corpus_sents_trans_df['sent_no'],\n",
        "                        y = corpus_sents_trans_df[mean_subset_col],\n",
        "                        line=dict(\n",
        "                              # color='#000000',\n",
        "                              width=5\n",
        "                              ),\n",
        "                        text = 'NA', # corpus_sents_trans_df['sent_raw'],\n",
        "                        name = mean_subset_col,\n",
        "                        hovertemplate = \"Model <b>\"+mean_subset_col+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f\"Sentence Transformer Sentiment Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "    xaxis_title=\"Sentence Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdRpXL6_yCWZ"
      },
      "source": [
        "robertaxml8lang_roll100_stdscale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16QqsYHTdYqY"
      },
      "source": [
        "##### **(ABOVE) Plotly SMA Sentence Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UWlGJ1IQG9t"
      },
      "source": [
        "##### **Comparison of Sentence Transformer Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqnwt6ygQJws"
      },
      "source": [
        "# Compare Sentence Transformer Standardized Sentiment Values\n",
        "\n",
        "model_transformers_ls = ['nlptown', 'robertalg15',\n",
        "                         'yelp', 'hinglish',\n",
        "                         'imdb2way', 'huggingface',\n",
        "                         't5imdb50k', 'robertaxml8lang']\n",
        "\n",
        "model_trans_standardized_roll_ls = []\n",
        "for amodel in model_transformers_ls:\n",
        "  col_name = f'{amodel}_lnorm_medianiqr_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_trans_standardized_roll_ls.append(col_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,amodel in enumerate(model_trans_standardized_roll_ls):\n",
        "  col_name_roll_stand = f'{col_name}_stand'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')\n",
        "  model_roll_stand_np = np.array(corpus_sents_trans_df[amodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_sents_trans_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_sents_trans_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Transformer Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxpxHLrpQG9x"
      },
      "source": [
        "# Create a comparison DataFrame of SentimentR Sentence Models\n",
        "\n",
        "model_transformers_ls = ['nlptown', 'robertalg15',\n",
        "                         'yelp', 'hinglish',\n",
        "                         'imdb2way', 'huggingface',\n",
        "                         't5imdb50k', 'robertaxml8lang']\n",
        "\n",
        "corr_df = pd.DataFrame()\n",
        "corr_df = corpus_sents_trans_df[model_transformers_ls].corr(method='spearman')\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhnAu46O7aq_"
      },
      "source": [
        "##### **Explore Sentence Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTFCZI667arB"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xeZ5Qhb7arB"
      },
      "source": [
        "Crux_Window_Percent = 6 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SentimentR_SMA_Model = \"RoBERTa Large 15 Databases\" #@param [\"NLPTown\", \"RoBERTa Large 15 Databases\", \"Distilled BERT SST\", \"BERT SST\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SentimentR_SMA_Model == 'NLPTown':\n",
        "  model_selected = f'nlptown'\n",
        "if SentimentR_SMA_Model == 'RoBERTa Large 15 Databases':\n",
        "  model_selected = f'robertalg15'\n",
        "if SentimentR_SMA_Model == 'Distilled BERT SST':\n",
        "  model_selected = f'distillbertsst'\n",
        "if SentimentR_SMA_Model == 'BERT SST':\n",
        "  model_selected = f'bertsst'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_lnorm_medianiqr_{roll_str}'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_stand_ls = [model_selected_fullname]\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "for amodel in corpus_models_stand_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_trans_df, \n",
        "                                         col_series=corpus_models_stand_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_height=0, \n",
        "                                         subtitle_str=' ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P34IC3Kh7arD"
      },
      "source": [
        "**Get Top-n Crux Peaks/Valleys with surrounding Context**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99RhVdHc7arE"
      },
      "source": [
        "Get_Peak_Cruxes = False #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 10 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "No_Paragraphs_on_Each_Side = 0 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        library_type='transformers',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y90bCSB7arF"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFi5kyx-7arG"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  1047#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRjHsOIpXovL"
      },
      "source": [
        "##### **Paragraph Transformers SMA Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBrSjSA4Qytu"
      },
      "source": [
        "# (Optional) Read Paragraph Sentiment Data generated by Transformers into DataFrame: corpus_parags_trans_df\n",
        "#            SKIP if no Transformer Paragraph Sentiment datafile to read in\n",
        "\n",
        "sum_sentiment_parags_transformers_filename = 'sum_sentiments_sents_transformers_ianmcewan_machineslikeme.csv'\n",
        "corpus_parags_trans_df = pd.read_csv(sum_sentiment_parags_transformers_filename)\n",
        "\n",
        "# Optional columns to drop\n",
        "corpus_parags_trans_df.drop(columns=['bertsst_pol', 'bertsst_prob'], axis=1, inplace=True)\n",
        "\n",
        "corpus_parags_trans_df.head(2)\n",
        "corpus_parags_trans_df.info()\n",
        "corpus_parags_trans_df.columns\n",
        "\n",
        "\"\"\"\n",
        "corpus_parags_trans_len = corpus_parags_trans_df.shape[0]\n",
        "\n",
        "if corpus_parags_trans_len != corpus_sents_df.shape[0]:\n",
        "  print('\\n\\n\\n======================================================================\\n')\n",
        "  print(f'ERROR: sentence sentiment values read into corpus_syuzhetr (len={corpus_transformers_len})')\n",
        "  print(f'       is not the same length as corpus_parags_trans_df (len={corpus_parags_trans_df.shape[0]}) ')\n",
        "  print(f'\\nRECOMMENDATION: Use the preprocessed corpus output created by this notebook ')\n",
        "  print(f'                as input to SyuzhetR in RStudio to generate sentiment series')\n",
        "  print(f'                and then retry importing')\n",
        "  print('\\n======================================================================\\n')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h1hIcSobYfQ"
      },
      "source": [
        "# Standardize all values with MedianIQR\n",
        "\n",
        "model_transformers_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "\n",
        "for model_transformer in model_transformers_ls:\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "  parags_len_ls = list(corpus_parags_trans_df['token_len'])\n",
        "  parags_sentiment_ls = list(corpus_parags_trans_df[model_transformer])\n",
        "  parags_sentiment_norm_ls = [parags_sentiment_ls[i]/parags_len_ls[i] for i in range(len(parags_len_ls))]\n",
        "\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  # corpus_parags_trans_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  col_medianiqr = f'{model_transformer}_medianiqr'\n",
        "  corpus_parags_trans_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_parags_trans_df[model_transformer]).reshape(-1, 1))\n",
        "  col_lnorm_medianiqr = f'{model_transformer}_lnorm_medianiqr'\n",
        "  corpus_parags_trans_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2ee-U3HUbre"
      },
      "source": [
        "# Calculate Transformer Rolling Windows = win_per of Corpus (default 5%)\n",
        "\n",
        "\n",
        "\n",
        "win_per = 10           \n",
        "win_roll = int(win_per/100 * corpus_parags_trans_df.shape[0])\n",
        "\n",
        "# model_transformers_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "model_transformers_ls = ['nlptown_lnorm_medianiqr', 'robertalg15_lnorm_medianiqr', 'distillbertsst_lnorm_medianiqr', 'bertsst_lnorm_medianiqr']\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_transformers_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  col_name_roll = f'{amodel}_roll050'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  corpus_parags_trans_df[col_name_roll] = corpus_parags_trans_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "corpus_parags_trans_df['mean_all_roll050'] = corpus_parags_trans_df[col_name_roll_ls].mean(axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Kh--NklUbmp"
      },
      "source": [
        "NLPTown_Arc = True #@param {type:\"boolean\"}\n",
        "RoBERTaLg15_Arc = True #@param {type:\"boolean\"}\n",
        "DistillBERTSST_Arc = True #@param {type:\"boolean\"}\n",
        "BERTSST_Arc = True #@param {type:\"boolean\"}\n",
        "Mean_Subset_Arc = True #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr4Wqoh36V29"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_transformers_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  if len(str(win_per)) == 1:\n",
        "    roll_str = 'roll0' + str(win_per) + '0'\n",
        "  else:\n",
        "    roll_str = 'roll' + str(win_per) + '0'\n",
        "  col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  corpus_sents_trans_df[col_name_roll] = corpus_sents_trans_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_' + roll_str\n",
        "model_transformers_lnorm_medianiqr_ls = []\n",
        "corpus_sents_trans_df[col_mean_roll] = corpus_sents_trans_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\n",
        "col_mean_lnorm_median_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "model_transformers_lnorm_medianiqr_ls = []\n",
        "for acol_name in model_transformers_ls:\n",
        "  model_transformers_lnorm_medianiqr_ls.append(acol_name+'_lnorm_medianiqr_'+roll_str)\n",
        "corpus_sents_trans_df[col_mean_lnorm_median_roll] = corpus_sents_trans_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IVvASZFUbdJ"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "win_per = SMA_Window_Percent\n",
        "win_roll = int(win_per/100 * corpus_parags_trans_df.shape[0])\n",
        "\n",
        "if len(str(win_per)) == 1:\n",
        "  roll_str = 'roll0' + str(win_per) + '0'\n",
        "else:\n",
        "  roll_str = 'roll' + str(win_per) + '0'\n",
        "\n",
        "# display(corpus_sents_df.head())\n",
        "\n",
        "model_transformers_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "\n",
        "# list of (scale, center) adjustments for each model so they can be compared on same graph\n",
        "model_transformers_scale_ls = [(0.3, -0.6), (1,0.1), (1,0.1), (1,0.1)]\n",
        "\n",
        "col_name_roll_ls = []\n",
        "# fig, ax = plt.subplots()\n",
        "for amodel in model_transformers_ls:\n",
        "  # if not(amodel.endswith('roll050')):\n",
        "  col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n",
        "  col_name_roll_ls.append(col_name_roll)\n",
        "  # else:\n",
        "  #   col_name_roll_ls.append(amodel)\n",
        "  print(f'creating: {col_name_roll}')\n",
        "  corpus_parags_trans_df[col_name_roll] = corpus_parags_trans_df[amodel].rolling(win_roll, center=True).mean()\n",
        "\n",
        "col_mean_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "# model_transformers_lnorm_medianiqr_ls = []\n",
        "corpus_parags_trans_df[col_mean_roll] = corpus_parags_trans_df[col_name_roll_ls].mean(axis=1)\n",
        "\n",
        "\"\"\"\n",
        "col_mean_lnorm_median_roll = 'mean_lnorm_medianiqr_' + roll_str\n",
        "# model_transformers_lnorm_medianiqr_ls = []\n",
        "for acol_name in model_transformers_ls:\n",
        "  model_transformers_lnorm_medianiqr_ls.append(acol_name)\n",
        "corpus_parags_trans_df[col_mean_lnorm_median_roll] = corpus_parags_trans_df[model_transformers_lnorm_medianiqr_ls].mean(axis=1)\n",
        "\"\"\";\n",
        "\n",
        "\n",
        "model_transformers_subset_ls = []\n",
        "if NLPTown_Arc == True:\n",
        "  # model_transformers_subset_ls.append('nlptown_lnorm_medianiqr_roll050')\n",
        "  model_transformers_subset_ls.append('nlptown_lnorm_medianiqr' + '_' + roll_str)\n",
        "if RoBERTaLg15_Arc == True:\n",
        "  model_transformers_subset_ls.append('robertalg15_lnorm_medianiqr' +'_' + roll_str)\n",
        "if DistillBERTSST_Arc == True:\n",
        "  model_transformers_subset_ls.append('distillbertsst_lnorm_medianiqr' + '_' + roll_str)\n",
        "if BERTSST_Arc == True:\n",
        "  model_transformers_subset_ls.append('bertsst_lnorm_medianiqr' + '_' + roll_str)\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Safe)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "# add traces\n",
        "for i,amodel in enumerate(model_transformers_subset_ls):\n",
        "  fig.add_traces(go.Line(x = corpus_parags_trans_df['sent_no'],\n",
        "                        y = model_transformers_scale_ls[i][0]*corpus_parags_trans_df[amodel]+model_transformers_scale_ls[i][1],\n",
        "                        text = corpus_parags_trans_df['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model: <b>\"+amodel+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "if Mean_Subset_Arc == True:\n",
        "  mean_subset_col = 'mean_subset_roll050'\n",
        "  corpus_parags_trans_df[mean_subset_col] = corpus_parags_trans_df[model_transformers_subset_ls].mean(axis=1)\n",
        "  fig.add_traces(go.Line(x=corpus_parags_trans_df['sent_no'],\n",
        "                        y = corpus_parags_trans_df[mean_subset_col],\n",
        "                        line=dict(\n",
        "                              # color='#000000',\n",
        "                              width=5\n",
        "                              ),\n",
        "                        text = 'NA', # corpus_parags_trans_df['sent_raw'],\n",
        "                        name = mean_subset_col,\n",
        "                        hovertemplate = \"Model <b>%{mean_subset_col}</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y:.4f}</b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Paragraph Transformer Sentiment Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "    xaxis_title=\"Paragraph Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89X8Sz2QdMn-"
      },
      "source": [
        "##### **(ABOVE) Plotly SMA Paragraph Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmbxdtyvQo3K"
      },
      "source": [
        "##### **Comparison of Paragraph Transformer Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIaxX1dEbvkE"
      },
      "source": [
        "# Compare Paragraph Transformer Standardized Sentiment Values\n",
        "\n",
        "model_trans_ls = ['nlptown', 'robertalg15', 'distillbertsst', 'bertsst']\n",
        "\n",
        "model_trans_standardized_roll_ls = []\n",
        "for amodel in model_trans_ls:\n",
        "  col_name = f'{amodel}_lnorm_medianiqr_{roll_str}'  # TODO: drop lnorm_medianiqr earlier and just Standardize here\n",
        "                                                     # NOTE: Simple SciPy StandardScaler works on SMA Series that don't have outliers like Raw Series\n",
        "  # print(f'col_name: {col_name}')\n",
        "  model_trans_standardized_roll_ls.append(col_name)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i,amodel in enumerate(model_trans_standardized_roll_ls):\n",
        "  col_name_roll_stand = f'{col_name}_stand'\n",
        "  # print(f'col_name_roll_stand: {col_name_roll_stand}')  # sent\n",
        "  model_roll_stand_np = np.array(corpus_parags_trans_df[amodel])\n",
        "  # .apply(lambda x: Scale_SentimentR*x))\n",
        "  \n",
        "  model_roll_stand_np = model_roll_stand_np.reshape((len(model_roll_stand_np), 1))\n",
        "\n",
        "  scaler = scaler.fit(model_roll_stand_np)\n",
        "  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n",
        "  model_roll_stand_xform_np = scaler.transform(model_roll_stand_np)\n",
        "\n",
        "  corpus_parags_trans_df[col_name_roll_stand] = pd.Series(model_roll_stand_xform_np.flatten())\n",
        "\n",
        "  # Plot\n",
        "  corpus_parags_trans_df[col_name_roll_stand].plot(label=amodel) # label=col_name_roll_stand))\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Paragraph Transformer Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtxzLN7CQo3R"
      },
      "source": [
        "# Create a comparison DataFrame of SentimentR Paragraph Models\n",
        "\n",
        "corr_transformers_models_ls = ['nlptown','robertalg15', 'distillbertsst','bertsst']\n",
        "\n",
        "corr_transformers_df = corpus_parags_trans_df[corr_transformers_models_ls].corr(method='spearman')\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dowF8jrw67W7"
      },
      "source": [
        "# Quick Sentence vs Paragraph Transformer Sentiment SMA Comparison\n",
        "\n",
        "plt.close()\n",
        "\n",
        "# Colab Jupyter wouldn't plot pd.Series from 2 different DataFrames on the same graph\n",
        "#   so combine into temporary DataFrame as a workaround\n",
        "\n",
        "temp_df = pd.DataFrame(columns = ['Sentences', 'Paragraphs'])\n",
        "\n",
        "y_col = f'mean_lnorm_medianiqr_{roll_str}'\n",
        "temp_df['Sentences'] = corpus_sents_trans_df[y_col]\n",
        "temp_df['Paragraphs'] = corpus_parags_trans_df[y_col]\n",
        "\n",
        "temp_df['Sentences'].plot(linewidth=10)\n",
        "temp_df['Paragraphs'].plot()\n",
        "\n",
        "\n",
        "# plt.plot(corpus_sents_trans_df[y_col], label='Sentences')\n",
        "\n",
        "# y_col = f'mean_lnorm_medianiqr_{roll_str}'\n",
        "# plt.plot(corpus_parags_trans_df[y_col], label='Paragraphs')\n",
        "\n",
        "# plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Paragraph Transformer Sentiments\\nMean Length-Normed MedianIQR Sentiments SMA ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJIK_j3wbx97"
      },
      "source": [
        "corpus_sents_trans_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZorifAHe7ye"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5CazR5rY_u2"
      },
      "source": [
        "##### **Explore Paragraph Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHihplfSY_u4"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvndYD2CY_u6"
      },
      "source": [
        "Crux_Window_Percent = 2 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "SentimentR_SMA_Model = \"RoBERTa Large 15 Databases\" #@param [\"NLPTown\", \"RoBERTa Large 15 Databases\", \"Distilled BERT SST\", \"BERT SST\"]\n",
        "Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "if SentimentR_SMA_Model == 'NLPTown':\n",
        "  model_selected = f'nlptown'\n",
        "if SentimentR_SMA_Model == 'RoBERTa Large 15 Databases':\n",
        "  model_selected = f'robertalg15'\n",
        "if SentimentR_SMA_Model == 'Distilled BERT SST':\n",
        "  model_selected = f'distillbertsst'\n",
        "if SentimentR_SMA_Model == 'BERT SST':\n",
        "  model_selected = f'bertsst'\n",
        "\n",
        "if Anomaly_Detection == False:\n",
        "  # (a) Use Sentence SMA smoothed Sentiment models to detect Crux Points\n",
        "  model_selected_fullname = f'{model_selected}_lnorm_medianiqr_{roll_str}'\n",
        "else:\n",
        "  # (b)Use Sentence Raw Sentiment models to detect outliers\n",
        "  model_selected_fullname = f'{model_selected}'\n",
        "\n",
        "\n",
        "# TODO: enable multiple overlay crux points with underlying mean/median arc\n",
        "corpus_models_stand_ls = [model_selected_fullname]\n",
        "\n",
        "# Warning: requires definitions of: x, section_sents_df\n",
        "#          so Baseline models must be run first\n",
        "\n",
        "for amodel in corpus_models_stand_ls:\n",
        "  corpus_cruxes_all_dt[amodel] = get_crux_points(ts_df=corpus_sents_trans_df, \n",
        "                                         col_series=corpus_models_stand_ls, \n",
        "                                         text_type='sentence', \n",
        "                                         win_per=Crux_Window_Percent, \n",
        "                                         sec_y_height=0, \n",
        "                                         subtitle_str=' ', \n",
        "                                         do_plot=True, \n",
        "                                         save2file=False)\n",
        "  \n",
        "model_crux_ls = corpus_cruxes_all_dt[amodel]\n",
        "# model_crux_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR0h5f7bY_u8"
      },
      "source": [
        "**Get Top-n Crux Peaks/Valleys with surrounding Context**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beSU4keNY_u-"
      },
      "source": [
        "# Crux Details\n",
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 6 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        library_type='transformers',\n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes, \n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "else:\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG8t02zqY_vA"
      },
      "source": [
        "**Zoom in on Context surrounding a particular Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Sm942CY_vB"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  4494#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEcDjmwcyysJ"
      },
      "source": [
        "### **Compare ALL Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAx6TUxysYoG"
      },
      "source": [
        "#### **Review, Processes and Combine into Unified DataFrame**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWeRmjD7sgUA"
      },
      "source": [
        "**Review**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiG9p1S20Bu_"
      },
      "source": [
        "corpus_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYoIb-jVr-3k"
      },
      "source": [
        "corpus_sents_sentimentr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFAQOy4fr-vs"
      },
      "source": [
        "corpus_sents_syuzhetr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rX0VMb5r-rM"
      },
      "source": [
        "corpus_sents_trans_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zURjifUsh3t"
      },
      "source": [
        "**Process**\n",
        "\n",
        "NOTE: Assume only base_model Raw Sentiment Series exist in each of the 4 Library DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCHRu2RwsW5l"
      },
      "source": [
        "groups_ls = ['models_baseline_ls',\n",
        "                'models_sentimentr_ls',\n",
        "                'models_syuzhetr_ls',\n",
        "                'models_transformer_ls']\n",
        "\n",
        "models_baseline_ls = ['sentimentr',\n",
        "                      'syuzhet',\n",
        "                      'bing',\n",
        "                      'sentiword',\n",
        "                      'senticnet',\n",
        "                      'nrc',\n",
        "                      'afinn',\n",
        "                      'vader',\n",
        "                      'textblob',\n",
        "                      'pattern',\n",
        "                      'stanza']\n",
        "\n",
        "models_sentimentr_ls = ['jockers_rinker',\n",
        "                        'jockers',\n",
        "                        'huliu',\n",
        "                        'senticnet',\n",
        "                        'sentiword',\n",
        "                        'nrc',\n",
        "                        'lmcd']\n",
        "\n",
        "models_syuzhetr_ls = ['syuzhet',\n",
        "                      'bing',\n",
        "                      'afinn',\n",
        "                      'nrc']\n",
        "\n",
        "models_transformer_ls = ['robertalg15', \n",
        "                         'nlptown', \n",
        "                         'yelp', \n",
        "                         'hinglish',\n",
        "                         'imdb2way', \n",
        "                         'huggingface', \n",
        "                         't5imdb50k', \n",
        "                         'robertaxml8lang']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk86ubNB_Hzn"
      },
      "source": [
        "# StandardScaler SMA for Baseline Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_sents_df.shape[0]*win_per/100)\n",
        "\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_baseline_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_sents_df[col_sma_winper] = corpus_sents_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{col_sma_winper}_stdscale'\n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_sents_df[col_sma_winper])\n",
        "  corpus_sents_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "\n",
        "  # Plot\n",
        "  # corpus_sents_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  corpus_sents_df[col_sma_winper_stdscale].plot()\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Baseline Sentiments\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\"\"\"\n",
        "for agroup in groups_ls:\n",
        "  for amodel in globals()[agroup]:\n",
        "    print(f'Processing Model: {amodel:>15} in Group: {agroup}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPqb0SeVALi5"
      },
      "source": [
        "# StandardScaler SMA for SentimentR Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_sents_sentimentr_df.shape[0]*win_per/100)\n",
        "\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_sentimentr_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_sents_sentimentr_df[col_sma_winper] = corpus_sents_sentimentr_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{col_sma_winper}_stdscale'\n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_sents_sentimentr_df[col_sma_winper])\n",
        "  corpus_sents_sentimentr_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "\n",
        "  # Plot\n",
        "  # corpus_sents_sentimentr_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  corpus_sents_sentimentr_df[col_sma_winper_stdscale].plot()\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Baseline Sentiments\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\"\"\"\n",
        "for agroup in groups_ls:\n",
        "  for amodel in globals()[agroup]:\n",
        "    print(f'Processing Model: {amodel:>15} in Group: {agroup}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkLEuh1Bgk9l"
      },
      "source": [
        "corpus_sents_syuzhetr_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnUuECApALc9"
      },
      "source": [
        "# StandardScaler SMA for SyuzhetR Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_sents_syuzhetr_df.shape[0]*win_per/100)\n",
        "\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_syuzhetr_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_sents_syuzhetr_df[col_sma_winper] = corpus_sents_syuzhetr_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{col_sma_winper}_stdscale'\n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_sents_syuzhetr_df[col_sma_winper])\n",
        "  corpus_sents_syuzhetr_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "\n",
        "  # Plot\n",
        "  # corpus_sents_syuzhetr_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  corpus_sents_syuzhetr_df[col_sma_winper_stdscale].plot()\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Baseline Sentiments\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\"\"\"\n",
        "for agroup in groups_ls:\n",
        "  for amodel in globals()[agroup]:\n",
        "    print(f'Processing Model: {amodel:>15} in Group: {agroup}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SN6gRLJALZP"
      },
      "source": [
        "# StandardScaler SMA for Transformer Models\n",
        "\n",
        "SMA_Window_Percentage = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "\n",
        "\n",
        "# Convert the SMA Window from Percentage of Corpus to No of Sentences\n",
        "win_per = SMA_Window_Percentage\n",
        "win_sents = int(corpus_sents_trans_df.shape[0]*win_per/100)\n",
        "\n",
        "# Loop over every Group and within each Group, loop over each Model\n",
        "for amodel in models_transformer_ls:\n",
        "\n",
        "  # Print the current Group/Model that is being used\n",
        "  # print(f'Processing Model: {amodel:>15} in Group: Baselines')\n",
        "\n",
        "  # Generate new SMA col name\n",
        "  if len(str(win_per)) < 2:\n",
        "    col_sma_winper = f'{amodel}_roll0{str(win_per)}0'\n",
        "  else:\n",
        "    col_sma_winper = f'{amodel}_roll{str(win_per)}0'\n",
        "\n",
        "  # Create new SMA Column\n",
        "  # print(f'creating roll col: {col_sma_winper}')\n",
        "  corpus_sents_trans_df[col_sma_winper] = corpus_sents_trans_df[amodel].rolling(win_sents, center=True).mean()\n",
        "\n",
        "  # Standardize SMA Column\n",
        "  col_sma_winper_stdscale = f'{col_sma_winper}_stdscale'\n",
        "  # print(f'creating stdscale col of roll: {col_sma_winper_stdscale}')\n",
        "  series_stdscale_ls = get_standardscaler(amodel, corpus_sents_trans_df[col_sma_winper])\n",
        "  corpus_sents_trans_df[col_sma_winper_stdscale] = pd.Series(series_stdscale_ls)\n",
        "\n",
        "  # Plot\n",
        "  # corpus_sents_trans_df.iloc[200:1000][col_sma_winper_stdscale].plot()\n",
        "  corpus_sents_trans_df[col_sma_winper_stdscale].plot()\n",
        "\n",
        "plt.grid()\n",
        "plt.title(f'{CORPUS_FULL} Compare Sentence Transformer Sentiments\\nStandardScaler of SMA Smoothed Arcs ({roll_str.capitalize()})')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\"\"\"\n",
        "for agroup in groups_ls:\n",
        "  for amodel in globals()[agroup]:\n",
        "    print(f'Processing Model: {amodel:>15} in Group: {agroup}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06NwK2PrmIV9"
      },
      "source": [
        "# Vertically Concatenate ALL 4 Sentiment Groups StandardizedScaled SMA Sentence Sentiment Series into 1 Big DataFrame\n",
        "\n",
        "# Get Baseline Model StandardScaled SMA column names\n",
        "cols_baseline_stdscale_ls = []\n",
        "for amodel in models_baseline_ls:\n",
        "  col_roll_stdscale = f'{amodel}_{roll_str}_stdscale'\n",
        "  cols_baseline_stdscale_ls.append(col_roll_stdscale)\n",
        "# print(f'\\nBaseline StdScaled SMA Columns:\\n    {cols_baseline_stdscale_ls}')\n",
        "\n",
        "temp_baseline_df = corpus_sents_df[cols_baseline_stdscale_ls].copy()\n",
        "temp_baseline_df = temp_baseline_df.add_prefix('baseline_')\n",
        "temp_baseline_df.columns\n",
        "\n",
        "# Get SentimentR Model StandardScaled SMA column names\n",
        "cols_sentimentr_stdscale_ls = []\n",
        "for amodel in models_sentimentr_ls:\n",
        "  col_roll_stdscale = f'{amodel}_{roll_str}_stdscale'\n",
        "  cols_sentimentr_stdscale_ls.append(col_roll_stdscale)\n",
        "# print(f'\\nSentimentR StdScaled SMA Columns:\\n    {cols_sentimentr_stdscale_ls}')\n",
        "\n",
        "temp_sentimentr_df = corpus_sents_sentimentr_df[cols_sentimentr_stdscale_ls].copy()\n",
        "temp_sentimentr_df = temp_sentimentr_df.add_prefix('sentimentr_')\n",
        "temp_sentimentr_df.columns\n",
        "\n",
        "# Get SyuzhetR Model StandardScaled SMA column names\n",
        "cols_syuzhetr_stdscale_ls = []\n",
        "for amodel in models_syuzhetr_ls:\n",
        "  col_roll_stdscale = f'{amodel}_{roll_str}_stdscale'\n",
        "  cols_syuzhetr_stdscale_ls.append(col_roll_stdscale)\n",
        "# print(f'\\nSyuzhetR StdScaled SMA Columns:\\n    {cols_syuzhetr_stdscale_ls}')\n",
        "\n",
        "temp_syuzhetr_df = corpus_sents_syuzhetr_df[cols_syuzhetr_stdscale_ls].copy()\n",
        "temp_syuzhetr_df = temp_syuzhetr_df.add_prefix('syuzhetr_')\n",
        "temp_syuzhetr_df.columns\n",
        "\n",
        "# Get Transformer Model StandardScaled SMA column names\n",
        "cols_transformer_stdscale_ls = []\n",
        "for amodel in models_transformer_ls:\n",
        "  col_roll_stdscale = f'{amodel}_{roll_str}_stdscale'\n",
        "  cols_transformer_stdscale_ls.append(col_roll_stdscale)\n",
        "# print(f'\\nTransformer StdScaled SMA Columns:\\n    {cols_transformer_stdscale_ls}')\n",
        "\n",
        "# If Transformers Sentiment DataFrame exists, add it to the Unified DataFrame\n",
        "var_name = 'corpus_sents_trans_df'\n",
        "if var_name in globals():\n",
        "  # print(f'{var_name} is declared globally')\n",
        "  # print(eval(f'{var_name}.shape[0]'))\n",
        "  corpus_sents_trans_df_len = eval(f'{var_name}.shape[0]')\n",
        "  print(f'{var_name} has {corpus_sents_trans_df_len} Sentences')\n",
        "\n",
        "  temp_transformer_df = corpus_sents_trans_df[cols_transformer_stdscale_ls].copy()\n",
        "  temp_transformer_df = temp_transformer_df.add_prefix('transformer_')\n",
        "  temp_transformer_df.columns\n",
        "\n",
        "  print(f'\\n\\n{var_name} IS declared\\n    so adding to Unified DataFrame')\n",
        "  corpus_sents_all_df = pd.concat([temp_baseline_df,\n",
        "                                  temp_sentimentr_df,\n",
        "                                  temp_syuzhetr_df,\n",
        "                                  temp_transformer_df],\n",
        "                                  axis=1)\n",
        "\n",
        "  temp_transformer_df = pd.DataFrame()\n",
        "\n",
        "else:\n",
        "  print(f'\\n\\n{var_name} IS NOT declared\\n    so NOT adding to Unified DataFrame')\n",
        "\n",
        "  corpus_sents_all_df = pd.concat([temp_baseline_df,\n",
        "                                  temp_sentimentr_df,\n",
        "                                  temp_syuzhetr_df],\n",
        "                                  axis=1)\n",
        "\n",
        "temp_baseline_df = pd.DataFrame()\n",
        "temp_sentimentr_df = pd.DataFrame()\n",
        "temp_syuzhetr_df = pd.DataFrame()\n",
        "\n",
        "print(f'\\ncorpus_sents_all_df.shape: {corpus_sents_all_df.shape}')\n",
        "# corpus_sents_all_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r34lNnz0oeQN"
      },
      "source": [
        "# Create a Correlation Heatmap for All Sentence Models\n",
        "\n",
        "# corr_df = corpus_sents_syuzhetr_df[syuzhetr_corr_models_ls].corr(method='spearman')\n",
        "corr_df = corpus_sents_all_df.corr(method='spearman')\n",
        "\n",
        "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
        "fig = sns.clustermap(corr_df,\n",
        "                     row_cluster=True,\n",
        "                     col_cluster=True,\n",
        "                     figsize=(10, 10))\n",
        "\n",
        "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
        "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nznsrhwwioqq"
      },
      "source": [
        "#### **Compare any StandardizedScale Sentence Sentiment Models**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select any combination of Sentiment Models in order of the following four Groups: Baseline, SentimentR, SyuzhetR and Transformers\n",
        "\n",
        "* All Sentiment Time Series are StandardizedScaled version of SMA created in the notebook above this cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3xvWEs5iPOp"
      },
      "source": [
        "corpus_sents_trans_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEaMXFw6hayU"
      },
      "source": [
        "Baseline_SentimentR = True #@param {type:\"boolean\"}\n",
        "Baseline_Syuzhet = False #@param {type:\"boolean\"}\n",
        "Baseline_Bing = False #@param {type:\"boolean\"}\n",
        "Baseline_SenticNet = False #@param {type:\"boolean\"}\n",
        "Baseline_SentiWord = False #@param {type:\"boolean\"}\n",
        "Baseline_NRC = False #@param {type:\"boolean\"}\n",
        "Baseline_AFINN = False #@param {type:\"boolean\"}\n",
        "Baseline_VADER = False #@param {type:\"boolean\"}\n",
        "Baseline_TextBlob = False #@param {type:\"boolean\"}\n",
        "Baseline_Pattern = False #@param {type:\"boolean\"}\n",
        "Baseline_Stanza = True #@param {type:\"boolean\"}\n",
        "# Baseline-Mean_All = False #@param {type:\"boolean\"}\n",
        "# Baseline-Mean_Subset = False #@param {type:\"boolean\"}\n",
        "# Baseline-MPQA = False #@param {type:\"boolean\"}\n",
        "# Baseline-SentiStrength = False #@param {type:\"boolean\"}\n",
        "\n",
        "SentimentR_JockersRinker = True #@param {type:\"boolean\"}\n",
        "SentimentR_Jockers = False #@param {type:\"boolean\"}\n",
        "SentimentR_HuLiu = False #@param {type:\"boolean\"}\n",
        "SentimentR_SenticNet = False #@param {type:\"boolean\"}\n",
        "SentimentR_SentiWord = True #@param {type:\"boolean\"}\n",
        "SentimentR_NRC = False #@param {type:\"boolean\"}\n",
        "SentimentR_LoughranMcDonald = False #@param {type:\"boolean\"}\n",
        "\n",
        "SyuzhetR_Syuzhet = True #@param {type:\"boolean\"}\n",
        "SyuzhetR_Bing = False #@param {type:\"boolean\"}\n",
        "SyuzhetR_AFINN = False #@param {type:\"boolean\"}\n",
        "SyuzhetR_NRC = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "Transformer_RoBERTaLg15 = True #@param {type:\"boolean\"}\n",
        "Transformer_T5IMDB50k = True #@param {type:\"boolean\"}\n",
        "Transformer_Huggingface = False #@param {type:\"boolean\"}\n",
        "Transformer_NLPTown = False #@param {type:\"boolean\"}\n",
        "Transformer_RoBERTaXML8lang = True #@param {type:\"boolean\"}\n",
        "Transformer_IMDB2way = False #@param {type:\"boolean\"}\n",
        "Transformer_Hinglish = False #@param {type:\"boolean\"}\n",
        "Transformer_Yelp = False #@param {type:\"boolean\"}\n",
        "# Mean_Subset_Arc = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkR7gXEehbbf"
      },
      "source": [
        "# Plotly Interactive/Zoom Sentiment Plots\n",
        "\n",
        "# SMA_Window_Percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# win_per = SMA_Window_Percent\n",
        "# win_roll = int(win_per/100 * corpus_sents_all_df.shape[0])\n",
        "\n",
        "# NOTE: all 4 Groups need to be run with the same SMA roll_per (usually 5 or 10%)\n",
        "#       to compare Models from the 4 different Groups\n",
        "\n",
        "\n",
        "model_all_subset_ls = []\n",
        "\n",
        "if Baseline_SentimentR == True:\n",
        "  model_all_subset_ls.append('baseline_sentimentr_roll100_stdscale')\n",
        "if Baseline_Syuzhet == True:\n",
        "  model_all_subset_ls.append('baseline_syuzhet_roll100_stdscale')\n",
        "if Baseline_Bing == True:\n",
        "  model_all_subset_ls.append('baseline_bing_roll100_stdscale')\n",
        "if Baseline_SenticNet == True:\n",
        "  model_all_subset_ls.append('baseline_senticnet_roll100_stdscale')\n",
        "if Baseline_SentiWord == True:\n",
        "  model_all_subset_ls.append('baseline_sentiword_roll100_stdscale')\n",
        "if Baseline_NRC == True:\n",
        "  model_all_subset_ls.append('baseline_nrc_roll100_stdscale')\n",
        "if Baseline_AFINN == True:\n",
        "  model_all_subset_ls.append('baseline_afinn_roll100_stdscale')\n",
        "if Baseline_VADER == True:\n",
        "  model_all_subset_ls.append('baseline_vader_roll100_stdscale')\n",
        "if Baseline_TextBlob == True:\n",
        "  model_all_subset_ls.append('baseline_stanza_roll100_stdscale')\n",
        "if Baseline_Pattern == True:\n",
        "  model_all_subset_ls.append('baseline_pattern_roll100_stdscale')\n",
        "if Baseline_Stanza == True:\n",
        "  model_all_subset_ls.append('baseline_stanza_roll100_stdscale')\n",
        "\n",
        "if SentimentR_JockersRinker == True:\n",
        "  model_all_subset_ls.append('sentimentr_jockers_rinker_roll100_stdscale')\n",
        "if SentimentR_Jockers == True:\n",
        "  model_all_subset_ls.append('sentimentr_jockers_roll100_stdscale')\n",
        "if SentimentR_HuLiu == True:\n",
        "  model_all_subset_ls.append('sentimentr_huliu_roll100_stdscale')\n",
        "if SentimentR_SenticNet == True:\n",
        "  model_all_subset_ls.append('sentimentr_senticnet_roll100_stdscale')\n",
        "if SentimentR_SentiWord == True:\n",
        "  model_all_subset_ls.append('sentimentr_sentiword_roll100_stdscale')\n",
        "if SentimentR_NRC == True:\n",
        "  model_all_subset_ls.append('sentimentr_nrc_roll100_stdscale')\n",
        "if SentimentR_LoughranMcDonald == True:\n",
        "  model_all_subset_ls.append('sentimentr_lmcd_roll100_stdscale')\n",
        "\n",
        "\n",
        "if SyuzhetR_Syuzhet == True:\n",
        "  model_all_subset_ls.append('syuzhetr_syuzhet_roll100_stdscale')\n",
        "if SyuzhetR_Bing == True:\n",
        "  model_all_subset_ls.append('syuzhetr_bing_roll100_stdscale')\n",
        "if SyuzhetR_AFINN == True:\n",
        "  model_all_subset_ls.append('syuzhetr_afinn_roll100_stdscale')\n",
        "if SyuzhetR_NRC == True:\n",
        "  model_all_subset_ls.append('syuzhetr_nrc_roll100_stdscale')\n",
        "\n",
        "# Exclude Transformer Models if not loaded/defined\n",
        "var_name = 'corpus_sents_trans_df'\n",
        "if var_name in globals():\n",
        "  # print(f'{var_name} is declared globally')\n",
        "  # print(eval(f'{var_name}.shape[0]'))\n",
        "  corpus_sents_trans_df_len = eval(f'{var_name}.shape[0]')\n",
        "  print(f'{var_name} has {corpus_sents_trans_df_len} Sentences')\n",
        "\n",
        "  if Transformer_RoBERTaLg15 == True:\n",
        "    model_all_subset_ls.append('transformer_robertalg15_roll100_stdscale')\n",
        "  if Transformer_T5IMDB50k == True:\n",
        "    model_all_subset_ls.append('transformer_t5imdb50k_roll100_stdscale')\n",
        "  if Transformer_Huggingface == True:\n",
        "    model_all_subset_ls.append('transformer_rhuggingface_roll100_stdscale')\n",
        "  if Transformer_NLPTown == True:\n",
        "    model_all_subset_ls.append('transformer_nlptown_roll100_stdscale')\n",
        "  if Transformer_RoBERTaXML8lang == True:\n",
        "    model_all_subset_ls.append('transformer_robertaxml8lang_roll100_stdscale')\n",
        "  if Transformer_IMDB2way == True:\n",
        "    model_all_subset_ls.append('transformer_imdb2way_roll100_stdscale')\n",
        "  if Transformer_Hinglish == True:\n",
        "    model_all_subset_ls.append('transformer_hinglish_roll100_stdscale')\n",
        "  if Transformer_Yelp == True:\n",
        "    model_all_subset_ls.append('transformer_yelp_roll100_stdscale')\n",
        "\n",
        "\n",
        "else:\n",
        "  print(f'ERROR: {var_name} IS NOT declared\\n    Go back and load Transformer Sentiment Datafile\\n    and re-run this code cell')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "palette = cycle(px.colors.qualitative.Safe)\n",
        "# palette = cycle(px.colors.sequential.PuBu)\n",
        "\n",
        "my_layout = go.Layout(\n",
        "    autosize=False,\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    margin=go.layout.Margin(\n",
        "        l=10,\n",
        "        r=50,\n",
        "        b=100,\n",
        "        t=100,\n",
        "        pad = 1\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=my_layout)\n",
        "\n",
        "\n",
        "# Add Sentiment Arc Plot Traces\n",
        "for i,amodel in enumerate(model_all_subset_ls):\n",
        "  fig.add_traces(go.Line(x = corpus_sents_all_df.index.values,\n",
        "                        y = corpus_sents_all_df[amodel],\n",
        "                        text = corpus_sents_df['sent_raw'], # corpus_sents_df.iloc[x]['sent_raw'],\n",
        "                        name = amodel,\n",
        "                        hovertemplate = \"Model: <b>\"+amodel+\"</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b><br>Text: <b><i>%{text}</i></b>\", \n",
        "                        marker_color=next(palette)))\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=CORPUS_FULL + \"<br>Compare Sentence StandardizedScaled SMA Sentiment Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n",
        "    xaxis_title=\"Sentence Number\",\n",
        "    # yaxis_title=\"Sentiment Value\",\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=16,\n",
        "        font_family=\"Rockwell\"\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7WUuj1j4XW7"
      },
      "source": [
        "### **Save Raw Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SIJZk6f2ccm"
      },
      "source": [
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "\n",
        "# Save the Sentences in the original Raw and Cleaned Corpus \n",
        "corpus_text_sentences_raw_filename = f'corpus_text_sentences_raw_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text raw sentences to file: {corpus_text_sentences_raw_filename}')\n",
        "corpus_sents_df['sent_raw'].to_csv(corpus_text_sentences_raw_filename)\n",
        "\n",
        "corpus_text_sentences_clean_filename = f'corpus_text_sentences_clean_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text clean sentences to file: {corpus_text_sentences_clean_filename}')\n",
        "corpus_sents_df['sent_clean'].to_csv(corpus_text_sentences_clean_filename)\n",
        "\n",
        "\n",
        "# Save the Paragraphs in the original Raw and Cleaned Corpus\n",
        "corpus_text_paragraphs_raw_filename = f'corpus_text_paragraphs_raw_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text raw paragraphs to file: {corpus_text_paragraphs_raw_filename}')\n",
        "corpus_parags_df['parag_raw'].to_csv(corpus_text_paragraphs_raw_filename)\n",
        "\n",
        "corpus_text_paragraphs_clean_filename = f'corpus_text_sentences_clean_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text clean sentences to file: {corpus_text_paragraphs_clean_filename}')\n",
        "corpus_parags_df['parag_clean'].to_csv(corpus_text_paragraphs_clean_filename)\n",
        "\n",
        "\n",
        "# Save the Sentences of each of the 4 Groups DataFrames \n",
        "corpus_sents_baseline_filename = f'sum_sentiments_sents_baseline_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Sentence Baselines to file: {corpus_sents_baseline_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_baseline_filename)\n",
        "\n",
        "corpus_sents_sentimentr_filename = f'sum_sentiments_sents_sentimentr_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Sentence SentimentR to file: {corpus_sents_sentimentr_filename}')\n",
        "corpus_sents_sentimentr_df.to_csv(corpus_sents_sentimentr_filename)\n",
        "\n",
        "corpus_sents_syuzhetr_filename = f'sum_sentiments_sents_syuzhetr_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Sentence SyuzhetR to file: {corpus_sents_syuzhetr_filename}')\n",
        "corpus_sents_syuzhetr_df.to_csv(corpus_sents_syuzhetr_filename)\n",
        "\n",
        "corpus_sents_transformer_filename = f'sum_sentiments_sents_transformer_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Sentence Transformer to file: {corpus_sents_transformer_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_transformer_filename)\n",
        "\n",
        "\n",
        "# Save StandardizedScaled SMA Sentences of ALL Models from the Unified DataFrame\n",
        "corpus_sents_all_filename = f'sum_sentiments_sents_all_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Sentence ALL Models to file: {corpus_sents_all_filename}')\n",
        "corpus_sents_all_df.to_csv(corpus_sents_all_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xWAbqvy9bzI"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k80YU4ZoHCs2"
      },
      "source": [
        "### **Calculate Lexical Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhthN28XHIVC"
      },
      "source": [
        "# models_ls = ['vader', 'textblob', 'stanza', 'afinn', 'sentimentr', 'syuzhet', 'bing', 'pattern', 'sentiword', 'senticnet', 'nrc']\n",
        "\n",
        "# corpus_lexicons_stats_dt = {}\n",
        "\n",
        "# for amodel in models_ls:\n",
        "#   get_lexstats(amodel)\n",
        "\n",
        "# Validate\n",
        "# corpus_lexicons_stats_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CokfceWPtfD"
      },
      "source": [
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsEbvCoCX7HY"
      },
      "source": [
        "## **(Optional) Calculate Baseline Sentiments (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZK1gA47xzkS"
      },
      "source": [
        "### **Select Sentiment Models (Manual)**\n",
        "\n",
        "NOTE:\n",
        "\n",
        "* Stanza (Stanford OpenNLP) can take upto 50 minutes to run\n",
        "\n",
        "* Listed in increasing order of (approx) run time\n",
        "\n",
        "* MPQA/SentiStrength not yet implemented (placeholders only for now)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0pStg1gJTZA"
      },
      "source": [
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "NRC_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0IACAN5JTZC"
      },
      "source": [
        "# Create and Verify custom list of Models to include\n",
        "\n",
        "MODELS_CUSTOM_LS = []\n",
        "\n",
        "if VADER_Arc:\n",
        "  MODELS_CUSTOM_LS.append('vader')\n",
        "if TextBlob_Arc:\n",
        "  MODELS_CUSTOM_LS.append('textblob')\n",
        "if Stanza_Arc:\n",
        "  MODELS_CUSTOM_LS.append('stanza')\n",
        "if SentimentR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentimentr')\n",
        "if Syuzhet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('syuzhet')\n",
        "if AFINN_Arc:\n",
        "  MODELS_CUSTOM_LS.append('afinn')\n",
        "if Bing_Arc:\n",
        "  MODELS_CUSTOM_LS.append('bing')\n",
        "if Pattern_Arc:\n",
        "  MODELS_CUSTOM_LS.append('pattern')\n",
        "if SentiWord_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentiword')\n",
        "if SenticNet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('senticnet')\n",
        "if NRC_Arc:\n",
        "  MODELS_CUSTOM_LS.append('nrc')\n",
        "\n",
        "print(f'Here are the Models we are using to ensemble and save:\\n\\n   {MODELS_CUSTOM_LS}')\n",
        "\n",
        "\"\"\"\n",
        "models_incl_ls = []\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  models_incl_ls.append(amodel[:2])\n",
        "models_incl_str = ''.join(models_incl_ls)\n",
        "\n",
        "print(f'Here is a custom name abbr: {models_incl_str}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw0JPNe6T2ap"
      },
      "source": [
        "# Calculate (win_(x)1per) 1% of Corpus length for smallest (odd-valued) rolling window\n",
        "\n",
        "# Sentences\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "win_raw_s1per = int(corpus_sents_len * 0.01)\n",
        "# print(f'1% Rolling Window: {win_raw_s1per}')\n",
        "\n",
        "if win_raw_s1per % 2:\n",
        "  win_s1per = win_raw_s1per\n",
        "else:\n",
        "  win_s1per = win_raw_s1per + 1\n",
        "\n",
        "# Paragraphs\n",
        "\n",
        "corpus_parags_len = corpus_parags_df.shape[0]\n",
        "\n",
        "win_raw_p1per = int(corpus_parags_len * 0.01)\n",
        "# print(f'1% Rolling Window: {win_raw_1per}')\n",
        "\n",
        "if win_raw_p1per % 2:\n",
        "  win_p1per = win_raw_p1per\n",
        "else:\n",
        "  win_p1per = win_raw_p1per + 1\n",
        "\n",
        "\n",
        "# Sections\n",
        "\n",
        "# NO NEED FOR SLIDING WINDOW ON SECTIONS\n",
        "\n",
        "\n",
        "print(f'Sentence 1 Percent window: {win_s1per}')\n",
        "print(f'Paragraph 1 Percent window: {win_p1per}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQJED-TeTVQD"
      },
      "source": [
        "LEXICONS_SUBDIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGjfPK9u21HH"
      },
      "source": [
        "# Verify Sentiment Lexicon hash files are accessable\n",
        "\n",
        "lexicons_path = f'/gdrive/MyDrive/{LEXICONS_SUBDIR[1:]}/hash*.csv'\n",
        "!pwd\n",
        "!ls $lexicons_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tAxuAxU7ueg"
      },
      "source": [
        "### **Calculate SentimentR (Jockers-Rinker) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foQeDPQpvyB3"
      },
      "source": [
        "model_base = 'sentimentr'\n",
        "model_name = 'sentimentr_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEnJj5rIMcSj"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_sentimentr.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq7dImOJozm5"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "\n",
        "  lexicon_sentimentr_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_sentimentr.csv')\n",
        "  lexicon_sentimentr_df['x'] = lexicon_sentimentr_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_sentimentr_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_sentimentr_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_sentimentr_df.head()\n",
        "    lexicon_sentimentr_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25_Zjja0o_Hx"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "\n",
        "  id = lexicon_sentimentr_df.word.values\n",
        "  values = lexicon_sentimentr_df.polarity.values\n",
        "\n",
        "  lexicon_sentimentr_dt = dict(zip(id, values))\n",
        "  # lexicon_sentimentr_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_sentimentr_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQYYP2hiO53j"
      },
      "source": [
        "corpus_sents_df['sent_clean'].isna().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnGOrKoM2zkR"
      },
      "source": [
        "corpus_chaps_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxe15XjNwByS"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_sentimentr(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_sentimentr_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SentimentR_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentimentr, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpV6ddmSOHLW"
      },
      "source": [
        "corpus_sents_df['sentimentr'].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbGz2Yi5wByY"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOSiIyhbN2-8"
      },
      "source": [
        "# Verify \n",
        "\n",
        "corpus_sents_df['sentimentr'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZjqwTvU76AR"
      },
      "source": [
        "### **Calculate Syuzhet (Jockers) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjOXGQX54lbX"
      },
      "source": [
        "# Define Model names\n",
        "model_base = 'syuzhet'\n",
        "model_name = 'syuzhet_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB5fBNQ-QNwk"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_syuzhet.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkhaGcuy4lbg"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "\n",
        "  lexicon_syuzhet_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_syuzhet.csv')\n",
        "  lexicon_syuzhet_df['word'] = lexicon_syuzhet_df['word'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_syuzhet_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_syuzhet_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_syuzhet_df.head()\n",
        "    lexicon_syuzhet_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9IuINuR4lbi"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "\n",
        "  id = lexicon_syuzhet_df.word.values\n",
        "  values = lexicon_syuzhet_df.value.values\n",
        "\n",
        "  lexicon_syuzhet_dt = dict(zip(id, values))\n",
        "  # lexicon_sentimentr_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_syuzhet_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO-txaFL4lbo"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_syuzhet(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_syuzhet_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_syuzhet, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfsLZSo04lbp"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-BwrbyXR5Hd"
      },
      "source": [
        "# Verify \n",
        "\n",
        "corpus_sents_df['syuzhet'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA3dWsnF78mi"
      },
      "source": [
        "### **Calculate Bing (HuLiu) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wvq1prK7n1k"
      },
      "source": [
        "model_base = 'bing'\n",
        "model_name = 'bing_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDfB2f0olded"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_bing.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZELGxS8y9PiS"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if Bing_Arc == True:\n",
        "  \n",
        "  lexicon_bing_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_bing.csv')\n",
        "  lexicon_bing_df['x'] = lexicon_bing_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_bing_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_bing_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_bing_df.head()\n",
        "    lexicon_bing_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJQTaeue9VjI"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if Bing_Arc == True:\n",
        "\n",
        "  id = lexicon_bing_df.word.values\n",
        "  values = lexicon_bing_df.polarity.values\n",
        "\n",
        "  lexicon_bing_dt = dict(zip(id, values))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Ffzu1m7n10"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_bing(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_bing_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Bing_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_bing, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWITnL2a9rrx"
      },
      "source": [
        "# Calculate Bing Sentiment [0,1,2]\n",
        "\n",
        "def bing_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += lex_discrete2continous_sentiment(str(aword), lexicon_bing_dt)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+1)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xpzm3hi7n1x"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Bing_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(bing_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UFEItnO-i7h"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Bing_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=bing_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4thQWz3-i7k"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKlKnqmWmBEP"
      },
      "source": [
        "# Verify \n",
        "\n",
        "corpus_sents_df['bing'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkNZVk128jV9"
      },
      "source": [
        "### **Calculate SentiWord Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnL4ORNp8MgB"
      },
      "source": [
        "model_base = 'sentiword'\n",
        "model_name = 'sentiword_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ootf8xdbnHPx"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_sentiword.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpVmt0fK8xi_"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "\n",
        "  lexicon_sentiword_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_sentiword.csv')\n",
        "  lexicon_sentiword_df['x'] = lexicon_sentiword_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_sentiword_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_sentiword_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_sentiword_df.head()\n",
        "    lexicon_sentiword_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwtK2M9h879M"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "\n",
        "  id = lexicon_sentiword_df.word.values\n",
        "  values = lexicon_sentiword_df.polarity.values\n",
        "\n",
        "  lexicon_sentiword_dt = dict(zip(id, values))\n",
        "  # lexicon_sentiword_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_sentiword_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I-QwB478MgP"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_sentiword(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_sentiword_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SentiWord_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentiword, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbEF88568MgS"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9GfZ5RQnn4G"
      },
      "source": [
        "# Verify \n",
        "\n",
        "corpus_sents_df['sentiword'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUcANLM_8mtT"
      },
      "source": [
        "### **Calculate SenticNet Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://sentic.net/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OqhSberA1hZ"
      },
      "source": [
        "model_base = 'senticnet'\n",
        "model_name = 'senticnet_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsnCzsfFnx7B"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_senticnet.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmMfvQvYBBoM"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "\n",
        "  lexicon_senticnet_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_senticnet.csv')\n",
        "  lexicon_senticnet_df['x'] = lexicon_senticnet_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_senticnet_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_senticnet_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_senticnet_df.head()\n",
        "    lexicon_senticnet_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2oS71STBIUS"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "\n",
        "  id = lexicon_senticnet_df.word.values\n",
        "  values = lexicon_senticnet_df.polarity.values\n",
        "\n",
        "  lexicon_senticnet_dt =dict(zip(id, values))\n",
        "  # lexicon_jockersrinker_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  text2sentiment(sent_test, lexicon_senticnet_dt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHQwXBl5BlRM"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_senticnet(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on senticnet lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_senticnet_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SenticNet_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_senticnet, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raHUj3a4A1hs"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9lSo0Kmn_T4"
      },
      "source": [
        "# Verify \n",
        "\n",
        "corpus_sents_df['senticnet'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn4KQYpH3glK"
      },
      "source": [
        "### **Calculate NRC Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USx6LRmoCFV8"
      },
      "source": [
        "model_base = 'nrc'\n",
        "model_name = 'nrc_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EdOTv3KoFEV"
      },
      "source": [
        "# Verify Lexicon subdirectory and datafiles\n",
        "!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n",
        "\n",
        "print('\\nTop of Dictionary datafile ----------')\n",
        "!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_nrc.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFEszSc13glL"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if NRC_Arc == True:\n",
        "\n",
        "  lexicon_nrc_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_nrc.csv')\n",
        "  lexicon_nrc_df['x'] = lexicon_nrc_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_nrc_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_nrc_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_nrc_df.head()\n",
        "    lexicon_nrc_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nAkx4Mv3glL"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if NRC_Arc == True:\n",
        "\n",
        "  id = lexicon_nrc_df.word.values\n",
        "  values = lexicon_nrc_df.polarity.values\n",
        "\n",
        "  lexicon_nrc_dt =dict(zip(id, values))\n",
        "  # lexicon_jockersrinker_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv1nVSATb7z4"
      },
      "source": [
        "# Calculate NRC Sentiment [0,1,2]\n",
        "\n",
        "def nrc_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += lex_discrete2continous_sentiment(str(aword), lexicon_nrc_dt)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+10)\n",
        "\n",
        "  return text_sentiment_norm\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss3Xixo_CX2q"
      },
      "source": [
        "# Test\n",
        "\n",
        "if NRC_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(nrc_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhqnljMSCX2w"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if NRC_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=nrc_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2q97Dm-CX2z"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBK7LQx4oXI-"
      },
      "source": [
        "# Verify \n",
        "\n",
        "corpus_sents_df['nrc'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRTjCPLb8cbB"
      },
      "source": [
        "### **Calculate Afinn Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://github.com/fnielsen/afinn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcnxqnyzDCde"
      },
      "source": [
        "model_base = 'afinn'\n",
        "model_name = 'afinn_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDG2KxvNBdj6"
      },
      "source": [
        "!pip install afinn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evnkXWL58CcX"
      },
      "source": [
        "# Install and configure for English\n",
        "\n",
        "from afinn import Afinn\n",
        "afinn = Afinn(language='en')\n",
        "\n",
        "# Test\n",
        "\n",
        "# afinn.score('I had the worst day.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKbeW_cMXhmI"
      },
      "source": [
        "# Calculate AFINN Sentiment [0,1,2]\n",
        "\n",
        "def afinn_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += afinn.score(aword)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+0.1)\n",
        "\n",
        "  return float(text_sentiment_norm)  # return float vs np.float64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRCd4VpwDO0Q"
      },
      "source": [
        "# Test\n",
        "\n",
        "if AFINN_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(afinn_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrP_wxB3DO0S"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if AFINN_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=afinn_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgZaMPYKDO0T"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clUzylGOog5h"
      },
      "source": [
        "# Verify \n",
        "\n",
        "corpus_sents_df['afinn'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAEiglIPDfFI"
      },
      "source": [
        "### **Calculate VADER Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgkvefzpDgx-"
      },
      "source": [
        "model_base = 'vader'\n",
        "model_name = 'vader_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wodGtjXhDmZN"
      },
      "source": [
        "# Sentiment evaluation function\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Test\n",
        "sid.polarity_scores('hello world')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS8e25MkDmZP"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "if VADER_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sid.polarity_scores, sentiment_type='compound')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Azyv2lDmZP"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq8KNwpjom4X"
      },
      "source": [
        "# Verify \n",
        "\n",
        "corpus_sents_df['vader'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCN4c-G48e7-"
      },
      "source": [
        "### **Calculate TextBlob Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MfVWZ34Vg8U"
      },
      "source": [
        "model_base = 'textblob'\n",
        "model_name = 'textblob_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "118Blghk7fjp"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhJsYxPoVhY4"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "def textblob_sentiment(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return a sentiment value between -1.0 to +1.0 using TextBlob\n",
        "  '''\n",
        "  return TextBlob(text_str).sentiment.polarity\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if TextBlob_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=textblob_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sx9UgXYNhqO"
      },
      "source": [
        "corpus_sects_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlHRf2aFVhY7"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_name, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_name, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_name, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_name, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMWewkTgord1"
      },
      "source": [
        "# Verify \n",
        "\n",
        "corpus_sents_df['textblob'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2blGfVlKb_s"
      },
      "source": [
        "### **Calculate Pattern Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU60-nqpCsl7"
      },
      "source": [
        "model_base = 'pattern'\n",
        "model_name = 'pattern_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KxnLfHoL3Fy"
      },
      "source": [
        "!pip install pattern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtwmIrSOKZRm"
      },
      "source": [
        "from pattern.en import sentiment as pattern_sa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vwtm_jBKZM2"
      },
      "source": [
        "# Test\n",
        "\n",
        "sent_test='I hate Mondays.'\n",
        "pattern_sa(sent_test)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXq-jDPxasVY"
      },
      "source": [
        "# Calculate Pattern Sentiment [0,1,2]\n",
        "\n",
        "def pattern_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += pattern_sa(str(aword))[0]\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+0.01)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-sXRBNWC08o"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(pattern_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db0hezLKC08p"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Pattern_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=pattern_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGHixqpOC08q"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-X3xNWkoyLS"
      },
      "source": [
        "# Verify \n",
        "\n",
        "corpus_sents_df['pattern'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Md97vbo9CP"
      },
      "source": [
        "# Explore far outlier\n",
        "corpus_sents_df[corpus_sents_df['pattern'] == corpus_sents_df['pattern'].min()]['sent_no']\n",
        "print('\\n')\n",
        "corpus_sents_df[corpus_sents_df['pattern'] == corpus_sents_df['pattern'].min()]['sent_raw']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQupH_HrqAmo"
      },
      "source": [
        "pattern_sa('Alas!')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC0F9rGgpUIR"
      },
      "source": [
        "corpus_sents_df.iloc[288:294]['pattern'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsaziON_Z263"
      },
      "source": [
        "### **Calculate Stanza/OpenNLP Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://github.com/piyushpathak03/NLP-using-STANZA/blob/main/Stanza.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZgGfcCuFnmI"
      },
      "source": [
        "if Stanza_Arc == True:\n",
        "  model_base = 'stanza'\n",
        "  model_name = 'stanza_lnorm_medianiqr'\n",
        "\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoZUi2AwZ_7L"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5txTb6aIZ2tN"
      },
      "source": [
        "%time\n",
        "\n",
        "import stanza\n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  stanza.download('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NORYbxsxZ2qg"
      },
      "source": [
        "if Stanza_Arc == True:\n",
        "  nlp = stanza.Pipeline('en', processors='tokenize,sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtOtBfYwZ2na"
      },
      "source": [
        "# Test stanza directly\n",
        "\n",
        "# doc = nlp('Ram is a bad boy')\n",
        "# for i, sentence in enumerate(doc.sentences):\n",
        "#     print(i, sentence.sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKnox67kayod"
      },
      "source": [
        "# Calculate Stanza Sentiment [0,1,2]\n",
        "\n",
        "def stanza_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_tot = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    adoc = nlp(aword)\n",
        "    for i, sentence in enumerate(adoc.sentences):\n",
        "      text_sentiment_tot += float(sentence.sentiment)\n",
        "  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.1)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNngkBBAF26C"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(stanza_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrY2mrhVF26D"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# NOTE: requires about 50mins (20210708 at 0730) Colab Pro: GPU+RAM\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Stanza_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=stanza_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSiYC73nF26D"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "  # Validate\n",
        "  corpus_lexicons_stats_dt\n",
        "\n",
        "  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5tUYZA1DExu"
      },
      "source": [
        "# Verify \n",
        "\n",
        "corpus_sents_df['stanza'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfMwbhVMwgXw"
      },
      "source": [
        "## **(Optional) Calculate SyuzhetR Sentiments (Auto)**\n",
        "\n",
        "```\n",
        "library(‘syuzhet’)\n",
        "\n",
        "getwd()\n",
        "list.files(pattern=’*.txt’)\n",
        "\n",
        "Rc_ddefoe_str <- syuzhet::get_text_as_string(‘ddefoe_robinsoncrusoe_final_hand.txt’)\n",
        "Rc_ddefoe_sents_v <- syuzhet::get_sentences(rc_ddefoe_str)\n",
        "\n",
        "Rc_ddefoe_all_df <- data.frame(syuzhet=syuzhet::get_sentiment(rc_ddefoe_sents_v, method=’syuzhet’)\n",
        "rc_ddefoe_all_df$bing <- syuzhet::get_sentiment(rc_ddefoe_sents_v, method=’bing’)\n",
        "rc_ddefoe_all_df$afinn <- syuzhet::get_sentiment(rc_ddefoe_sents_v, method=’afinn’)\n",
        "rc_ddefoe_all_df$nrc <- syuzhet::get_sentiment(rc_ddefoe_sents_v, method=’nrc’)\n",
        "\n",
        "write.csv(rc_ddefoe_all_df, ‘sum_sentiments_rc_ddefoe_syuzhetR_4models.csv’)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHPmbx5Pwsq9"
      },
      "source": [
        "## **(Optional) Calculate SentimentR Sentiments (Auto)**\n",
        "\n",
        "```\n",
        "library(‘sentimentr’)\n",
        "# conflict with syuzhet::get_sentences\n",
        "\n",
        "# continuing from syuzhet code above we inherit global var: rc_ddefoe_sents_v\n",
        "# SentimentR recommends reparsing these with sentimentr::get_sentences(rc_ddefoe_sents_v)\n",
        "\n",
        "Rc_dedefoe_sentimentr_sents_v <- sentimentr::get_sentences(rc_ddefoe_sents_v)\n",
        "\n",
        "# Create data.frame with jockers_rinker sentiments\n",
        "Rc_dedefoe_sentimentr_all_df <- data.frame(jockersrinker = sentimentr::sentiment(rc_dedefoe_sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_jockers_rinker, hyphen=” “, neutral.nonverb.like=TRUE)$sentiment)\n",
        "\n",
        "# Add other lexicon sentiments\n",
        "rc_dedefoe_sentimentr_all_df$jockers <- sentimentr::sentiment(rc_dedefoe_sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_jockers, hyphen=” “, neutral.nonverb.like=TRUE)$sentiment\n",
        "\n",
        "rc_dedefoe_sentimentr_all_df$huliu <- sentimentr::sentiment(rc_dedefoe_sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_huliu, hyphen=” “, neutral.nonverb.like=TRUE)$sentiment\n",
        "\n",
        "rc_dedefoe_sentimentr_all_df$lmcd <- sentimentr::sentiment(rc_dedefoe_sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_loughran_mcdonald, hyphen=” “, neutral.nonverb.like=TRUE)$sentiment\n",
        "\n",
        "rc_dedefoe_sentimentr_all_df$nrc <- sentimentr::sentiment(rc_dedefoe_sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_nrc, hyphen=” “, neutral.nonverb.like=TRUE)$sentiment\n",
        "\n",
        "rc_dedefoe_sentimentr_all_df$senticnet <- sentimentr::sentiment(rc_dedefoe_sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_senticnet, hyphen=” “, neutral.nonverb.like=TRUE)$sentiment\n",
        "\n",
        "rc_dedefoe_sentimentr_all_df$sentiword <- sentimentr::sentiment(rc_dedefoe_sentimentr_sents_v, polarity_dt=lexicon::hash_sentiment_sentiword, hyphen=” “, neutral.nonverb.like=TRUE)$sentiment\n",
        "\n",
        "write.csv(rc_ddefoe_sentimentr_all_df, ‘sum_sentiments_rc_ddefoe_sentimentR_7models.csv’)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNCGf1KZEpld"
      },
      "source": [
        "# **Save Raw Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REL23tcN9NS-"
      },
      "source": [
        "## **Sentence and Paragraph DataFrames (incl raw/clean text)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm61JKUqA6wn"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbHVOSMJAKhQ"
      },
      "source": [
        "corpus_sents_df['sent_raw'].isna().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyzp_6RhBIkl"
      },
      "source": [
        "CORPUS_TITLE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNkAJzWhEpld"
      },
      "source": [
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "\n",
        "# Save Sentences of original Raw and Cleaned Corpus\n",
        "corpus_text_sentences_raw_filename = f'corpus_text_sentences_raw_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text raw sentences to file: {corpus_text_sentences_raw_filename}')\n",
        "corpus_sents_df['sent_raw'].to_csv(corpus_text_sentences_raw_filename)\n",
        "\n",
        "corpus_text_sentences_clean_filename = f'corpus_text_sentences_clean_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text clean sentences to file: {corpus_text_sentences_clean_filename}')\n",
        "corpus_sents_df['sent_clean'].to_csv(corpus_text_sentences_clean_filename)\n",
        "\n",
        "\n",
        "# Save Paragraphs of original Raw and Cleaned Corpus\n",
        "corpus_text_paragraphs_raw_filename = f'corpus_text_paragraphs_raw_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text raw paragraphs to file: {corpus_text_paragraphs_raw_filename}')\n",
        "corpus_parags_df['parag_raw'].to_csv(corpus_text_paragraphs_raw_filename)\n",
        "\n",
        "corpus_text_paragraphs_clean_filename = f'corpus_text_paragraphs_clean_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text clean sentences to file: {corpus_text_paragraphs_clean_filename}')\n",
        "corpus_parags_df['parag_clean'].to_csv(corpus_text_paragraphs_clean_filename)\n",
        "\n",
        "\n",
        "\n",
        "# Save the Sentiment Values at both the Sentence and Paragraph Levels\n",
        "corpus_sents_filename = f'sum_sentiments_sents_baselines_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sentences to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "corpus_parags_filename = f'sum_sentiments_parags_baselines_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Paragraphs to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysa0Yp9yXWAe"
      },
      "source": [
        "# Verify exported Sentence Sentiments file\n",
        "\n",
        "!head -n 5 $corpus_sents_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9cZmIkx7Too"
      },
      "source": [
        "# **EDA (Repeat for each Sentiment Model)** (Auto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJRV2n0M_xN6"
      },
      "source": [
        "#### **Histograms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJTcj9faMh61"
      },
      "source": [
        "**Sentiment Histogram Plots**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Histograms are provided for the (a) Length-Normed and (b) Scaled (Median Interquartile Range) Sentiment values for Sentences, Paragraphs and Sections. \n",
        "\n",
        "* There we used extensively early on to compare which Sentiment Time series preprocessing techniques worked best with our various Novel corpora according to two criteria: \n",
        "\n",
        "* (a) Vertical Scaling Method with the ability to transform histograms of sentiment values to well-behaved near-gaussian distributions and clipping outliers. After experimenting with various techniques including: mean/STD, median/MAD, and various two stage outlier/normalization methods median/IQR proved best (define).\n",
        "\n",
        "* (b) Horizontal Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPEv0DsBRhhQ"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU46yaCPPdSm"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYs2V2ILENaZ"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8b4CjTYwlgP"
      },
      "source": [
        "# Plot Histogram of Sentence lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuwVO3Q4Rmu4"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MhJ4T18PjTM"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SajUdBthwlgR"
      },
      "source": [
        "# Plot Histogram of Paragraph lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nuOQ7cZRrue"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfhjLMdjRwZX"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBFb-SLywlgS"
      },
      "source": [
        "# Plot Histogram of Section lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43YL_Iuf0JHZ"
      },
      "source": [
        "#### **Raw Sentiment Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AHHijpONvpt"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GorGKbFbR28W"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN27M4WlwlgT"
      },
      "source": [
        "# Plot Raw Sentence Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0QCx2OMN_jm"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxL5FryPR-ln"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d8uXvYJwlgU"
      },
      "source": [
        "# Plot Raw Paragraph Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_raw_sentiments(model_name=model_name, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp7xjR7GvxkT"
      },
      "source": [
        "# TODO: Add Section Crux Nos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2hFkRQHSLVE"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adhoCpB5SFQv"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "# col_meanstd = f'{model_name}_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='section', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_meanstd, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjZZqY6cwlgW"
      },
      "source": [
        "# Plot Raw Standardized Section Sentiments\n",
        "\n",
        "# NOTE: Compared with Length-Normed, the Raw Standardizations lose most SATS features\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "# col_lnorm_meanstd = f'{model_name}_lnorm_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='section', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_lnorm_meanstd, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDF_-tZhr44H"
      },
      "source": [
        "# Plot Raw Standardized Chapter Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "# col_lnorm_meanstd = f'{model_name}_lnorm_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='chapter', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_lnorm_meanstd, semantic_type='chapter', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1tIJZepmvLu"
      },
      "source": [
        "#### **Crux Points and Surrounding Contexts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FBWrNEJyit6"
      },
      "source": [
        "# Veify all the model sentiment variations\n",
        "\n",
        "[x for x in corpus_sects_df.columns if x.startswith(model_base)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKazAFV_qpCn"
      },
      "source": [
        "**Compare Chapters vs Sections Crux Points**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At the highest level, the Corpus is divided into Chapters which may then futher subdivided into Sections (e.g. extra spaces, punctuation like '* * *' or special printer glyph/fleuron).  Horizonal dark blue lines indicate Chapter divisions while Section boundries lie at both dark and light blue vertical lines.\n",
        "\n",
        "* Since each Chapter may contain multiple Sections, the Section Sentiment plot is more detailed/jagged than the Chapter Sentiment plots. By plotting both together, the smoother Chapter Sentiment plot gives a more general sense of the Corpus Sentiment Arc while the next, more-detailed Section Sentiment plot enables a more detailed investigation/localization of Crux Point neighborhoods.\n",
        "\n",
        "* At this early stage both the Chapter and Section Sentiment plots are too general to provide accurate/fixed Crux localization. As such, only aggregrate Sentiment values for each Chapter/Section are assigned to the mid-point of each Chapter/Section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sme-1HVRZOGs"
      },
      "source": [
        "# col_lnorm_medianiqr = 'pattern_lnorm_medianiqr'\n",
        "col_lnorm_medianiqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08xDu5WgkGAg"
      },
      "source": [
        "# col_lnorm_medianiqr = 'vader_lnorm_medianiqr'\n",
        "\n",
        "# corpus_chaps_df.drop(columns=['textblob_lnorm_medianiqr_lnorm_medianiqr', 'textblob_lnorm_medianiqr_medianiqr', 'textblob_lnorm_medianiqr_lnorm_meanstd'], inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06m1xDRzmWbq"
      },
      "source": [
        "corpus_chaps_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tojSdIGkPDz"
      },
      "source": [
        "corpus_chaps_df.columns\n",
        "# corpus_chaps_df.iloc[:20][['chap_no','sent_no_start','sent_no_mid','char_len','token_len','vader', 'vader_lnorm_medianiqr', 'textblob', 'textblob_lnorm_medianiqr']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu6F5kWerAPZ"
      },
      "source": [
        "corpus_chaps_df.vader_lnorm_medianiqr.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLgGy3HgO9--"
      },
      "source": [
        "col_lnorm_medianiqr = 'syuzhet_lnorm_medianiqr'\n",
        "model_name = 'syuzhet_lnorm_medianiqr'\n",
        "model_base = 'syuzhet'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M1bg8RcPH_2"
      },
      "source": [
        "# Plot Annotated Section Cruxes of Raw Sentiment Time Series\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[model_base], semantic_type='chapter', label_token_ct=3, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[model_base], semantic_type='section', label_token_ct=-1, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jku9ZQ9v9Wg"
      },
      "source": [
        "# Plot Annotated Section Cruxes of Standardized Sentiment Time Series\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='chapter', label_token_ct=3, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=-1, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxA0sDvlcdLx"
      },
      "source": [
        "**Sections Crux Points in Detail**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2tKXTD8v9RH"
      },
      "source": [
        "# Corpus Section Standardized Sentiment Time Series\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSw-wARBF2fW"
      },
      "source": [
        "**Verify Crux Point Sentence Number and Text Match**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At [Crux_Sentence_Text] enter the first few words that uniquely identify the Crux Sentence and confirm the Sentence No matches the information in the plot above. (NOTE: Search is for an exact match including case and puncutation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU6l4UaPdmAF"
      },
      "source": [
        "Crux_Sentence_Text = \"haiku\" #@param {type:\"string\"}\n",
        "\n",
        "# Verify individual Crux Sentence Numbers matches Content\n",
        "\n",
        "crux_sent_no = int(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Crux_Sentence_Text)]['sent_no'])\n",
        "\n",
        "print(f'The Sentence:\\n\\n    {Crux_Sentence_Text}\\n\\nMatches Sentence #{crux_sent_no}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Q_weR3H5Qu"
      },
      "source": [
        "**Review Context Around Any Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYAqkxi2FSw1"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  4494#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7-PoXAcq4S4"
      },
      "source": [
        "**Compare Paragraph vs Section Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzgfs-U9QZeq"
      },
      "source": [
        "# Verify the valid ranges for Sentences and Paragraphs\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "print(f'There are {corpus_sents_len} Sentences in the Corpus')\n",
        "corpus_parags_len = corpus_parags_df.shape[0]\n",
        "print(f'There are {corpus_parags_len} Paragraphs in the Corpus')\n",
        "\n",
        "# Create a new Corpus Paragraph DataFrame (corpus_parags_zoom_df) that is streteched out to have as many sample points as there are Sentences\n",
        "#   That is, go from an original corpus_parags_df of #Paragraph datapoints to an expanded corpus_parags_zoom_df of #Sentences datapoints using scipy.ndimage.interpolation.zoom\n",
        "\n",
        "corpus_parags_zoom_df = pd.DataFrame()\n",
        "\n",
        "resample_ratio = corpus_sents_df.shape[0]/corpus_parags_df.shape[0]   # ratio = no_sents/no_parags\n",
        "\n",
        "corpus_parags_df_numcols_ls = corpus_parags_df.select_dtypes(include=['number']).columns\n",
        "\n",
        "for acol in corpus_parags_df_numcols_ls:\n",
        "  parags_zoom_temp_np = zoom(np.array(corpus_parags_df[acol]), resample_ratio)\n",
        "  corpus_parags_zoom_df[acol] = pd.Series(parags_zoom_temp_np)\n",
        "\n",
        "print('\\n')\n",
        "print(f'New expanded corpus_parags_zoom_df.shape = {corpus_parags_zoom_df.shape}')\n",
        "print(f'           matches corpus_sents_df.shape = {corpus_sents_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wskHXVNSmjfQ"
      },
      "source": [
        "**Adjust the Paragraph Sentiment Plot to compare it with the Section Sentiment Plot for this Corpus**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Adjust [Scale_Vertical_Rolling_Paragraph] until the vertical min/max spans of the two plots are approximately equal\n",
        "\n",
        "* Adjust [Set_Paragraph_Rolling_Window_Percent] to set horizonal smoothness of Rolling Paragraph (typically 5,10 or 20%)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9P9k98ibh4n"
      },
      "source": [
        "col_lnorm_medianiqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgaBEAVCTMRt"
      },
      "source": [
        "Scale_Vertical_Rolling_Paragraph = 2.7 #@param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "Set_Paragraph_Rolling_Window_Percent = 3 #@param {type:\"slider\", min:0, max:30, step:1}\n",
        "\n",
        "# Compare Section Midpoints vs Sentence SMA Sentiment Values\n",
        "\n",
        "scale_sma_paragraph = Scale_Vertical_Rolling_Paragraph\n",
        "sentence_count = corpus_sents_df.shape[0]\n",
        "if Set_Paragraph_Rolling_Window_Percent == 0:\n",
        "  sma_parag_win = 1\n",
        "else:\n",
        "  sma_parag_win = int((Set_Paragraph_Rolling_Window_Percent/100)*sentence_count)\n",
        "\n",
        "\n",
        "# corpus_parags_zoom_df['vader_lnorm_medianiqr'].rolling(window=sma_parag_win, center=True).mean().apply(lambda x: x*scale_sma_paragraph).plot(label=f'{model_base} Paragraphs', alpha=0.3)\n",
        "corpus_parags_zoom_df[col_lnorm_medianiqr].rolling(window=sma_parag_win, center=True).mean().apply(lambda x: x*scale_sma_paragraph).plot(label=f'{model_base} Paragraphs', alpha=0.3)\n",
        "\n",
        "_ = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=0, title_xpos=0.5, title_ypos=1.0, sec_y_height=0, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} \\n SMA vs midpoint Paragraph MedianIQR Sentiment with Crux Points via SciPy.peaks')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUyZF6tE1rL0"
      },
      "source": [
        "#### **Zoom into a Section**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At [Select_Section_No] pick a Section of the Corpus to zoom into\n",
        "\n",
        "* Adjust [Scale_Vertical_Rolling_Sentences] until the vertical min/max spans of the two plots are approximately equal \n",
        "\n",
        "* Adjust [Set_Sentence_Rolling_Window_Percent] to set horizonal smoothness of Rolling Paragraph (typically 5,10 or 20%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu7DYRELQBoH"
      },
      "source": [
        "# Explore a Corpus Section Up-Close\n",
        "\n",
        "section_count = corpus_sects_df.shape[0]\n",
        "print(f'There are {section_count} Sections in this corpus,\\n  pick one numbered between 0 and {section_count-1}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Q2a854Zxl1"
      },
      "source": [
        "Select_Section_No =  24#@param {type:\"integer\"}\n",
        "Scale_Vertical_Rolling_Sentences = 0.2 #@param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "Set_Sentence_Rolling_Window_Percent = 3 #@param {type:\"slider\", min:0, max:30, step:1}\n",
        "\n",
        "# Make Copies instead of just using References / Only Reference, not copy()\n",
        "# section_sents_df = pd.DataFrame()\n",
        "# section_parags_df = pd.DataFrame()\n",
        "\n",
        "section_sents_df, section_parags_df = get_section_timeseries(Select_Section_No)\n",
        "\n",
        "# section_sents_df.head()\n",
        "\n",
        "print(f'section_sents_df.shape: {section_sents_df.shape}')\n",
        "print(f'section_parags_df.shape: {section_parags_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MbSPmp2dN9Z"
      },
      "source": [
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE-CDUmvFDOe"
      },
      "source": [
        "# Add expanded Paragraph sentiment to corpus_sents_df\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr_parag'] = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df')\n",
        "\n",
        "# NOTE: Define section_sents_df, MUST BE EXECUTED BEFORE ANY CRUX POINT DETECTION!!!\n",
        "\n",
        "parags_midpoint_ls = []\n",
        "col_name_parag = f'{model_name}_parag'\n",
        "section_sents_df[col_name_parag], parags_midpoint_ls = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df', model_name=model_name)\n",
        "\n",
        "# Verify Sentences and Expanded Paragraph lengths match\n",
        "print(f'\\nIn Section #{Select_Section_No}\\n')\n",
        "print(f'            Sentence Count: {section_sents_df.shape[0]}')\n",
        "print(f\"(expanded) Paragraph Count: {str(section_sents_df['parag_no'].unique().shape[0])}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOBqRxQz3gbh"
      },
      "source": [
        "##### **Section Histograms**\n",
        "\n",
        "EDA Unbalanced Paragraph and Sentence Features within selected Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MoMM3l8Px2R"
      },
      "source": [
        "# Verify the non-uniform distribution of Paragraph lengths within selected Section (thus necessity for noralizing Paragraph Sentiment by Paragraph length)\n",
        "\n",
        "section_sents_df['parag_no'].value_counts().hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Number of Sentences per Paragraph in Section #{Select_Section_No}')\n",
        "plt.xlabel('Number of Sentences per Paragraph')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5eC0i5KO9cm"
      },
      "source": [
        "section_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoVdCaReDJvl"
      },
      "source": [
        "section_sents_df[model_name].hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.xlabel('Sentence Sentiment')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSrXWnnEEm00"
      },
      "source": [
        "# Histogram of Paragraph Sentiments within selected Section\n",
        " \n",
        "# create a unified Section DataFrame with equal length Sentences and (expanded) Paragraphs Sentiment Series\n",
        "# section_sents_parags_df = section_sents_df.copy()\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr_parag_approx'] = parag_sentiment_expanded_ls\n",
        "\n",
        "\n",
        "section_sents_df[col_name_parag].hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Length-Normed Paragraph Sentiment in Section #{Select_Section_No}')\n",
        "plt.xlabel('Length-Normed Sentiment of Paragraph')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcao3RNuyCr_"
      },
      "source": [
        "**Naive Raw and LOWESS Smoothed Paragraph Sentiment plots within selected Section**\n",
        "\n",
        "NOTE: Horizonal x-axis narrative time axis not adjusted for variable paragraph lengths - simply used midpoints assuming equal length Paragraphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag47-b4EEO6k"
      },
      "source": [
        "**Raw and SMA Sentence Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkpQiVCoZTJ0"
      },
      "source": [
        "# section_crux_sents_dt\n",
        "\n",
        "# type(section_crux_sents_dt['vader_lnorm_medianiqr_roll50_frac14_win10'][0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB4fEZzJQPkg"
      },
      "source": [
        "##### **Section SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAIAX2OhW2k0"
      },
      "source": [
        "# SMA with Raw Sentiment values over entire Corpus\n",
        "\n",
        "sec_y_ht = -0.06\n",
        "\n",
        "plot_smas(section_view=False, model_name=model_base, text_unit='sentence', wins_ls=[5,10,15,20], alpha=0.5, y_height=sec_y_ht, subtitle_str=f'(Model: {model_base.capitalize()})', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8xWHqNAYV_p"
      },
      "source": [
        "# SMA with Length-Normed MedianIQR Sentiment values over entire Corpus\n",
        "\n",
        "sec_y_ht = -0.11\n",
        "\n",
        "plot_smas(section_view=False, model_name=model_name, text_unit='sentence', wins_ls=[5,10,15,20], alpha=0.5, y_height=sec_y_ht, subtitle_str=f'(Model: {model_base.capitalize()})', save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nA285mHvM4T"
      },
      "source": [
        "# Within the Selected_Section_No, plot Sentence SMA with vertical Paragraph boundries indicated\n",
        "\n",
        "sec_y_ht = 0.65\n",
        "\n",
        "# At Section boundries draw blue vertical lines \n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "  # 'BigNews1', xy=(sent_no, 0.5), xytext=(-10, 25), textcoords='offset points',                   rotation=90, va='bottom', ha='center', annotation_clip=True)\n",
        "\n",
        "  # plt.text(sent_no, -.5, 'goodbye',rotation=90, zorder=0)\n",
        "      \n",
        "for win_size in range(50,250,25):\n",
        "  section_sents_df[model_name].rolling(win_size, center=True).mean().plot(alpha=0.5, label=f'SMA win={win_size}')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base.capitalize()})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfZiEeDhWJJM"
      },
      "source": [
        "# Within the Selected_Section_No, compare Sentence SMA with vertical Paragraph boundries indicated\n",
        "#    3 Sentence SMAs: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "\n",
        "sec_y_ht = -0.5\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "awins_ls = [10]\n",
        "get_smas(section_sents_df, model_name=model_name, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=col_medianiqr, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=1.8, subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=model_base, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=8., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False);\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqlsQS-RYT7x"
      },
      "source": [
        "# Within the Selected_Section_No, compare Paragraph SMA: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "#     Plot Paragraph by Sentence Sentiment within selected Section\n",
        "\n",
        "get_smas(section_sents_df, model_name=model_name, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=col_medianiqr, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=1.8, subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=model_base, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=8., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTHlWyjzYf7C"
      },
      "source": [
        "# Within the Selected_Section_No, compare Paragraph SMA: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "#     Plot Paragraph by Paragraph Sentiment within selected Section\n",
        "\n",
        "\"\"\"\n",
        "# TODO: Fix ValueError: Image size of 1311x82731 pixels is too large. It must be less than 2^16 in each direction.\n",
        "\n",
        "\n",
        "sec_y_ht = -15\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(aparag_no, sec_y_ht, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(aparag_no, color='blue', alpha=0.1)\n",
        "\n",
        "awins_ls = [5]\n",
        "get_smas(section_parags_df, model_name=model_name, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_parags_df, model_name=col_medianiqr, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=8., subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_parags_df, model_name=model_base, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=10., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Paragraph Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhIM7zwuQUBq"
      },
      "source": [
        "##### **Raw Sentiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp0JvXFrV-B4"
      },
      "source": [
        "# Within the Selected_Section_No, compare Sentence Sentiment: (a)Raw vs (b)Standardized MedianIQR vs (c)Length-Normed Standardized MedianIQR\n",
        "#     Plot Raw vs MedianIQR Sentence Sentiment within selected Section\n",
        "\n",
        "sec_y_ht = -4.0\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_base, data=section_sents_df, alpha=0.3, label=f'Raw Sentence Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_sents_df, alpha=0.5, label=f'MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.plot(col_lnorm_medianiqr, data=section_sents_df, alpha=0.5, label=f'MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw vs MedianIQR Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzoXo8mVZMq_"
      },
      "source": [
        "# Within the Selected_Section_No, compare Sentence Sentiment: \n",
        "#     MedianIQR vs Length-Normed MedianIQR Sentence Sentiment within selected Section\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_name, data=section_sents_df, alpha=0.3, label=f'Length-Normed MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_sents_df, alpha=0.3, label=f'MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLength-Normed vs non-Normed Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tag3LC8Y9Is"
      },
      "source": [
        "# Within the Selected_Section_No, compare Paragraph Sentiment\n",
        "#    Standardized MedianIQR vs Length-Normed Standardized MedianIQR\n",
        "\n",
        "sec_y_ht = 0\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(aparag_no, sec_y_ht, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(aparag_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_name, data=section_parags_df, alpha=0.3, label=f'Length-Normed MedianIQR Paragraph Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_parags_df, alpha=0.3, label=f'MedianIQR Paragraph Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLength-Normed vs non-Normed Paragraph Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJmDFQeH6gI"
      },
      "source": [
        "##### **LOWESS Smoothed**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhYnx-MgH_G0"
      },
      "source": [
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0bylmqBf7Js"
      },
      "source": [
        "# Within the Selected_Section_No, use Lowess Smoothing and SciPy find_peaks()\n",
        "#    to get Sentence Crux Points \n",
        "\n",
        "win_lowess_start = 20\n",
        "win_lowess_end = 50\n",
        "win_lowess_step = 10\n",
        "\n",
        "win_lowess_no = 10\n",
        "sec_y_ht = 0\n",
        "\n",
        "for win_lowess_no in range(win_lowess_start, win_lowess_end, win_lowess_step):\n",
        "  section_crux_ls = get_lowess_cruxes(ts_df=section_sents_df, col_series=model_name, text_type='sentence', win_lowess=win_lowess_no, sec_y_height=sec_y_ht, subtitle_str=f'win={win_lowess_no}', do_plot=True, save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DQ4zGtRizlM"
      },
      "source": [
        "# TODO: Only printing sentiment for first crux point\n",
        "\n",
        "section_sents_df.iloc[62]['sent_raw']\n",
        "print('\\n')\n",
        "section_sents_df.iloc[62]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B0vm5RjgCLS"
      },
      "source": [
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "\n",
        "\n",
        "crux_sortsents_report(section_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa124qbuKzzH"
      },
      "source": [
        "# LOWESS Smoothed Sentences within chosen Selection No\n",
        "\n",
        "my_afrac = 1./12   # 1./12 ~ 0.08\n",
        "\n",
        "temp_df = get_lowess(section_sents_df, [model_name], plot_subtitle='LOWESS Smoothed MedianIRQ Sentence Sentiment', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                afrac=my_afrac, ait=7, alpha=0.8, do_plot=True, save2file=False)\n",
        "temp_df.columns\n",
        "col_lowess = f'{model_name}_{my_afrac:2.3f}lowess'\n",
        "col_lowess_clean = col_lowess.replace('_0.','_')\n",
        "section_sents_df[col_lowess_clean] = temp_df['median']\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Sentence Sentiments with selected Section #{Select_Section_No} (LOWESS frac={my_afrac:.2f})')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend('',frameon=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLeyWMtMaEqZ"
      },
      "source": [
        "# LOWESS Smoothed Paragraphs within chosen Selection No\n",
        "\n",
        "my_afrac = 1./8 # 1./8 ~ 0.125\n",
        "\n",
        "temp_df = get_lowess(section_parags_df, [model_name], plot_subtitle='LOWESS Smoothed Mean Rolling Sentence Sentiment', alabel=f'LOWESS Smoothed (afrac={my_afrac})', \n",
        "                afrac=my_afrac, ait=7, alpha=0.8, do_plot=True, save2file=False)\n",
        "\n",
        "section_parags_df[f'{model_name}_{my_afrac:.2f}_lowess'] = temp_df['median']\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Sentence Sentiments with selected Section #{Select_Section_No} (LOWESS frac={my_afrac:.2f})')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend('',frameon=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GfIuHNNmu-H"
      },
      "source": [
        "section_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEMWOxr7m2cl"
      },
      "source": [
        "section_crux_sents_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zouNSYD9Yloj"
      },
      "source": [
        "section_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vVdmcTcYb40"
      },
      "source": [
        "model_name = 'sentimentr_lnorm_medianiqr'\n",
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJa85Yx-rkkx"
      },
      "source": [
        "!pip install hdbscan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGA7kXGjrNET"
      },
      "source": [
        "from hdbscan import HDBSCAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZYo_LR8rF_J"
      },
      "source": [
        "\n",
        "\n",
        "y_ls = [1,2,4,7,9,5,4,7,9,56,57,54,60,200,297,275,243]\n",
        "y = np.reshape(y_ls, (-1, 1))\n",
        "type(y)\n",
        "y.shape\n",
        "\n",
        "clusterer = HDBSCAN(min_cluster_size=3)\n",
        "cluster_labels = clusterer.fit_predict(y)\n",
        "\n",
        "best_cluster = clusterer.exemplars_[cluster_labels[y.argmax()]].ravel()\n",
        "print(best_cluster)\n",
        "cluster_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsvTX-vtz8lq"
      },
      "source": [
        "type(y_ls)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo2zOaWVtUsm"
      },
      "source": [
        "y = np.reshape(crux_points_x_ls, (-1,1))\n",
        "\n",
        "clusterer = HDBSCAN(min_cluster_size=3)\n",
        "cluster_labels = clusterer.fit_predict(y)\n",
        "\n",
        "best_cluster = clusterer.exemplars_[cluster_labels[y.argmax()]].ravel()\n",
        "print(best_cluster)\n",
        "cluster_labels\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBXI8l6ywagn"
      },
      "source": [
        "len(color_palette)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB5VL_sV0tAG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRoIWeY1vx8M"
      },
      "source": [
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=3).fit(y)\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')\n",
        "\n",
        "color_palette = sns.color_palette('deep', 9)\n",
        "cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in cluster_labels]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, clusterer.probabilities_)]\n",
        "x = np.zeros_like(y) + 12.5\n",
        "plt.scatter(*y.T, x, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN7Rpw7B0xu-"
      },
      "source": [
        "print(crux_points_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLafViJ2zxrQ"
      },
      "source": [
        "len(crux_points_ls)\n",
        "crux_points_np.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irX4QSEbtTiy"
      },
      "source": [
        "crux_points_np = np.reshape(crux_points_ls, (-1,1))\n",
        "# np.reshape(crux_points_x_ls, (-1,1))\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=3).fit(crux_points_np)\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')\n",
        "\n",
        "color_palette = sns.color_palette('deep', 9)\n",
        "cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in cluster_labels]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, clusterer.probabilities_)]\n",
        "y_np = np.zeros_like(crux_points_np) + 12.5\n",
        "plt.scatter(*crux_points_np.T, y_np, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NRZ0Auka-uG"
      },
      "source": [
        "section_crux_sents_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGDccDTMfMT4"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10, 1./14, 1./16]\n",
        "# grid_fracs = [1./10, 1./14, 1./16]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.3]\n",
        "win_lowess=9\n",
        "\n",
        "\n",
        "# section_sents_df['vader_lnorm_medianiqr'].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot('vader_lnorm_medianiqr', data=section_sents_df)\n",
        "plt.title(f'LOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "section_crux_sents_dt = {}\n",
        "\n",
        "for afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  #   _ = get_lowess(section_sents_df, ['vader_lnorm_medianiqr'], plot_subtitle='Naive Raw + MedianIQR Midpoints', alabel=f'LOWESS Smoothed (afrac={my_afrac})', \n",
        "  #                afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # afrac = 1./7\n",
        "  # model_name = 'vader_lnorm_medianiqr' # model_name\n",
        "  sm_x, sm_y = sm_lowess(endog=section_sents_df[model_name].values, exog=section_sents_df.index.values, frac=afrac, it=3, return_sorted = True).T\n",
        "  col_lowess_frac = f'{model_name}_frac{int((afrac-int(afrac))*100)}_win{win_lowess}'\n",
        "  section_sents_df[col_lowess_frac] = sm_y\n",
        "  # _ = get_lowess(ts_df='section_sents_df', models_ls=[col_roll_str], text_unit='sentence', plot_subtitle='', alabel='', afrac=1./10, ait=5, alpha=0.5, do_plot=True, save2file=False)\n",
        "  section_crux_ls = list(get_lowess_cruxes(section_sents_df, col_series=col_lowess_frac, win_lowess=win_lowess, do_plot=False))\n",
        "  # col_lowess_frac = f'{model_name}_frac{int((afrac-int(afrac))*100)}_win{win_lowess}'\n",
        "  # print(f\"col_lowess_frac: {col_lowess_frac}\")\n",
        "  section_crux_sents_dt[col_lowess_frac] = section_crux_ls # list(zip(sm_x, sm_y))\n",
        "  # x, y = zip(*data)\n",
        "\n",
        "  # Set vertical y-axis magnification\n",
        "  y_mag = 30\n",
        "  plt.plot(sm_x, y_mag*sm_y)\n",
        "  plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nDifferent LOWESS Smoothed SMA Sentence Sentiments and Crux Points within selected Section #{Select_Section_No}')\n",
        "  plt.legend(loc='best')\n",
        "\n",
        "\n",
        "# Plot Crux Points for all LOWESS Curves on the x-axis\n",
        "crux_points_ls = []\n",
        "for key,value in section_crux_sents_dt.items():\n",
        "  model_lowess_name = key\n",
        "  crux_points_ls.extend(value)\n",
        "  plt.scatter(*zip(*value))\n",
        "\n",
        "# Plot Automatic HDBSCAN Clusters of Crux Points\n",
        "crux_points_np = np.reshape(crux_points_ls, (-1, 1))\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=3).fit(crux_points_np)\n",
        "print(f'HDBSCAN found {clusterer.labels_.max()} clusters.')\n",
        "\n",
        "color_palette = sns.color_palette('deep', 9)\n",
        "cluster_colors = [color_palette[x] if x >= 0\n",
        "                  else (0.5, 0.5, 0.5)\n",
        "                  for x in cluster_labels]\n",
        "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
        "                         zip(cluster_colors, clusterer.probabilities_)]\n",
        "y_np = np.zeros_like(crux_points_np) + 12.5\n",
        "plt.scatter(*crux_points_np.T, y_np, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} Sentence Crux Detection within selected Section #{Select_Section_No}\\nLOWESS Smoothed (Model: {model_lowess_name})')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\n",
        "# Plot the mean of all SMA MedianIQR Sentiment Time Series\n",
        "# col_meanroll = f'{model_name}_rollmean'\n",
        "# section_sents_df[col_meanroll] = section_sents_df[col_rolls_ls].mean(axis=1)\n",
        "# section_sents_df[col_meanroll].plot(label='mean', color='black', linewidth=3)\n",
        "\n",
        "# Plot corresponding Crux Points\n",
        "# model_name = 'vader_lnorm_medianiqr'\n",
        "section_crux_ls = get_lowess_cruxes(section_sents_df, col_series=model_name, win_lowess=10, do_plot=False) # 'vader_lnorm_medianiqr_0.07_lowess')\n",
        "section_sents_df.shape[0]\n",
        "print('\\n');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDx21JbaeL9s"
      },
      "source": [
        "!pip install kmeans1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFPbi6WqQVUs"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPcTifCyeUJh"
      },
      "source": [
        "import kmeans1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN4M4LEAeoCa"
      },
      "source": [
        "crux_points_x_ls = [x[0] for x in crux_points_ls]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htu7N02odHaU"
      },
      "source": [
        "**Enter how many Clusters of potential Crux Points you see in the Plot above**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBSVG9LXa2FD"
      },
      "source": [
        "Cluster_Count = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "clusters, centroids = kmeans1d.cluster(crux_points_x_ls, Cluster_Count)\n",
        "\n",
        "print(clusters)  \n",
        "print(centroids)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVCq0YUXBpX8"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2]\n",
        "\n",
        "# section_sents_df['vader_lnorm_medianiqr'].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot('vader_lnorm_medianiqr', data=section_sents_df)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "for my_afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  _ = get_lowess(section_sents_df, [model_name], plot_subtitle='Naive Raw + MedianIQR Midpoints', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                 afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # corpus_cruxes_dt['vader_lnorm_medianiqr'] = plot_crux_sections(model_names_ls=['vader_lnorm_medianiqr'], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL}\\n LOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpzOnRqZS3jZ"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.12, 0.14, 0.16, 0.18, 0.2]\n",
        "\n",
        "# section_sents_df[model_name].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot(model_name, data=section_sents_df)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "for my_afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  _ = get_lowess(section_parags_df, [model_name], plot_subtitle='MedianIQR Midpoints', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                 afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # corpus_cruxes_dt[model_name] = plot_crux_sections(model_names_ls=[model_name], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL}\\n LOWESS Smoothed Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O85J51__Ka-z"
      },
      "source": [
        "section_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48u_zr6-KkH-"
      },
      "source": [
        "# SMA Sentences\n",
        "\n",
        "# grid_afracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.3]\n",
        "# grid_fracs = [1./6, 1./7, 1./8, 1./9, 1./10, 1./15, 1./20]\n",
        "scale_roll = 1.\n",
        "win_lowess_per = 30\n",
        "win_lowess = int(win_lowess/100 * section_sents_df.shape[0])\n",
        "\n",
        "col_meanroll_lowess_ls = []\n",
        "\n",
        "col_meanroll = f'{model_name}_mean_roll050'\n",
        "for afrac in grid_fracs:\n",
        "  lowess_smooth_df = get_lowess(section_sents_df, [col_meanroll], plot_subtitle='SMA Mean of MedianIQR ', alabel=f'LOWESS afrac={afrac:.3f}', \n",
        "                afrac=afrac, ait=7, alpha=0.3, do_plot=True, save2file=False)\n",
        "  # print(f'type: {lowess_smooth_df.columns}')\n",
        "\n",
        "# col_lowess_str = f'{col_mean_roll}_lowess_frac{10*win_lowess}'\n",
        "col_meanroll_lowess_str = f'{col_meanroll}_frac{int((afrac-int(afrac))*100)}_win{win_lowess}'\n",
        "col_meanroll_lowess_ls.append(col_meanroll_lowess_str)\n",
        "section_sents_df[col_meanroll_lowess_str] = section_sents_df[model_name].apply(lambda x: x*scale_roll).rolling(win_lowess, center=True).mean()\n",
        "section_sents_df[col_meanroll_lowess_str].plot(alpha=0.7)\n",
        "get_lowess(section_sents_df, [col_meanroll_lowess_str], plot_subtitle='SMA Mean of MedianIQR ', alabel=f'LOWESS (afrac={my_afrac:.3f})', \n",
        "                afrac=afrac, ait=7, alpha=1.0, do_plot=True, save2file=False);\n",
        "\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KlWvNkcMI-M"
      },
      "source": [
        "**Raw Paragraph Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhRcTp4ALYUQ"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "win_sents_ls = [5,10,15,20,25]\n",
        "scale_roll = 6\n",
        "\n",
        "plt.plot(model_name, data=section_parags_df, alpha=0.3, label=f'Raw Paragraph Sentiment within selected Segment #{Select_Section_No}')\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5TcmNEsMMiE"
      },
      "source": [
        "**Length-Normed Paragraph Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BemEIw0Tknk"
      },
      "source": [
        "corpus_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyRTfn6HI50X"
      },
      "source": [
        "# Plot and Compare Naive Raw and LOWESS Smoothed Paragraph Sentiment Time Series within selected Section\n",
        "\n",
        "section_parag_lowess_df = pd.DataFrame()\n",
        "section_parag_lowess_df['parag_no'] = section_parags_df['parag_no'].copy()\n",
        "\n",
        "parags_midpoint_sentiment_ls = []\n",
        "for parags_midpoint_idx in parags_midpoint_ls:\n",
        "  parags_midpoint_sentiment_ls.append(float(corpus_parags_df[corpus_parags_df['parag_no'] == parags_midpoint_idx][model_name]))\n",
        "\n",
        "col_midapprox = f'{model_name}_midapprox'\n",
        "section_parag_lowess_df[col_midapprox] = parags_midpoint_sentiment_ls\n",
        "\n",
        "\n",
        "section_parag_lowess_df[col_midapprox].plot(label='Raw Midpoints')\n",
        "plt.xlabel(f'Niave Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "\n",
        "_ = get_lowess(section_parag_lowess_df, [col_midapprox], plot_subtitle=f'{model_base.capitalize()} Naive Raw + MedianIQR Midpoints', alabel='LOWESS Midpoints', afrac=1./4, ait=7, do_plot=True, save2file=False);\n",
        "\n",
        "# section_lowess_parags_df = get_lowess(section_sents_parags_df, ['vader_lnorm_medianiqr'], plot_subtitle='Approximate Paragraph MedianIQR', afrac=1./4, ait=7, do_plot=True, save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24M7uauXygFc"
      },
      "source": [
        "**Length-Noramlized Raw and LOWESS Smoothed Paragraph Sentiment plots within selected Section**\n",
        "\n",
        "NOTE: Horizonal x-axis narrative time axis adjusted for variable paragraph lengths - used midpoints of unequal length Paragraphs to more accurately visualize Sentiment Arc and precisely localize Crux Points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOsXfELh-tt_"
      },
      "source": [
        "# Verify details on currently selected Section\n",
        "print(f'Details on Section #{Select_Section_No}')\n",
        "print('------------------------')\n",
        "print(f' Paragraph Count: {section_parags_df.shape[0]}')\n",
        "print(f' Sentence Count:  {section_sents_df.shape[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfQS_qAo-Qdh"
      },
      "source": [
        "# %load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq2JjzKIHEdO"
      },
      "source": [
        "# Plot Raw and Rolling Sentence Sentiments within selected Section\n",
        "\n",
        "win_per = 5  # Rolling Window size in percentage of total Corpus length\n",
        "\n",
        "section_sents_parags_df.plot(x='sent_no', y='vader_lnorm_medianiqr')\n",
        "\n",
        "plt.title(f'Raw and Rolling Sentence Sentiments within selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No} (Length-Normalized in terms of Paragraphs)')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "\n",
        "section_sents_parags_df['vader_lnorm_medianiqr'].rolling(int((win_per/100)*section_sents_parags_df.shape[0])).mean().plot(label=\"Approx Paragraph VADER SMA (win=5%)\");\n",
        "\n",
        "# section_sents_parags_df.plot(x='sent_no', y='vader_lnorm_medianiqr', label='Sentence VADER MedianIQR')\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr'].rolling(int(0.05*section_sents_parags_df.shape[0])).mean().plot(label=\"Sentence VADER SMA (win=5%)\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laYb3dm101Qa"
      },
      "source": [
        "**Get Crux Points within selected Section**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peaks] to search for Peaks (unselect to search for Valley)\n",
        "\n",
        "* Pick [Crux_Rank] (1-5) to get the 1st to 5th biggest Peak or Valley Crux Paragraph\n",
        "\n",
        "* Pick [Context_Paragraphs_Each_Side] (0-5) to get n paragraphs before and n paragraphs after the selected Crux Paragraph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyekwnvX4wkj"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# ARCHIVED\n",
        "\n",
        "def get_sentnocontext(ts_df, model_name='vader', get_peaks=True, crux_rank=1, n_sideparags=1):\n",
        "  # get_cruxparags_section()\n",
        "  '''\n",
        "  Given a Section DataFrame with model_name sentiment column and crux peak/valley, rank and side paragraphs context\n",
        "  Return a list with the appropriate Crux Paragraph within this Section, and context\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "  Given a sentence number in the Corpus\n",
        "  Return the containing paragraph and n-paragraphs on either side\n",
        "  (e.g. if n=2, return 2+1+2=5 paragraphs)\n",
        "  '''\n",
        "\n",
        "  crux_parags_context_ls = []\n",
        "\n",
        "  if get_peaks == True:\n",
        "    sort_asc_flag=False\n",
        "  else:\n",
        "    sort_asc_flag=True\n",
        "\n",
        "  crux_parag_no = ts_df.sort_values(by=[model_name], ascending=sort_asc_flag).iloc[crux_rank-1]['parag_no']\n",
        "\n",
        "  print(f'crux_parag_no: {crux_parag_no}')\n",
        "\n",
        "  if n_sideparags == 0:\n",
        "    crux_parags_context_ls = list(corpus_parags_df[corpus_parags_df['parag_no'] == crux_parag_no]['parag_raw'])\n",
        "\n",
        "  else:\n",
        "    parag_start = crux_parag_no - n_sideparags\n",
        "    parag_end = crux_parag_no + n_sideparags + 1\n",
        "    crux_parags_context_ls = list(corpus_parags_df.iloc[parag_start:parag_end]['parag_raw'])\n",
        "\n",
        "  return crux_parags_context_ls\n",
        "\n",
        "# Test\n",
        "parags_context_ls = get_sentnocontext(ts_df=section_parags_df, model_name='vader_lnorm_medianiqr', get_peaks=True, crux_rank=1, n_sideparags=1)\n",
        "parags_context_ls\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxeabYXf2iqB"
      },
      "source": [
        "# def get_crux_parags_report(ts_df, model_name='vader', get_peaks=True, crux_rank=1, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence):\n",
        "\n",
        "# def get_sentnocontext_report(ts_df, model_name='vader', get_peaks=True, crux_rank=1, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence):\n",
        "'''\n",
        "Given a DataFrame with model_name sentiment column and crux peak/valley, rank and side paragraphs context\n",
        "Return a list with the appropriate Crux Paragraph, and context\n",
        "'''\n",
        "\"\"\"\n",
        "\n",
        "def get_sentnocontext_report(the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n",
        "  '''\n",
        "  Wrapper function around  get_sentnocontext()\n",
        "  Prints a nicely formatted context report\n",
        "  '''\n",
        "\n",
        "  context_noparags = the_n_sideparags*2+1\n",
        "\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f'The {context_noparags} Paragraph(s) Context around the Sentence #{Crux_Sentence_No} Crux Point:')\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f\"\\nCrux Sentence Raw Text: -------------------------------\\n\\n    {corpus_sents_df[corpus_sents_df['sent_no'] == the_sent_no]['sent_raw']}\") # iloc[the_sent_no]['sent_raw']}\")\n",
        "\n",
        "  print(f\"\\n{context_noparags} Paragraph(s) Context: ------------------------------\")\n",
        "  # context_parags_ls = get_sentnocontext(sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_parags_ls = get_sentnocontext(sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_len = len(context_parags_ls)\n",
        "  context_mid = context_len//2\n",
        "  for i, aparag in enumerate(context_parags_ls):\n",
        "    if i==context_mid:\n",
        "      # print(f'\\n>>> Paragraph #{i}: <<< Crux Point Sentence CAPITALIZED within this Paragraph\\n\\n    {aparag}')\n",
        "      print(f'\\n<*> {aparag}')\n",
        "    else:\n",
        "      # print(f'\\n    Paragraph #{i}:\\n\\n    {aparag}')\n",
        "      print(f'\\n    {aparag}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# get_sentnocontext_report(sent_no=1051, n_sideparags=1, sent_highlight=True)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxQGzYhgHEUD"
      },
      "source": [
        "# Get_Peaks = True #@param {type:\"boolean\"}\n",
        "Crux_Rank = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "\n",
        "# try:\n",
        "  \n",
        "# get_sentnocontext_report(ts_df=section_sents_df, model_name=model_name, get_peaks=Get_Peaks, crux_rank=Crux_Rank, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "get_sentnocontext_report(corpus_sents_df, the_sent_no=Crux_Rank, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "\n",
        "# except:\n",
        "#   print('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cPnJ7-QAHBp"
      },
      "source": [
        "section_parags_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga65SGcg5sM9"
      },
      "source": [
        "print(section_parags_df.sort_values(by=['vader_lnorm_medianiqr'], ascending=False).iloc[0]) # ['parag_no'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGuFadpM8QoB"
      },
      "source": [
        "parags_context_ls = get_cruxparag_context(ts_df=section_sents_parags_df, model_name='vader_lnorm_medianiqr', get_peaks=True, crux_rank=1, n_sideparags=2, sent_highlight=True)\n",
        "parags_context_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPE_ODwK2imT"
      },
      "source": [
        "section_sents_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gz8UImlq5Uy"
      },
      "source": [
        "# **Save Crux Points and Contexts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NH8ltVX9SvD"
      },
      "source": [
        "## **Section, Chapter Crux DataFrames (summary stats/sentiments only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovjd7uvr2hS8"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLb7mQgV7_JK"
      },
      "source": [
        "# Create a Chapter Summary DataFrame extracting only key information (no text)\n",
        "\n",
        "corpus_chaps_summary_df = corpus_chaps_df[['chap_no','sent_no_start','sent_no_mid','char_len','token_len',\n",
        "                 'sentimentr','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'syuzhet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'bing','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'sentiword','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'senticnet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'nrc','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'afinn','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'vader','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'textblob','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'pattern','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'stanza','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 ]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSvXSwp-2ePp"
      },
      "source": [
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5s8R38P81M7"
      },
      "source": [
        "# Create a Section Summary DataFrame extracting only key information (no text)\n",
        "\n",
        "corpus_sects_summary_df = corpus_sects_df[['sect_no','sent_no_start','sent_no_mid','char_len','token_len',\n",
        "                 'sentimentr','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'syuzhet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'bing','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'sentiword','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'senticnet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'nrc','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'afinn','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'vader','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'textblob','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'pattern','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'stanza','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 ]]\n",
        "\n",
        "corpus_sects_summary_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Op-IgH9iP2"
      },
      "source": [
        "# Save the original Corpus text at 4 levels of semantic grouping: sentences, paragraphs, sections and chapters\n",
        "\n",
        "# Save Section and Chapter DataFrames Metainformation (e.g. sent_no_start) and Sentiment Values\n",
        "corpus_sects_summary_filename = f'corpus_section_summary_lexrules_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Section Summary to file: {corpus_sects_summary_filename}')\n",
        "corpus_sects_summary_df.to_csv(corpus_sects_summary_filename)\n",
        "\n",
        "corpus_chaps_summary_filename = f'corpus_chapter_summary_lexrules_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Chapters Summary to file: {corpus_chaps_summary_filename}')\n",
        "corpus_chaps_summary_df.to_csv(corpus_chaps_summary_filename)\n",
        "\n",
        "# Save Corpus Cruxes Dictionary is saved to a JSON file\n",
        "corpus_cruxes_summary_filename = f'corpus_cruxes_summary_lexrules_{author_abbr_str}_{title_str}.json' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes Summary to file: {corpus_cruxes_summary_filename}')\n",
        "with open(corpus_cruxes_summary_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSO0vT7oXOXO"
      },
      "source": [
        "# Verify exported Section Summary file\n",
        "\n",
        "!head -n 5 $corpus_sects_summary_filename\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3RIJOdsI_Ud"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0A0M6TPu_Ml"
      },
      "source": [
        "## **Compare Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpX60Cld_B7z"
      },
      "source": [
        "def drop_dupcols(df):\n",
        "  '''\n",
        "  Given a DataFrame\n",
        "  Drop repeatitive columns\n",
        "  '''\n",
        "\n",
        "  col_drop_ls = []\n",
        "\n",
        "  col_ls = list(df.columns)\n",
        "  print(f'BEFORE: Columns #{len(df.columns)}')\n",
        "\n",
        "  for i, acol in enumerate(col_ls):\n",
        "    acol_word_ls = acol.split('_')\n",
        "    # print(f'acol_word_ls: {acol_word_ls}')\n",
        "    if (len(acol_word_ls)) == len(set(acol_word_ls)):\n",
        "      continue\n",
        "    else:\n",
        "      col_drop_ls.append(acol)\n",
        "\n",
        "  df.drop(columns=col_drop_ls, inplace=True, axis=1)\n",
        "\n",
        "  print(f'AFTER: Columns #{len(df.columns)}')\n",
        "\n",
        "  return col_drop_ls\n",
        "\n",
        "dropped_cols_ls = drop_dupcols(corpus_parags_df)\n",
        "print(f'dropped: {dropped_cols_ls}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIOFkFzYinfd"
      },
      "source": [
        "corpus_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lpvN8PR9VRk"
      },
      "source": [
        "# List of Tuples (Model, Scaling Factor) to plot together with same size for comparison\n",
        "\n",
        "models_sma_ls = [('vader_lnorm_medianiqr',1),\n",
        "                 ('textblob_lnorm_medianiqr',20),\n",
        "                 ('afinn_lnorm_medianiqr',4),\n",
        "                 ('sentimentr_lnorm_medianiqr',1),\n",
        "                 ('syuzhet_lnorm_medianiqr',1),\n",
        "                 ('bing_lnorm_medianiqr',0.1),\n",
        "                 ('sentiword_lnorm_medianiqr',10),\n",
        "                 ('senticnet_lnorm_medianiqr',.5),\n",
        "                 ('nrc_lnorm_medianiqr',0.2),\n",
        "                 ('pattern_lnorm_medianiqr',20),\n",
        "                 ('stanza_lnorm_medianiqr',0.5),\n",
        "                 ('hfbert_lnorm_medianiqr',5),\n",
        "                 ('nlptown_lnorm_medianiqr',5),\n",
        "                 ('robertalg15_lnorm_medianiqr',5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1pjqF9nTQg-"
      },
      "source": [
        "# List of Tuples (Model, Scaling Factor) to plot together with same size for comparison\n",
        "\n",
        "models_sma_ls = [('vader',1),\n",
        "                 ('textblob',20),\n",
        "                 ('afinn',4),\n",
        "                 ('sentimentr',1),\n",
        "                 ('syuzhet',1),\n",
        "                 ('bing',0.1),\n",
        "                 ('sentiword',10),\n",
        "                 ('senticnet',.5),\n",
        "                 ('nrc',0.2),\n",
        "                 ('pattern',20),\n",
        "                 ('stanza',0.5),\n",
        "                 ('hfbert',5),\n",
        "                 ('nlptown',5),\n",
        "                 ('robertalg15',5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RihyT4wZFP5k"
      },
      "source": [
        "corpus_sents_df[['stanza_lnorm_medianiqr','stanza']].rolling(500,center=True).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaxzZkVlBcNf"
      },
      "source": [
        "def plot_autoscaled_ts(ts_df=corpus_sents_df, ts_ls=['vader_lnorm_medianiqr', 'textblob_lnorm_medianiqr', \n",
        "                                                     'syuzhet_lnorm_medianiqr', 'sentimentr_lnorm_medianiqr',\n",
        "                                                     'bing_lnorm_medianiqr', 'afinn_lnorm_medianiqr',\n",
        "                                                     'pattern_lnorm_medianiqr', 'stanza_lnorm_medianiqr']):\n",
        "  '''\n",
        "  Given a DataFrame and list of Columns/Time Series\n",
        "  Automatically scale all to the same range and plot together\n",
        "  '''\n",
        "\n",
        "  ts_spans_ls = []\n",
        "\n",
        "  current_min = ts_df[ts_ls[0]].min()\n",
        "  current_max = ts_df[ts_ls[0]].max()\n",
        "  ts_spans_ls.append(float(current_max - current_min))\n",
        "\n",
        "  for ats in ts_ls[1:]:\n",
        "    current_min = ts_df[ats].min()\n",
        "    current_max = ts_df[ats].max()\n",
        "    ts_spans_ls.append(float(current_max - current_min))\n",
        "\n",
        "  # find index of maximum span\n",
        "  max_index = ts_spans_ls.index(max(ts_spans_ls))\n",
        "  max_span_value = ts_spans_ls[max_index]\n",
        "  max_span_model = ts_ls[max_index]\n",
        "  print(f'max span is: {max_span_value} from {max_span_model}')\n",
        "\n",
        "  for i, ats in enumerate(ts_ls):\n",
        "    y_scaling_factor = max_span_value/ts_spans_ls[i]\n",
        "    print(f'ats={ats} with scaling={y_scaling_factor}')\n",
        "    ts_y_scaled_ser = ts_df[ats].apply(lambda x: x*y_scaling_factor)\n",
        "    # plt.plot()\n",
        "    plot_df = pd.DataFrame()\n",
        "    plot_df['x_value'] = ts_df.index\n",
        "    plot_df['y_scaled'] = ts_y_scaled_ser\n",
        "    plot_df['y_scaled_roll050'] = plot_df['y_scaled'].rolling(350, center=True).mean()\n",
        "    plot_df['y_scaled_roll050'].plot(label=f'{ats}')\n",
        "    plt.legend(loc='best')\n",
        "    # sns.lineplot(data=plot_df, x='x_value', y='y_scaled', alpha=0.5, label=f'{ats}')\n",
        "\n",
        "  return ts_spans_ls, ts_ls\n",
        "\n",
        "# Test\n",
        "ts_spans_ls, ts_ls = plot_autoscaled_ts()\n",
        "zip_ls = zip(ts_spans_ls, ts_ls)\n",
        "for aspan, amodel in zip_ls:\n",
        "  print(f'model: {amodel} with span: {aspan}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emkaDWvDILDc"
      },
      "source": [
        "ts_df=corpus_sents_df, ts_ls=['vader_lnorm_medianiqr', 'textblob_lnorm_medianiqr', \n",
        "                                                     'syuzhet_lnorm_medianiqr', 'sentimentr_lnorm_medianiqr',\n",
        "                                                     'bing_lnorm_medianiqr', 'afinn_lnorm_medianiqr',\n",
        "                                                     'pattern_lnorm_medianiqr', 'stanza_lnorm_medianiqr']):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T8pf6DsIGmq"
      },
      "source": [
        "# Test\n",
        "ts_spans_ls, ts_ls = plot_autoscaled_ts(ts_df=corpus_parags_df)\n",
        "zip_ls = zip(ts_spans_ls, ts_ls)\n",
        "for aspan, amodel in zip_ls:\n",
        "  print(f'model: {amodel} with span: {aspan}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWGdzLS4Ibr-"
      },
      "source": [
        "corpus_sents_df['pattern_lnorm_medianiqr'].hist(bins=100) # rolling(100, center=True).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42RrUksBOxfo"
      },
      "source": [
        "models_sma_ls = [('vader_lnorm_medianiqr',1),\n",
        "                 ('textblob_lnorm_medianiqr',20),\n",
        "                 ('sentimentr_lnorm_medianiqr',1),\n",
        "                 ('syuzhet_lnorm_medianiqr',1),\n",
        "                 ('stanza_lnorm_medianiqr',0.2)]\n",
        "\n",
        "win_per = 5  # 5=5% of full corpus length\n",
        "win_roll = int(corpus_sents_df.shape[0]* win_per/100)\n",
        "\n",
        "for amodel, amag in models_sma_ls:\n",
        "  corpus_parags_df[amodel].rolling(win_roll, center=True).mean().apply(lambda x: amag*x).plot(linewidth=4, label=amodel)\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQgdwsJGZN_"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpOCp1-88rrF"
      },
      "source": [
        "# **Standardize and Remove Outliers (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mul5MSZrgKsw"
      },
      "source": [
        "## **Remove Outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07hyJuT1c5rJ"
      },
      "source": [
        "### **Before Removing Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKX8f0Q1I53M"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  print(f'Plotting {model_name}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_name, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzUoTpEWeyCy"
      },
      "source": [
        "# sns.lineplot(data=corpus_sents_df, x='sent_no', y='y_scaled', legend='brief', label='y_scaled')\n",
        "      \n",
        "# plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nSMA Smoothed Sentence Sentiment Plot (windows={win_ls})')\n",
        "# plt.legend(loc='best')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr1k-3Rsc5XL"
      },
      "source": [
        "# Plot all Raw Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_LS:\n",
        "  print(f'Plotting {model_name}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_name, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FrNFec7tNij"
      },
      "source": [
        "### **Remove Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYXa7qoevyuo"
      },
      "source": [
        "# Trim outliers to max of 3*Median Abs Variance in Standardized Sentiment Time Series\n",
        "#   and overwrite results in model_name column\n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\n",
        "# Sentence\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Sentence: {col_noouts_str} --------------------')\n",
        "  corpus_sents_df[col_noouts_str] = clip_outliers(corpus_sents_df[amodel_str])\n",
        "  \n",
        "  print(f'  old Standardized max: {corpus_sents_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sents_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_sents_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_sents_df[col_noouts_str].min()}')\n",
        "  \n",
        "# col_rename_dt = rename_cols(corpus_sents_df, models_ls) # ERROR: created 1 new col with col_rename_dt dictionary name instead of mapping correctly\n",
        "# col_rename_dt\n",
        "# _ = corpus_sents_df.rename(columns=col_rename_dt, inplace=True, errors='raise');\n",
        "\n",
        "# Paragraph\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Paragraph: {col_noouts_str} --------------------')\n",
        "  corpus_parags_df[col_noouts_str] = clip_outliers(corpus_parags_df[amodel_str])\n",
        "\n",
        "  print(f'  old Standardized max: {corpus_parags_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_parags_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_parags_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_parags_df[col_noouts_str].min()}')\n",
        "\n",
        "# Section\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Section: {col_noouts_str} --------------------')\n",
        "  corpus_sects_df[col_noouts_str] = clip_outliers(corpus_sects_df[amodel_str])\n",
        "\n",
        "  print(f'  old Standardized max: {corpus_sects_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sects_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_sects_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_sects_df[col_noouts_str].min()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyPsLI9E4tbB"
      },
      "source": [
        "# Trim outliers to max of 3*Median Abs Variance in Standardized Sentiment Time Series\n",
        "#   and overwrite results in model_name column\n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\"\"\"\n",
        "# Sentences\n",
        "for amodel in MODELS_LS:\n",
        "  col_stand = amodel + '_stand'\n",
        "  col_standout = amodel + '_standout'\n",
        "  print(f'Sentences: {col_stand} --------------------')\n",
        "  # corpus_sents_df[amodel] = corpus_sents_df[col_stand]\n",
        "  corpus_sents_df[col_standout] = clip_outliers(corpus_sents_df[col_stand])\n",
        "  \n",
        "  print(f'  old Standardized max: {corpus_sents_df[col_stand].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sents_df[col_stand].min()}')\n",
        "  print(f'  new max: {corpus_sents_df[col_standout].max()}')\n",
        "  print(f'  new min: {corpus_sents_df[col_standout].min()}')\n",
        "  \n",
        "# col_rename_dt = rename_cols(corpus_sents_df, models_ls) # ERROR: created 1 new col with col_rename_dt dictionary name instead of mapping correctly\n",
        "# col_rename_dt\n",
        "# _ = corpus_sents_df.rename(columns=col_rename_dt, inplace=True, errors='raise');\n",
        "\n",
        "# Paragraphs\n",
        "for amodel in MODELS_LS:\n",
        "  col_stand = amodel + '_stand'\n",
        "  col_standout = amodel + '_standout'\n",
        "  print(f'Paragraphs: {col_stand} --------------------')\n",
        "  # corpus_parags_df[amodel] = corpus_parags_df[col_stand]\n",
        "  corpus_parags_df[col_standout] = clip_outliers(corpus_parags_df[col_stand])\n",
        "  print(f'  old Standardized max: {corpus_parags_df[col_stand].max()}')\n",
        "  print(f'  old Standardized min: {corpus_parags_df[col_stand].min()}')\n",
        "  print(f'  new max: {corpus_parags_df[col_standout].max()}')\n",
        "  print(f'  new min: {corpus_parags_df[col_standout].min()}')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPKABrvstSIo"
      },
      "source": [
        "### **After Removing Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ccxOjqghbgP"
      },
      "source": [
        "# Plot all Raw Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "# Exlopre to find which Sentiment Time Series still have outliers after initial 2.5*Median Abs Dev Clipping\n",
        "MODELS_SENTS_EXCLUDE_LS = ['nrc','bing','afinn','stanza']  # Likely these TS are not normal or heavy tailed so 2.5*MedAbsDev did not clip well\n",
        "MODELS_SENTS_CUSTOM_LS = [x for x in MODELS_LS if x not in MODELS_SENTS_EXCLUDE_LS] \n",
        "\n",
        "for model_name in MODELS_SENTS_CUSTOM_LS:\n",
        "  model_noouts = f'{model_name}_noouts'\n",
        "  print(f'Plotting {model_noouts}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_noouts, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment w/Trimmed Outliers Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJHXLvayIhOC"
      },
      "source": [
        "# Plot all Raw Paragraph Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_LS:\n",
        "  model_standout = f'{model_name}_standout'\n",
        "  print(f'Plotting {model_standout}')\n",
        "  sns.lineplot(data=corpus_parags_df, x='parag_no', y=model_standout, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Paragraph Sentiment Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNp9zwBfmgpI"
      },
      "source": [
        "## **Standardize Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ_wcgiyx-NJ"
      },
      "source": [
        "### **Before Standardizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEUbDF4hx-NN"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  model_noouts_str = f'{model_name}_noouts'\n",
        "  print(f'Plotting {model_noouts_str}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_noouts_str, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_noouts_str}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IrnbsMByTmX"
      },
      "source": [
        "### **Standardized the NoOutliers Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhkW7faSmPWh"
      },
      "source": [
        "# Standardize Sentence and Paragraphs Sentiment Time Series (Section TS was Standardized above)\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "# orig_cols_ls = list(set(corpus_all_df.columns) - set(['sent_no','parag_no','sent_raw','sent_clean']))\n",
        "# cols_ls = []\n",
        "\n",
        "# Sentences\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_sents_df[acol].values.reshape(-1,1)))\n",
        "    corpus_sents_df[acol_new] = pd.Series(temp_np.squeeze())\n",
        "\n",
        "# Paragraphs\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_parags_df[acol].values.reshape(-1,1)))\n",
        "    corpus_parags_df[acol_new] = pd.Series(temp_np.squeeze())\n",
        "\n",
        "# Paragraphs\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_sects_df[acol].values.reshape(-1,1)))\n",
        "    corpus_sects_df[acol_new] = pd.Series(temp_np.squeeze())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn6EWXYaybhj"
      },
      "source": [
        "### **After Standardizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcGzZNZJO7fd"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  model_standouts_str = f'{model_name}_standouts'\n",
        "  print(f'Plotting {model_standouts_str}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_standouts_str, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_standouts_str}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37_ztlEpzYHx"
      },
      "source": [
        "MODELS_LS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Be4bSszK-A"
      },
      "source": [
        "## **Deselect Poorly Behaved Sentiment Time Series Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY_HdLarzK-D"
      },
      "source": [
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "NCR_Arc = True #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6z5qYX3zm9n"
      },
      "source": [
        "# Create and Verify custom list of Models to include\n",
        "\n",
        "MODELS_CUSTOM_LS = []\n",
        "\n",
        "if VADER_Arc:\n",
        "  MODELS_CUSTOM_LS.append('vader')\n",
        "if TextBlob_Arc:\n",
        "  MODELS_CUSTOM_LS.append('textblob')\n",
        "if Stanza_Arc:\n",
        "  MODELS_CUSTOM_LS.append('stanza')\n",
        "if SentimentR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentimentr')\n",
        "if Syuzhet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('syuzhet')\n",
        "if AFINN_Arc:\n",
        "  MODELS_CUSTOM_LS.append('afinn')\n",
        "if Bing_Arc:\n",
        "  MODELS_CUSTOM_LS.append('bing')\n",
        "if Pattern_Arc:\n",
        "  MODELS_CUSTOM_LS.append('pattern')\n",
        "if SentiWord_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentiword')\n",
        "if SenticNet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('senticnet')\n",
        "if NCR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('nrc')\n",
        "\n",
        "print(f'Here are the Models we are using to ensemble and save:\\n   {MODELS_CUSTOM_LS}')\n",
        "\n",
        "models_incl_ls = []\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  models_incl_ls.append(amodel[:2])\n",
        "models_incl_str = ''.join(models_incl_ls)\n",
        "\n",
        "print(f'Here is a custom name abbr: {models_incl_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfiyOjMBBraW"
      },
      "source": [
        "## **Calculate Median of All (Trimmed Outliers then Standardized) Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1tX6BmT4U5h"
      },
      "source": [
        "# Create a list of models with Outliers trimmed and Standardized Time Series \n",
        "\n",
        "MODELS_CUSTOM_STANDOUTS_LS = []\n",
        "\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  model_standout_str = f'{amodel}_standouts'\n",
        "  MODELS_CUSTOM_STANDOUTS_LS.append(model_standout_str)\n",
        "\n",
        "print(f'List of NoOutliers/Standardized Models to compute Median on: {MODELS_CUSTOM_STANDOUTS_LS}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx3-Qug9Y6h0"
      },
      "source": [
        "# Disregard poorly behaved time series identified and stored in MODELS_SENTS_EXCLUDE_LS \n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\n",
        "corpus_sents_df['median_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_sents_df.head(2)\n",
        "\n",
        "corpus_sents_df['std_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_sents_df.head(2)\n",
        "\n",
        "corpus_sents_df['mean_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_sents_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2qunU4VfJ4J"
      },
      "source": [
        "# (For now) Paragraph Sentiment TS are better behaved and don't require excluding any Model\n",
        "\n",
        "corpus_parags_df['median_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_parags_df['std_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_parags_df['mean_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_parags_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtTY4t4k1QBc"
      },
      "source": [
        "# (For now) Paragraph Sentiment TS are better behaved and don't require excluding any Model\n",
        "\n",
        "corpus_sects_df['median_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_sects_df['std_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_sects_df['mean_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_parags_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp_gYLYbkjxd"
      },
      "source": [
        "## **Save Processed Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmyasWR1kjxh"
      },
      "source": [
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "corpus_sents_filename = f'corpus_sentences_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sentences to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "corpus_parags_filename = f'corpus_paragraphs_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Paragraphs to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "corpus_sects_filename = f'corpus_sections_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sections to file: {corpus_sects_filename}')\n",
        "corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "corpus_cruxes_filename = f'corpus_cruxes_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes to file: {corpus_cruxes_filename}')\n",
        "with open(corpus_cruxes_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUnbmOFckt2t"
      },
      "source": [
        "# **EDA Visualizations and Comparisons**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww1a9l8Lkjxk"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiHul8A1kjxn"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbY7D82jkjxp"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_sects_df.head(2)\n",
        "corpus_sects_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvvfzj7xkjxp"
      },
      "source": [
        "corpus_cruxes_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDiTp97_zlUm"
      },
      "source": [
        "# norm_cols_ls = ['distbertsst_norm', 'nlptown_norm','xlnet_sst5_norm','bert_imdb_norm', 'bertuc_googapps_norm', 'roberta_lg15_norm']\n",
        "\"\"\"\n",
        "\n",
        "# ARCHIVE\n",
        "\n",
        "cols_norm_ls = []\n",
        "cols_stand_ls = []\n",
        "\n",
        "for acol in corpus_all_df.columns:\n",
        "  if acol.endswith('_norm'):\n",
        "    print(f'Adding {acol} to norm_cols_ls')\n",
        "    cols_norm_ls.append(acol)\n",
        "  elif acol.endswith('_stand'):\n",
        "    print(f'Adding {acol} to stand_cols_ls')\n",
        "    cols_stand_ls.append(acol)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "print(f'\\nNormalized Columns: {cols_norm_ls}')\n",
        "\n",
        "print(f'\\nStandardized Columns: {cols_stand_ls}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O3l61be6_GQ"
      },
      "source": [
        "### **LOWESS Smoothed Single Plot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUjsXyRG81zT"
      },
      "source": [
        "**Normalized Sentiment Smoothed with LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5MYEwGuZ2vh"
      },
      "source": [
        "MODELS_STAND_LS = []\n",
        "for amodel in MODELS_LS:\n",
        "  MODELS_STAND_LS.append(f'{amodel}_stand')\n",
        "\n",
        "print(f'MODELS_STAND_LS: {MODELS_STAND_LS}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBT-Qd4o8919"
      },
      "source": [
        "**Standardized Sentiment Smoothed with LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ-Oe8icVpHn"
      },
      "source": [
        "# Plot and Compare all LOWESS Smoothed *Standardized* Sentiment Time Series\n",
        "\n",
        "corpus_lowess_stand_df = get_lowess(corpus_all_df, cols_stand_ls, plot_subtitle='Standardized', afrac=1./10, ait=5, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JizqEQE6X3oQ"
      },
      "source": [
        "### **High Level: Section View**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIwEIY4cd2PB"
      },
      "source": [
        "##### **Raw Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryWst950gpYx"
      },
      "source": [
        "# Plot Raw Section Sentiments\n",
        "\"\"\"\n",
        "\n",
        "# ARCHIVED\n",
        "\n",
        "for amodel in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  amodel_stand_str = f'{amodel}'\n",
        "  plot_raw_sentiments(model_name=amodel_stand_str, semantic_type='section', save2file=False)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx4mryQwVz7h"
      },
      "source": [
        "# Raw Standardized Section Sentiment Time Series\n",
        "\n",
        "_ = plot_stand_crux_sections(ts_df=corpus_sects_df, model_names_ls=MODELS_CUSTOM_STANDOUTS_LS, semantic_type='section', label_token_ct=5, title_xpos=0.5, title_ypos=1, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0HK0lbzjXXF"
      },
      "source": [
        "# Plot the Median of the Customized Set of NoOutliers/Standardized Section Sentiment Time Series\n",
        "\n",
        "corpus_sects_df['median_standouts_custom_lex'].plot()\n",
        "plt.title(f'{CORPUS_FULL}\\n Median of NoOutliers/Standardized Section Sentiment Time Series');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKz34IzZdxIg"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeOWF6frEm0S"
      },
      "source": [
        "Window_Width = 4 #@param {type:\"slider\", min:2, max:20, step:1}\n",
        "\n",
        "# DEFAULT 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1lngzR7XM5l"
      },
      "source": [
        "# SMA of Custom Set of NoOutliers/Standardized Section Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/Adjust the win_ls to explore different window_size by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  # print(f'Plotting {acol_name}')\n",
        "  get_smas(corpus_sects_df, model_name, text_unit='section', subtitle_str='(NOTE: different x-scale)', win_ls=[Window_Width])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5Qw6frQdz1C"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAuz19B5ntxk"
      },
      "source": [
        "# Standardized Section LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sects_df, models_ls=MODELS_STAND_LS, text_unit='section', afrac=1./4, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-u2ScQZD8Lv"
      },
      "source": [
        "LOWESS_fraction = 0.15 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 (approx 0.17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L82SbpnkO_3"
      },
      "source": [
        "# Standardized Median Section LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sects_df, models_ls=['median'], text_unit='section', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6Ipaa-lWVwP"
      },
      "source": [
        "### **Middle Level: Paragraph Views**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDYYpJYRfrZ-"
      },
      "source": [
        "##### **Raw Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxyJD-fjf4ev"
      },
      "source": [
        "# Plot Custom Set of NoOutliers/Standardized Paragraph Sentiments Time Series \n",
        "\n",
        "for amodel in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  plot_raw_sentiments(model_name=amodel, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwD9lNuJkCfo"
      },
      "source": [
        "# Plot the Median for the Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series\n",
        "\n",
        "corpus_parags_df['median_standouts_custom_lex'].plot()\n",
        "plt.title(f'{CORPUS_FULL}\\n Median of Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jii7hmhleyHo"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJn79SwJDKHt"
      },
      "source": [
        "Window_Width = 10 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "490cQGkgIBK2"
      },
      "source": [
        "# SMA Standardized Paragraph Sentiment Arcs\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots (5-20 defaults)\n",
        "\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  get_smas(corpus_parags_df, model_name, text_unit='paragraph', win_ls=[Window_Width])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDo7OHyHCtUJ"
      },
      "source": [
        "Window_Percentage = 5 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT: 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mahzCm5W9-QL"
      },
      "source": [
        "# SMA Plot the Median for the Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots\n",
        "\n",
        "win_percentage = Window_Percentage  # 5 means rolling window size is 5% of corpus length (5-20 default)\n",
        "\n",
        "win_size = int(corpus_parags_df.shape[0]*(win_percentage*0.01))\n",
        "\n",
        "plot_title = f'{CORPUS_FULL}\\n Median of Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series (win={win_percentage}%)'\n",
        "corpus_parags_df['median_standouts_custom_lex'].rolling(win_size).mean().plot(title=plot_title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2CIR3U9e4WC"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIzHfrCdCC9d"
      },
      "source": [
        "LOWESS_fraction = 0.17 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 (approx 0.17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2337wydGe4WC"
      },
      "source": [
        "# Standardized Paragraph LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots (0.10-0.20 default)\n",
        "\n",
        "_ = get_lowess(corpus_parags_df, models_ls=MODELS_CUSTOM_STANDOUTS_LS, text_unit='paragraph', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0emSLGZCh6-Y"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vq8s_aShoEJ"
      },
      "source": [
        "# Plot it\n",
        "\n",
        "_ = get_lowess(corpus_parags_df, models_ls=['median_standouts_custom_lex'], text_unit='paragraph', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzcDX02pZGbr"
      },
      "source": [
        "### **Low Level: Sentence Views**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV6spOW1fSKf"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE_-_JJ9FeVj"
      },
      "source": [
        "Window_Percentage = 10 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT: 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gYKbdptopin"
      },
      "source": [
        "# Plot all Standardized Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "models_stand_ls = []\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  get_smas(corpus_sents_df, model_name, text_unit='sentence', alpha=0.3, win_ls=[Window_Percentage])\n",
        "\n",
        "# print(f'models_stand_ls: {models_stand_ls}')\n",
        "corpus_sents_df['median_stand'] = corpus_sents_df[models_stand_ls].median()\n",
        "plt.plot(corpus_sents_df.sent_no, corpus_sents_df.median_stand, color='black', label='Median')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0fL5RXKfZG6"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-NGw8yEFnJD"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDlGY9CMfUj9"
      },
      "source": [
        "# Standardized Paragraph LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sents_df, models_ls=MODELS_CUSTOM_STANDOUTS_LS, text_unit='sentence', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9SQeAvgGFb6"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-GtGa5mlEhZ"
      },
      "source": [
        "# Median of Custom Set of NoOutlier/Standardized Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sents_df, models_ls=['median_standouts_custom_lex'], text_unit='paragraph', afrac=1./8, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1VhYgLai3RK"
      },
      "source": [
        "## **Save Standardized-NoOutliers Sentiment Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3zmDWROOEvM"
      },
      "source": [
        "# Save all Processed DataFrames\n",
        "\n",
        "# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "corpus_sents_filename = f'corpus_sentences_only_lexrules_standouts_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Paragraphs DataFrame\n",
        "corpus_parags_filename = f'corpus_paragraphs_only_lexrules_standouts_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Section DataFrame\n",
        "corpus_sects_filename = f'corpus_sections_only_lexrules_standouts_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_sects_filename}')\n",
        "corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "\n",
        "# Save Cruxes\n",
        "corpus_cruxes_filename = f'corpus_cruxes_lexrules_{models_incl_str}_{author_abbr_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes to file: {corpus_cruxes_filename}')\n",
        "with open(corpus_cruxes_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vUzh39HHJXz"
      },
      "source": [
        "# **Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJI_13B0HMnU"
      },
      "source": [
        "## **Sentiment Stability**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R6SMWtSHJLK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9sEH8_IG6Qq"
      },
      "source": [
        "## **Crux Point Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRjKHnQaOTu2"
      },
      "source": [
        "### **Gather (n) Highest/Lowest Sentiment Values for Each Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG6Y0vPFOkzn"
      },
      "source": [
        "def getn_cruxes(crux_dt, model_name='vader', get_n=6):\n",
        "  '''\n",
        "  Given a Crux Dictionary, a Model item/dict within, and and integer n\n",
        "  Return the n highest and n lowest sentiment values\n",
        "  NOTE: if get_n == 0, return all Crux Points for all Models\n",
        "  '''\n",
        "\n",
        "  cruxes_all_df = pd.DataFrame\n",
        "  cruxes_n_top_df = pd.DataFrame()\n",
        "\n",
        "  cruxes_all_df = pd.DataFrame.from_dict(crux_dt[model_name])\n",
        "\n",
        "  cruxes_all_df = cruxes_all_df.transpose().reset_index().rename(columns={'index':'var'})\n",
        "\n",
        "  cruxes_all_df.rename(columns={'var':'sent_no',0:model_name,1:'sent_raw'}, inplace=True)\n",
        "  cruxes_all_df.drop(columns=['sent_raw'], inplace=True)\n",
        "  cruxes_all_df.rename(columns={model_name:'sentiment'}, inplace=True)\n",
        "  cruxes_all_df['sentiment'] = cruxes_all_df['sentiment'].astype('float')\n",
        "  cruxes_all_df['model_name'] = model_name\n",
        "  cruxes_all_df = cruxes_all_df[['sent_no','model_name','sentiment']]\n",
        "\n",
        "  if get_n > 0:\n",
        "    cruxes_n_top_df = cruxes_all_df.nlargest(get_n, 'sentiment')\n",
        "    cruxes_n_top_df = cruxes_n_top_df.append(cruxes_all_df.nsmallest(get_n, 'sentiment'))\n",
        "  elif get_n ==0:\n",
        "    cruxes_n_top_df = cruxes_all_df\n",
        "  else:\n",
        "    print(f'ERROR: argument get_n must be either 0 (return all Cruxes) or greater than 0')\n",
        "    \n",
        "  return cruxes_n_top_df\n",
        "\n",
        "# Test\n",
        "\n",
        "cruxes_n_top_df = getn_cruxes(corpus_cruxes_dt, model_name='vader', get_n=3)\n",
        "cruxes_n_top_df.head(6)\n",
        "cruxes_n_top_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H67tOfiaSaHB"
      },
      "source": [
        "# Acculumate all the Crux Points from All Models into one cruxes_n_top_all_df DataFrame\n",
        "\n",
        "cruxes_n_top_all_df = pd.DataFrame()\n",
        "\n",
        "for amodel in MODELS_LS:\n",
        "  print(f'Appending Cruxes from {amodel}')\n",
        "  cruxes_n_top_df = getn_cruxes(corpus_cruxes_dt, model_name=amodel, get_n=12)\n",
        "  cruxes_n_top_all_df = cruxes_n_top_all_df.append(cruxes_n_top_df, ignore_index=True)\n",
        "\n",
        "for amodel in MODELS_LS:\n",
        "  crux_ct = len(cruxes_n_top_all_df[cruxes_n_top_all_df['model_name'] == amodel])\n",
        "  print(f'{amodel.capitalize()} has {crux_ct} Cruxes')\n",
        "\n",
        "print(f'There are a total of {cruxes_n_top_all_df.shape[0]} in all Models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQDWi6YLYVn0"
      },
      "source": [
        "# Plot Crux Points in 2D Space: Scatterplot\n",
        "\n",
        "sns.lmplot('sent_no', 'sentiment', data=cruxes_n_top_all_df, fit_reg=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab2oHmU1Yqcl"
      },
      "source": [
        "# Plot Crux Points in 1D Space: Histogram\n",
        "\n",
        "sns.distplot(cruxes_n_top_all_df.sent_no, bins=2000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtdL2OjAJm55"
      },
      "source": [
        "sns.clustermap(cruxes_n_top_all_df.sent_no)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiTZC_X4G6EC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE8EJC1wHCEn"
      },
      "source": [
        "## **Sentiment Arc Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFKeAFHlG-vm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TfBnxjYG5_u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wjBEXGyvU4x"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRo_Tt35VW8"
      },
      "source": [
        "**Bi/Tri-Polarity Lexicons**\n",
        "\n",
        "NRC: \n",
        "* https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
        "\n",
        "MPQA (Upitt):\n",
        "* https://mpqa.cs.pitt.edu/lexicons/effect_lexicon/\n",
        "* https://github.com/nlpcl-lab/mpqa2.0-preprocessing\n",
        "* https://github.com/kvangundy/basic-sentiment-analyzer/blob/master/sentimentDict.csv\n",
        "\n",
        "SentimentAnalysis.R (20210217 124s):\n",
        "* QDAP \n",
        "* DictionaryGI: Harvard-IV dictionary asused in the General Inquirer software (2005-/1637+)\n",
        "* DictionaryLM: Loughran-McDonald Financial dictionary (2355-/354+/297?)\n",
        "* DictionaryHE: Henry's Financial Dictionary (85-/105+)\n",
        "\n",
        "Custom Lexicons:\n",
        "* https://nealcaren.org/lessons/wordlists/ \n",
        "\n",
        "Lexicons\n",
        "* https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsOKzM-RYHf5"
      },
      "source": [
        "# Smooth Raw Sentiment Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWKU0OzT_9pe"
      },
      "source": [
        "**Simple Moving Average (SMA) by Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGluyBsFkQ7C"
      },
      "source": [
        "# Line Plots of Sentiment Values\n",
        "\n",
        "# set a grey background (use sns.set_theme() if seaborn version 0.11.0 or above) \n",
        "sns.set(style=\"darkgrid\")\n",
        "# df = sns.load_dataset(\"iris\")\n",
        "\n",
        "fig, axs = plt.subplots(4, 2, figsize=(12, 18))\n",
        "\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"median\", color=\"skyblue\", ax=axs[0, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"vader_mean_roll050\", color=\"skyblue\", ax=axs[0, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"sentimentr_mean_roll050\", color=\"olive\", ax=axs[1, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"syuzhet_mean_roll050\", color=\"olive\", ax=axs[1, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"bing_mean_roll050\", color=\"gold\", ax=axs[2, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"textblob_mean_roll050\", color=\"gold\", ax=axs[2, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"sentiword_mean_roll050\", color=\"teal\", ax=axs[3, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"senticnet_mean_roll050\", color=\"teal\", ax=axs[3, 1])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySuYAv2YxCO0"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iX1nCK6ljY8"
      },
      "source": [
        "%whos dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC3r9eWywyXB"
      },
      "source": [
        "\n",
        "# g=sns.pointplot(x=0, y=1, data=df, dodge=True,plot_kws=dict(alpha=0.3))\n",
        "# plt.setp(g.collections, alpha=.3) #for the markers\n",
        "# plt.setp(g.lines, alpha=.3)       #for the lines\n",
        "\n",
        "for i, sa_model in enumerate(corpus_sents_df.columns):\n",
        "  if (sa_model.endswith('_roll050')):\n",
        "    # if (sa_model != 'sent_no'):\n",
        "    if (sa_model == 'median'):\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, color='black')\n",
        "    else:\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, alpha=0.3)\n",
        "\n",
        "# print(f'{i}: {sa_model}')\n",
        "\n",
        "'''\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='median')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='textb')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTKI4Fu2ACKP"
      },
      "source": [
        "**Exponential Moving Average**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvVMrD58AL97"
      },
      "source": [
        "# Not Necessary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wEab5F8_3Wl"
      },
      "source": [
        "**LOWESS and LOESS Smoothing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPOo6VAMmWh_"
      },
      "source": [
        "sa_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlPwfjS8mKpY"
      },
      "source": [
        "def plot_lowess(df, df_cols_ls, aplot=True, afrac=1./10, ait=5):\n",
        "  '''\n",
        "  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n",
        "  Return a DataFrame with LOWESS values\n",
        "  If 'plot=True', also output plot\n",
        "  '''\n",
        "\n",
        "  # global corpus_sents_norm_df\n",
        "\n",
        "  lowess_df = pd.DataFrame()\n",
        "\n",
        "  for i,acol in enumerate(df_cols_ls):\n",
        "    sm_x, sm_y = sm_lowess(endog=df[acol].values, exog=df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n",
        "    col_new = f'{acol}_lowess'\n",
        "    lowess_df[col_new] = pd.Series(sm_x)\n",
        "    if aplot:\n",
        "      plt.plot(sm_x, sm_y, label=acol, alpha=0.5, linewidth=2)\n",
        "\n",
        "      frac_str = str(round(100*afrac))\n",
        "      plt.title(f'{CORPUS_FULL} \\n LOWESS (frac={frac_str} Sentence Sentiment (Model: {sa_model})')\n",
        "      plt.legend(title='Sentiment Series')\n",
        "\n",
        "  return lowess_df\n",
        "\n",
        "# Test\n",
        "new_lowess_col = f'{sa_model}_lowess'\n",
        "my_frac = 1./10\n",
        "my_frac_per = round(100*my_frac)\n",
        "new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n",
        "corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], afrac=my_frac)\n",
        "corpus_sents_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qRUhRfmmyUA"
      },
      "source": [
        "corpus_sents_norm_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxbZdPVbnL6w"
      },
      "source": [
        "corpus_sents_norm_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25CDCoDJmTcI"
      },
      "source": [
        "norm_cols_ls = []\n",
        "for acol in corpus_sents_norm_df.columns:\n",
        "  if acol.endswith('_2norm'):\n",
        "    norm_cols_ls.append(acol)\n",
        "\n",
        "print(f'All norm_cols_ls')\n",
        "\n",
        "temp_cols_ls = list(set(norm_cols_ls) - set(['stanza_2norm','afinn_2norm']))\n",
        "\n",
        "print(f'Trimmed temp_cols_ls:')\n",
        "print(temp_cols_ls)\n",
        "\n",
        "\n",
        "plot_lowess(corpus_sents_norm_df, temp_cols_ls, 'Normed')\n",
        "\n",
        "'''\n",
        "for i, sa_model in enumerate(corpus_sents_df.columns):\n",
        "  if (sa_model.endswith('_roll050')):\n",
        "    # if (sa_model != 'sent_no'):\n",
        "    if (sa_model == 'median'):\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, color='black')\n",
        "    else:\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, alpha=0.3)\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7hs9FIxpugn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq8fWCKCpuaW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvKW23iwAGnk"
      },
      "source": [
        "%time\n",
        "\n",
        "plt.title(f'{BOOK_TITLE_FULL} \\n {sa_model} with statsmodels LOWESS (frac=0.1/iter=5)')\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['median'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "plt.plot(sm_x, sm_y, label='Median', color='black', linewidth=3)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['vader_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='VADER', color='royalblue', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['jockers_rinker_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='Jockers-Rinker', color='red', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['syuzhet_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='Syuzhet', color='tomato', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['huliu_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='HuLiu', color='teal', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['textblob_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='TextBlob', color='lime', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['sentiword_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='SentiWord', color='goldenrod', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['senticnet_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='SenticNet', color='forestgreen', alpha=0.5)\n",
        "\n",
        "# y_upper = corpus_sentiments_df['norm_score'].max() + 0.01\n",
        "# y_lower = corpus_sentiments_df['norm_score'].min() - 0.01\n",
        "# y_range = y_upper - y_lower + 0.02\n",
        "# plt.ylim([0.71, 0.74])\n",
        "# plt.ylim([y_lower, y_upper])\n",
        "# plt.plot(x, y, 'k.');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iORvQhmNFrw"
      },
      "source": [
        "**Discrete Cosine Transform (DCT)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917t5E9t_Pez"
      },
      "source": [
        "Libraries:\n",
        "\n",
        "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html#scipy.fft.dct\n",
        "* https://github.com/search?q=discrete+cosine \n",
        "\n",
        "Tutorial\n",
        "\n",
        "* https://realpython.com/python-scipy-fft/#the-discrete-cosine-and-sine-transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI-UtGpXwsJ6"
      },
      "source": [
        "from sktime.transformations.series.cos import CosineTransformer\n",
        "from sktime.datasets import load_airline\n",
        "y = load_airline()\n",
        "transformer = CosineTransformer()\n",
        "y_hat = transformer.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HUsqY_c3wsGA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YfHqTtxwsC4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKYGvaNxwr_m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noayFmY5v6Wc"
      },
      "source": [
        "!pip install pyts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-DLnLsjVWB-"
      },
      "source": [
        "**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n",
        "\n",
        "* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n",
        "* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n",
        "* http://futuredata.stanford.edu/asap/ \n",
        "* https://www.datadoghq.com/blog/auto-smoother-asap/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsz35wSyWlvA"
      },
      "source": [
        "import scipy.stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLFb9ZitVV3E"
      },
      "source": [
        "# ASAP Simple (Brute Force)\n",
        "def moving_average(data, _range):\n",
        "    ret = np.cumsum(data, dtype=float)\n",
        "    ret[_range:] = ret[_range:] - ret[:-_range]\n",
        "    return ret[_range - 1:] / _range\n",
        "\n",
        "def SMA(data, _range, slide):\n",
        "    ret = moving_average(data, _range)[::slide]\n",
        "    return list(ret)\n",
        "\n",
        "def kurtosis(values):\n",
        "    return scipy.stats.kurtosis(values)\n",
        "\n",
        "def roughness(vals):\n",
        "    return np.std(np.diff(vals))\n",
        "\n",
        "def smooth_simple(data, max_window=5, resolution=None):\n",
        "    data = np.array(data)\n",
        "    # Preaggregate according to resolution\n",
        "    window_size = 1\n",
        "    slide_size = 1\n",
        "    if resolution:\n",
        "        slide_size = int(len(data) / resolution)\n",
        "        if slide_size > 1:\n",
        "            data = SMA(data, slide_size, slide_size)\n",
        "    orig_kurt   = kurtosis(data)\n",
        "    min_obj     = roughness(data)\n",
        "    for w in range(2, int(len(data) / max_window + 1)):\n",
        "        smoothed = SMA(data, w, 1)\n",
        "        if kurtosis(smoothed) >= orig_kurt:\n",
        "            r = roughness(smoothed)\n",
        "            if r < min_obj:\n",
        "                min_obj = r\n",
        "                window_size = w\n",
        "    return window_size, slide_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHdCbShgVVrR"
      },
      "source": [
        "# Plot time series before and after smoothing\n",
        "def plot(data, window_size, slide_size, plot_title):\n",
        "    plt.clf()\n",
        "    plt.figure()\n",
        "    data = SMA(data, slide_size, slide_size)\n",
        "    method_names = [\"Original\", \"ASAP Smoothed\"]\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
        "    smoothed = SMA(data, window_size, 1)\n",
        "    smoothed_range = range(int(window_size/2), int(window_size/2) + len(smoothed))\n",
        "    ax1.set_xlim(0, len(data))\n",
        "    ax1.plot(data, linestyle='-', linewidth=1.5)\n",
        "    ax2.plot(smoothed_range, smoothed, linestyle='-', linewidth=1.5)\n",
        "    axes = [ax1, ax2]\n",
        "    for i in range(2):\n",
        "        axes[i].get_xaxis().set_visible(False)\n",
        "        axes[i].text(0.02, 0.8, \"%s\" %(method_names[i]),\n",
        "            verticalalignment='center', horizontalalignment='left',\n",
        "            transform=axes[i].transAxes, fontsize=25)\n",
        "\n",
        "    fig.set_size_inches(16, 12)\n",
        "    plt.tight_layout(w_pad=1)\n",
        "    plt.title(plot_title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpSktA4gWV3J"
      },
      "source": [
        "corpus_sentiments_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "710ghW5qVVoF"
      },
      "source": [
        "# Taxi\n",
        "# raw_data = load_csv('Taxi.csv')\n",
        "# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n",
        "# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n",
        "window_size, slide_size = smooth_simple(list(corpus_sentiments_df['median']), resolution=1000)\n",
        "print(\"Window Size: \", window_size)\n",
        "plot_title = f'{BOOK_TITLE_FULL} \\n Median Sentiment Smoothed with ASAP from Stanford (res=1000)'\n",
        "plot(list(corpus_sentiments_df['median']), window_size, slide_size, plot_title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wkkx80YYLCg"
      },
      "source": [
        "# Calculate Error Metrics based on Distance from Median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0K9P4jA_bDj"
      },
      "source": [
        "Libraries:\n",
        "\n",
        "* https://github.com/wannesm/dtaidistance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmSSemdcLQ9x"
      },
      "source": [
        "# sentiment_lowess_df['median']\n",
        "sentiment_lowess_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tvyv-LtJiqJ"
      },
      "source": [
        "# Rank each Sentiment Model by Error/Distance Metrics from Median\n",
        "\n",
        "sentiment_lowess_df['min'] = sentiment_lowess_df[['vader','jockers-rinker','syuzhet','huliu','textblob','sentiword','senticnet']].min(axis=1)\n",
        "sentiment_lowess_df['max'] = sentiment_lowess_df[['vader','jockers-rinker','syuzhet','huliu','textblob','sentiword','senticnet']].max(axis=1)\n",
        "sentiment_lowess_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18y52tS9JTeO"
      },
      "source": [
        "# LOWESS Smoothed Median Curve with Min/Max Confidence Intervals\n",
        "\n",
        "plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Min/Max Confidence Intervals')\n",
        "\n",
        "sns.lineplot(data=sentiment_lowess_df, x='x_value', y='median', linewidth=3, color='black')\n",
        "plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cQqCyABTkzT"
      },
      "source": [
        "# Error Metrics of each Model relative to the Median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BBTjp7nTkhK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHVeb7Jy9zaa"
      },
      "source": [
        "# Group and Classify Sentiment Arcs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xeV-709-_M2"
      },
      "source": [
        "Libraries\n",
        "\n",
        "* https://github.com/alan-turing-institute/sktime\n",
        "\n",
        "* https://github.com/johannfaouzi/pyts \n",
        "\n",
        "Code\n",
        "\n",
        "* https://colab.research.google.com/drive/1oEFfK5KTJyFQGs2xunc1cDW2OcbkZKCY\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFEtVAt29zMP"
      },
      "source": [
        "# https://colab.research.google.com/drive/1oEFfK5KTJyFQGs2xunc1cDW2OcbkZKCY\n",
        "\n",
        "# https://github.com/johannfaouzi/pyts \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "804w9PYAYP_p"
      },
      "source": [
        "# Export Manual and Automatic Sentiment Polarities and Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wnHGCQnO9aA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}