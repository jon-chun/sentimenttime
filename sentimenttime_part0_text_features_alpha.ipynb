{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sentimenttime_part0_text_features_alpha.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNnMOrWWZ6JP2Sm4Ecod4jN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jon-chun/sentimenttime/blob/main/sentimenttime_part0_text_features_alpha.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANyfZ3ypflJQ"
      },
      "source": [
        "# **DTW Clustering with dtaidistance**\n",
        "\n",
        "* https://github.com/wannesm/dtaidistance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYHqJ5Mzo7wI"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3aFFUqcxPvY"
      },
      "source": [
        "# !git clone https://github.com/alan-turing-institute/sktime.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHwrwVLoxqrZ"
      },
      "source": [
        "# %cd sktime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDwkN22hxzhL"
      },
      "source": [
        "# !pip install --editable ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zyaHPkSwKdC"
      },
      "source": [
        "# Missing Transformers\n",
        "\n",
        "!pip install sktime[all_extras]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei9M_yaOvqA3"
      },
      "source": [
        "!pip install tsfresh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JMa5kwUsb7n"
      },
      "source": [
        "!pip install dtaidistance[all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE_sRmbff9ss"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al4u0YJZhvS3"
      },
      "source": [
        "import random\n",
        "import array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1NBN-UxgA3o"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPPMlaqVJHfH"
      },
      "source": [
        "pd.set_option('display.width',1000)\n",
        "pd.set_option('max_colwidth', 1000) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaqYEZfrJOJg"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7eEO4t7gZs2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzsWh1uYJk_I"
      },
      "source": [
        "stopwords_st = set(stopwords.words('english'))\n",
        "stopwords_st.discard('not')\n",
        "stopwords_st.discard('no')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09sVK1QXgPDT"
      },
      "source": [
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41y-ocEfRmn-"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyltcFqD5XcV"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (30,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmHosz9b17U-"
      },
      "source": [
        "# **Read Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CojWyOCM19QR"
      },
      "source": [
        "# Connect to Google gDrive\n",
        "\n",
        "# Flag to indicate first run through code \n",
        "flag_first_run = True\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw1QfXVK2ECa"
      },
      "source": [
        "gdrive_subdir = \"./research/2021/sa_book_code/books_sa/cdickens_greatexpectations\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW-a610X2D_B"
      },
      "source": [
        "flag_first_run = True\n",
        "\n",
        "CORPUS_FULL = 'Great Expectations by Charles Dickens (1861)'\n",
        "CORPUS_SUBDIR = gdrive_subdir\n",
        "corpus_filename = CORPUS_SUBDIR\n",
        "\n",
        "# Change to working subdirectory\n",
        "if flag_first_run == True:\n",
        "  full_path_str = gdrive_subdir\n",
        "  flag_first_run = False\n",
        "else:\n",
        "  full_path_str = f'/gdrive/MyDrive{gdrive_subdir[1:]}'\n",
        "\n",
        "%cd $full_path_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVUiL7OA2D79"
      },
      "source": [
        "!ls -altr *.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn0KNRhuREQM"
      },
      "source": [
        "corpus_sents_df = pd.read_csv('corpus_text_sents_raw_cdickens_greatexpectations.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDeHO6ipSF3r"
      },
      "source": [
        "corpus_sents_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0QDqtqhwVvv"
      },
      "source": [
        "corpus_unified_df = pd.read_csv('sum_sentiments_all31_sents_cdickens_cdickens_greatexpectations.csv')\n",
        "corpus_unified_df.drop(columns=['Unnamed: 0'],inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buTP6-jewVpp"
      },
      "source": [
        "corpus_unified_df.head(2)\n",
        "corpus_unified_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpFNz_8RR39s"
      },
      "source": [
        "# **Descriptive Statistics: Linguistic**\n",
        "\n",
        "* https://github.com/Perevalov/LinguaF\n",
        "* https://github.com/LSYS/LexicalRichness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRCUub92gd8l"
      },
      "source": [
        "!pip install pip install linguaf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTApLiKJNK6e"
      },
      "source": [
        "### **General Counts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFOrcVuOgo2I"
      },
      "source": [
        "!pip install lexicalrichness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSZrKdeCRLaw"
      },
      "source": [
        "corpus_sents_df.head()\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk8F1kIGSMov"
      },
      "source": [
        "corpus_sents_df['len_char'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNAI1y3fXDOG"
      },
      "source": [
        "sum_stat_str = str(corpus_sents_df['len_char'].describe())\n",
        "print(sum_stat_str)\n",
        "print('\\n')\n",
        "stat_len_char_str = '\\n'.join(x.split('.')[0] for x in sum_stat_str.split('\\n')[:-1])\n",
        "print(stat_len_char_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwGBZSOvSw9f"
      },
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "sns.kdeplot(data=corpus_sents_df, x=\"len_char\", ax=ax1)\n",
        "ax1.set_xlim((corpus_sents_df[\"sent_no\"].min(), int(corpus_sents_df[\"sent_no\"].max()/8)))\n",
        "ax2 = ax1.twinx()\n",
        "sns.histplot(data=corpus_sents_df, x=\"len_char\", discrete=True, ax=ax2)\n",
        "ax2.set_title(f'{CORPUS_FULL}\\nSentence Length (chars) Histogram')\n",
        "ax2.text(.9, 0.8,f'Descriptive Statistics:\\n-----------------------\\n {stat_len_char_str}', fontsize=12, ha='center', va='center', transform = ax2.transAxes);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I49Ssmq1SMnr"
      },
      "source": [
        "corpus_sents_df['len_word'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x.split()))\n",
        "sum_stat_str = str(corpus_sents_df['len_word'].describe())\n",
        "print(sum_stat_str)\n",
        "print('\\n')\n",
        "stat_len_word_str = '\\n'.join(x.split('.')[0] for x in sum_stat_str.split('\\n')[:-1])\n",
        "print(stat_len_word_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbtBRPV0SxSc"
      },
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "sns.kdeplot(data=corpus_sents_df, x=\"len_char\", ax=ax1)\n",
        "ax1.set_xlim((corpus_sents_df[\"sent_no\"].min(), int(corpus_sents_df[\"sent_no\"].max()/8)))\n",
        "ax2 = ax1.twinx()\n",
        "sns.histplot(data=corpus_sents_df, x=\"len_char\", discrete=True, ax=ax2)\n",
        "ax2.set_title(f'{CORPUS_FULL}\\nSentence Length (words) Histogram')\n",
        "ax2.text(.9, 0.8,f'Descriptive Statistics:\\n-----------------------\\n {stat_len_word_str}', fontsize=12, ha='center', va='center', transform = ax2.transAxes);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aHAjYYqSABk"
      },
      "source": [
        "corpus_sents_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvGLDlOQNWn1"
      },
      "source": [
        "### **Library: LexicalRichness**\n",
        "\n",
        "* https://github.com/LSYS/LexicalRichness (20210815 26s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_rwD-qOSfMW"
      },
      "source": [
        "from lexicalrichness import LexicalRichness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFBlWxsjSfIZ"
      },
      "source": [
        "corpus_raw_str = ' '.join(corpus_sents_df['sent_raw'])\n",
        "corpus_raw_str[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz8fCoYvhCUM"
      },
      "source": [
        "lex = LexicalRichness(corpus_raw_str)\n",
        "\n",
        "# Return word count.\n",
        "corpus_word_ct = lex.words\n",
        "print(f'corpus_word_ct: {corpus_word_ct}')\n",
        "\n",
        "# Return (unique) word count.\n",
        "corpus_unique_word_cd = lex.terms\n",
        "print(f'corpus_unique_word_cd: {corpus_unique_word_cd}')\n",
        "\n",
        "# Return type-token ratio (TTR) of text.\n",
        "corpus_ttr = lex.ttr\n",
        "print(f'corpus_ttr: {corpus_ttr}')\n",
        "\n",
        "# Return root type-token ratio (RTTR) of text.\n",
        "corpus_rttr = lex.rttr\n",
        "print(f'corpus_rttr: {corpus_rttr}')\n",
        "\n",
        "# Return corrected type-token ratio (CTTR) of text.\n",
        "corpus_rttr = lex.cttr\n",
        "print(f'corpus_rttr: {corpus_rttr}')\n",
        "\n",
        "# Return mean segmental type-token ratio (MSTTR).\n",
        "corpus_msttr = lex.msttr(segment_window=25)\n",
        "print(f'corpus_msttr: {corpus_msttr}')\n",
        "\n",
        "# Return moving average type-token ratio (MATTR).\n",
        "corpus_mattr = lex.mattr(window_size=25)\n",
        "print(f'corpus_mattr: {corpus_mattr}')\n",
        "\n",
        "# Return Measure of Textual Lexical Diversity (MTLD).\n",
        "corpus_mtld = lex.mtld(threshold=0.72)\n",
        "print(f'corpus_mtld: {corpus_mtld}')\n",
        "\n",
        "# Return hypergeometric distribution diversity (HD-D) measure.\n",
        "corpus_hdd = lex.hdd(draws=42)\n",
        "print(f'corpus_hdd: {corpus_hdd}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brQG4CMaNaGP"
      },
      "source": [
        "### **Library: LinguaF**\n",
        "\n",
        "* https://github.com/Perevalov/LinguaF (20210614 2s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7cRZ7othCRJ"
      },
      "source": [
        "from linguaf import descriptive_statistics as ds\n",
        "\n",
        "documents = list(corpus_sents_df['sent_raw'])\n",
        "\n",
        "corpus_char_ct = ds.char_count(documents)\n",
        "print(f'corpus_char_ct: {corpus_char_ct}')\n",
        "\n",
        "corpus_letter_ct = ds.letter_count(documents)\n",
        "print(f'corpus_letter_ct: {corpus_letter_ct}')\n",
        "\n",
        "corpus_punct_ct = ds.punctuation_count(documents)\n",
        "print(f'corpus_punct_ct: {corpus_punct_ct}')\n",
        "\n",
        "corpus_digit_ct = ds.digit_count(documents)\n",
        "print(f'corpus_digit_ct: {corpus_digit_ct}')\n",
        "\n",
        "corpus_syllable_ct = ds.syllable_count(documents)\n",
        "print(f'corpus_syllable_ct: {corpus_syllable_ct}')\n",
        "\n",
        "corpus_sent_ct = len(documents)\n",
        "print(f'corpus_sent_ct: {corpus_sent_ct}')\n",
        "\n",
        "corpus_avg_syllable_word = ds.avg_syllable_per_word(documents)\n",
        "print(f'corpus_avg_syllable_word: {corpus_avg_syllable_word}')\n",
        "\n",
        "corpus_avg_word_len = ds.avg_word_length(documents)\n",
        "print(f'corpus_avg_word_len: {corpus_avg_word_len}')\n",
        "\n",
        "corpus_avg_sent_len = ds.avg_sentence_length(documents)\n",
        "print(f'corpus_avg_sent_len: {corpus_avg_sent_len}')\n",
        "\n",
        "corpus_avg_word_sent = ds.avg_words_per_sentence(documents)\n",
        "print(f'corpus_avg_word_sent: {corpus_avg_word_sent}')\n",
        "\n",
        "corpus_avg_syllable_ct = ds.avg_syllable_per_word(documents)\n",
        "print(f'corpus_avg_syllable_ct: {corpus_avg_syllable_ct}')\n",
        "\n",
        "\"\"\"\n",
        "    Number of characters char_count\n",
        "    Number of letters letter_count\n",
        "    Number of punctuation characters punctuation_count\n",
        "    Number of digits digit_count\n",
        "    Number of syllables syllable_count\n",
        "    Number of sentences sentence_count\n",
        "    Number of n-syllable words number_of_n_syllable_words\n",
        "    Average syllables per word avg_syllable_per_word\n",
        "    Average word length avg_word_length\n",
        "    Average sentence length avg_sentence_length\n",
        "    Average words per sentence avg_words_per_sentence\n",
        "\n",
        "Additional methods:\n",
        "\n",
        "    Get lexical items (nouns, adjectives, verbs, adverbs) get_lexical_items\n",
        "    Get n-grams get_ngrams\n",
        "    Get sentences get_sentences\n",
        "    Get words get_words\n",
        "    Tokenize tokenize\n",
        "    Remove punctuation remove_punctuation\n",
        "    Remove digits remove_digits\n",
        "\"\"\";\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgDmKV-EhCOG"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 7987s (>>1hr)\n",
        "\n",
        "from linguaf import syntactical_complexity as sc\n",
        "\n",
        "corpus_mdd_complexity = sc.mean_dependency_distance(documents)\n",
        "print(f'corpus_mdd_complexity: {corpus_mdd_complexity}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtV7y4-IhCLS"
      },
      "source": [
        "%time\n",
        "\n",
        "# NOTE: 23s\n",
        "\n",
        "from linguaf import lexical_diversity as ld\n",
        "\n",
        "corpus_lexical_density = ld.lexical_density(documents)\n",
        "print(f'corpus_lexical_density: {corpus_lexical_density}')\n",
        "\n",
        "corpus_type_token_ratio = ld.type_token_ratio(documents)\n",
        "print(f'corpus_type_token_ratio: {corpus_type_token_ratio}')\n",
        "\n",
        "corpus_log_type_token_ratio = ld.log_type_token_ratio(documents)\n",
        "print(f'corpus_log_type_token_ratio: {corpus_log_type_token_ratio}')\n",
        "\n",
        "corpus_summer_index = ld.summer_index(documents)\n",
        "print(f'corpus_summer_index: {corpus_summer_index}')\n",
        "\n",
        "corpus_root_type_token_ratio = ld.root_type_token_ratio(documents)\n",
        "print(f'corpus_root_type_token_ratio: {corpus_root_type_token_ratio}')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    Lexical Density (LD) lexical_density\n",
        "    Type Token Ratio (TTR) type_token_ratio\n",
        "    Herdan's Constant or Log Type Token Ratio (LogTTR) log_type_token_ratio\n",
        "    Summer's Index summer_index\n",
        "    Root Type Token Ratio (RootTTR) root_type_token_ratio\n",
        "\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLiwqBSWhCIe"
      },
      "source": [
        "from linguaf import readability as r\n",
        "\n",
        "# NOTE: 74s\n",
        "\n",
        "corpus_flesch_reading_ease = r.flesch_reading_ease(documents)\n",
        "print(f'corpus_flesch_reading_ease: {corpus_flesch_reading_ease}')\n",
        "\n",
        "corpus_read_flesch = r.flesch_kincaid_grade(documents)\n",
        "print(f'corpus_read_flesch: {corpus_read_flesch}')\n",
        "\n",
        "corpus_automated_readability_index = r.automated_readability_index(documents)\n",
        "print(f'corpus_automated_readability_index: {corpus_automated_readability_index}')\n",
        "\n",
        "corpus_automated_readability_index_simple = r.automated_readability_index_simple(documents)\n",
        "print(f'corpus_automated_readability_index_simple: {corpus_automated_readability_index_simple}')\n",
        "\n",
        "corpus_coleman_readability = r.coleman_readability(documents)\n",
        "print(f'corpus_coleman_readability: {corpus_coleman_readability}')\n",
        "\n",
        "corpus_easy_listening = r.easy_listening(documents)\n",
        "print(f'corpus_easy_listening: {corpus_easy_listening}')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    Flesch Reading Ease (FRE) flesch_reading_ease\n",
        "    Flesch-Kincaid Grade (FKG) flesch_kincaid_grade\n",
        "    Automated Readability Index (ARI) automated_readability_index\n",
        "    Simple Automated Readability Index (sARI) automated_readability_index_simple\n",
        "    Coleman's Readability Score coleman_readability\n",
        "    Easy Listening Score easy_listening\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVFy-cGINBJx"
      },
      "source": [
        "### **Library: textstat**\n",
        "\n",
        "* https://github.com/shivam5992/textstat (20210816 689s) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbxuGPLEhCF0"
      },
      "source": [
        "!pip install textstat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbzprDDPhCCw"
      },
      "source": [
        "import textstat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_U-5zViLjCH"
      },
      "source": [
        "test_data = (\n",
        "    \"Playing games has always been thought to be important to \"\n",
        "    \"the development of well-balanced and creative children; \"\n",
        "    \"however, what part, if any, they should play in the lives \"\n",
        "    \"of adults has never been researched that deeply. I believe \"\n",
        "    \"that playing games is every bit as important for adults \"\n",
        "    \"as for children. Not only is taking time out to play games \"\n",
        "    \"with our children and other adults valuable to building \"\n",
        "    \"interpersonal relationships but is also a wonderful way \"\n",
        "    \"to release built up tension.\"\n",
        ")\n",
        "\n",
        "type(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpjEHu0lt50f"
      },
      "source": [
        "test_data[:75]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prBBsOifuKoX"
      },
      "source": [
        "corpus_raw_str[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q6GKaSsLi-m"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: \n",
        "\n",
        "test_data = corpus_raw_str\n",
        "\n",
        "corpus_flesch_reading_ease = textstat.flesch_reading_ease(test_data)\n",
        "print(f'corpus_flesch_reading_ease: {corpus_flesch_reading_ease}')\n",
        "\n",
        "corpus_flesch_kincaid_grade = textstat.flesch_kincaid_grade(test_data)\n",
        "print(f'corpus_flesch_kincaid_grade: {corpus_flesch_kincaid_grade}')\n",
        "\n",
        "corpus_smog_index = textstat.smog_index(test_data)\n",
        "print(f'corpus_smog_index: {corpus_smog_index}')\n",
        "\n",
        "corpus_coleman_liau_index = textstat.coleman_liau_index(test_data)\n",
        "print(f'corpus_coleman_liau_index: {corpus_coleman_liau_index}')\n",
        "\n",
        "corpus_automated_readability_index = textstat.automated_readability_index(test_data)\n",
        "print(f'corpus_automated_readability_index: {corpus_automated_readability_index}')\n",
        "\n",
        "corpus_dale_chall_readability_score = textstat.dale_chall_readability_score(test_data)\n",
        "print(f'corpus_dale_chall_readability_score: {corpus_dale_chall_readability_score}')\n",
        "\n",
        "corpus_difficult_words = textstat.difficult_words(test_data)\n",
        "print(f'corpus_difficult_words: {corpus_difficult_words}')\n",
        "\n",
        "corpus_linsear_write_formula = textstat.linsear_write_formula(test_data)\n",
        "print(f'corpus_linsear_write_formula: {corpus_linsear_write_formula}')\n",
        "\n",
        "corpus_gunning_fog = textstat.gunning_fog(test_data)\n",
        "print(f'corpus_gunning_fog: {corpus_gunning_fog}')\n",
        "\n",
        "corpus_text_standard = textstat.text_standard(test_data)\n",
        "print(f'corpus_text_standard: {corpus_text_standard}')\n",
        "\n",
        "corpus_fernandez_huerta = textstat.fernandez_huerta(test_data)\n",
        "print(f'corpus_fernandez_huerta: {corpus_fernandez_huerta}')\n",
        "\n",
        "corpus_szigriszt_pazos = textstat.szigriszt_pazos(test_data)\n",
        "print(f'corpus_szigriszt_pazos: {corpus_szigriszt_pazos}')\n",
        "\n",
        "corpus_gutierrez_polini = textstat.gutierrez_polini(test_data)\n",
        "print(f'corpus_gutierrez_polini: {corpus_gutierrez_polini}')\n",
        "\n",
        "corpus_crawford = textstat.crawford(test_data)\n",
        "print(f'corpus_crawford: {corpus_crawford}')\n",
        "\n",
        "corpus_gulpease_index = textstat.gulpease_index(test_data)\n",
        "print(f'corpus_gulpease_index: {corpus_gulpease_index}')\n",
        "\n",
        "corpus_osmane = textstat.osman(test_data)\n",
        "print(f'corpus_osmane: {corpus_osmane}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFIVH4k2OTwN"
      },
      "source": [
        "### **Library: NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLUW60wIOTeP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HFLkeCZOVlJ"
      },
      "source": [
        "### **Library: PyLex**\n",
        "\n",
        "* https://github.com/techcentaur/PyLex (20180606 58s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfAa5tqmOTbG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNfE-MqLSgKI"
      },
      "source": [
        "# **Feature Extraction: Time Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WGoVirjwgVM"
      },
      "source": [
        "## **Simple Summary Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh5vqBLn2D2L"
      },
      "source": [
        "corpus_unified_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRaLIqgQafqy"
      },
      "source": [
        "\n",
        "sns.set(rc = {'figure.figsize':(20,10)})\n",
        "# plt.figure(figsize = (30,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCmjilbYaHLw"
      },
      "source": [
        "# sns.set(rc={'figure.figsize':(30,10)})\n",
        "p = sns.displot(corpus_unified_df, x='baseline_sentimentr_stdscaler_roll10', kind='kde');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhwKZBgUdi9M"
      },
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "ax = sns.distplot(corpus_unified_df[\"baseline_sentimentr_stdscaler_roll10\"], fit=norm, kde=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bHHeZChcq5u"
      },
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "ax1 = sns.displot(corpus_unified_df[\"baseline_sentimentr_stdscaler_roll10\"])\n",
        "ax1.set_xlim((corpus_unified_df[\"sent_no\"].min(), int(corpus_unified_df[\"sent_no\"].max()/8)))\n",
        "ax2 = ax1.twinx()\n",
        "sns.histplot(data=corpus_unified_df, x=\"baseline_sentimentr_stdscaler_roll10\", discrete=False, ax=ax2)\n",
        "ax2.set_title(f'{CORPUS_FULL}\\nSentence Length (words) Histogram')\n",
        "ax2.text(.9, 0.9,f'Descriptive Statistics:\\n-----------------------\\n {stat_len_word_str}', fontsize=12, ha='center', va='center', transform = ax.transAxes);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y39AsvgmZE40"
      },
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "# sns.kdeplot(data=corpus_unified_df[\"baseline_sentimentr_stdscaler_roll10\"], x=corpus_unified_df[\"sent_no\"], ax=ax1)\n",
        "sns.displot(data=corpus_unified_df, x='baseline_sentimentr_stdscaler_roll10', kind='kde', ax=ax1)\n",
        "ax1.set_xlim((corpus_unified_df[\"sent_no\"].min(), int(corpus_unified_df[\"sent_no\"].max()/2)))\n",
        "ax2 = ax1.twinx()\n",
        "sns.histplot(data=corpus_unified_df, x=\"baseline_sentimentr_stdscaler_roll10\", bins=100, ax=ax2)\n",
        "# ax2.set_title(f'{CORPUS_FULL}\\nSentence Sentiment (baseline_sentimentr_stdscaler_roll10) Histogram');\n",
        "# ax2.text(.9, 0.9,f'Descriptive Statistics:\\n-----------------------\\n {stat_len_word_str}', fontsize=12, ha='center', va='center', transform = ax.transAxes);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XjdNmBC19Ld"
      },
      "source": [
        "corpus_unified_df['baseline_sentimentr_stdscaler_roll10'].plot()\n",
        "corpus_unified_df['baseline_syuzhet_stdscaler_roll10'].plot()\n",
        "plt.legend(loc='best')\n",
        "plt.title(f'Great Expectations by Charles Dickens\\nDiachronic Sentiment over {sent_ct} Sentences using Standard Scaler + SMA 10%');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a9oTwP6-29Zr"
      },
      "source": [
        "ts_stdscaler_roll_df = corpus_unified_df.filter(like='roll10').copy()\n",
        "sent_ct = ts_stdscaler_roll_df.shape[0]\n",
        "# print(f'Time Series Count: {sent_ct}')\n",
        "ts_stdscaler_roll_df.filter(regex='^(sentimentr|syuzhet|transformer)',axis=1).plot()\n",
        "plt.legend(loc='best')\n",
        "plt.title(f'Great Expectations by Charles Dickens\\nDiachronic Sentiment over {sent_ct} Sentences using Standard Scaler + SMA 10%');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ifg4NstuwUe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrrlpA_euxJv"
      },
      "source": [
        "## **Library: TSfresh**\n",
        "\n",
        "* https://github.com/blue-yonder/tsfresh (20210709 509s) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMzn8yQsyIJj"
      },
      "source": [
        "# !pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGOU9aKmuwTQ"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sktime.datasets import load_basic_motions\n",
        "from sktime.datasets import load_arrow_head\n",
        "from sktime.transformers.series_as_features.summarize import TSFreshFeatureExtractor\n",
        "from sktime.forecasting.base import ForecastingHorizon\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sktime.forecasting.compose import ReducedTimeSeriesRegressionForecaster\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sktime.datasets import load_airline\n",
        "from sktime.forecasting.model_selection import temporal_train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlE7DkU9uwNN"
      },
      "source": [
        "X, y = load_arrow_head(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkSXG3LguwKS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdJwqF32uwHO"
      },
      "source": [
        "from tsfresh.examples.robot_execution_failures import download_robot_execution_failures, \\\n",
        "    load_robot_execution_failures\n",
        "download_robot_execution_failures()\n",
        "timeseries, y = load_robot_execution_failures()\n",
        "\n",
        "timeseries.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GQUWoTnuwEC"
      },
      "source": [
        "print(timeseries.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzZ62xTJuwBJ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "timeseries[timeseries['id'] == 3].plot(subplots=True, sharex=True, figsize=(10,10))\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6rILygJuv-K"
      },
      "source": [
        "timeseries[timeseries['id'] == 20].plot(subplots=True, sharex=True, figsize=(10,10))\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71xtjZN-uv7T"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 47s\n",
        "\n",
        "from tsfresh import extract_features\n",
        "extracted_features = extract_features(timeseries, column_id=\"id\", column_sort=\"time\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tciEKXp0G_C"
      },
      "source": [
        "# Extracted features\n",
        "print(f'Original Feature Count: {timeseries.shape}')\n",
        "\n",
        "print(f'Extracted Feature Count: {extracted_features.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmcn9Sb6uv4R"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 30s\n",
        "\n",
        "from tsfresh import select_features\n",
        "from tsfresh.utilities.dataframe_functions import impute\n",
        "\n",
        "impute(extracted_features)\n",
        "features_filtered = select_features(extracted_features, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tLzBtPo0G77"
      },
      "source": [
        "# Relevant features\n",
        "print(f'Original Feature Count: {timeseries.shape}')\n",
        "\n",
        "print(f'Relevant Feature Count: {features_filtered.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VH-SnQT0G49"
      },
      "source": [
        "from tsfresh.feature_extraction import feature_calculators"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIsyEDqD2Rk8"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPh0EUIA2a7O"
      },
      "source": [
        "corpus_unified_df.fillna(0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-txGK6PK0G1w"
      },
      "source": [
        "ts_model_ser = corpus_unified_df['baseline_sentimentr_stdscaler_roll10']\n",
        "\n",
        "ts_abs_eng = feature_calculators.abs_energy(ts_model_ser)\n",
        "print(f'ts_abs_eng: {ts_abs_eng}')\n",
        "\n",
        "ts_sum_of_changes = feature_calculators.absolute_sum_of_changes(ts_model_ser)\n",
        "print(f'ts_sum_of_changes: {ts_sum_of_changes}')\n",
        "\n",
        "ts_acf = feature_calculators.acf(ts_model_ser)\n",
        "print(f'ts_acf: {ts_acf}')\n",
        "\n",
        "ts_adfuller = feature_calculators.adfuller(ts_model_ser)\n",
        "print(f'ts_adfuller: {ts_adfuller}')\n",
        "\n",
        "# ts_approximate_entropy = feature_calculators.approximate_entropy(ts_model_ser)\n",
        "# print(f'ts_approximate_entropy: {ts_approximate_entropy}')\n",
        "\n",
        "# ts_agg_autocorrelation = feature_calculators.agg_autocorrelation(ts_model_ser)\n",
        "# print(f'ts_agg_autocorrelation: {ts_agg_autocorrelation}')\n",
        "\n",
        "# ts_agg_linear_trend = feature_calculators.agg_linear_trend(ts_model_ser)\n",
        "# print(f'agg_linear_trend: {agg_linear_trend}')\n",
        "\n",
        "# ts_augmented_dickey_fuller = feature_calculators.augmented_dickey_fuller(ts_model_ser)\n",
        "# print(f'ts_augmented_dickey_fuller: {ts_augmented_dickey_fuller}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ShSuKKNc-P"
      },
      "source": [
        "## **Library: sktime**\n",
        "\n",
        "* https://github.com/alan-turing-institute/sktime (20210818 4.4k)\n",
        "\n",
        "Our aim is to make the time series analysis ecosystem more interoperable and usable as a whole. sktime provides a unified interface for distinct but related time series learning tasks. It features dedicated time series algorithms and tools for composite model building including pipelining, ensembling, tuning and reduction that enables users to apply an algorithm for one task to another.\n",
        "\n",
        "sktime also provides interfaces to related libraries, for example scikit-learn, statsmodels, tsfresh, PyOD and fbprophet, among others.\n",
        "\n",
        "For deep learning, see our companion package: sktime-dl."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHCTrid6NheB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU7Ojfe7NqRB"
      },
      "source": [
        "### **Library: sktime-dl**\n",
        "\n",
        "* https://github.com/sktime/sktime-dl (20210812 453s) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkc51XwdNhkJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9NzOfTyOAey"
      },
      "source": [
        "### **Library: dl-4-tsc**\n",
        "\n",
        "* https://github.com/hfawaz/dl-4-tsc (20200406 872s)\n",
        "\n",
        "This is the companion repository for our paper titled \"Deep learning for time series classification: a review\" published in Data Mining and Knowledge Discovery, also available on ArXiv."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuEBBI-yNhg5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R68sZafOUIC"
      },
      "source": [
        "## **Library: Darts**\n",
        "\n",
        "* https://github.com/unit8co/dart (20210818 2.3k) \n",
        "\n",
        "darts is a Python library for easy manipulation and forecasting of time series. It contains a variety of models, from classics such as ARIMA to deep neural networks. The models can all be used in the same way, using fit() and predict() functions, similar to scikit-learn. The library also makes it easy to backtest models, and combine the predictions of several models and external regressors. Darts supports both univariate and multivariate time series and models. The neural networks can be trained on multiple time series, and some of the models offer probabilistic forecasts.\n",
        "\n",
        "Currently, the library contains the following features:\n",
        "\n",
        "Forecasting Models: A large collection of forecasting models; from statistical models (such as ARIMA) to deep learning models (such as N-BEATS). See table of models below.\n",
        "\n",
        "Data processing: Tools to easily apply (and revert) common transformations on time series data (scaling, boxcox, …)\n",
        "\n",
        "Metrics: A variety of metrics for evaluating time series' goodness of fit; from R2-scores to Mean Absolute Scaled Error.\n",
        "\n",
        "Backtesting: Utilities for simulating historical forecasts, using moving time windows.\n",
        "\n",
        "Regression Models: Possibility to predict a time series from lagged versions of itself and of some external covariate series, using arbitrary regression models (e.g. scikit-learn models).\n",
        "\n",
        "Multiple series training: All neural networks, as well as RegressionModels (incl. LinearRegressionModel and RandomForest) support being trained on multiple series.\n",
        "\n",
        "Past and Future Covariates support: Some models support past-observed and/or future-known covariate time series as inputs for producing forecasts.\n",
        "\n",
        "Multivariate Support: Tools to create, manipulate and forecast multivariate time series.\n",
        "\n",
        "Probabilistic Support: TimeSeries objects can (optionally) represent stochastic time series; this can for instance be used to get confidence intervals.\n",
        "\n",
        "Filtering Models: Darts offers three filtering models: KalmanFilter, GaussianProcessFilter, and MovingAverage, which allow to filter time series, and in some cases obtain probabilistic inferences of the underlying states/values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo9Kp8RpOTyQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6U8fHrrLcGE"
      },
      "source": [
        "## **Library: Cesium**\n",
        "\n",
        "* http://cesium-ml.org/docs/feature_table.html\n",
        "* http://cesium-ml.org/docs/auto_examples/plot_EEG_Example.html#sphx-glr-auto-examples-plot-eeg-example-py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTmGeQsf2afd"
      },
      "source": [
        "from cesium import featurize\n",
        "\n",
        "features_to_use = [\"amplitude\",\n",
        "                   \"percent_beyond_1_std\",\n",
        "                   \"maximum\",\n",
        "                   \"max_slope\",\n",
        "                   \"median\",\n",
        "                   \"median_absolute_deviation\",\n",
        "                   \"percent_close_to_median\",\n",
        "                   \"minimum\",\n",
        "                   \"skew\",\n",
        "                   \"std\",\n",
        "                   \"weighted_average\"]\n",
        "\n",
        "fset_cesium = featurize.featurize_time_series(times=eeg[\"times\"],\n",
        "                                              values=eeg[\"measurements\"],\n",
        "                                              errors=None,\n",
        "                                              features_to_use=features_to_use)\n",
        "\n",
        "print(fset_cesium.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ODqfd6vMzRo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux7F6eeRMzOk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFy96nofMzLi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od0VrJoLMzIc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toKuosWPMzFv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZC9vv82MIFY"
      },
      "source": [
        "## **Library: FeatureTools**\n",
        "\n",
        "* https://featuretools.alteryx.com/en/stable/ (Extract Deep Features)\n",
        "* https://www.featuretools.com/demos/ (Forecast examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57uxvOwq2acV"
      },
      "source": [
        "import featuretools as ft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HovcIGXj2aZc"
      },
      "source": [
        "data = ft.demo.load_mock_customer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tDghLTK2aWW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0abyswiy2aTp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB5Ftwy9uv1P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfbQxMpal3Jz"
      },
      "source": [
        "# **Forecasting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQbUxAwwim0l"
      },
      "source": [
        "## **Library: TSLearn**\n",
        "\n",
        "* https://github.com/tslearn-team/tslearn (20210818 1.8k)\n",
        "\n",
        "tslearn is a Python package that provides machine learning tools for the analysis of time series. This package builds on (and hence depends on) scikit-learn, numpy and scipy libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRsz4kDAizPM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dT9JMvVi9qJ"
      },
      "source": [
        "## **Library: Informer2020**\n",
        "\n",
        "* https://github.com/zhouhaoyi/Informer2020 (20210812 1.6k)\n",
        "\n",
        "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (AAAI'21 Best Paper). This is the origin Pytorch implementation of Informer in the following paper: Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. Special thanks to Jieqi Peng@cookieminions for building this repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG_1Pvxhl7lY"
      },
      "source": [
        "# **Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu61pRwbmKzO"
      },
      "source": [
        "## **Library: tsai**\n",
        "\n",
        "* https://github.com/timeseriesAI/tsai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_StEG9pizFv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17gY7GZVm0Kl"
      },
      "source": [
        "# **AutoML Time Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOsQrqA_m35-"
      },
      "source": [
        "## **Library: MS NNI**\n",
        "\n",
        "* https://github.com/microsoft/nni"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCUrJ2xqmS8r"
      },
      "source": [
        "# **END**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGRZaC4io96t"
      },
      "source": [
        "## **Compute Distance Matrix between 2 Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDuHnGabfkbl"
      },
      "source": [
        "from dtaidistance import dtw\n",
        "from dtaidistance import dtw_visualisation as dtwvis\n",
        "import numpy as np\n",
        "s1 = np.array([0., 0, 1, 2, 1, 0, 1, 0, 0, 2, 1, 0, 0])\n",
        "s2 = np.array([0., 1, 2, 3, 1, 0, 0, 0, 2, 1, 0, 0, 0])\n",
        "path = dtw.warping_path(s1, s2)\n",
        "dtwvis.plot_warping(s1, s2, path, filename=\"warp.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKeB7_EcgGxJ"
      },
      "source": [
        "Image(filename='warp.png') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flVzRjEXhIkW"
      },
      "source": [
        "res = [random.randrange(1, 50, 1) for i in range(7)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gxUxLUBf73A"
      },
      "source": [
        "# Option #1: pandas\n",
        "\n",
        "s1 = [0, 0, 1, 2, 1, 0, 1, 0, 0]\n",
        "s2 = [0, 1, 2, 0, 0, 0, 0, 0, 0]\n",
        "distance = dtw.distance(s1, s2)\n",
        "print(distance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVxOylPNt3QS"
      },
      "source": [
        "print(dtw.distance.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnUSRo5Tha4r"
      },
      "source": [
        "%%timeit\n",
        "\n",
        "# 100 datapoints: 159ms\n",
        "# 300 datapoints: 1.5s\n",
        "# 500 datapoints: 4.3s\n",
        "# 1k datapoints: 17.7s\n",
        "# 5k datapoints: ?(1.55s)\n",
        "# 10k datapoints: >15m\n",
        "\n",
        "dist_ls = []\n",
        "\n",
        "for i in range(10):\n",
        "  r1 = [random.randrange(1, 50, 1) for i in range(10000)]\n",
        "  r2 = [random.randrange(1, 50, 1) for i in range(10000)]\n",
        "  dist_fl = dtw.distance(r1, r2)\n",
        "  dist_ls.append(dist_fl)\n",
        "\n",
        "print(f'Mean: {sum(dist_ls)/len(dist_ls)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REe3SYCAmliL"
      },
      "source": [
        "%%timeit\n",
        "\n",
        "# 100 datapoints: 159ms\n",
        "# 300 datapoints: 1.5s\n",
        "# 500 datapoints: 4.3s\n",
        "# 1k datapoints: 17.7s\n",
        "# 5k datapoints: ?(1.55s)\n",
        "# 10k datapoints: \n",
        "\n",
        "dist_ls = []\n",
        "\n",
        "for i in range(10):\n",
        "  r1 = [random.randrange(1, 50, 1) for i in range(5000)]\n",
        "  r2 = [random.randrange(1, 50, 1) for i in range(5000)]\n",
        "  dist_fl = dtw.distance(r1, r2)\n",
        "  dist_ls.append(dist_fl)\n",
        "\n",
        "print(f'Mean: {sum(dist_ls)/len(dist_ls)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyLImJ-Cf8zD"
      },
      "source": [
        "# Option #2 (30-3000x faster) c implementation requires array w/doubles (and optionally max_dist pruning)\n",
        "\n",
        "s1 = array.array('d',[0, 0, 1, 2, 1, 0, 1, 0, 0])\n",
        "s2 = array.array('d',[0, 1, 2, 0, 0, 0, 0, 0, 0])\n",
        "d = dtw.distance_fast(s1, s2, use_pruning=True)\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--4UKgBTh3AH"
      },
      "source": [
        "%%timeit\n",
        "\n",
        "# 100 datapoints: 3ms\n",
        "# 300 datapoints: 12.3ms\n",
        "# 500 datapoints: 26ms\n",
        "# 1k datapoints: 81.3ms\n",
        "# 5k datapoints: 1.55s\n",
        "# 10k datapoints: 6s\n",
        "\n",
        "dist_fast_ls = []\n",
        "\n",
        "for i in range(10):\n",
        "  r1 = array.array('d',[random.randrange(1, 50, 1) for i in range(1000)])\n",
        "  r2 = array.array('d',[random.randrange(1, 50, 1) for i in range(1000)])\n",
        "  dist_fl = dtw.distance_fast(r1, r2)\n",
        "  dist_fast_ls.append(dist_fl)\n",
        "\n",
        "print(f'Mean: {sum(dist_fast_ls)/len(dist_fast_ls)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SJG_JLLgmRp"
      },
      "source": [
        "# Option #3: Numpy array with doubles or floats\n",
        "\n",
        "s1 = np.array([0, 0, 1, 2, 1, 0, 1, 0, 0], dtype=np.double)\n",
        "s2 = np.array([0.0, 1, 2, 0, 0, 0, 0, 0, 0])\n",
        "d = dtw.distance_fast(s1, s2, use_pruning=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuAIW4tSj-XR"
      },
      "source": [
        "%%timeit\n",
        "\n",
        "# 100 datapoints: 3ms\n",
        "# 300 datapoints: 12.3ms\n",
        "# 500 datapoints: 26ms\n",
        "# 1k datapoints: 82ms\n",
        "# 5k datapoints: 1.55s\n",
        "# 10k datapoints: 6s\n",
        "\n",
        "dist_c_ls = []\n",
        "\n",
        "for i in range(10):\n",
        "  r1 = np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double)\n",
        "  r2 = np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double)\n",
        "  dist_fl = dtw.distance_fast(r1, r2)\n",
        "  dist_c_ls.append(dist_fl)\n",
        "\n",
        "print(f'Mean: {sum(dist_c_ls)/len(dist_c_ls)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjCozLeno1Hq"
      },
      "source": [
        "## **Visualize Warping Paths**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-3pDsX9kGDk"
      },
      "source": [
        "s1 = [0, 0, 1, 2, 1, 0, 1, 0, 0]\n",
        "s2 = [0, 1, 2, 0, 0, 0, 0, 0, 0]\n",
        "distance, paths = dtw.warping_paths(s1, s2)\n",
        "print(distance)\n",
        "print(paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew8Q5LOBo0uT"
      },
      "source": [
        "x = np.arange(0, 20, .5)\n",
        "s1 = np.sin(x)\n",
        "s2 = np.sin(x - 1)\n",
        "random.seed(1)\n",
        "for idx in range(len(s2)):\n",
        "    if random.random() < 0.05:\n",
        "        s2[idx] += (random.random() - 0.5) / 2\n",
        "d, paths = dtw.warping_paths(s1, s2, window=25, psi=2)\n",
        "best_path = dtw.best_path(paths)\n",
        "dtwvis.plot_warpingpaths(s1, s2, paths, best_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTBPezg0qXfO"
      },
      "source": [
        "## **Compute Distance Matrix between Set of Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRbu28tuPtR"
      },
      "source": [
        "from dtaidistance import dtw\n",
        "import numpy as np\n",
        "series = [\n",
        "    np.array([0, 0, 1, 2, 1, 0, 1, 0, 0], dtype=np.double),\n",
        "    np.array([0.0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
        "    np.array([0.0, 0, 1, 2, 1, 0, 0, 0])]\n",
        "ds = dtw.distance_matrix_fast(series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99TFFc0YqXR_"
      },
      "source": [
        "series = np.matrix([\n",
        "    [0.0, 0, 1, 2, 1, 0, 1, 0, 0],\n",
        "    [0.0, 1, 2, 0, 0, 0, 0, 0, 0],\n",
        "    [0.0, 0, 1, 2, 1, 0, 0, 0, 0]])\n",
        "\n",
        "ds = dtw.distance_matrix_fast(series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoDfclurrzvE"
      },
      "source": [
        "ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SlTIgyCs6jN"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pm-7wEcsmMb"
      },
      "source": [
        "[random.randrange(1, 50, 1) for i in range(10)]\n",
        "print('\\n')\n",
        "[random.randrange(1, 50, 1) for i in range(10)]\n",
        "print('\\n')\n",
        "[random.randrange(1, 50, 1) for i in range(10)]\n",
        "print('\\n')\n",
        "[random.randrange(1, 50, 1) for i in range(10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQNMEfLPsHJ1"
      },
      "source": [
        "series = [\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double)]\n",
        "ds = dtw.distance_matrix_fast(series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzliDjqhr42m"
      },
      "source": [
        "ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXue2jMsqXNB"
      },
      "source": [
        "# Can be distributed and parallelized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwNxwguG5wV7"
      },
      "source": [
        "ts_stdscaler_roll_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH7AO9K56Mse"
      },
      "source": [
        "ts_sentiments_np = ts_stdscaler_roll_df.to_numpy()\n",
        "ts_sentiments_np.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU09t0rb6o8I"
      },
      "source": [
        "series = ts_sentiments_np\n",
        "ds = dtw.distance_matrix_fast(series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDh4-NrQ5yY8"
      },
      "source": [
        "from dtaidistance import dtw\n",
        "import numpy as np\n",
        "series = [\n",
        "    np.array(ts_sentiment_df[''], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double),\n",
        "    np.array([random.randrange(1, 50, 1) for i in range(1000)], dtype=np.double)]\n",
        "ds = dtw.distance_matrix_fast(series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELeFrNpN5yQp"
      },
      "source": [
        "ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncx8Hb09Pw85"
      },
      "source": [
        "# **Stylometry**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrw9bHenRSbn"
      },
      "source": [
        "## **Library: StyloR**\n",
        "\n",
        "* https://github.com/computationalstylistics/stylo (20210804 106s)\n",
        "* https://github.com/JoannaBy/DHSI2021-Stylometry (2021 Tutorial)\n",
        "\n",
        "Python:\n",
        "\n",
        "* https://github.com/worldwise001/stylometry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNq5zqtHRSSx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ9PIHEESNLx"
      },
      "source": [
        "## **Web: Rolling Stylometry**\n",
        "\n",
        "* https://github.com/stylo-explorer/rolling-stylometry-explorer (20210808 6s) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYQMk-kNSMyb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzJN1epfQ0R3"
      },
      "source": [
        "## **Library: ScatterText**\n",
        "\n",
        "* https://github.com/JasonKessler/scattertext (20210707 1.6k)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckKwvJQlQ0Hp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWUIzzTCP08d"
      },
      "source": [
        "## **Library: PySty**\n",
        "\n",
        "* https://github.com/mikekestemont/pystyl (20180426 52s)\n",
        "* https://github.com/mikekestemont/pystyl/blob/master/A%20Walk%20Through%20PyStyl.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytlr6-hDP0wG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uPs-hOGV2Z-"
      },
      "source": [
        "## **Library: TestFeatureSelection**\n",
        "\n",
        "* https://github.com/StatguyUser/TextFeatureSelection (20210812 21s)\n",
        "\n",
        "Python library for feature selection for text features. It has filter method and genetic algorithm for improving text classification models. Helps improve your machine learning models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSChpli6P0tF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_GhMPjcP0qw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1kayEpFQR-3"
      },
      "source": [
        "## **Library: Stylometry**\n",
        "\n",
        "* https://github.com/worldwise001/stylometry (20150409 13s) \n",
        "* https://github.com/jpotts18/stylometry (20191215 103s) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnO6Ptp0P0nM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cHV1BtxSpca"
      },
      "source": [
        "## **Library: PASTEL (Persona: Gender, Age, Country, Politics, Education), Ethic, TOW)**\n",
        "\n",
        "* https://github.com/dykang/PASTEL (20200316 22s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK3mLkaiSpMB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2lCzRWwThCV"
      },
      "source": [
        "## **Jupyter: Authorship Chi-Squared Test**\n",
        "\n",
        "* https://github.com/travisrussell/pale_fire_analysis/blob/master/code/pale_fire_chi_squared.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ZIA_dLTgng"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WptYVfosUah4"
      },
      "source": [
        "## **Library: Translate Author Style 4 Anonymity PyTorch**\n",
        "\n",
        "* https://github.com/rakshithShetty/A4NT-author-masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFz_FvzjUaEO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TpoONY6QhyO"
      },
      "source": [
        "## **Library: Tweet Bot Detector**\n",
        "\n",
        "* https://github.com/omerjaved11/Author_Profiling_clef19 (20190817 3s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6MSRcGWUt_Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3UgzohwUuUB"
      },
      "source": [
        "## **Kaggle Competitions**\n",
        "\n",
        "Spooky Author Identification\n",
        "* https://www.kaggle.com/c/spooky-author-identification/code\n",
        "* https://www.kaggle.com/christopher22/stylometry-identify-authors-by-sentence-structure\n",
        "\n",
        "CommonLit Readibility:\n",
        "* https://www.kaggle.com/c/commonlitreadabilityprize/code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw10OpZOUu4j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoijIJAwRElY"
      },
      "source": [
        "# **NLP Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htbFgDlxRJ9d"
      },
      "source": [
        "## **Library: TextFeatureSelection**\n",
        "\n",
        "* https://github.com/StatguyUser/TextFeatureSelection\n",
        "\n",
        "TextFeatureSelection is a Python library which helps improve text classification models through feature selection. It has 3 methods TextFeatureSelection, TextFeatureSelectionGA (Genetic Algorithm) and TextFeatureSelectionEnsemble methods respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFE2UJFBREY6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e1IzGiyREVq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Xeef6BRETC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB8IQNsSpOom"
      },
      "source": [
        "# **Visualize Hierarchical Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7y6i0GFuviq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCBYyvrCuVdT"
      },
      "source": [
        "from dtaidistance import clustering\n",
        "# Custom Hierarchical clustering\n",
        "model1 = clustering.Hierarchical(dtw.distance_matrix_fast, {})\n",
        "cluster_idx = model1.fit(series)\n",
        "# Augment Hierarchical object to keep track of the full tree\n",
        "model2 = clustering.HierarchicalTree(model1)\n",
        "cluster_idx = model2.fit(series)\n",
        "# SciPy linkage clustering\n",
        "model3 = clustering.LinkageTree(dtw.distance_matrix_fast, {})\n",
        "cluster_idx = model3.fit(series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N4XfaECuXKG"
      },
      "source": [
        "model3.plot(\"myplot.png\")\n",
        "Image(filename='myplot.png') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqH6FmhbwqPv"
      },
      "source": [
        "ts_labels = ['SentimentR',\n",
        "             'SyuzhetR',\n",
        "             'TextBlob',\n",
        "             'Flair',\n",
        "             'Stanza',\n",
        "             'Logistic Regression',\n",
        "             'LSTM',\n",
        "             'CNN',\n",
        "             'RoBERTa 15 Large',\n",
        "             'T5']\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(30, 10))\n",
        "# show_ts_label = lambda idx: \"ts-\" + str(idx)\n",
        "show_ts_label = lambda idx: ts_labels[idx]\n",
        "model3.plot(\"hierarchy.png\", axes=ax, show_ts_label=show_ts_label,\n",
        "           show_tr_label=True, ts_label_margin=-100,\n",
        "           ts_left_margin=5, ts_sample_length=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2xnjrsCwqMt"
      },
      "source": [
        "Image(filename='hierarchy.png') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaTxbw7twqJ4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vp5pXaHwqHE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKPojcl6pMGs"
      },
      "source": [
        "from dtaidistance import clustering\n",
        "\n",
        "# Custom Hierarchical clustering\n",
        "# model1 = clustering.Hierarchical(dtw.distance_matrix_fast, {})\n",
        "# cluster_idx = model1.fit(series)\n",
        "\n",
        "# Augment Hierarchical object to keep track of the full tree\n",
        "# model2 = clustering.HierarchicalTree(model1)\n",
        "# cluster_idx = model2.fit(series)\n",
        "\n",
        "\n",
        "\n",
        "# SciPy linkage clustering\n",
        "model3 = clustering.LinkageTree(dtw.distance_matrix_fast, {})\n",
        "cluster_idx = model3.fit(series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GeW0x7Cpkgg"
      },
      "source": [
        "model3.plot(\"myplot.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhAAht3rqlyX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}