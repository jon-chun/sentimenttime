{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentimenttime_part1_lexrules_20210708_ver5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "u932nJxdh0Ac",
        "XKV1uMBEO8TR",
        "Bsm8awD4AZ8O",
        "yXwKR4gA8Ouk",
        "FU3aHagiRjqR",
        "jHscLkclSYqN",
        "yL8R_ANtfYG6",
        "j2tIua7tTSRz",
        "B0BukxX9ZKYA",
        "Ueo8fTSqqaak",
        "pnI6dd-kFe5E",
        "qiLWzvjFGRLy",
        "k80YU4ZoHCs2",
        "zZK1gA47xzkS",
        "ZJRV2n0M_xN6",
        "43YL_Iuf0JHZ",
        "r1tIJZepmvLu",
        "QZjqwTvU76AR",
        "SkNZVk128jV9",
        "pPKABrvstSIo",
        "SGiIwIcRmsu5",
        "oQ_wcgiyx-NJ",
        "nn6EWXYaybhj",
        "34Be4bSszK-A",
        "gfiyOjMBBraW",
        "sp_gYLYbkjxd",
        "4O3l61be6_GQ",
        "JizqEQE6X3oQ",
        "EIwEIY4cd2PB",
        "XKz34IzZdxIg",
        "H5Qw6frQdz1C",
        "B6Ipaa-lWVwP",
        "XDYYpJYRfrZ-",
        "Jii7hmhleyHo",
        "f2CIR3U9e4WC",
        "XzcDX02pZGbr",
        "OV6spOW1fSKf",
        "s0fL5RXKfZG6",
        "x1VhYgLai3RK",
        "7vUzh39HHJXz",
        "VJI_13B0HMnU",
        "B9sEH8_IG6Qq",
        "yRjKHnQaOTu2",
        "EE8EJC1wHCEn",
        "3wjBEXGyvU4x",
        "1iWFYbF1dknh",
        "hZPDCvYBCdPY",
        "dJiJ2tUBYEPu",
        "vsOKzM-RYHf5",
        "wHVeb7Jy9zaa",
        "PJbFlIpAYNYO",
        "804w9PYAYP_p"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "69b2558215214d0c863a7c35a8e96cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d9d23dbb0a8a4397b5b39bd5eb1a5963",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ec76b73afff746958a46b5fd875a854f",
              "IPY_MODEL_5256fcb8c519453dacb9133be40f37a9"
            ]
          }
        },
        "d9d23dbb0a8a4397b5b39bd5eb1a5963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec76b73afff746958a46b5fd875a854f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9add1f94ebd14ed383119c7f851fafb3",
            "_dom_classes": [],
            "description": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.1.json: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 23895,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 23895,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_607da4c564754163afadc841a2140c08"
          }
        },
        "5256fcb8c519453dacb9133be40f37a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_34ec2c61e6f04ced8162c48c8764b130",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 139k/? [01:25&lt;00:00, 1.63kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b618404af54418bb0307fbe1f6a5b1d"
          }
        },
        "9add1f94ebd14ed383119c7f851fafb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "607da4c564754163afadc841a2140c08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34ec2c61e6f04ced8162c48c8764b130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b618404af54418bb0307fbe1f6a5b1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08b6afbf521b4395baf4e10b80d3e7dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_72a9299f1a274944b96bbb0719ed41ed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7d979efc197d48abb73d30b4a5a7ee44",
              "IPY_MODEL_00a6ce99bfb942a087e56ff3093f863e"
            ]
          }
        },
        "72a9299f1a274944b96bbb0719ed41ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d979efc197d48abb73d30b4a5a7ee44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3f98e668b0c44992947806a96e3b271a",
            "_dom_classes": [],
            "description": "Downloading http://nlp.stanford.edu/software/stanza/1.2.1/en/default.zip: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 411784510,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 411784510,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_756e477b37ef403eb25926ce277bd12d"
          }
        },
        "00a6ce99bfb942a087e56ff3093f863e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5c0920806ac742c6bf1e0cb9e536e8e4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 412M/412M [01:24&lt;00:00, 4.88MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4cb4558e33694ec2ab5f242ae991630e"
          }
        },
        "3f98e668b0c44992947806a96e3b271a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "756e477b37ef403eb25926ce277bd12d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c0920806ac742c6bf1e0cb9e536e8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4cb4558e33694ec2ab5f242ae991630e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jon-chun/sentimenttime/blob/main/sentimenttime_part1_lexrules_20210708_ver5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibHFmIWoU3Vx"
      },
      "source": [
        "# **An Analytic Methodology to Extract Narratives from Text: Using Sentiment Analysis to find the Arcs and Crux Points in Novels, Social Media and Chat Transcripts**\n",
        "\n",
        "By: Jon Chun\n",
        "12 Jun 2021\n",
        "\n",
        "References:\n",
        "\n",
        "* Coming...\n",
        "\n",
        "TODO:\n",
        "* Filter out non-printable characters\n",
        "* Roll-over Crux-Points (SentNo+Sent/Parag) (plotly)\n",
        "* Label/Roll-over Chapter/Sect No at Boundries\n",
        "* Generate Report PDF/csv\n",
        "* Option to select raw or discrete2continous transformation (Bing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u932nJxdh0Ac"
      },
      "source": [
        "# Configuration (Auto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_a9eQyBiiTG"
      },
      "source": [
        "**Global Configuration Constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT2MyyjpihFj"
      },
      "source": [
        "# Hardcoded Sentiment Analysis Models\n",
        "\n",
        "MODELS_LS = ['vader','textblob','stanza','afinn','bing','sentimentr','syuzhet','pattern','sentiword','senticnet','nrc']\n",
        "            \n",
        "# Minimum lengths for Sentences and Paragraphs\n",
        "#   (Shorter Sents/Parags will be deleted)\n",
        "\n",
        "MIN_CHAP_LEN = 5000\n",
        "MIN_SECT_LEN = 5000  # Minimum char length to be included in section DataFrame\n",
        "MIN_PARAG_LEN = 2\n",
        "MIN_SENT_LEN = 2\n",
        "\n",
        "# Min/Max statistics on each lexicon's sentiment values applied to corpus\n",
        "corpus_lexicons_stats_dt = {}\n",
        "corpus_cruxes_dt = {}\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOPa6HH-OjZp"
      },
      "source": [
        "**Install Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEjSzsusOOJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0584f913-69ea-4aa1-967a-c2fb4ae2df45"
      },
      "source": [
        "# INSTALL LIBRARIES\n",
        "\n",
        "!pip install sklearn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ0UVdasuTTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858383f7-8a4a-43ce-a28e-a32a3ee9f195"
      },
      "source": [
        "%pip install contractions"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/93/f4/0ec4a458e4368cc3be2c799411ecf0bc961930e566dadb9624563821b3a6/contractions-0.0.52-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 5.1MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl (283kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 51.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85392 sha256=08b1ea4c154473b83234fce028efc9573132ef494eda6e1fbf9314573a1dcf14\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.2.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmtqmvu6OlR9"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7bf4lfgwMEz"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import io\n",
        "import glob\n",
        "import json\n",
        "import contextlib"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOmyq4h7OOFi"
      },
      "source": [
        "# IMPORT LIBRARIES\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OslLdEsvOuFU"
      },
      "source": [
        "import re\n",
        "import string"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YelenXz5BcmE"
      },
      "source": [
        "import collections\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Suximbjnw8D"
      },
      "source": [
        "# Import libraries for logging\n",
        "\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import time                     # (TODO: check no dependencies and delete)\n",
        "from time import gmtime, strftime"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPZmScjVDYyw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf62e110-bedb-46d0-c03f-7e011e08b9f4"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Download for sentence tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download for nltk/VADER sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMl2mfF8Haw8"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler   # To normalize time series\n",
        "from sklearn.preprocessing import StandardScaler # To Standardize time series: center(sub mean) and rescale within 1 SD (only for well-behaved guassian distributions)\n",
        "from sklearn.preprocessing import RobustScaler   # To Standardize time series: center(sub median) and rescale within 25%-75% (1st-3rd) IQR (better for noisy, outliers distributions)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nckwluDXwa1c"
      },
      "source": [
        "mean_std_scaler = StandardScaler()\n",
        "median_iqr_scaler = RobustScaler()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U589lvKXmFV-"
      },
      "source": [
        "# Zoom interpolates new datapoints between existing datapoints to expand a time series \n",
        "\n",
        "from scipy.ndimage.interpolation import zoom"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wcZfSOuBlW7"
      },
      "source": [
        "from scipy import interpolate\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy import signal\n",
        "from scipy.signal import argrelextrema"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY3UyvYjAvDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0467b704-9c5c-4399-8e52-28234bcc00b8"
      },
      "source": [
        "from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess\n",
        "from statsmodels import robust"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcSc4jsggSy2"
      },
      "source": [
        "**Define Library-Dependent Objects**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjGN2sN3uRpN"
      },
      "source": [
        "import contractions"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02LJQlYpgQGs"
      },
      "source": [
        "corpus_sects_df = pd.DataFrame()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwl0MBDyOwtX"
      },
      "source": [
        "**Configure Jupyter Notebook**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD1cyqWsfjxA"
      },
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzfybE5kfmE-"
      },
      "source": [
        "# Configure matplotlib and seaborn\n",
        "\n",
        "# Plotting pretty figures and avoid blurry images\n",
        "# %config InlineBackend.figure_format = 'retina'\n",
        "# Larger scale for plots in notebooks\n",
        "# sns.set_context('talk')\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [16, 8]\n",
        "plt.rcParams['figure.dpi'] = 100"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIIjSbyeP2fg"
      },
      "source": [
        "# Configure Jupyter\n",
        "\n",
        "# Enable multiple outputs from one code cell\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets, interactive\n",
        "\n",
        "# Configure Google Colab\n",
        "\n",
        "%load_ext google.colab.data_table"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS_El2PiQlyP"
      },
      "source": [
        "# Text wrap\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dLkfn4KFmDf"
      },
      "source": [
        "**Configuration Details Snapshot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FNPovQBFZky",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "891f0e3a-9d56-4958-e226-898d799bdc7a"
      },
      "source": [
        "# Snap Shot of Time, Machine, Data and Library/Version Blueprint\n",
        "# TODO:"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wiSBHxoOGZz"
      },
      "source": [
        "# Pick ONE Method (a) or (b) to Get Corpus Textfile\n",
        "\n",
        "**Choose either (a) OR (b), not both**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KRfiXXQOZcq"
      },
      "source": [
        "## **Option (a): Connect to Google gDrive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G64etjAUOOSm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "55168ee4-c58e-4d70-e9bb-dd611631a7c3"
      },
      "source": [
        "# Connect to Google gDrive\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0fvFZq-eFaw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3e194c0c-cb1f-4f7c-ac8e-8ad73329d10f"
      },
      "source": [
        "# Select the Corpus subdirectory on your Google gDrive\n",
        "\n",
        "gdrive_subdir = \"./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\" #@param {type:\"string\"}\n",
        "CORPUS_SUBDIR = gdrive_subdir\n",
        "corpus_filename = ''\n",
        "%cd $gdrive_subdir\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/research/2021/sa_book_code/books_sa/imcewan_machineslikeme\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKV1uMBEO8TR"
      },
      "source": [
        "## **Option (b): Upload Corpus Textfile**\n",
        "\n",
        "***Only do this if your Google subdirectory doesn't already contain a plain text file of your Corpus or you wish to overwrite it and use a newer version***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH6dlB2fO6Ln"
      },
      "source": [
        "# Execute this code cell to upload plain text file of corpus\n",
        "#   Should be *.txt format with paragraphs separated by at least 2 newlines\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3LORQ4fRGBW"
      },
      "source": [
        "# Verify file was uploaded\n",
        "\n",
        "# Get uploaded filename\n",
        "corpus_filename = list(uploaded.keys())[0]\n",
        "print(f'Uploaded Corpus filename is: {corpus_filename}')\n",
        "CORPUS_FILENAME = corpus_filename\n",
        "\n",
        "!ls -al $corpus_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsm8awD4AZ8O"
      },
      "source": [
        "# **Configuration (Manual)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfilGg6Mkxnd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4dee5ba0-67a5-4ffc-8e46-353883ca7778"
      },
      "source": [
        "# Verify subdirectory change\n",
        "\n",
        "!pwd\n",
        "!ls *.txt\n",
        "\n",
        "# TODO: Intelligently automate the filling of form based upon directory"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/research/2021/sa_book_code/books_sa/imcewan_machineslikeme\n",
            "'fsfitzgerald_thegreatgatsby (1).txt'\n",
            " fsfitzgerald_thegreatgatsby.txt\n",
            " imcewan_machineslikeme_sents.txt\n",
            " machines_like_me_all.txt\n",
            " machines_like_me_clean_final_hand.txt\n",
            " machines_like_me_clean.txt\n",
            " mlm_final_handclean_nosections.txt\n",
            " mlm_final_handclean_sections.txt\n",
            " mlm_final_hand.txt\n",
            " mlm_sentences.txt\n",
            " report_sentiment_cruxes_ianmcewan_machineslikeme_XLNet_SST_20210622_1232.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP3WLEv_g5aq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5bd976b4-39fd-48dc-df64-b9fbc0a6d0c3"
      },
      "source": [
        "CORPUS_TITLE = 'Machines Like Me' #@param {type:\"string\"}\n",
        "CORPUS_AUTHOR = \"Ian McEwan\" #@param {type:\"string\"}\n",
        "CORPUS_FILENAME = \"mlm_final_handclean_sections.txt\" #@param {type:\"string\"}\n",
        "CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\" #@param {type:\"string\"}\n",
        "\n",
        "CORPUS_FULL = f'{CORPUS_TITLE} by: {CORPUS_AUTHOR}'\n",
        "\n",
        "PLOT_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n",
        "\n",
        "FILE_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n",
        "\n",
        "gdrive_subdir = CORPUS_SUBDIR\n",
        "corpus_filename = ''\n",
        "author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "\n",
        "print(f'\\nWorking Corpus Datafile: {CORPUS_SUBDIR}')\n",
        "print(f'\\nFull Corpus Title/Author: {CORPUS_FULL}')\n",
        "\n",
        "# Verify contents of Corpus File is Correctly Formatted\n",
        "#   \n",
        "# TODO: ./utils/verify_format.py\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Working Corpus Datafile: ./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\n",
            "\n",
            "Full Corpus Title/Author: Machines Like Me by: Ian McEwan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKYXs5dHKPuk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "070a947a-7f10-415f-83c1-ff692f8c16c0"
      },
      "source": [
        "FILE_OUTPUT=\"Major\""
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8owpM75RILKn"
      },
      "source": [
        "# **Utility Functions (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMLbyx6gIPqj"
      },
      "source": [
        "## **Files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaRD7RL9IOeg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f67b49e6-a3db-40b0-f6f5-4783610ec66b"
      },
      "source": [
        "# Generate full path and timestamp for new filepath/filename\n",
        "\n",
        "def gen_pathfiletime(file_str, subdir_str=''):\n",
        "\n",
        "  # Geenreate compressed author and title substrings\n",
        "  author_raw_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "  title_raw_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "\n",
        "  # Generate current/unique datetime string\n",
        "  datetime_str = str(datetime.now().strftime('%Y%m%d%H%M%S'))\n",
        "\n",
        "  # Built fullpath+filename string\n",
        "  file_base, file_ext = file_str.split('.')\n",
        "\n",
        "  author_str = re.sub('[^A-Za-z0-9]+', '', author_raw_str)\n",
        "  title_str = re.sub('[^A-Za-z0-9]+', '', title_raw_str)\n",
        "\n",
        "  full_filepath_str = f'{subdir_str}{file_base}_{author_str}_{title_str}_{datetime_str}.{file_ext}'\n",
        "\n",
        "  # print(f'Returning from gen_savepath() with full_filepath={full_filepath}')\n",
        "\n",
        "  return full_filepath_str\n",
        "\n",
        "# Test\n",
        "# pathfilename_str = gen_pathfiletime('hist_paraglen.png')\n",
        "# print(pathfilename_str)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaBHFZBtKut8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "52a64330-75b9-468c-ac11-4af10f67f3c2"
      },
      "source": [
        "# Tokenize into Sentences\n",
        "\n",
        "def parag2sents(corpus_parags_ls):\n",
        "  '''\n",
        "  Given a list of paragraphs,\n",
        "  Return a list of lists of Sentences [sent_no, parag_no, asent(text)]\n",
        "  '''\n",
        "\n",
        "  sent_no = 0\n",
        "  # sent_base = 0\n",
        "  corpus_sents_row_ls = []\n",
        "  for parag_no,aparag in enumerate(corpus_parags_ls):\n",
        "    sents_ls = sent_tokenize(aparag)\n",
        "    # Delete (whitespace only) sentences\n",
        "    sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n",
        "    # Delete (punctuation only) sentences\n",
        "    sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n",
        "    # Delete numbers (int or float) sentences\n",
        "    sents_ls = [x for x in sents_ls if not (x.strip().isnumeric())]\n",
        "    # TODO: may want to keep\n",
        "    for s,asent in enumerate(sents_ls):\n",
        "      corpus_sents_row_ls.append([sent_no, parag_no, asent])\n",
        "      sent_no += 1\n",
        "\n",
        "    # print(f'Returning with corpus_sents_row_ls length = {len(corpus_sents_row_ls)}')\n",
        "  \n",
        "  return corpus_sents_row_ls\n",
        "\n",
        "# Test\n",
        "\n",
        "'''\n",
        "print(f'Length {len(corpus_parags_raw_ls)}')\n",
        "corpus_sents_row_ls = parag2sents(corpus_parags_raw_ls)\n",
        "\n",
        "print(f'First row {corpus_sents_row_ls[0]}')\n",
        "print('\\n')\n",
        "print(f'Last row {corpus_sents_row_ls[-1]}')\n",
        "''';"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9FDtpQ6QaEJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ddf4857f-3f41-43a6-b1c6-08547cff0a8e"
      },
      "source": [
        "#This function converts to lower-case, removes square bracket, removes numbers and punctuation\n",
        " \n",
        "def text_clean(text):\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)  # Expand contrations\n",
        "    text = re.sub(\"\\\\'s\", \" own\", text)  # After expanding normal apostrophes, expand possessive apostrophes \"Mary's car\" -> \"Mary own car\"\n",
        "\n",
        "    # TODO: More formally\n",
        "    # https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0\n",
        "    text = re.sub(\"-\\n\", \"\", text)       # Join end of line words split by continuation hyphens\n",
        "    text = re.sub(\"-\\n\\r\", \"\", text)\n",
        "    text = re.sub(\"-\\r\", \"\", text)\n",
        "    text = re.sub(\"\\[.*?\\]\", \" \", text)\n",
        "\n",
        "    text = re.sub(\"-\", \" \", text)  # Special care for hypenated words well-known: choose option (a)\n",
        "                                   # (a) 'well known', (b) 'wellknown' (c) 'well known' and 'wellknown' cf: https://datascience.stackexchange.com/questions/81072/how-to-process-the-hyphenated-english-words-for-any-nlp-problem\n",
        "    text = re.sub(\"/\", \" \", text)  # sociability/conversation/interesting -> sociability conversation interesting                             \n",
        "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
        "    text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n",
        "    text = re.sub(\"[\\n]\", \" \", text)  # Replace newline with space\n",
        "    return text"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StEtSX9EQrsl"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rz8A-nUIkUu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "35b4d0e8-7ac1-4114-8776-5e3c55bcb9eb"
      },
      "source": [
        "# Read corpus into a single string then split into sections (chapters and subchapter)\n",
        "\n",
        "'''\n",
        "if len(corpus_filename) == 0:\n",
        "  # If now file uploaded, use the file in Google gDrive\n",
        "  corpus_filename = CORPUS_FILENAME\n",
        "else:\n",
        "  # The uploaded file has priority over the gDrive Corpus file\n",
        "  pass\n",
        "'''\n",
        "\n",
        "def corpus2sects(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of min preprocessed raw sections/CHAPTERs (corpus_parags_raw_temp_ls)\n",
        "  '''\n",
        "\n",
        "  with open(corpus_filename, \"r\", encoding='windows-1252') as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  # print(f'len(corpus_raw_str) = {len(corpus_raw_str)}')\n",
        "  corpus_sects_ls = re.split(r'(CHAPTER [\\d]{1,2}|-----)', corpus_raw_str) # , flags=re.I)\n",
        "\n",
        "  # Filter out the Section separator '-----' lines\n",
        "  corpus_sects_noseclines_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('-----'))]\n",
        "  # Filter out the CHAPTER lines\n",
        "  corpus_sects_nochheads_ls = [x for x in corpus_sects_noseclines_ls if not (x.strip().startswith('CHAPTER '))]\n",
        "\n",
        "  return corpus_sects_nochheads_ls\n",
        "\n",
        "'''\n",
        "  corpus_parags_raw_temp_ls = corpus_raw_str.split('\\n\\n')\n",
        "  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_raw_temp_ls)}')\n",
        "\n",
        "  # Strip excess whitespace and drop empty lines\n",
        "  corpus_parags_raw_temp_ls = [x.strip() for x in corpus_parags_raw_temp_ls if len(x.strip()) > MIN_PARAG_LEN]\n",
        "  print(f'Corpus Paragraph -(whitespace only) Count: {len(corpus_parags_raw_temp_ls)}')\n",
        "\n",
        "  # Drop lines that only contain punctuation (e.g. '\"', '.', '...', etc)\n",
        "  corpus_parags_raw_temp_ls = [x for x in corpus_parags_raw_temp_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  print(f'Corpus Paragraph -(punctuation only) Count: {len(corpus_parags_raw_temp_ls)}')\n",
        "\n",
        "  return corpus_parags_raw_temp_ls\n",
        "\n",
        "# Test  \n",
        "\n",
        "corpus_parags_raw_ls = read_corpus_parags(CORPUS_FILENAME)\n",
        "print(f'We found #{len(corpus_parags_raw_ls)} lines\\n')\n",
        "\n",
        "print('\\nThe first 10 lines of the Corpus:')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_raw_ls[:10]\n",
        "\n",
        "print('\\nThe last 10 lines of the Corpus:')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_raw_ls[-10:]\n",
        "print('\\n')\n",
        "print(sorted(corpus_parags_raw_ls, key=lambda x: (len(x), x)))\n",
        "''';"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTRoTl90jgB1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bf5f156d-8259-4108-ba85-c00340c75a76"
      },
      "source": [
        "def corpus2chaps(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of min preprocessed raw CHAPTERs (corpus_parags_raw_temp_ls)\n",
        "  '''\n",
        "\n",
        "  with open(corpus_filename, \"r\", encoding='windows-1252') as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  # print(f'len(corpus_raw_str) = {len(corpus_raw_str)}')\n",
        "  corpus_chaps_ls = re.split(r'(CHAPTER [\\d]{1,2})', corpus_raw_str, flags=re.I) # , flags=re.I)\n",
        "\n",
        "  corpus_chaps_nochheads_ls = [x for x in corpus_chaps_ls if not (x.strip().startswith('CHAPTER '))]\n",
        "\n",
        "  return corpus_chaps_nochheads_ls"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMJphODKQvqS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "621413ca-d37b-4c15-e371-71ebbaff0df2"
      },
      "source": [
        "CORPUS_FILENAME"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mlm_final_handclean_sections.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S95HvpvwSBqp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "db68662d-d791-4bde-bfed-392152584800"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/research/2021/sa_book_code/books_sa/imcewan_machineslikeme\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXPS0M_QQnzR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9d0003d6-d7cf-41cb-f0c9-f06ec9f0bc2e"
      },
      "source": [
        "!ls -altr *.txt"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 531671 Jun  5 20:10  machines_like_me_all.txt\n",
            "-rw------- 1 root root 512517 Jun  5 20:10  machines_like_me_clean.txt\n",
            "-rw------- 1 root root 513945 Jun  5 21:02  machines_like_me_clean_final_hand.txt\n",
            "-rw------- 1 root root 486767 Jun  5 22:02  mlm_sentences.txt\n",
            "-rw------- 1 root root 486767 Jun  5 22:02  imcewan_machineslikeme_sents.txt\n",
            "-rw------- 1 root root 268684 Jun 16 13:52 'fsfitzgerald_thegreatgatsby (1).txt'\n",
            "-rw------- 1 root root 274935 Jun 16 14:52  fsfitzgerald_thegreatgatsby.txt\n",
            "-rw------- 1 root root    438 Jun 22 12:32  report_sentiment_cruxes_ianmcewan_machineslikeme_XLNet_SST_20210622_1232.txt\n",
            "-rw------- 1 root root 513921 Jun 27 18:18  mlm_final_hand.txt\n",
            "-rw------- 1 root root 513921 Jun 28 15:43  mlm_final_handclean_nosections.txt\n",
            "-rw------- 1 root root 514194 Jun 28 15:44  mlm_final_handclean_sections.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4IFHwutR0vW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "27ee13a8-3f98-4759-d5cd-1af4baa3ebdc"
      },
      "source": [
        "!head -n 10 mlm_final_handclean_sections.txt"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "But remember, please, the Law by which we live, We are not built to comprehend a lie ...\r\n",
            "\r\n",
            "--Rudyard Kipling, \"The Secret of the Machines\"\r\n",
            "\r\n",
            "CHAPTER 1\r\n",
            "\r\n",
            "It was religious yearning granted hope, it was the holy grail of science. Our ambitions ran high and low--for a creation myth made real, for a monstrous act of self-love. As soon as it was feasible, we had no choice but to follow our desires and hang the consequences. In loftiest terms, we aimed to escape our mortality, confront or even replace the Godhead with a perfect self. More practically, we intended to devise an improved, more modern version of ourselves and exult in the joy of invention, the thrill of mastery. In the autumn of the twentieth century, it came about at last, the first step towards the fulfilment of an ancient dream, the beginning of the long lesson we would teach ourselves that however complicated we were, however faulty and difficult to describe in even our simplest actions and modes of being, we could be imitated and bettered. And I was there as a young man, an early and eager adopter in that chilly dawn.\r\n",
            "\r\n",
            "But artificial humans were a cliche long before they arrived, so when they did, they seemed to some a disappointment. The imagination, fleeter than history, than technological advance, had already rehearsed this future in books, then films and TV dramas, as if human actors, walking with a certain glazed look, phony head movements, some stiffness in the lower back, could prepare us for life with our cousins from the future.\r\n",
            "\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntW-l6qnJwX2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a1d7da1d-3558-417e-b3ac-2d620130100c"
      },
      "source": [
        "sections_ls = corpus2sects('./mlm_final_handclean_sections.txt') # (CORPUS_FILENAME)\n",
        "len(sections_ls)\n",
        "# r1 = re.findall(r'^CHAPTER 1$' ,corpus_raw_str)\n",
        "# r1 = re.findall(r'-----',corpus_raw_str)\n",
        "# print(r1)\n",
        "\n",
        "# corpus_raw_str[:500]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otWe7mGOTuUV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9349e519-af94-4e97-c0b1-2faa70f2320f"
      },
      "source": [
        "len(sections_ls)\n",
        "min(sections_ls, key=len) "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'But remember, please, the Law by which we live, We are not built to comprehend a lie ...\\n\\n--Rudyard Kipling, \"The Secret of the Machines\"\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axLWTdFHMNGx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bc509e2c-3c33-4fdb-f365-2006124f956a"
      },
      "source": [
        "\n",
        "target_string = \"My name is maximums and my luck numbers are 12 45 78\"\n",
        "# split on white-space \n",
        "word_list = re.split(r\"\\s+\", target_string)\n",
        "print(word_list)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['My', 'name', 'is', 'maximums', 'and', 'my', 'luck', 'numbers', 'are', '12', '45', '78']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf5a5ClONwiA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b0631bf1-ddbb-422c-ab18-a1712383b5bd"
      },
      "source": [
        "# Read corpus into a single string then split into paragraphs\n",
        "\n",
        "'''\n",
        "if len(corpus_filename) == 0:\n",
        "  # If now file uploaded, use the file in Google gDrive\n",
        "  corpus_filename = CORPUS_FILENAME\n",
        "else:\n",
        "  # The uploaded file has priority over the gDrive Corpus file\n",
        "  pass\n",
        "'''\n",
        "\n",
        "def corpus2parags(corpus_filename):\n",
        "  '''\n",
        "  Given a corpus_filename (assuming already %cd into correct subdir)\n",
        "  Return a list of min preprocessed raw paragraphs (corpus_parags_raw_temp_ls)\n",
        "  '''\n",
        "\n",
        "  with open(corpus_filename, \"r\", encoding='windows-1252') as infp:\n",
        "    corpus_raw_str = infp.read()\n",
        "\n",
        "  corpus_parags_raw_temp_ls = corpus_raw_str.split('\\n\\n')\n",
        "  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_raw_temp_ls)}')\n",
        "\n",
        "  # Strip excess whitespace and drop empty lines\n",
        "  corpus_parags_raw_temp_ls = [x.strip() for x in corpus_parags_raw_temp_ls if len(x.strip()) > MIN_PARAG_LEN]\n",
        "  print(f'Corpus Paragraph -(whitespace only) Count: {len(corpus_parags_raw_temp_ls)}')\n",
        "\n",
        "  # Drop lines that only contain punctuation (e.g. '\"', '.', '...', etc)\n",
        "  corpus_parags_raw_temp_ls = [x for x in corpus_parags_raw_temp_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n",
        "  print(f'Corpus Paragraph -(punctuation only) Count: {len(corpus_parags_raw_temp_ls)}')\n",
        "\n",
        "  return corpus_parags_raw_temp_ls\n",
        "\n",
        "# Test  \n",
        "'''\n",
        "corpus_parags_raw_ls = read_corpus_parags(CORPUS_FILENAME)\n",
        "print(f'We found #{len(corpus_parags_raw_ls)} lines\\n')\n",
        "\n",
        "print('\\nThe first 10 lines of the Corpus:')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_raw_ls[:10]\n",
        "\n",
        "print('\\nThe last 10 lines of the Corpus:')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_raw_ls[-10:]\n",
        "print('\\n')\n",
        "print(sorted(corpus_parags_raw_ls, key=lambda x: (len(x), x)))\n",
        "''';"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afWoWCZ7R_U9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8e2fa9e8-257d-40bc-ac7e-64404dfd4717"
      },
      "source": [
        "# Verify saved under newest filename\n",
        "\n",
        "def get_recentfile(file_type='csv'):\n",
        "  '''\n",
        "  Given a file extension type,\n",
        "  Return the most recently created file of that type \n",
        "  in the current directory\n",
        "  '''\n",
        "  file_pattern = \"./*.\" + file_type\n",
        "  print(f'file_pattern: {file_pattern}')\n",
        "  list_of_files = glob.glob(file_pattern) # * means all if need specific format then *.csv\n",
        "  latest_file = max(list_of_files, key=os.path.getmtime)\n",
        "\n",
        "  return latest_file\n",
        "\n",
        "# Test\n",
        "\n",
        "# get_recentfile('txt')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUvKJEybUIeP"
      },
      "source": [
        "## **Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "G8TwYCrwwUco",
        "outputId": "20e234b4-6e23-4b56-9dcf-c95d42381539"
      },
      "source": [
        "# Test to find longest String in Corpus (in terms of #tokens) \n",
        "#      must be <510 tokens for Transformers\n",
        "\n",
        "# corpus_sents_df['sent_raw'].astype(str).apply(lambda x: len(x.split())).max() # split().len()).max()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLASKwzZF-Lh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "26c79cb4-c103-4d3f-a73c-97903ad2e511"
      },
      "source": [
        "def get_sentiments(model_base, sentiment_fn, sentiment_type='lexicon'):\n",
        "  '''\n",
        "  Given a model_base name and sentiment evaluation function\n",
        "  Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "  '''\n",
        "\n",
        "  # Calculate Sentiment Polarities\n",
        "\n",
        "  if sentiment_type == 'lexicon':\n",
        "    corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "  \n",
        "  elif sentiment_type == 'compound':\n",
        "    # VADER\n",
        "\n",
        "    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n",
        "    corpus_sents_df['scores'] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_parags_df['scores'] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_sects_df['scores'] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_chaps_df['scores'] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "\n",
        "    # Extract Compound Sentiment\n",
        "    corpus_sents_df[model_base]  = corpus_sents_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_parags_df[model_base]  = corpus_parags_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_sects_df[model_base]  = corpus_sects_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_chaps_df[model_base]  = corpus_chaps_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "\n",
        "  elif sentiment_type == 'function':\n",
        "    # TextBlob\n",
        "\n",
        "    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n",
        "    corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: sentiment_type={sentiment_type} but must be one of (lexicon, compound, function)')\n",
        "    return\n",
        "\n",
        "  # Create new column names\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "\n",
        "\n",
        "  # Get Chapter Standardization with MeanSTD and RobustStandardization with MedianIQRScaling\n",
        "  corpus_chaps_df[col_meanstd]  = mean_std_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Chapter Sentiment by dividing by Chapter Length\n",
        "  chaps_len_ls = list(corpus_chaps_df['token_len'])\n",
        "  chaps_sentiment_ls = list(corpus_chaps_df[model_base])\n",
        "  chaps_sentiment_norm_ls = [chaps_sentiment_ls[i]/chaps_len_ls[i] for i in range(len(chaps_len_ls))]\n",
        "  # RobustStandardize Chapter sentiment values\n",
        "  corpus_chaps_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  # Get Section Standardization with MeanSTD and RobustStandardization with MedianIQRScaling\n",
        "  corpus_sects_df[col_meanstd]  = mean_std_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sects_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Section Sentiment by dividing by Section Length\n",
        "  sects_len_ls = list(corpus_sects_df['token_len'])\n",
        "  sects_sentiment_ls = list(corpus_sects_df[model_base])\n",
        "  sects_sentiment_norm_ls = [sects_sentiment_ls[i]/sects_len_ls[i] for i in range(len(sects_len_ls))]\n",
        "  # RobustStandardize Section sentiment values\n",
        "  corpus_sects_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_sects_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sects_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  # Normalize the Paragraph Sentiment by dividing by Chapter Length\n",
        "  parags_len_ls = list(corpus_parags_df['token_len'])\n",
        "  parags_sentiment_ls = list(corpus_parags_df[model_base])\n",
        "  parags_sentiment_norm_ls = [parags_sentiment_ls[i]/parags_len_ls[i] for i in range(len(parags_len_ls))]\n",
        "  # RobustStandardize Paragraph sentiment values\n",
        "  corpus_parags_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_parags_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_parags_df[model_base]).reshape(-1, 1))\n",
        "  corpus_parags_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "  sents_len_ls = list(corpus_sents_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_sents_df[model_base])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/sents_len_ls[i] for i in range(len(sents_len_ls))]\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_sents_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sents_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  return"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJNepWBIkVjm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "93048b4d-dd61-49a6-e3a8-3812d01331bf"
      },
      "source": [
        "# Read in lexicon at given path into Dict[word]=polarity\n",
        "\n",
        "def get_lexicon(lexicon_name, lexicon_format=2):\n",
        "    \"\"\"\n",
        "    Read sentiment lexicon.csv file at lexicon_path\n",
        "    into appropriate Dict[word]=polarity\n",
        "\n",
        "    1. lexicon_dt[word] = <polarity value>\n",
        "\n",
        "    Args:\n",
        "        sa_lib (str, optional): [description]. Defaults to 'syuzhet'.\n",
        "    \"\"\"\n",
        "    \n",
        "    # global lexicon_df\n",
        "\n",
        "    lexicon_df = pd.DataFrame()\n",
        "    \n",
        "    # print(os.getcwd())\n",
        "    lexicons_ls = os.listdir('../sa_lexicons/')\n",
        "    if (lexicon_name in lexicons_ls):\n",
        "      print(f'Found {lexicon_name} in lexicon_directory)')\n",
        "    # print(glob.glob('*.csv'))\n",
        "    cp_cmd = f'copy ../sa_lexicons/{lexicon_name} ./'\n",
        "    print(f'cp_cmd = {cp_cmd}')\n",
        "    os.system(cp_cmd)\n",
        "    os.system('cp ../sa_lexicons/' + lexicon_name.strip() + ' ./')\n",
        "    os.listdir('.')  \n",
        "\n",
        "    try:\n",
        "      lexicon_df = pd.read_csv(lexicon_name)\n",
        "      lexicon_df.info()\n",
        "      # lexicon_df = lexicon_tmp_df.copy()\n",
        "      # print(lexicon_df.head())\n",
        "      return lexicon_df\n",
        "    except:\n",
        "      print(f'ERROR: Cannot read lexicon.csv at {lexicon_name}')\n",
        "      return -1\n",
        "\n",
        "'''\n",
        "    print\n",
        "    if (sa_lexicon == 'default'):\n",
        "        lexicon_df = pd.read_csv(LEXICON_PATH)\n",
        "        lexicon_df.columns = ['index_no', 'word', 'polarity']\n",
        "        lexicon_df.drop(['index_no'], axis=1, inplace=True)\n",
        "        lexicon_df.dropna(inplace=True)\n",
        "        lexicon_dt = lexicon_df.set_index('word').T.to_dict('list')\n",
        "        # unlist the polarity to type: float\n",
        "        for key in lexicon_dt:\n",
        "            lexicon_dt[key] = float(lexicon_dt[key][0])\n",
        "        \n",
        "    ### print(f\"Exit get_sa_lex() with {len(lexicon_dt.keys())} entries in syuzhet_dt\")\n",
        "    return lexicon_dt\n",
        "''';"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWdzLF0jwI-u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "435e9e08-81e0-49de-a7f7-58a6b30aae10"
      },
      "source": [
        "# Sentence to Sentiment Polarity according to passed in Lexicon Dictionary\n",
        "\n",
        "def text2sentiment(text_str, lexicon_dt):\n",
        "  '''\n",
        "  Given a text_str and lexicon_dt, calculate \n",
        "  the sentimety polarity.\n",
        "  '''\n",
        "\n",
        "  # Remove all not alphanumeric and whitespace characters\n",
        "  text_str = re.sub(r'[^\\w\\s]', '', text_str) \n",
        "\n",
        "  text_str = text_str.strip().lower()\n",
        "  if (len(text_str) < 1):\n",
        "      print(f\"ERROR: text2sentiment() given empty/null/invalid string: {text_str}\")\n",
        "\n",
        "  text_ls = text_str.split()\n",
        "  # print(f'text_ls: {text_ls}')\n",
        "\n",
        "  # Accumulated Total Sentiment Polarity for entire Sentence\n",
        "  text_sa_tot = 0.0\n",
        "\n",
        "  for aword in text_ls:\n",
        "      # print(f'getting sa for word: {aword}')\n",
        "      try:\n",
        "          word_sa_fl = float(lexicon_dt[aword])\n",
        "          text_sa_tot += word_sa_fl\n",
        "          # print(f\">>{aword} has a sentiment value of {word_sa_fl}\")\n",
        "      except TypeError: # KeyError:\n",
        "          # aword is not in lexicon so it adds 0 to the sentence sa sum\n",
        "          # print(f\"TypeError: cannot convert {lexicon_dt[aword]} to float\")\n",
        "          continue\n",
        "      except KeyError:\n",
        "          # print(f\"KeyError: missing key {aword} in defaultdict syuzhet_dt\")\n",
        "          continue\n",
        "      except:\n",
        "          e = sys.exc_info()[0]\n",
        "          # print(f\"ERROR {e}: sent2lex_sa() cannot catch aword indexing into syuzhet_dt error\")\n",
        "  \n",
        "  # print(f\"Leaving sent2lex_sa() with sentence sa value = {str(text_sa_tot)}\")\n",
        "  \n",
        "  return text_sa_tot\n",
        "\n",
        "\n",
        "# Test\n",
        "\n",
        "# sent2sentiment('I hate and despise and abhor and dislike and am disgusted by Mondays.', lexicon_jockersrinker_dt)\n",
        "# sent2sentiment('hate Mondays.', lexicon_jockersrinker_dt)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6AEBZzvEz4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9fb38967-d895-4590-ce83-6fba781f4883"
      },
      "source": [
        "def plot_smas(section_view=True, model_name='vader', text_unit='sentence', wins_ls=[20], alpha=0.5, subtitle_str='', y_height=0, save2file=False):\n",
        "  '''\n",
        "  Given a model, text_unit\n",
        "  Plot a SMA using default values and wrapping the function get_smas()\n",
        "  '''\n",
        "\n",
        "  if (section_view == True) and not any(x == text_unit for x in ['sentence', 'paragraph']):\n",
        "    print(f'ERROR: You can only plot SMA within a Section with Sentence or Paragraph text units')\n",
        "    return -99\n",
        "\n",
        "  if text_unit == 'sentence':\n",
        "    if section_view == False:\n",
        "      ts_df = corpus_sents_df\n",
        "    else:\n",
        "      ts_df = section_sents_df\n",
        "    wins_ls = [5,10,20]\n",
        "  elif text_unit == 'paragraph':\n",
        "    if section_view == False:\n",
        "      ts_df = corpus_parags_df\n",
        "    else:\n",
        "      ts_df = section_parags_df\n",
        "    wins_ls = [5,10,20]\n",
        "  elif text_unit == 'section':\n",
        "    ts_df = corpus_sects_df\n",
        "    wins_ls=[20]\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n",
        "\n",
        "  sectno_loc = ts_df[model_name].min()\n",
        "\n",
        "  if section_view ==False:\n",
        "    # At Section boundries draw blue vertical lines \n",
        "    section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "    for i, sent_no in enumerate(section_boundries_ls):\n",
        "      plt.text(sent_no, y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n",
        "      plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "      # 'BigNews1', xy=(sent_no, 0.5), xytext=(-10, 25), textcoords='offset points',                   rotation=90, va='bottom', ha='center', annotation_clip=True)\n",
        "\n",
        "      # plt.text(sent_no, -.5, 'goodbye',rotation=90, zorder=0)\n",
        "\n",
        "    # At Chapter boundaries draw red vertical lines\n",
        "    chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n",
        "    for i, sent_no in enumerate(chapter_boundries_ls):\n",
        "      plt.axvline(sent_no, color='navy', alpha=0.1)\n",
        "      # plt.text(sent_no, .5, 'hello', rotation=90, zorder=0)\n",
        "\n",
        "  get_smas(ts_df, model_name=model_name, text_unit=text_unit, wins_ls=wins_ls, alpha=alpha, subtitle_str=subtitle_str, save2file=save2file)\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_sma_sents_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcAMyjyn8ugj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2ad49078-6ef6-48a5-9906-7aecee4f72a5"
      },
      "source": [
        "# SMA 5% Sentiment of Sentence Sentiment\n",
        "\n",
        "def get_smas(ts_df, model_name, text_unit='sentence', wins_ls=[5,10], alpha=0.5, scale_factor=1., subtitle_str='', mean_adj=0., do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a model_name and time series DataFrame and list of win_rolls in percentages\n",
        "  Return the rolling means of the time series using the window sizes in win_rolls\n",
        "  '''\n",
        "\n",
        "  temp_roll_df = pd.DataFrame() # TODO: save sma rolling values into temp_df and return this value\n",
        "\n",
        "  win_1per = int(ts_df.shape[0]*0.01)\n",
        "  if text_unit ==  'sentence':\n",
        "    # win_1per = win_s1per\n",
        "    x_idx = 'sent_no'\n",
        "    fname_abbr = 'sents'\n",
        "  elif text_unit == 'paragraph':\n",
        "    # win_1per = win_p1per\n",
        "    x_idx = 'parag_no'\n",
        "    fname_abbr = 'parags'\n",
        "  elif text_unit == 'section':\n",
        "    win_1per = 1\n",
        "    wins_ls = [int(0.1 * corpus_sects_df.shape[0])]  # Edge case to deal with very few Section data points\n",
        "    x_idx = 'sect_no'\n",
        "    fname_abbr = 'sects'\n",
        "  else:\n",
        "    print(f'ERROR: text_unit={text_unit} but must be either sentence, paragraph or section')\n",
        "  \n",
        "  for i, awin_size in enumerate(wins_ls):\n",
        "    if len(str(awin_size)) == 1:\n",
        "      awin_str = '0'+str(awin_size)+'0'\n",
        "    else:\n",
        "      awin_str = str(awin_size)+ '0'\n",
        "    col_roll_str = f'{model_name}_mean_roll{awin_str}'\n",
        "    win_size = awin_size*win_1per\n",
        "    ts_df[col_roll_str] = ts_df[model_name].rolling(window=win_size, center=True).mean()\n",
        "  \n",
        "    if do_plot == True:\n",
        "      alabel = f'{model_name} (win={awin_size})'\n",
        "      ts_df['y_scaled'] = ts_df[col_roll_str]*scale_factor + mean_adj \n",
        "      sns.lineplot(data=ts_df, x=x_idx, y='y_scaled', legend='brief', label=alabel, alpha=alpha)\n",
        "      \n",
        "  plt.title(f'{CORPUS_FULL} (Model: {model_name}: {subtitle_str}) \\nSMA Smoothed {text_unit} Sentiment Plot (windows={wins_ls})')\n",
        "  # plt.legend(loc='best')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_{fname_abbr}_sa_mean_050100sma.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return temp_roll_df"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULzfHeDK8udN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "79810502-8482-4dba-fc3e-eb2c23daef1a"
      },
      "source": [
        "def get_lexstats(ts_df, model_name, text_unit='sentence'):\n",
        "  '''\n",
        "  Given a model name\n",
        "  calculate, store and return time series stats\n",
        "  '''\n",
        "  \n",
        "  global corpus_lexicons_stats_dt\n",
        "\n",
        "  temp_dt = {}\n",
        "  \n",
        "  if text_unit == 'sentence':\n",
        "    stat_idx = f'{model_name}_sents'\n",
        "  elif text_unit == 'paragraph':\n",
        "    stat_idx = f'{model_name}_parags'\n",
        "  elif text_unit == 'section':\n",
        "    stat_idx = f'{model_name}_sects'\n",
        "  elif text_unit == 'chapter':\n",
        "    stat_idx = f'{model_name}_chaps'\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must either be sentence, paragraph, or section')\n",
        "\n",
        "  sentiment_min = ts_df[model_name].min()\n",
        "  sentiment_max = ts_df[model_name].max()\n",
        "\n",
        "  temp_dt = {'sentiment_min' : sentiment_min,\n",
        "             'sentiment_max' : sentiment_max}\n",
        "\n",
        "  corpus_lexicons_stats_dt[stat_idx] = temp_dt\n",
        "                                     \n",
        "  return \n",
        "\n",
        "# Test\n",
        "# get_lexstats('afinn')\n",
        "# corpus_lexicons_stats_dt"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltdJn-7ePNM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "538500a8-524d-4bff-f94d-1b522fe50d03"
      },
      "source": [
        "def lex_discrete2continous_sentiment(text, lexicon):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_tot = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    word_sentiment = text2sentiment(str(aword), lexicon)\n",
        "    text_sentiment_tot += word_sentiment\n",
        "  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.01)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6xMI98l8uaH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1fc36f8b-904c-42ef-904b-428b55d07774"
      },
      "source": [
        "def clip_outliers(floats_ser):\n",
        "  '''\n",
        "  Given a pd.Series of float values\n",
        "  Return a list with outliers removed, values limited within 3 median absolute deviations from median\n",
        "  '''\n",
        "  # https://www.statsmodels.org/stable/generated/statsmodels.robust.scale.mad.html#statsmodels.robust.scale.mad\n",
        "\n",
        "  # Old mean/std, less robust\n",
        "  # ser_std = floats_ser.std()\n",
        "  # ser_median = floats_ser.mean() # TODO: more robust: asym/outliers -> median/IQR or median/median abs deviation\n",
        "\n",
        "  floats_np = np.array(floats_ser)\n",
        "  ser_median = floats_ser.median()\n",
        "  ser_mad = robust.mad(floats_np)\n",
        "  print(f'ser_median = {ser_median}')\n",
        "  print(f'ser_mad = {ser_mad}')\n",
        "\n",
        "  if ser_mad == 0:\n",
        "    # for TS with small ranges (e.g. -1.0 to +1.0) Median Abs Deviation = 0\n",
        "    #   so pass back the original time series\n",
        "    floats_clip_ls = list(floats_ser)\n",
        "\n",
        "  else:\n",
        "    ser_oldmax = floats_ser.max()\n",
        "    ser_oldmin = floats_ser.min()\n",
        "    print(f'ser_max = {ser_oldmax}')\n",
        "    print(f'ser_min = {ser_oldmin}')\n",
        "\n",
        "    ser_upperlim = ser_median + 2.5*ser_mad\n",
        "    ser_lowerlim = ser_median - 2.5*ser_mad\n",
        "    print(f'ser_upperlim = {ser_upperlim}')\n",
        "    print(f'ser_lowerlim = {ser_lowerlim}')\n",
        "\n",
        "    # Clip outliers to max or min values\n",
        "    floats_clip_ls = np.clip(floats_np, ser_lowerlim, ser_upperlim)\n",
        "    # print(f'max floast_ls {floats_ls.max()}')\n",
        "\n",
        "    # def map2range(value, low, high, new_low, new_high):\n",
        "    #   '''map a value from one range to another'''\n",
        "    #   return value * 1.0 / (high - low + 1) * (new_high - new_low + 1)\n",
        "\n",
        "    # Map all float values to range [-1.0 to 1.0]\n",
        "    # floats_clip_sig_ls = [map2range(i, ser_oldmin, ser_oldmax, ser_upperlim, ser_lowerlim) for i in floats_clip_ls]\n",
        "\n",
        "    # listmax_fl = float(max(floats_ls))\n",
        "    # floats_ls = [i/listmax_fl for i in floats_ls]\n",
        "    #floats_ls = [1/(1+math.exp(-i)) for i in floats_ls]\n",
        "\n",
        "  return floats_clip_ls  # floats_clip_sig_ls\n",
        "\n",
        "# Test\n",
        "# Will not work on first run as corpus_sents_df is not defined yet\n",
        "'''\n",
        "data = np.array([1, 4, 4, 7, 12, 13, 16, 19, 22, 24])\n",
        "test_ls = clip_outliers(corpus_sents_df['vader'])\n",
        "print(f'new min is {min(test_ls)}')\n",
        "print(f'new max is {max(test_ls)}')\n",
        "''';"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXwKR4gA8Ouk"
      },
      "source": [
        "## **Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8Hf8nU98uXI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ca088cf0-389d-46e7-a2da-2e61376b0b6e"
      },
      "source": [
        "def rename_cols(ts_df, col_old_ls, suffix_str='_raw'):\n",
        "  '''\n",
        "  Given a DataFrame, list of columns in DataFrame and a suffix,\n",
        "  Return a Dictionary mapping old col names to new col name (orig+suffix)\n",
        "  '''\n",
        "\n",
        "  col_new_ls = []\n",
        "  for acol in col_old_ls:\n",
        "    acol_new = acol + suffix_str\n",
        "    col_new_ls.append(acol_new)\n",
        "\n",
        "  # Create dict for col mapping: keys=old col names, value=new col names\n",
        "  col_rename_dt = dict(zip(col_old_ls, col_new_ls))\n",
        "\n",
        "  # ts_df.rename(columns=col_rename_dt, errors=\"raise\")\n",
        "\n",
        "  return col_rename_dt\n",
        "\n",
        "# test_ls = [col for col in corpus_sents_df.columns if not(renaming_fun(col) is None)]\n",
        "# print(f'test_ls: {test_ls}')\n",
        "\n",
        "# Test\n",
        "# col_rename_dt = rename_cols(corpus_sents_df, sentiment_only_cols_ls)\n",
        "# col_rename_dt"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YJJcvDVnUuT"
      },
      "source": [
        "## **Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIo6-zGKnZps",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "17818bdb-fed6-4770-f456-51d9e24cf674"
      },
      "source": [
        "def norm2negpos1(data_ser):\n",
        "  '''\n",
        "  Given a series of floating number\n",
        "  Return a a list of same values normed between -1.0 and +1.0\n",
        "  '''\n",
        "  # data_np = np.matrix(data_ser)\n",
        "\n",
        "  scaler=MinMaxScaler(feature_range=(-1.0, 1.0))\n",
        "  temp_ser = scaler.fit_transform(np.matrix(data_ser))\n",
        "  \n",
        "  return temp_ser\n",
        "\n",
        "# Test\n",
        "'''\n",
        "temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n",
        "print(type(temp_np))\n",
        "temp_np.shape\n",
        "''';"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpVHeYKYnUhU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "75d33763-8b39-4b8a-8b9f-3b6bddd3fdcf"
      },
      "source": [
        "def standardize_ts(data_ser):\n",
        "  '''\n",
        "  Given a series of floating number\n",
        "  Return a a list of same values normed between -1.0 and +1.0\n",
        "  '''\n",
        "  # data_np = np.matrix(data_ser)\n",
        "\n",
        "  std_scaler = StandardScaler()\n",
        "  df_std = std_scaler.fit_transform(np.array(data_ser))\n",
        "  \n",
        "  return df_std\n",
        "\n",
        "# Test\n",
        "'''\n",
        "temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n",
        "print(type(temp_np))\n",
        "temp_np.shape\n",
        "temp_np\n",
        "''';"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63hXO7mwpBj7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3eeffa33-5b0f-422f-e55e-7e7c5d75f20a"
      },
      "source": [
        "MODELS_LS"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vader',\n",
              " 'textblob',\n",
              " 'stanza',\n",
              " 'afinn',\n",
              " 'bing',\n",
              " 'sentimentr',\n",
              " 'syuzhet',\n",
              " 'pattern',\n",
              " 'sentiword',\n",
              " 'senticnet',\n",
              " 'nrc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylt_kLuEFDrj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dd1fe652-1e52-407e-a371-937454ae8e8f"
      },
      "source": [
        "# This must be defined AFTER the corpus_sects_df DataFrame is created in the Preprocessing Step below\n",
        "\n",
        "# Raw Plot of Section Sentiments (Adjusted for (x-axis) mid-Section Sentence No and (y-axis) Sentiment weighted by Section length )\n",
        "\n",
        "# corpus_sects_df = pd.DataFrame()  # Create empty early as required by some utility functions\n",
        "\n",
        "def plot_crux_sections(model_names_ls, semantic_type='section', subtitle_str='', label_token_ct=0, title_xpos = 0.8, title_ypos=0.2, sec_y_height=0, save2file=False):\n",
        "  '''\n",
        "  Given a Sections DataFrame, model_name and semantic type,\n",
        "  Return a Plot of the Cruxes\n",
        "  '''\n",
        "\n",
        "  crux_points_dt = {}\n",
        "  model_stand_names_ls = []\n",
        "  section_boundries_ls = []\n",
        "\n",
        "\n",
        "  # print(f'Using model_names: {model_names_ls}')\n",
        "\n",
        "  # sns.lineplot(data=ts_df, x='sent_no_mid', y=amodel_stand, markers=['o'], alpha=0.5, label=amodel_stand); # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment (Bing Lexicon)')\n",
        "\n",
        "\n",
        "  # At Section boundries draw blue vertical lines \n",
        "  section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "  for i, sent_no in enumerate(section_boundries_ls):\n",
        "    plt.text(sent_no, sec_y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1);\n",
        "\n",
        "  # At Chapter boundaries draw red vertical lines\n",
        "  chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n",
        "  for i, sent_no in enumerate(chapter_boundries_ls):\n",
        "    plt.axvline(sent_no, color='navy', alpha=0.1);\n",
        "\n",
        "  # Error check and assign DataFrame associated with each semantic_type\n",
        "  if semantic_type == 'section':\n",
        "    # Get midpoints of each Section\n",
        "    ts_df=corpus_sects_df\n",
        "    midpoints_ls = list(corpus_sects_df['sent_no_mid'])\n",
        "  elif semantic_type == 'chapter':\n",
        "    # Get midpoints of each Chapter\n",
        "    ts_df=corpus_chaps_df\n",
        "    midpoints_ls = list(corpus_chaps_df['sent_no_mid'])\n",
        "  else:\n",
        "    print(f\"ERROR: semantic_type={semantic_type} must be either 'section' or 'chapter'\")\n",
        "    return -1\n",
        "\n",
        "  # How many sentiment time series are we plotting?\n",
        "  if len(model_names_ls) == 1:\n",
        "    \n",
        "    # Plotting only one model\n",
        "    model_name_full = str(model_names_ls[0])\n",
        "    model_name_root = model_name_full.split('_')[0]\n",
        "    print(f'model_name_full: {model_name_full} and model_name_root: {model_name_root}')\n",
        "    if model_name_root in MODELS_LS:\n",
        "      # Plot\n",
        "      print(f'about to sns.lineplot model: ') # {ts_df}')\n",
        "      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "      # g._legend.remove()\n",
        "      # print(f'model_name_full={model_name_full}')\n",
        "      # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], markers=\"o\", alpha=0.5, label=model_name_full)\n",
        "    else:\n",
        "      print(f'ERROR: model_names_ls[0]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n",
        "      return -1\n",
        "\n",
        "    # If plotting only one model, add labels\n",
        "    midpoints_sentiment_ls = list(ts_df[model_name_full])\n",
        "    sect_ct = 0\n",
        "    for x,y in zip(midpoints_ls, midpoints_sentiment_ls): \n",
        "      label_token_int = int(label_token_ct)\n",
        "      if label_token_int < 0:\n",
        "        label = ''\n",
        "      elif label_token_int == 0:\n",
        "        # if arg label_token_ct == 0, just print sent_no\n",
        "        label = f\"#{x}({sect_ct})\"\n",
        "      else:\n",
        "        # if arg label_token_ct > 0, print the first label_token_ct words of sentence at crux point\n",
        "        label = f\"#{x}({sect_ct}) {' '.join(corpus_sents_df.iloc[x-1]['sent_raw'].split()[:label_token_int])}\"; # \\nPolarity: {y:.2f}'\n",
        "\n",
        "      # Save Crux point in crux_points_dt Dictionary if plotting Cruxes for a single/specific Model\n",
        "      crux_full_str = ' '.join(corpus_sents_df.iloc[x]['sent_raw'].split())\n",
        "      crux_points_dt[x] = [y, crux_full_str]\n",
        "\n",
        "      plt.annotate(label,\n",
        "                   (x,y),\n",
        "                   textcoords='offset points',\n",
        "                   xytext=(0,10),\n",
        "                   ha='center',\n",
        "                   rotation=90)\n",
        "      sect_ct += 1\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment ({model_name_full.capitalize()})\\n{subtitle_str}', x=title_xpos, y=title_ypos);\n",
        "    # Plot\n",
        "    plt.plot(midpoints_ls, midpoints_sentiment_ls, marker=\"o\", ms=6) # , markevery=[0,1])\n",
        "\n",
        "  else:\n",
        "    # If plotting multiple models\n",
        "    model_names_str = 'Multiple Models'\n",
        "    for i, model_name_full in enumerate(model_names_ls):\n",
        "      # Error check and assign correct model names\n",
        "      model_name_root = model_name_full.split('_')[0]\n",
        "      if model_name_root in MODELS_LS:\n",
        "        # Plot\n",
        "        g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "        # g._legend.remove()\n",
        "        # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], marker=\"o\", alpha=0.5, label=model_name_full)\n",
        "      else:\n",
        "        print(f'ERROR: model_names_ls[]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n",
        "        return -1\n",
        "\n",
        "      # Plot\n",
        "      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "      # g._legend.remove()\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment (Standardized Models)\\n{subtitle_str}', x=title_xpos, y=title_ypos)\n",
        "\n",
        "  # plt.legend(loc='best');\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    models_names_ls = [x[:2] for x in model_names_ls]\n",
        "    models_names_str = ''.join(models_names_ls)\n",
        "    plot_filename = f'plot_cruxes_{semantic_type}_{models_names_str}_{models_names_str}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return crux_points_dt"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUNMIlJKHyz3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b909379c-9c06-4191-aa2c-3003bb49bc6e"
      },
      "source": [
        "def plot_histogram(model_name='vader', text_unit='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a model, text_unit\n",
        "  Plot a Histogram using the default DataFrame\n",
        "  '''\n",
        "\n",
        "  if text_unit == 'sentence':\n",
        "    ts_df = corpus_sents_df\n",
        "\n",
        "  elif text_unit == 'paragraph':\n",
        "    ts_df = corpus_parags_df\n",
        "\n",
        "  elif text_unit == 'section':\n",
        "    ts_df = corpus_sects_df\n",
        "\n",
        "  elif text_unit == 'chapter':\n",
        "    ts_df = corpus_chaps_df\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n",
        "\n",
        "  sns.histplot(ts_df[model_name], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram {text_unit.capitalize()} Sentiment (Model {model_name.capitalize()})')\n",
        "  # get_smas(ts_df, model_name=model_name, text_unit=text_unit, win_ls=wins_def_ls)\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_hist_{text_unit}_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGTkfsSWFCeO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "242a5b9c-4c59-47ed-c9aa-e70905aa48ee"
      },
      "source": [
        "# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n",
        "\n",
        "def plot_raw_sections(ts_df='corpus_sents_df', model_name='vader', semantic_type='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, model_name column, semantic_type \n",
        "  Plot the raw sentiment types\n",
        "  Options to save2file\n",
        "  ''' \n",
        "  \n",
        "  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n",
        "  sns.lineplot(data=ts_df, x='sect_no', y=model_name, alpha=0.5).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_nostand_sects_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmrtifYoIjOT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "da6cbc1f-f020-4859-9db5-961252fb65ae"
      },
      "source": [
        "# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n",
        "\n",
        "def plot_raw_sentiments(model_name='vader', semantic_type='sentence', save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, model_name column, semantic_type \n",
        "  Plot the raw sentiment types\n",
        "  Options to save2file\n",
        "  ''' \n",
        "  \n",
        "  if semantic_type == 'sentence':\n",
        "    ts_df = corpus_sents_df\n",
        "    x_units = 'sent_no'\n",
        "  elif semantic_type == 'paragraph':\n",
        "    ts_df = corpus_parags_df\n",
        "    x_units = 'parag_no'\n",
        "  elif (semantic_type == 'section') | (semantic_type == 'section_stand'):\n",
        "    ts_df = corpus_sects_df\n",
        "    x_units = 'sect_no'\n",
        "  elif (semantic_type == 'chapter') | (semantic_type == 'chapter_stand'):\n",
        "    ts_df = corpus_chaps_df\n",
        "    x_units = 'chap_no'\n",
        "    \n",
        "  else:\n",
        "    print(f'ERROR: {semantic_type} must be sentence, paragraph or section')\n",
        "\n",
        "\n",
        "  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n",
        "  sns.lineplot(data=ts_df, x=x_units, y=model_name, alpha=0.5, label=model_name).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n",
        "  \n",
        "  plt.legend(loc='best')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_raw_sentiments_{semantic_type}_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB8ibstaeiVd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f722f0f5-3af4-4194-fab2-5cff24be2fa6"
      },
      "source": [
        "# TODO: must plot in order to save, cannot save without first plotting\n",
        "\n",
        "def get_lowess(ts_df='corpus_parags_df', models_ls=MODELS_LS, text_unit='paragraph', plot_subtitle='', alabel='', afrac=1./10, ait=5, alpha=0.5, do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n",
        "  Return a DataFrame with LOWESS values\n",
        "  If 'plot=True', also output plot\n",
        "  '''\n",
        "\n",
        "  # global corpus_all_df\n",
        "\n",
        "  lowess_df = pd.DataFrame()\n",
        "\n",
        "  # Step 1: Calculate LOWESS smoothed values\n",
        "  for i,acol in enumerate(models_ls):\n",
        "    sm_x, sm_y = sm_lowess(endog=ts_df[acol].values, exog=ts_df.index.values, frac=afrac, it=ait, return_sorted = True).T\n",
        "    col_new = f'{acol}_lowess'\n",
        "    lowess_df[col_new] = pd.Series(sm_y)\n",
        "    # Optionally plot LOWESS for all models\n",
        "    if do_plot:\n",
        "      if alabel == '':\n",
        "        alable == acol\n",
        "      plt.plot(sm_x, sm_y, label=alabel, alpha=alpha, linewidth=2)\n",
        "\n",
        "  lowess_df['median'] = lowess_df.median(axis=1) # sm_y # corpus_all_df[df_cols_ls].median(axis=1)\n",
        "  \n",
        "  # Step 2: Optionally plot LOWESS for median\n",
        "  if do_plot:\n",
        "    # sm_x, sm_y = sm_lowess(endog=lowess_df.median, exog=lowess_df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n",
        "    # plt.plot(sm_x, sm_y, label='median', alpha=0.9, linewidth=2, color='black')\n",
        "    \n",
        "    frac_str = str(round(100*afrac))\n",
        "    plt.title(f'{CORPUS_FULL} \\n {plot_subtitle} {text_unit} Standardized Sentiment Smoothed with LOWESS (frac={frac_str})')\n",
        "    plt.legend(title='Sentiment Model')\n",
        "\n",
        "  # Step 3: Optionally save to file\n",
        "  if save2file:\n",
        "    # Save Plot to file.\n",
        "    plot_filename = f'plot_{text_unit}_lowess_{plot_subtitle.split()[0].lower()}_{author_str}_{title_str}.png'\n",
        "    # plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plot_filename, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "\n",
        "  return lowess_df\n",
        "\n",
        "# Test\n",
        "'''\n",
        "new_lowess_col = f'{sa_model}_lowess'\n",
        "my_frac = 1./10\n",
        "my_frac_per = round(100*my_frac)\n",
        "new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n",
        "corpus_all_df[new_lowess_col] = plot_lowess(corpus_all_df, [sa_model], afrac=my_frac)\n",
        "corpus_all_df.head()\n",
        "''';"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cANCC2iz6nwo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "47cfdf05-b54f-4f1d-ef1d-589d16d50947"
      },
      "source": [
        "def get_sent2dets(sent_no):\n",
        "  '''\n",
        "  Given a Sentence Number\n",
        "  Return the corresponding Paragraph, Section and Chapter Numbers that contain it\n",
        "  '''\n",
        "\n",
        "  # Get Paragraph No containing given Sentence No\n",
        "  sent_parag_no = int(corpus_sents_df[corpus_sents_df['sent_no']==sent_no]['parag_no'])\n",
        "\n",
        "  # Get Section No containing given Sentence No.\n",
        "  corpus_sects_ls = list(corpus_sects_df['sect_no'])\n",
        "  for asect_no in corpus_sects_ls:\n",
        "    if (int(corpus_sects_df[corpus_sects_df['sect_no'] == asect_no]['sent_no_start']) > sent_no):\n",
        "      break\n",
        "    sent_sect_no = asect_no\n",
        "    # print(f'asect={asect_no}')\n",
        "\n",
        "  # Get Chapter No containing given Sentence No.\n",
        "  corpus_chaps_ls = list(corpus_chaps_df['chap_no'])\n",
        "  for achap_no in corpus_chaps_ls:\n",
        "    if (int(corpus_chaps_df[corpus_chaps_df['chap_no'] == achap_no]['sent_no_start']) > sent_no):\n",
        "      break\n",
        "    sent_chap_no = achap_no\n",
        "    # print(f'achap={achap_no}')\n",
        "\n",
        "\n",
        "  return sent_parag_no, sent_sect_no, sent_chap_no\n",
        "\n",
        "# Test\n",
        "# sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(1408)\n",
        "# print(f'sent_parag_no={sent_parag_no}\\nsent_sect_no={sent_sect_no}\\nsent_chap_no={sent_chap_no}')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sijR4OknJive",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e7122430-0df4-4ee1-b8fa-320ae2cfece6"
      },
      "source": [
        "def get_sentnocontext(sent_no=1, n_sideparags=1, sent_highlight=True):\n",
        "  '''\n",
        "  Given a sentence number in the Corpus\n",
        "  Return the containing paragraph and n-paragraphs on either side\n",
        "  (e.g. if n=2, return 2+1+2=5 paragraphs)\n",
        "  '''\n",
        "\n",
        "  parag_target_no = int(corpus_sents_df[corpus_sents_df['sent_no'] == sent_no]['parag_no'])\n",
        "  # print(f'parag_target_no = {parag_target_no} and type: {type(parag_target_no)}')\n",
        "\n",
        "  if n_sideparags == 0:\n",
        "    parags_context_ls = list(corpus_parags_df[corpus_parags_df['parag_no'] == parag_target_no]['parag_raw'])\n",
        "\n",
        "  else:\n",
        "    parag_start = parag_target_no - n_sideparags\n",
        "    parag_end = parag_target_no + n_sideparags + 1\n",
        "    parags_context_ls = list(corpus_parags_df.iloc[parag_start:parag_end]['parag_raw'])\n",
        "\n",
        "\n",
        "  if sent_highlight == True:\n",
        "    parag_match_str = str(parags_context_ls[n_sideparags])\n",
        "    # print(f'parag_match_str:\\n  {parag_match_str}')\n",
        "    sent_idx = sent_no\n",
        "    sent_str = (corpus_sents_df[corpus_sents_df['sent_no']==sent_idx]['sent_raw'].values)[0]\n",
        "    sent_str_up = sent_str.upper()\n",
        "    # print(f'sent_str:\\n  {sent_str}')\n",
        "    # parags_context_ls[n_sideparags] \n",
        "    parags_context_ls[n_sideparags] = parag_match_str.replace(sent_str, sent_str_up)\n",
        "\n",
        "  return parags_context_ls\n",
        "\n",
        "# Te\n",
        "# context_highlighted = get_sentnoparags(sent_no=1051, n_sideparags=1)\n",
        "# print(context_highlighted)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM_I8uDfJztH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a3da4015-97b2-4efa-bafe-838a669c65a3"
      },
      "source": [
        "def get_sentnocontext_report(the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n",
        "  '''\n",
        "  Wrapper function around  get_sentnocontext()\n",
        "  Prints a nicely formatted context report\n",
        "  '''\n",
        "\n",
        "  context_noparags = the_n_sideparags*2+1\n",
        "\n",
        "  # print('-------------------------------------------------------------')\n",
        "  print(f'The {context_noparags} Paragraph(s) Context around the Sentence #{Crux_Sentence_No} Crux Point:')\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f\"\\nCrux Sentence #{the_sent_no} Raw Text: -------------------------------\\n\\n    {str(corpus_sents_df[corpus_sents_df['sent_no'] == the_sent_no]['sent_raw'].values[0])}\\n\") # iloc[the_sent_no]['sent_raw']}\")\n",
        "\n",
        "  sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(the_sent_no)\n",
        "  print(f\"\\nCrux Sentence #{the_sent_no} is Contained in: ---------------------------\\n\\n    Paragraph #{sent_parag_no}\\n      Section #{sent_sect_no}\\n      Chapter #{sent_chap_no}\\n\")\n",
        "\n",
        "  print(f\"\\n{context_noparags} Paragraph(s) Context: ------------------------------\")\n",
        "  context_parags_ls = get_sentnocontext(sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_len = len(context_parags_ls)\n",
        "  context_mid = context_len//2\n",
        "  for i, aparag in enumerate(context_parags_ls):\n",
        "    if i==context_mid:\n",
        "      # print(f'\\n>>> Paragraph #{i}: <<< Crux Point Sentence CAPITALIZED within this Paragraph\\n\\n    {aparag}')\n",
        "      print(f'\\n<*> {aparag}')\n",
        "    else:\n",
        "      # print(f'\\n    Paragraph #{i}:\\n\\n    {aparag}')\n",
        "      print(f'\\n    {aparag}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# get_sentnocontext_report(sent_no=1051, n_sideparags=1, sent_highlight=True)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y04GiohGypNX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2f847b4f-5dd6-4d1c-9e84-d983faf06fc7"
      },
      "source": [
        "def get_section_timeseries(sect_no):\n",
        "  '''\n",
        "  Given a Section No in the current Corpus\n",
        "  Return the start,mid and ending Sent No for this Section as well as the Sentiment Time Series between the start/end Sentence for this Section\n",
        "  '''\n",
        "  \n",
        "  section_count = corpus_sects_df.shape[0]\n",
        "\n",
        "  # Compute the start, mid and end Sentence numbers for the selected Section\n",
        "  if Select_Section_No >= section_count:\n",
        "    print(f'ERROR: You picked Section #{Select_Section_No}.\\n  Section for this Corpus must be between 0 and {section_count-1}')\n",
        "    return -1\n",
        "\n",
        "  else:\n",
        "\n",
        "    # Get the starting and middle Sentence No of this Section\n",
        "    sect_sent_start = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_start'].values)\n",
        "    # sect_sent_mid = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_mid'].values)\n",
        "\n",
        "    # Calculate last Sentence No of this Section\n",
        "    if Select_Section_No == (section_count-1):   \n",
        "      print(f'You selected the last Section of this Corpus')\n",
        "      sect_sent_end = corpus_sents_df.shape[0] - 1\n",
        "    else:\n",
        "      sect_sent_end = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No+1]['sent_no_start'].values) # - 1\n",
        "      \n",
        "    print(f'Section #{sect_no}:----------')\n",
        "    print(f'\\nsect_sent_start: {sect_sent_start}')\n",
        "    # print(f'sect_sent_mid: {sect_sent_mid}')\n",
        "    print(f'sect_sent_end: {sect_sent_end}')\n",
        "\n",
        "\n",
        "  # Comput the start, and end Paragraph numbers for the selected Section\n",
        "  sect_parag_start = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_start]['parag_no'].values)\n",
        "  sect_parag_end = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_end]['parag_no'].values)\n",
        "\n",
        "  print(f'\\nsect_parag_start: {sect_parag_start}')\n",
        "  print(f'sect_parag_end: {sect_parag_end}')\n",
        "\n",
        "\n",
        "  # Extract and Return both a Sentence and Paragraph DataFrame for this Section \n",
        "\n",
        "  section_sents_df = corpus_sents_df.iloc[sect_sent_start:sect_sent_end]\n",
        "\n",
        "  section_parags_df = corpus_parags_df.iloc[sect_parag_start:sect_parag_end]\n",
        "\n",
        "\n",
        "  return section_sents_df, section_parags_df\n",
        "\n",
        "# Test\n",
        "\n",
        "# section_sents_df, section_parags_df = get_section_timeseries(Select_Section_No)\n",
        "\n",
        "# section_sents_df.head()\n",
        "\n",
        "# print(f'\\nsection_sents_df.shape: {section_sents_df.shape}')\n",
        "# print(f'section_parags_df.shape: {section_parags_df.shape}')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Et56z3-Jr1E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "adb16477-75b1-4ddb-925b-650f86963ac6"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_crux_points(col_series, semantic_type='sentence', win_lowess=5, do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  crux_ls = []\n",
        "\n",
        "  if semantic_type == 'sentence':\n",
        "    ts_df = corpus_sents_df\n",
        "    x_units = 'sent_no'\n",
        "  elif semantic_type == 'paragraph':\n",
        "    ts_df = corpus_parags_df\n",
        "    x_units = 'parag_no'\n",
        "  elif (semantic_type == 'section') | (semantic_type == 'section_stand'):\n",
        "    ts_df = corpus_sects_df\n",
        "    x_units = 'sect_no'\n",
        "  elif (semantic_type == 'chapter') | (semantic_type == 'chapter_stand'):\n",
        "    ts_df = corpus_chaps_df\n",
        "    x_units = 'chap_no'\n",
        "    \n",
        "  else:\n",
        "    print(f'ERROR: {semantic_type} must be sentence, paragraph or section')\n",
        "\n",
        "\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "\n",
        "  series_no_min = ts_df[x_units].min()\n",
        "  seires_no_max = ts_df[x_units].max()\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values\n",
        "\n",
        "  half_win = int((win_lowess/100)*series_len)\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win) # np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL}\\nRaw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig('argrelextrema.png')\n",
        "\n",
        "  return crux_coord_ls\n",
        "\n",
        "\n",
        "  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n",
        "  sns.lineplot(data=ts_df, x=x_units, y=model_name, alpha=0.5, label=model_name).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n",
        "  \n",
        "  plt.legend(loc='best')\n",
        "\n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_raw_sentiments_{semantic_type}_{model_name}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgFMgdQ3X33F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0d1edaf6-1372-43f1-f285-8a3adfc969b2"
      },
      "source": [
        "\n",
        "def get_crux_points(ts_df, col_series, text_type='sentence', win_lowess=5, y_height=0, do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "\n",
        "  sent_no_min = ts_df.sent_no.min()\n",
        "  sent_no_max = ts_df.sent_no.max()\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values\n",
        "\n",
        "  half_win = int((win_lowess/100)*series_len)\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  # Find valleys(min).\n",
        "  valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    if text_type == 'sentence':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(sent_no, y_height, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "    elif text_type == 'paragraph':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(aparag_no, y_height, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(aparag_no, color='blue', alpha=0.1)    \n",
        "    else:\n",
        "      print(f\"ERROR: text_type is {text_type} but must be either 'sentence' or 'paragarph'\")\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL}\\nRaw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig('argrelextrema.png')\n",
        "\n",
        "  return crux_coord_ls"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv376c5_bfrg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cc1842fb-9bc1-47c6-9759-beaa8c80b948"
      },
      "source": [
        "def crux_sortsents(crux_ls, top_n=3, get_peaks=True):\n",
        "  '''\n",
        "  Given a list of tuples (sent_no, sentiment value), top_n cruxes to retrieve and bool flag get_peaks\n",
        "  Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n",
        "  '''\n",
        "\n",
        "  crux_old_ls = []\n",
        "  crux_new_ls = []\n",
        "\n",
        "  crux_old_ls = sorted(crux_ls, key=lambda tup: (tup[1]), reverse=get_peaks)\n",
        "\n",
        "  if get_peaks == True:\n",
        "    crux_old_ls = [x for x in crux_old_ls if x[1] > 0]\n",
        "  else:\n",
        "    crux_old_ls = [x for x in crux_old_ls if x[1] < 0]\n",
        "\n",
        "  # Return only the n_top cruxes if more cruxes than n_top else return all cruxes\n",
        "  if len(crux_old_ls) >= top_n:\n",
        "    crux_old_ls = crux_old_ls[:top_n]\n",
        "\n",
        "  for asent_no, asentiment_val in crux_old_ls:\n",
        "    asent_raw = str(corpus_sents_df[corpus_sents_df['sent_no'] == asent_no]['sent_raw'].values[0])\n",
        "    crux_new_ls.append((int(asent_no), float(f'{asentiment_val:.3f}'), str(asent_raw),))\n",
        "\n",
        "  return crux_new_ls\n",
        "\n",
        "# Test\n",
        "# crux_n_top_ls = crux_sortsents(section_crux_ls, top_n=3, get_peaks=True)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFDjK1o8gnQ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8da391f0-ac84-410e-b45d-60b18fdd0e77"
      },
      "source": [
        "def crux_sortsents_report(crux_ls, top_n=3, get_peaks=True, n_sideparags=1):\n",
        "  '''\n",
        "  Wrapper function to produce report based upon 'crux_sortsents() described as:\n",
        "    Given a list of tuples (sent_no, sentiment value), top_n cruxes to retrieve and bool flag get_peaks\n",
        "    Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n",
        "  '''\n",
        "\n",
        "  crux_n_top_ls = crux_sortsents(crux_ls=crux_ls, top_n=top_n, get_peaks=get_peaks)\n",
        "\n",
        "  if get_peaks == True:\n",
        "    crux_label = 'Peak'\n",
        "  else:\n",
        "    crux_label = 'Valley'\n",
        "\n",
        "  print('------------------------------')\n",
        "  print(f'Section #{Select_Section_No} Top {top_n} {crux_label}s\\n')\n",
        "  for i,crux_sent_tup in enumerate(crux_n_top_ls):\n",
        "    print(f'   {crux_label} #{i} at Sentence #{crux_sent_tup[0]} with Sentiment Value {crux_sent_tup[1]}')\n",
        "  # print('------------------------------\\n')\n",
        "  # print('Sent_No  Sentiment   Sentence (Raw Text)\\n')\n",
        "  \n",
        "  for sent_no, sent_pol, sent_raw in crux_n_top_ls: \n",
        "    sent_no = int(sent_no)\n",
        "    print('\\n\\n-------------------------------------------------------------')\n",
        "    print(f'Sentence #{sent_no}   Sentiment: {sent_pol:.3f}\\n') #     {sent_raw}\\n')\n",
        "    # print('------------------------------')\n",
        "    get_sentnocontext_report(the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "    # get_sentnocontext(sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=Highlight_Crux_Sentence)\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJw2WDlwHH5y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6c13fe3e-792d-4873-fde4-ed296b95d924"
      },
      "source": [
        "# For the selected Section, create an expanded Paragraph DataFrame to match the number of Sentences in the Section\n",
        "\n",
        "def expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df', model_name='vader_lnorm_medianiqr'):\n",
        "  '''\n",
        "  Given a Corpus Paragraph DataFrame and a longer Sentence DataFrame that cover the same Section of a Corpus\n",
        "  Return an expanded version of the Paragraph DataFrame of equal length to the Sentence DataFrame so they can be plotted/compared along the same x-axis\n",
        "  '''\n",
        "\n",
        "  parag_sentiment_expanded_ls = []\n",
        "  parags_midpoint_ls = []\n",
        "  sent_sum = 0\n",
        "  parag_start = section_parags_df.parag_no.min()\n",
        "  print(f'parag_start: {parag_start}')\n",
        "  parag_end = section_parags_df.parag_no.max() + 1 # shape[0] + 3\n",
        "  print(f'parag_end: {parag_end}')\n",
        "  parags_range_ls = list(range(parag_start, parag_end, 1))\n",
        "  print(f'parags_range_ls: {parags_range_ls}')\n",
        "  for i, aparag_no in enumerate(parags_range_ls):\n",
        "    aparag_sentiment_fl = float(corpus_parags_df[corpus_parags_df['parag_no']==aparag_no][model_name])\n",
        "    sent_ct = len(corpus_sents_df[corpus_sents_df.parag_no == aparag_no])\n",
        "    parag_midpoint_int = int(sent_ct//2 + sent_sum)\n",
        "    parags_midpoint_ls.append(parag_midpoint_int)\n",
        "    for asent in range(sent_ct):\n",
        "      parag_sentiment_expanded_ls.append(aparag_sentiment_fl)\n",
        "    sent_sum += sent_ct\n",
        "    print(f'#{i}: Paragraph #{aparag_no} has {sent_ct} Sentences and Avg Sentiment: {aparag_sentiment_fl:.3f}')\n",
        "\n",
        "  print(f'\\nSentence Total: {sent_sum} vs Original section_sents_df: {section_sents_df.shape[0]}')\n",
        "  print(f'  Paragraph Sentiment length: {len(parag_sentiment_expanded_ls)}')\n",
        "\n",
        "  # section_sents_parags_df = section_sents_df.copy()\n",
        "  \n",
        "  # section_sents_parags_df.head(1);\n",
        "\n",
        "  # corpus_sents_df['']\n",
        "\n",
        "  return parag_sentiment_expanded_ls, parags_midpoint_ls\n",
        "\n",
        "# Test\n",
        "# section_sents_df['vader_lnorm_medianiqr_parag'] = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df')\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU3aHagiRjqR"
      },
      "source": [
        "# **Preprocess and Review Corpus Text (Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nok4C3snMjVK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7483aafd-65a7-4354-88ad-2f88dd7ae7a8"
      },
      "source": [
        "corpus_filename"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPOXcjDZSXrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d4bb2c1a-0a3f-42ba-ba94-66624053aa80"
      },
      "source": [
        "# TODO: Spell check and correct common OCR errors\n",
        "\n",
        "# SymSpellPy\n",
        "# JamSpell\n",
        "# OCR - https://github.com/Alvant/MIL-OCR"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtQmkG5hSXfZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a87a4883-9615-4eae-a2b7-cba168e159f2"
      },
      "source": [
        "# !pip install -U symspellpy"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gnibvUESa1M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e125bad1-136a-48e6-9f2b-449a05778949"
      },
      "source": [
        "# Did not need these\n",
        "\n",
        "# dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77m4NXkbSayg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "14aa6157-a306-452d-d24f-cb9ae4e794f2"
      },
      "source": [
        "\"\"\"\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# term_index is the column of the term and count_index is the\n",
        "# column of the term frequency\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# lookup suggestions for single-word input strings\n",
        "input_term = \"memebers\"  # misspelling of \"members\"\n",
        "input_term = \"summermorning\"\n",
        "# max edit distance per lookup\n",
        "# (max_edit_distance_lookup <= max_dictionary_edit_distance)\n",
        "suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST,\n",
        "                               max_edit_distance=2)\n",
        "# display suggestion term, term frequency, and edit distance\n",
        "for suggestion in suggestions:\n",
        "    print(suggestion)\n",
        "\n",
        "\n",
        "\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell\n",
        "\n",
        "# Set max_dictionary_edit_distance to avoid spelling correction\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "# term_index is the column of the term and count_index is the\n",
        "# column of the term frequency\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# a sentence without any spaces\n",
        "input_term = \"thequickbrownfoxjumpsoverthelazydog\"\n",
        "input_term = \"summermorning\"\n",
        "result = sym_spell.word_segmentation(input_term)\n",
        "print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n",
        "                          result.log_prob_sum))\n",
        "\"\"\";"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Y1D-q5kH7n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3e822f02-1417-4743-f93e-dd471d6b5da6"
      },
      "source": [
        "# Read corpus into a single string then split into CHAPTERs\n",
        "\n",
        "# corpus_path = f'{CORPUS_SUBDIR}/{CORPUS_FILENAME}'\n",
        "# corpus_path = f'./{CORPUS_FILENAME}'\n",
        "\n",
        "corpus_chaps_preraw_ls  = corpus2chaps(CORPUS_FILENAME)\n",
        "print(f'We found #{len(corpus_chaps_preraw_ls)} prelen sections\\n')\n",
        "\n",
        "corpus_chaps_raw_ls = [item for item in corpus_chaps_preraw_ls if len(item)>=MIN_CHAP_LEN]\n",
        "\n",
        "print(f'We found #{len(corpus_chaps_raw_ls)} chapters\\n')\n",
        "\n",
        "print('\\nThe first 3 Chapters of the Corpus begin:')\n",
        "print('-----------------------------------\\n')\n",
        "# corpus_chaps_raw_ls[:10]\n",
        "\n",
        "print('\\nThe last 3 Chapters of the Corpus begin:')\n",
        "print('-----------------------------------\\n')\n",
        "# corpus_chaps_raw_ls[-10:]\n",
        "print('\\n')\n",
        "\n",
        "n_shortest = 1\n",
        "print(f'The {n_shortest} shortest Chapters in the Corpus are:')\n",
        "print('--------------------------------------------')\n",
        "temp_ls = sorted(corpus_chaps_raw_ls, key=lambda x: (len(x), x))\n",
        "\"\"\"\n",
        "for i, asent in enumerate(temp_ls[:n_shortest]):\n",
        "  print(f'Shortest Section #{i}: {asent}')\n",
        "\"\"\"\n",
        "print(f'Chapter Count is {len(corpus_chaps_raw_ls)}')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "We found #11 prelen sections\n",
            "\n",
            "We found #10 chapters\n",
            "\n",
            "\n",
            "The first 3 Chapters of the Corpus begin:\n",
            "-----------------------------------\n",
            "\n",
            "\n",
            "The last 3 Chapters of the Corpus begin:\n",
            "-----------------------------------\n",
            "\n",
            "\n",
            "\n",
            "The 1 shortest Chapters in the Corpus are:\n",
            "--------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor i, asent in enumerate(temp_ls[:n_shortest]):\\n  print(f'Shortest Section #{i}: {asent}')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "stream",
          "text": [
            "Chapter Count is 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_l6UcZNnebv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f1c85726-76e3-4a0b-b065-1fabfccd8850"
      },
      "source": [
        "corpus_chaps_raw_ls[9][:100]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nOur immediate duty was to introduce Maxfield to the notion that I was not a robot and that I was g'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD_jkrnjIc6I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2b3eb8a8-063a-40af-b272-d2bf6f5b8482"
      },
      "source": [
        "# Read corpus into a single string then split into Sections (Chapter or sub-chapter divisions)\n",
        "\n",
        "# corpus_path = f'{CORPUS_SUBDIR}/{CORPUS_FILENAME}'\n",
        "# corpus_path = f'./{CORPUS_FILENAME}'\n",
        "\n",
        "corpus_sects_preraw_ls  = corpus2sects(CORPUS_FILENAME)\n",
        "print(f'We found #{len(corpus_sects_preraw_ls)} prelen sections\\n')\n",
        "\n",
        "'''\n",
        "corpus_sects_raw_ls = []\n",
        "for i,asect in enumerate(corpus_sects_preraw_ls):\n",
        "  if len(asect) > MIN_SECT_LEN:\n",
        "    print(f'Including Section #{i}: Length is {len(asect)}')\n",
        "    corpus_sects_raw_ls.append(asect)\n",
        "  else:\n",
        "    print(f'EXCLUDING Section #{i}: Length is {len(asect)}')\n",
        "'''\n",
        "\n",
        "corpus_sects_raw_ls = [item for item in corpus_sects_preraw_ls if len(item)>=MIN_SECT_LEN]\n",
        "\n",
        "print(f'We found #{len(corpus_sects_raw_ls)} sections\\n')\n",
        "\n",
        "print('\\nThe first 3 Sections of the Corpus:')\n",
        "print('-----------------------------------\\n')\n",
        "# corpus_sects_raw_ls[:10]\n",
        "\n",
        "print('\\nThe last 3 Sections of the Corpus:')\n",
        "print('-----------------------------------\\n')\n",
        "# corpus_sects_raw_ls[-10:]\n",
        "print('\\n')\n",
        "\n",
        "n_shortest = 2\n",
        "print(f'The {n_shortest} shortest Sections in the Corpus are:')\n",
        "print('--------------------------------------------')\n",
        "temp_ls = sorted(corpus_sects_raw_ls, key=lambda x: (len(x), x))\n",
        "for i, asent in enumerate(temp_ls[:n_shortest]):\n",
        "  print(f'Shortest Section #{i}: {asent}')\n",
        "\n",
        "print(f'Section Count is {len(corpus_sects_raw_ls)}')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "We found #31 prelen sections\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ncorpus_sects_raw_ls = []\\nfor i,asect in enumerate(corpus_sects_preraw_ls):\\n  if len(asect) > MIN_SECT_LEN:\\n    print(f'Including Section #{i}: Length is {len(asect)}')\\n    corpus_sects_raw_ls.append(asect)\\n  else:\\n    print(f'EXCLUDING Section #{i}: Length is {len(asect)}')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "stream",
          "text": [
            "We found #30 sections\n",
            "\n",
            "\n",
            "The first 3 Sections of the Corpus:\n",
            "-----------------------------------\n",
            "\n",
            "\n",
            "The last 3 Sections of the Corpus:\n",
            "-----------------------------------\n",
            "\n",
            "\n",
            "\n",
            "The 2 shortest Sections in the Corpus are:\n",
            "--------------------------------------------\n",
            "Shortest Section #0: \n",
            "\n",
            "When I entered the kitchen in the morning, later than usual, Adam's eyes were open. They were pale blue, flecked with minuscule vertical rods of black. The eyelashes were long and thick, like a child's. But his blink mechanism had not yet kicked in. It was set at irregular intervals and adjusted for mood and gestures, and primed to react to the actions and speech of others. Reluctantly, I would read the handbook into the night. He was equipped with a blink reflex to protect his eyes from flying objects. At present, his gaze was empty of meaning or intent and therefore unaffecting, as lifeless as the stare of a shop-window mannequin. So far, he was showing none of the fractional movements that warmly typify the human head. Elsewhere, no body language at all. When I felt for the pulse in his wrist, I found nothing--a heartbeat without a pulse. His arm was heavy to lift, resistant at the elbow joint, as though rigor mortis was about to set in.\n",
            "\n",
            "I turned my back on him and made coffee. Miranda was on my mind. Everything had changed. Nothing had changed.\n",
            "\n",
            "During my near-sleepless night, I would remember that she was visiting her father. She would have gone straight to Salisbury from her seminar. I saw her on her train from Waterloo, sitting with an unread book on her lap, staring at the rushing landscape, the dip and rise of telephone lines, not thinking of me. Or thinking only of me. Or recalling a boy at her seminar who'd tried to out-stare her.\n",
            "\n",
            "I watched the TV news on my phone. A brilliant mosaic in sound and sparkling seaside light. Portsmouth. The Task Force ready to depart. Most of the country was in a dream-theatre, in historical dress. Late medieval. Seventeenth century. Early nineteenth. Ruffs, hose, hooped skirts, powdered wigs, eyepatch, wooden legs. Accuracy was unpatriotic. Historically, we were special and the fleet was bound for success. TV and press encouraged a vague collective memory of enemies defeated-- the Spanish, the Dutch, the Germans twice this century, the French from Agincourt to Waterloo. A fly-past by fighter jets. A young man in combat gear, fresh out of Sandhurst, narrowed his eyes as he told an interviewer of the difficulties ahead. A superior officer spoke of his men's unshakeable resolve. I was moved, even as I disliked it. When a massed band of Highland pipers marched towards their ship's gangplank, my spirits swelled. Then back to the studio for charts, arrows, logistics, objectives, sane voices in agreement. For diplomatic moves. For the prime minister in her trim blue suit on the steps of Downing Street.\n",
            "\n",
            "I warmed to it, even though I often declared myself against it all. I loved my country. What a venture, what wild courage. Eight thousand miles. What decent people putting their lives at risk. I took a second coffee next door, made the bed to give the room the appearance of a workplace, and sat down to reflect a while on the state of the world's markets. The prospect of war had sent the FTSE down a further one per cent. Still in a patriotic mood, I assumed an Argie defeat and took a position on a toy and novelty group that made Union Jacks on sticks for people to wave. I also invested in two champagne importers, and bet on a big recovery generally. Merchant-navy ships had been requisitioned to transport troops to the South Atlantic. A friend who worked in asset management in the City told me that his company was predicting that some would be sunk. It made sense to short the major players in the insurance markets and invest in South Korean shipbuilders. I was in no mood for such cynicism.\n",
            "\n",
            "My desktop computer, bought second-hand from a Brixton junk shop, dated from the mid-sixties and was slow. It took me an hour to arrange the position on the flag-maker. I would have been quicker if Iâ€™d had my thoughts under control. When I was not thinking about Miranda and listening out for her footsteps in the flat above me, I was thinking about Adam and whether I should sell him off or start making decisions about his personality. I sold sterling and thought more about Adam. I bought gold and thought again about Miranda. I sat on the lavatory and wondered about Swiss francs. Over a third coffee I asked myself what else a victorious nation might spend its money on. Beef. Pubs. I V sets. I took positions on all three and felt virtuous, a part of the war effort. Soon it was time for lunch.\n",
            "\n",
            "I sat facing Adam again while I ate a cheese and pickle sandwich. Any further signs of life? Not at first glance. His gaze, directed over my left shoulder, was still dead. No movement. But five minutes later I glanced up by chance and was actually looking at him when he began to breathe. I heard first a series of rapid clicks, then a mosquito-like whine as his lips parted. For half a minute nothing happened, then his chin trembled and he made an authentic gulping sound as he snatched his first mouthful of air. He did not need oxygen, of course. That metabolic necessity was years away. His first exhalation was so long in coming that I stopped eating and tensely waited. It came at last--silently, through his nostrils. Soon his breathing assumed a steady rhythm, his chest expanded and contracted appropriately. I was spooked. With his lifeless eyes, Adam had the appearance of a breathing corpse.\n",
            "\n",
            "How much of life we ascribe to the eyes. If only his were closed, I thought, he would at least have the appearance of a man in a trance. I left my sandwich and went to stand by him and, out of curiosity, put my hand close to his mouth. His breath was moist and warm. Clever. In the user's manual I would read that he urinated once a day in the late morning. Also clever. As I went to close his right eye, my forefinger brushed against his eyebrow. He flinched and violently jerked his head away from me. Startled, I moved back. Then I waited. For twenty seconds or more nothing happened, then, with a smooth, soundless movement, infinitesimally slow, the tilt of his shoulders, the angle of his head moved towards their former positions. His rate of breathing was undisturbed. Mine and my pulse had accelerated. I was standing several feet away, fascinated by the way he settled back, like a balloon gently deflating. I decided against closing his eyes. While I was waiting for something more from him, I heard Miranda moving around in the flat upstairs. Back from Salisbury. Wandering in and out of her bedroom. Once again I felt the troubled thrill of undeclared love, and that was when I had the first stirrings of an idea.\n",
            "\n",
            "\n",
            "Shortest Section #1: \n",
            "\n",
            "Not long after I was thrown out of the legal profession I formed a company with two friends. The idea was to buy romantic apartments in Rome and Paris at local prices, do them up to a high standard, dress them with antique furniture and sell them to wealthy, cultured Americans or to agencies that would do the same. It was not exactly the quick route to our first million. Most cultured Americans were not rich. Those who were did not share our tastes. The work was complicated and exhausting, especially in Rome, where we had to learn how and whom to bribe among the officials in local government. In Paris it was the bureaucracy that wore us down.\n",
            "\n",
            "One weekend I flew to Rome to close a deal. It was important for this particular client that I stayed in his expensive hotel. This one was a well-established place at the top of the Spanish Steps. The client was staying there in a grand suite. I came into the city on a Friday evening, hot and harassed from my ride on a crowded airport bus. I was dressed in jeans and t-shirt, with a cheap Norwegian airline bag hanging from my shoulder. I stepped into a beautiful reception area. Just by chance, the manager happened to be standing by the check-in desk. He was not waiting for me--I was not important enough for that. I just happened to breeze in and since he was a courteous gentleman, extremely well dressed and correct, he welcomed me warmly in Italian to his hotel. I only partly understood what he was saying. His voice was expressionless, with little variation in pitch, and my Italian was poor. A receptionist came over and explained that the manager was congenitally deaf but he spoke nine languages, most of them European. Since childhood he would been adept at lip-reading. Rut before he could read mine I would have to indicate which language I was speaking. Otherwise he could not begin to understand me.\n",
            "\n",
            "He ran through his list. Norwegian? I shook my head. Finnish? English came fifth. He said he could have sworn I was a Nordic sort. So our conversation--pleasant, of no real consequence--could begin. Rut in theory, an entire world was open to us, and one piece of information had unlocked it all. Without it, his great gift could not come into play.\n",
            "\n",
            "Miranda's story was a version of such a key. Our conversation, in the form of our love, could properly begin. Her secretiveness, withdrawals and silence, her diffidence, that air she had of seeming older than her years, her tendency to drift out of reach, even in moments of tenderness, were forms of grieving. It pained me that she had carried her sadness alone. I admired the boldness and courage of her revenge. It was a dangerous plan, executed with such focus and brilliant disregard for consequences. I loved her more. I loved her poor friend. I would do everything to protect Miranda from this beast Gorringe. It touched me, to be the first to know her story.\n",
            "\n",
            "Telling it was a liberation for Miranda too. Half an hour after she had finished, when we were alone in the bedroom, she looped her arms around my neck, drew me to her and kissed me. We knew we were starting again. Adam was next door, charging up, lost to his thoughts. It was true, the old cliche about stress and desire. We undressed each other impatiently and, as usual, my plaster cast made me clumsy. Afterwards we lay on our sides, face to face. Her father still did not know what had happened. Miranda still had no contact with Mariam's family. The visits to the mosque had at first brought Mariam closer, then they seemed futile. She wished Gorringe had got a longer sentence. She remained tormented by her school-girlish vow of silence. A simple message, to Sana or Yasir or to a teacher, would have saved Mariam's life. The cruellest recollection, the one she tortured herself with, was when Sana, embracing her at the extremes of grief, had whispered the question in her ear. It was Sana who found Mariam in the bath. That imagined sight, the crimson water, the lithe brown body half submerged, was another torture, the because of night-long waking terrors and hideous dreams.\n",
            "\n",
            "Lying on the bed in the darkening room, lost to all else, we seemed to be heading towards the dawn. But it was not yet nine of the clock. Mostly she talked, I listened and asked occasional questions. Would Gorringe return to live in Salisbury? Yes. His parents were still away and he was living in the family house. Was Mariam's family still in town? No, they had moved to be closer to relatives in Leicester. Had she visited the grave? Many times, always approaching with caution in case one of the family was there. She always left flowers.\n",
            "\n",
            "In a long conversation it can be difficult to trace how or when the subject comes to shift. It may have been mention of Surayya, the love of Mariam's life. That little girl must have led us to Mark. Miranda said she missed him. I said I often thought about him. We had failed to find out where he was and what had happened. He had disappeared into the system, into a cloud of privacy regulation and the unreachable sanctuary of family law. We talked about luck, the hold it had over a child's life--what he is born into, whether he is loved and how intelligently.\n",
            "\n",
            "After a pause, Miranda said, \"And when it is all against him, whether someone can rescue him.\"\n",
            "\n",
            "I asked her if she thought her father's love came near to making up for her absent mother. She did not reply. Her breathing was suddenly rhythmic. In just a few seconds, she had fallen asleep and was curled against me. Gently, I rolled onto my back, staying as close to her as I could. In the half-light, the ceiling looked charmingly ancient rather than stained and disintegrating. I followed the jagged line of a crack that ran from a corner of the room towards the centre.\n",
            "\n",
            "If Adam had been driven by cogs and flywheels, I would have heard them turning in the silence that had followed Miranda's story. His arms were folded, his eyes were closed. The tough-guy look he had in repose, recently softened by adoration, appeared harshly reinstated. The flattened nose looked flatter still. The Bosphorus dockworker. What could it mean, to say that he was thinking? Sifting through remote memory banks? Logic gates flashing open and closed? Precedents retrieved, then compared, rejected or stored? Without self-awareness, it would not be thinking at all so much as data processing. But Adam had told me he was in love. He had haikus to prove it. Love was not possible without a self, and nor was thinking. I still had not settled this basic question. Perhaps it was beyond reach. No one would know what it was we had created. Whatever subjective life Adam and his kind possessed could not be ours to verify. In which case he was what was fashionably referred to as a black box--from the outside it seemed to work. That was as far as we would ever get.\n",
            "\n",
            "When Miranda had finished her story, there was the silence, and then we had talked. After a while, I had turned to Adam. \"Well?\"\n",
            "\n",
            "He took a few seconds, then he had said, \"Very dark.\"\n",
            "\n",
            "A rape, a suicide, a wrongly kept secret--of course it was dark. I was in an emotional state and I did not ask him to explain. Now, lying next to Miranda as she slept, I wondered if he meant something more significant, the consequence of his thinking, if that was really what it . . . depends on definitions .. . That was when I too fell asleep.\n",
            "\n",
            "Perhaps half an hour passed. What woke me was a sound outside the room. My arm in its cast was wedged uncomfortably against my side. Miranda had rolled away from me, into a deeper sleep. I heard the sound again, the familiar creak of a floorboard. My sleep had been light and I felt no anxiety, but the abrupt click of the door handle turning woke Miranda into a state of confusion and fear. She sat upright, one hand gripping mine.\n",
            "\n",
            "\"it is him,\" she whispered.\n",
            "\n",
            "I knew it could not be. \"it is fine,\" I said. I freed myself from her and stood to knot a towel around my waist. As I went towards the door it opened. It was Adam, offering me the kitchen phone.\n",
            "\n",
            "\"I did not want to disturb you,\" he said softly. \"But I think it is a call you would want to take.\"\n",
            "\n",
            "I closed the door on him and came back towards the bed with the phone against my ear.\n",
            "\n",
            "\"Mr. Charles Friend?\" The voice was tentative.\n",
            "\n",
            "\"Yes.\"\n",
            "\n",
            "\"I hope it is not too late to call. This is Alan Turing. We saw you briefly in Greek Street. I wondered if we might meet up for a chat.\"\n",
            "\n",
            "\n",
            "Section Count is 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK1cGWw_5eqn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "37d2b75a-bf26-4450-8808-f741fe8c0e6c"
      },
      "source": [
        "# Read corpus into a single string then split into paragraphs\n",
        "\n",
        "corpus_parags_raw_ls = corpus2parags(CORPUS_FILENAME)\n",
        "print(f'We found #{len(corpus_parags_raw_ls)} paragraphs\\n')\n",
        "\n",
        "print('\\nThe first 10 Paragraphs of the Corpus:')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_raw_ls[:10]\n",
        "\n",
        "print('\\nThe last 10 Paragraphs of the Corpus:')\n",
        "print('-----------------------------------\\n')\n",
        "corpus_parags_raw_ls[-10:]\n",
        "print('\\n')\n",
        "\n",
        "n_shortest = 10\n",
        "print(f'The {n_shortest} shortest Paragraphs in the Corpus are:')\n",
        "print('--------------------------------------------')\n",
        "temp_ls = sorted(corpus_parags_raw_ls, key=lambda x: (len(x), x))\n",
        "for i, asent in enumerate(temp_ls[:n_shortest]):\n",
        "  print(f'Shortest Paragraph #{i}: {asent}')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Corpus Paragraph Raw Count: 1455\n",
            "Corpus Paragraph -(whitespace only) Count: 1453\n",
            "Corpus Paragraph -(punctuation only) Count: 1428\n",
            "We found #1428 paragraphs\n",
            "\n",
            "\n",
            "The first 10 Paragraphs of the Corpus:\n",
            "-----------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['But remember, please, the Law by which we live, We are not built to comprehend a lie ...',\n",
              " '--Rudyard Kipling, \"The Secret of the Machines\"',\n",
              " 'CHAPTER 1',\n",
              " 'It was religious yearning granted hope, it was the holy grail of science. Our ambitions ran high and low--for a creation myth made real, for a monstrous act of self-love. As soon as it was feasible, we had no choice but to follow our desires and hang the consequences. In loftiest terms, we aimed to escape our mortality, confront or even replace the Godhead with a perfect self. More practically, we intended to devise an improved, more modern version of ourselves and exult in the joy of invention, the thrill of mastery. In the autumn of the twentieth century, it came about at last, the first step towards the fulfilment of an ancient dream, the beginning of the long lesson we would teach ourselves that however complicated we were, however faulty and difficult to describe in even our simplest actions and modes of being, we could be imitated and bettered. And I was there as a young man, an early and eager adopter in that chilly dawn.',\n",
              " 'But artificial humans were a cliche long before they arrived, so when they did, they seemed to some a disappointment. The imagination, fleeter than history, than technological advance, had already rehearsed this future in books, then films and TV dramas, as if human actors, walking with a certain glazed look, phony head movements, some stiffness in the lower back, could prepare us for life with our cousins from the future.',\n",
              " \"I was among the optimists, blessed by unexpected funds following my mother's death and the sale of the family home, which turned out to be on a valuable development site. The first truly viable manufactured human with plausible intelligence and looks, believable motion and shifts of expression, went on sale the week before the Falklands Task Force set off on its hopeless mission. Adam cost PS86,000. I brought him home in a hired van to my unpleasant flat in north Clapham. I would made a reckless decision, but I was encouraged by reports that Sir Alan Turing, war hero and presiding genius of the digital age, had taken delivery of the same model. Fie probably wanted to have his lab take it apart to examine its workings fully.\",\n",
              " 'Twelve of this first edition were called Adam, thirteen were called Eve. Corny, everyone agreed, but commercial. Notions of biological race being scientifically discredited, the twenty-five were designed to cover a range of ethnicities. There were rumours, then complaints, that the Arab could not be told apart from the Jew. Random programming as well as life experience would grant to all complete latitude in sexual preference. By the end of the first week, all the Eves sold out. At a careless glance, I might have taken my Adam for a Turk or a Greek. He weighed 170 pounds, so I had to ask my upstairs neighbour, Miranda, to help me carry him in from the street on the disposable stretcher that came with the purchase. While his batteries began to charge, I made us coffee, then scrolled through the 470-page online handbook. Its language was mostly clear and precise. But Adam was created across different agencies and in places the instructions had the charm of a nonsense poem. \"Unreveal upside of B347k vest to gain carefree emoticon with motherboard output to attenuate mood-swing penumbra.\"',\n",
              " \"At last, with cardboard and polystyrene wrapping strewn around his ankles, he sat naked at my tiny dining table, eyes closed, a black power line trailing from the entry point in his umbilicus to a thirteen-amp socket in the wall. It would take sixteen hours to fire him up. Then sessions of download updates and personal preferences. I wanted him now, and so did Miranda. Like eager young parents, we were avid for his first words. There was no loudspeaker cheaply buried in his chest. We knew from the excited publicity that he formed sounds with breath, tongue, teeth and palate. Already his lifelike skin was warm to the touch and as smooth as a child's. Miranda claimed to see his eyelashes flicker. I was certain she was seeing vibrations from the Tube trains rolling a hundred feet below us, but I said nothing.\",\n",
              " 'Adam was not a sex toy. However, he was capable of sex and possessed functional mucous membranes, in the maintenance of which he consumed half a litre of water each day. While he sat at the table, I observed that he was uncircumcised, fairly well endowed, with copious dark pubic hair. This highly advanced model of artificial human was likely to reflect the appetites of its young creators of code. The Adams and Eves, it was thought, would be lively.',\n",
              " 'He was advertised as a companion, an intellectual sparring partner, friend and factotum who could wash dishes, make beds and \"think.\" Every moment of his existence, everything he heard and saw, he recorded and could retrieve. He could not drive as yet and was not allowed to swim or shower or go out in the rain without an umbrella, or operate a chainsaw unsupervised. As for range, thanks to breakthroughs in electrical storage, he could run seventeen kilometres in two hours without a charge or, its energy equivalent, converse non-stop for twelve days. He had a working life of twenty years. He was compactly built, square-shouldered, dark-skinned, with thick black hair swept back; narrow in the face, with a hint of hooked nose suggestive of fierce intelligence, pensively hooded eyes, tight lips that, even as we watched, were draining of their deathly yellowish-white tint and acquiring rich human colour, perhaps even relaxing a little at the corners. Miranda said he resembled \"a docker from the Bosphorus.\"']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The last 10 Paragraphs of the Corpus:\n",
            "-----------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\"So--knowing not much about the mind, you want to embody an artificial one in social life. Machine learning can only take you so far. you will need to give this mind some rules to live by. How about a prohibition against lying? According to the Old Testament--Proverbs, I think--it is an abomination to God. But social life teems with harmless or even helpful untruths. How do we separate them out? Who is going to write the algorithm for the little white lie that spares the blushes of a friend? Or the lie that sends a rapist to prison who\\'d otherwise go free? We do not yet know how to teach machines to lie. And what about revenge? Permissible sometimes, according to you, if you love the person who is exacting it. Never, according to your Adam.\"',\n",
              " 'He paused and looked away from me again. From his profile, not only from his tone, I sensed a change was coming and my pulse was suddenly heavy. I could hear it in my ears. He proceeded calmly.',\n",
              " '\"My hope is that one day, what you did to Adam with a hammer will constitute a serious crime. Was it because you paid for him? Was that your entitlement?\"',\n",
              " 'He was looking at me, expecting an answer. I was not going to give one. If I did, I would have to lie. As his anger grew, so his voice grew quieter. I was intimidated. Holding his gaze was all I could do.',\n",
              " '\"You were not simply smashing up your own toy, like a spoiled child. You did not just negate an important argument for the rule of law. You tried to destroy a life. He was sentient. He had a self. How it is produced, wet neurons, microprocessors, DNA networks, it does not matter. Do you think we are alone with our special gift? Ask any dog owner. This was a good mind, Mr. Friend, better than yours or mine, I suspect. Here was a conscious existence and you did your best to wipe it out. I rather think I despise you for that. If it was down to me--\"',\n",
              " 'At that point, Turing\\'s desk phone rang. He snatched it up, listened, frowned. \"Thomas . . . Yes.\" He ran his palm across his mouth, and listened more. \"Well, I warned you . . .\" He broke off to look at me, or through me, and with a backhand wave dismissed me from his office. \"I have to take this.\" I went out into the corridor, then along it to be out of earshot. I felt unsteady and sickened. Guilt, in other words. He had drawn me in with a personal story and I would felt honoured. But it was merely a prelude. He softened me up, then delivered a materialist\\'s curse. It went through me. Like a blade. What sharpened it was that I understood. Adam was conscious. I would hovered near or in that position for a long time, then conveniently set it aside to do the deed. I should have told him how we mourned the loss, how Miranda had been tearful. I would forgotten to mention the last poem. How close we had leaned in to hear it. Between us, we had reconstructed it and written it down.',\n",
              " \"I could still hear him talking to Thomas Reah. I moved further away. I was beginning to doubt that I could face Turing again. He had delivered his judgement in tranquil tones that could barely conceal his contempt. What a twisted feeling it was, to be loathed by the man you most admired. Better to leave the building, walk away now. Without thinking, I put my hands in my pockets in search of change for a bus or the Tube. Nothing but a few coppers. I would spent the last of my money in the pub on Museum Street. I would have to walk to Vauxhall to collect the van. Its keys, I now discovered, were not in my pockets. If I would left them in Turing's office, I was not going back to retrieve them. I knew I should get going before he came off the phone. What a coward I was.\",\n",
              " 'But for the moment, I remained in the corridor, in a daze, sitting on a bench, staring through an open door opposite, trying to understand what it was, what it meant, to be accused of an attempted murder for which I would never stand trial. I took out my phone and saw Miranda\\'s text. \"Appeal success! Jasmin just brought Mark round. In bad state. Punched me. Kicked swore will not talk or let me touch him. Now having screaming fit. Complete meltdown. Come soon my love, M.\" We would find out for ourselves how long it would take Mark to forgive Miranda her long absence from his life. I felt oddly calm about the prospect--and confident. I owed something. Beyond my own concerns. A clear, clean purpose, to bring Mark back to that look he gave me across the jigsaw, to that carefree arm looped around Miranda\\'s neck, back to the generous space where he would dance again. From nowhere there came to me the image of a coin I once held in my hand, the Fields Medal, the highest distinction in mathematics, and the inscription, attributed to Archimedes. The translation read, \"Rise above yourself and grasp the world.\"',\n",
              " 'A minute passed before I realised that I was looking into the lab where the stainless-steel tables were. It seemed a long time since I would been there. In another life. I stood, paused, then, rejecting all thoughts of authority and permission, stepped in and approached. The long room, with its exposed industrial ceiling ducts and cables, remained fluorescent lit and was deserted but for a lab assistant busy at the far end. From the streets below came the sound of distant sirens and a repeated chant, hard to make out. Someone or something must go. I walked slowly, soundlessly, across the polished floor. Adam remained as he had been, lying on his back. His power line had been removed from his abdomen and trailed on the floor. The Charlie Parker head had gone and I was glad. I did not want to be in the line of that gaze.',\n",
              " \"I stood by Adam's side, and rested my hand on his lapel, above the stilled heart. Good cloth, was my irrelevant thought. I leaned over the table and looked down into the sightless cloudy green eyes. I had no particular intentions. Sometimes the body knows, ahead of the mind, what to do. I suppose I thought it was right to forgive him, despite the harm he had done to Mark, in the hope that he or the inheritor of his memories would forgive Miranda and me our terrible deed. Hesitating several seconds, I lowered my face over his and kissed his soft, all-too-human lips. I imagined some warmth in the flesh, and his hand coming up to touch my arm, as if to keep me there. I straightened and stood by the steel table, reluctant to leave. The streets below were suddenly silent. Above my head, the systems of the modern building murmured and growled like a living beast. My exhaustion welled up and my eyes closed briefly. In a moment of synaesthesia, jumbled phrases, scattered impulses of love and regret, became cascading curtains of coloured light that collapsed and folded then vanished. I was not too embarrassed to speak out loud to the dead to give shape and definition to my guilt. But I said nothing. The matter was too contorted. The next phase of my life, surely the most demanding, was already beginning. And I had lingered too long. Any moment, Turing would come out of his office to find me and damn me further. I turned away from Adam and walked the length of the lab at a pace without looking back. I ran along the empty corridor, found the emergency stairs, took them two at a time down into the street and set off on my journey southwards across London towards my troubled home.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The 10 shortest Paragraphs in the Corpus are:\n",
            "--------------------------------------------\n",
            "Shortest Paragraph #0: \"And?\"\n",
            "Shortest Paragraph #1: \"Wee.\"\n",
            "Shortest Paragraph #2: \"Why?\"\n",
            "Shortest Paragraph #3: \"Yes!\"\n",
            "Shortest Paragraph #4: \"Yes.\"\n",
            "Shortest Paragraph #5: \"Yes.\"\n",
            "Shortest Paragraph #6: \"Yes.\"\n",
            "Shortest Paragraph #7: \"Yes.\"\n",
            "Shortest Paragraph #8: \"Yes.\"\n",
            "Shortest Paragraph #9: \"Yes.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv8Vluu55enB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b4ffc25d-04b4-4c70-9a1f-1ddbe733edf9"
      },
      "source": [
        "# Tokenize Paragraphs into Sentences\n",
        "\n",
        "'''\n",
        "sent_no = 0\n",
        "# sent_base = 0\n",
        "corpus_sents_row_ls = []\n",
        "for parag_no,aparag in enumerate(corpus_parags_raw_ls):\n",
        "  sents_ls = sent_tokenize(aparag)\n",
        "  # Delete (whitespace only) sentences\n",
        "  sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n",
        "  # print(f'Corpus Sentences -(whitespace only) Count: {len(sents_ls)}')\n",
        "  # Delete (punctuation only) sentences\n",
        "  sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n",
        "  # print(f'Corpus Sentences -(punctuation only) Count: {len(sents_ls)}')\n",
        "  # sent_no = sent_base\n",
        "  for s,asent in enumerate(sents_ls):\n",
        "    corpus_sents_row_ls.append([sent_no, parag_no, asent])\n",
        "    sent_no += 1\n",
        "  # sent_base = sent_no \n",
        "\n",
        "\n",
        "print(f'{len(corpus_sents_row_ls)}')\n",
        "\n",
        "print(f'First row {corpus_sents_row_ls[0]}')\n",
        "print('\\n')\n",
        "print(f'Last row {corpus_sents_row_ls[-1]}')\n",
        "'''\n",
        "\n",
        "corpus_sents_row_ls = parag2sents(corpus_parags_raw_ls)\n",
        "print(f'{len(corpus_sents_row_ls)}')\n",
        "\n",
        "print(f'First row {corpus_sents_row_ls[0]}')\n",
        "print('\\n')\n",
        "print(f'Last row {corpus_sents_row_ls[-1]}');"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "7681\n",
            "First row [0, 0, 'But remember, please, the Law by which we live, We are not built to comprehend a lie ...']\n",
            "\n",
            "\n",
            "Last row [7680, 1427, 'I ran along the empty corridor, found the emergency stairs, took them two at a time down into the street and set off on my journey southwards across London towards my troubled home.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq4tmIjtI5WG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "210522ff-8fd2-4200-a4fe-9b2aa61d1bba"
      },
      "source": [
        "# Create Corpus Sentence DataFrame\n",
        "\n",
        "corpus_sents_df = pd.DataFrame(corpus_sents_row_ls)\n",
        "corpus_sents_df.columns = ['sent_no', 'parag_no', 'sent_raw']\n",
        "corpus_sents_df['sent_raw'] = corpus_sents_df['sent_raw'].astype('string')\n",
        "# Double check to drop any rows where raw Sentence is NaN or empty string ''\n",
        "corpus_sents_df.dropna(subset=['sent_raw'], inplace=True)\n",
        "\n",
        "\n",
        "print(f'First 10 Sentences of {CORPUS_FULL}')\n",
        "corpus_sents_df.head(10)\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "First 10 Sentences of Machines Like Me by: Ian McEwan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"But remember, please, the Law by which we live, We are not built to comprehend a lie ...\"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n\"--Rudyard Kipling, \\\"The Secret of the Machines\\\"\"],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n\"CHAPTER 1\"],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"It was religious yearning granted hope, it was the holy grail of science.\"],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"Our ambitions ran high and low--for a creation myth made real, for a monstrous act of self-love.\"],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"As soon as it was feasible, we had no choice but to follow our desires and hang the consequences.\"],\n [{\n            'v': 6,\n            'f': \"6\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"In loftiest terms, we aimed to escape our mortality, confront or even replace the Godhead with a perfect self.\"],\n [{\n            'v': 7,\n            'f': \"7\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"More practically, we intended to devise an improved, more modern version of ourselves and exult in the joy of invention, the thrill of mastery.\"],\n [{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"In the autumn of the twentieth century, it came about at last, the first step towards the fulfilment of an ancient dream, the beginning of the long lesson we would teach ourselves that however complicated we were, however faulty and difficult to describe in even our simplest actions and modes of being, we could be imitated and bettered.\"],\n [{\n            'v': 9,\n            'f': \"9\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"And I was there as a young man, an early and eager adopter in that chilly dawn.\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"number\", \"parag_no\"], [\"string\", \"sent_raw\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sent_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>But remember, please, the Law by which we live...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>--Rudyard Kipling, \"The Secret of the Machines\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>CHAPTER 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>It was religious yearning granted hope, it was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>Our ambitions ran high and low--for a creation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>As soon as it was feasible, we had no choice b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>In loftiest terms, we aimed to escape our mort...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>More practically, we intended to devise an imp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>In the autumn of the twentieth century, it cam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>And I was there as a young man, an early and e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sent_no  parag_no                                           sent_raw\n",
              "0        0         0  But remember, please, the Law by which we live...\n",
              "1        1         1    --Rudyard Kipling, \"The Secret of the Machines\"\n",
              "2        2         2                                          CHAPTER 1\n",
              "3        3         3  It was religious yearning granted hope, it was...\n",
              "4        4         3  Our ambitions ran high and low--for a creation...\n",
              "5        5         3  As soon as it was feasible, we had no choice b...\n",
              "6        6         3  In loftiest terms, we aimed to escape our mort...\n",
              "7        7         3  More practically, we intended to devise an imp...\n",
              "8        8         3  In the autumn of the twentieth century, it cam...\n",
              "9        9         3  And I was there as a young man, an early and e..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 7681 entries, 0 to 7680\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   sent_no   7681 non-null   int64 \n",
            " 1   parag_no  7681 non-null   int64 \n",
            " 2   sent_raw  7681 non-null   string\n",
            "dtypes: int64(2), string(1)\n",
            "memory usage: 240.0 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg4Q0nYNmGfj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9ff64753-ed76-4cbb-b7b2-e9cc31bfe0e2"
      },
      "source": [
        "corpus_sents_df[corpus_sents_df['sent_raw'].str.contains('summer')]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 1400,\n            'f': \"1400\",\n        },\n{\n            'v': 1400,\n            'f': \"1400\",\n        },\n{\n            'v': 235,\n            'f': \"235\",\n        },\n\"I went into my bedroom, undressed, leaving my clothes in a heap on my desk, and lay down naked under the summer duvet.\"],\n [{\n            'v': 1652,\n            'f': \"1652\",\n        },\n{\n            'v': 1652,\n            'f': \"1652\",\n        },\n{\n            'v': 272,\n            'f': \"272\",\n        },\n\"On the windowsill, where yellow gingham curtains hung still in our late-summer heat wave, a radio was playing the Beatles, recently regrouped after twelve years apart.\"],\n [{\n            'v': 1792,\n            'f': \"1792\",\n        },\n{\n            'v': 1792,\n            'f': \"1792\",\n        },\n{\n            'v': 292,\n            'f': \"292\",\n        },\n\"For me, it was far too early for bed and it was hot, like a summer evening in Marrakech.\"],\n [{\n            'v': 2095,\n            'f': \"2095\",\n        },\n{\n            'v': 2095,\n            'f': \"2095\",\n        },\n{\n            'v': 344,\n            'f': \"344\",\n        },\n\"Here she was beside me, close enough for me to feel her summer-morning body warmth.\"],\n [{\n            'v': 2613,\n            'f': \"2613\",\n        },\n{\n            'v': 2613,\n            'f': \"2613\",\n        },\n{\n            'v': 486,\n            'f': \"486\",\n        },\n\"The summer was hot and something was coming to the boil.\"],\n [{\n            'v': 2952,\n            'f': \"2952\",\n        },\n{\n            'v': 2952,\n            'f': \"2952\",\n        },\n{\n            'v': 559,\n            'f': \"559\",\n        },\n\"If the court would suspend proceedings and serve an order on the British subsidiary of the phone company to release its copies of the texts, these disputed versions of a summer evening would be settled.\"],\n [{\n            'v': 3698,\n            'f': \"3698\",\n        },\n{\n            'v': 3698,\n            'f': \"3698\",\n        },\n{\n            'v': 714,\n            'f': \"714\",\n        },\n\"Our last summer at school came around.\"],\n [{\n            'v': 3866,\n            'f': \"3866\",\n        },\n{\n            'v': 3866,\n            'f': \"3866\",\n        },\n{\n            'v': 725,\n            'f': \"725\",\n        },\n\"I would had two boyfriends by that summer and I knew what to do.\"],\n [{\n            'v': 4410,\n            'f': \"4410\",\n        },\n{\n            'v': 4410,\n            'f': \"4410\",\n        },\n{\n            'v': 801,\n            'f': \"801\",\n        },\n\"Now she was writing a short essay, to be read aloud in a summer-course seminar, that argued against empathy as a means of historical exploration.\"],\n [{\n            'v': 4676,\n            'f': \"4676\",\n        },\n{\n            'v': 4676,\n            'f': \"4676\",\n        },\n{\n            'v': 850,\n            'f': \"850\",\n        },\n\"I wanted to lie down on the worn-out grass of late summer and close my eyes.\"],\n [{\n            'v': 6243,\n            'f': \"6243\",\n        },\n{\n            'v': 6243,\n            'f': \"6243\",\n        },\n{\n            'v': 1182,\n            'f': \"1182\",\n        },\n\"He was happy in a borrowed summer frock.\"],\n [{\n            'v': 7201,\n            'f': \"7201\",\n        },\n{\n            'v': 7201,\n            'f': \"7201\",\n        },\n{\n            'v': 1383,\n            'f': \"1383\",\n        },\n\"It was the time of an exceptional Indian summer.\"],\n [{\n            'v': 7220,\n            'f': \"7220\",\n        },\n{\n            'v': 7220,\n            'f': \"7220\",\n        },\n{\n            'v': 1384,\n            'f': \"1384\",\n        },\n\"During the summer, I was drawn into the kind of labyrinthine bureaucracy I would have associated with the declining Ottoman Empire.\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"number\", \"parag_no\"], [\"string\", \"sent_raw\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sent_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1400</th>\n",
              "      <td>1400</td>\n",
              "      <td>235</td>\n",
              "      <td>I went into my bedroom, undressed, leaving my ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>1652</td>\n",
              "      <td>272</td>\n",
              "      <td>On the windowsill, where yellow gingham curtai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1792</th>\n",
              "      <td>1792</td>\n",
              "      <td>292</td>\n",
              "      <td>For me, it was far too early for bed and it wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2095</th>\n",
              "      <td>2095</td>\n",
              "      <td>344</td>\n",
              "      <td>Here she was beside me, close enough for me to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2613</th>\n",
              "      <td>2613</td>\n",
              "      <td>486</td>\n",
              "      <td>The summer was hot and something was coming to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2952</th>\n",
              "      <td>2952</td>\n",
              "      <td>559</td>\n",
              "      <td>If the court would suspend proceedings and ser...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3698</th>\n",
              "      <td>3698</td>\n",
              "      <td>714</td>\n",
              "      <td>Our last summer at school came around.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3866</th>\n",
              "      <td>3866</td>\n",
              "      <td>725</td>\n",
              "      <td>I would had two boyfriends by that summer and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4410</th>\n",
              "      <td>4410</td>\n",
              "      <td>801</td>\n",
              "      <td>Now she was writing a short essay, to be read ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4676</th>\n",
              "      <td>4676</td>\n",
              "      <td>850</td>\n",
              "      <td>I wanted to lie down on the worn-out grass of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6243</th>\n",
              "      <td>6243</td>\n",
              "      <td>1182</td>\n",
              "      <td>He was happy in a borrowed summer frock.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7201</th>\n",
              "      <td>7201</td>\n",
              "      <td>1383</td>\n",
              "      <td>It was the time of an exceptional Indian summer.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7220</th>\n",
              "      <td>7220</td>\n",
              "      <td>1384</td>\n",
              "      <td>During the summer, I was drawn into the kind o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      sent_no  parag_no                                           sent_raw\n",
              "1400     1400       235  I went into my bedroom, undressed, leaving my ...\n",
              "1652     1652       272  On the windowsill, where yellow gingham curtai...\n",
              "1792     1792       292  For me, it was far too early for bed and it wa...\n",
              "2095     2095       344  Here she was beside me, close enough for me to...\n",
              "2613     2613       486  The summer was hot and something was coming to...\n",
              "2952     2952       559  If the court would suspend proceedings and ser...\n",
              "3698     3698       714             Our last summer at school came around.\n",
              "3866     3866       725  I would had two boyfriends by that summer and ...\n",
              "4410     4410       801  Now she was writing a short essay, to be read ...\n",
              "4676     4676       850  I wanted to lie down on the worn-out grass of ...\n",
              "6243     6243      1182           He was happy in a borrowed summer frock.\n",
              "7201     7201      1383   It was the time of an exceptional Indian summer.\n",
              "7220     7220      1384  During the summer, I was drawn into the kind o..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3yjAh3qsEHV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "33d9a27a-d215-4bb0-ab05-97ff92b320f5"
      },
      "source": [
        "# Create Corpus Paragraph DataFrame\n",
        "\n",
        "parag_no_ls = []\n",
        "parag_raw_ls = []\n",
        "\n",
        "corpus_parags_df = pd.DataFrame()\n",
        "\n",
        "for i, aparag in enumerate(corpus_parags_raw_ls):\n",
        "  parag_no_ls.append(i)\n",
        "  parag_raw_ls.append(aparag)\n",
        "\n",
        "corpus_parags_df = pd.DataFrame(\n",
        "    {'parag_no': parag_no_ls,\n",
        "     'parag_raw': parag_raw_ls,\n",
        "    })\n",
        "\n",
        "# Test \n",
        "print(f'First 10 Paragraphs of {CORPUS_FULL}')\n",
        "corpus_parags_df.head(10)\n",
        "corpus_parags_df.info()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "First 10 Paragraphs of Machines Like Me by: Ian McEwan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"But remember, please, the Law by which we live, We are not built to comprehend a lie ...\"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n\"--Rudyard Kipling, \\\"The Secret of the Machines\\\"\"],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n\"CHAPTER 1\"],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"It was religious yearning granted hope, it was the holy grail of science. Our ambitions ran high and low--for a creation myth made real, for a monstrous act of self-love. As soon as it was feasible, we had no choice but to follow our desires and hang the consequences. In loftiest terms, we aimed to escape our mortality, confront or even replace the Godhead with a perfect self. More practically, we intended to devise an improved, more modern version of ourselves and exult in the joy of invention, the thrill of mastery. In the autumn of the twentieth century, it came about at last, the first step towards the fulfilment of an ancient dream, the beginning of the long lesson we would teach ourselves that however complicated we were, however faulty and difficult to describe in even our simplest actions and modes of being, we could be imitated and bettered. And I was there as a young man, an early and eager adopter in that chilly dawn.\"],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n\"But artificial humans were a cliche long before they arrived, so when they did, they seemed to some a disappointment. The imagination, fleeter than history, than technological advance, had already rehearsed this future in books, then films and TV dramas, as if human actors, walking with a certain glazed look, phony head movements, some stiffness in the lower back, could prepare us for life with our cousins from the future.\"],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n\"I was among the optimists, blessed by unexpected funds following my mother's death and the sale of the family home, which turned out to be on a valuable development site. The first truly viable manufactured human with plausible intelligence and looks, believable motion and shifts of expression, went on sale the week before the Falklands Task Force set off on its hopeless mission. Adam cost PS86,000. I brought him home in a hired van to my unpleasant flat in north Clapham. I would made a reckless decision, but I was encouraged by reports that Sir Alan Turing, war hero and presiding genius of the digital age, had taken delivery of the same model. Fie probably wanted to have his lab take it apart to examine its workings fully.\"],\n [{\n            'v': 6,\n            'f': \"6\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        },\n\"Twelve of this first edition were called Adam, thirteen were called Eve. Corny, everyone agreed, but commercial. Notions of biological race being scientifically discredited, the twenty-five were designed to cover a range of ethnicities. There were rumours, then complaints, that the Arab could not be told apart from the Jew. Random programming as well as life experience would grant to all complete latitude in sexual preference. By the end of the first week, all the Eves sold out. At a careless glance, I might have taken my Adam for a Turk or a Greek. He weighed 170 pounds, so I had to ask my upstairs neighbour, Miranda, to help me carry him in from the street on the disposable stretcher that came with the purchase. While his batteries began to charge, I made us coffee, then scrolled through the 470-page online handbook. Its language was mostly clear and precise. But Adam was created across different agencies and in places the instructions had the charm of a nonsense poem. \\\"Unreveal upside of B347k vest to gain carefree emoticon with motherboard output to attenuate mood-swing penumbra.\\\"\"],\n [{\n            'v': 7,\n            'f': \"7\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n\"At last, with cardboard and polystyrene wrapping strewn around his ankles, he sat naked at my tiny dining table, eyes closed, a black power line trailing from the entry point in his umbilicus to a thirteen-amp socket in the wall. It would take sixteen hours to fire him up. Then sessions of download updates and personal preferences. I wanted him now, and so did Miranda. Like eager young parents, we were avid for his first words. There was no loudspeaker cheaply buried in his chest. We knew from the excited publicity that he formed sounds with breath, tongue, teeth and palate. Already his lifelike skin was warm to the touch and as smooth as a child's. Miranda claimed to see his eyelashes flicker. I was certain she was seeing vibrations from the Tube trains rolling a hundred feet below us, but I said nothing.\"],\n [{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n\"Adam was not a sex toy. However, he was capable of sex and possessed functional mucous membranes, in the maintenance of which he consumed half a litre of water each day. While he sat at the table, I observed that he was uncircumcised, fairly well endowed, with copious dark pubic hair. This highly advanced model of artificial human was likely to reflect the appetites of its young creators of code. The Adams and Eves, it was thought, would be lively.\"],\n [{\n            'v': 9,\n            'f': \"9\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        },\n\"He was advertised as a companion, an intellectual sparring partner, friend and factotum who could wash dishes, make beds and \\\"think.\\\" Every moment of his existence, everything he heard and saw, he recorded and could retrieve. He could not drive as yet and was not allowed to swim or shower or go out in the rain without an umbrella, or operate a chainsaw unsupervised. As for range, thanks to breakthroughs in electrical storage, he could run seventeen kilometres in two hours without a charge or, its energy equivalent, converse non-stop for twelve days. He had a working life of twenty years. He was compactly built, square-shouldered, dark-skinned, with thick black hair swept back; narrow in the face, with a hint of hooked nose suggestive of fierce intelligence, pensively hooded eyes, tight lips that, even as we watched, were draining of their deathly yellowish-white tint and acquiring rich human colour, perhaps even relaxing a little at the corners. Miranda said he resembled \\\"a docker from the Bosphorus.\\\"\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"parag_no\"], [\"string\", \"parag_raw\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parag_no</th>\n",
              "      <th>parag_raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>But remember, please, the Law by which we live...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>--Rudyard Kipling, \"The Secret of the Machines\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>CHAPTER 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>It was religious yearning granted hope, it was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>But artificial humans were a cliche long befor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>I was among the optimists, blessed by unexpect...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>Twelve of this first edition were called Adam,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>At last, with cardboard and polystyrene wrappi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>Adam was not a sex toy. However, he was capabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>He was advertised as a companion, an intellect...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   parag_no                                          parag_raw\n",
              "0         0  But remember, please, the Law by which we live...\n",
              "1         1    --Rudyard Kipling, \"The Secret of the Machines\"\n",
              "2         2                                          CHAPTER 1\n",
              "3         3  It was religious yearning granted hope, it was...\n",
              "4         4  But artificial humans were a cliche long befor...\n",
              "5         5  I was among the optimists, blessed by unexpect...\n",
              "6         6  Twelve of this first edition were called Adam,...\n",
              "7         7  At last, with cardboard and polystyrene wrappi...\n",
              "8         8  Adam was not a sex toy. However, he was capabl...\n",
              "9         9  He was advertised as a companion, an intellect..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1428 entries, 0 to 1427\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   parag_no   1428 non-null   int64 \n",
            " 1   parag_raw  1428 non-null   object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 22.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYi6hDgiYQ82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "85cd1792-efa6-4ac9-ccbf-4b9e336b3b9b"
      },
      "source": [
        "# Create Corpus Section DataFrame\n",
        "\n",
        "sect_no_ls = []\n",
        "sect_raw_ls = []\n",
        "\n",
        "# corpus_sects_df = pd.DataFrame()\n",
        "\n",
        "for i, asect in enumerate(corpus_sects_raw_ls):\n",
        "  sect_no_ls.append(i)\n",
        "  sect_raw_ls.append(asect)\n",
        "\n",
        "\n",
        "corpus_sects_df = pd.DataFrame(\n",
        "    {'sect_no': sect_no_ls,\n",
        "     'sect_raw': sect_raw_ls,\n",
        "    })\n",
        "\n",
        "\n",
        "# Calculate the sentence number at the mid-point of each Section\n",
        "\n",
        "sect_mid_sentnos_ls = []\n",
        "sect_start_sentnos_ls = []\n",
        "sect_sentno_base = 0\n",
        "for i, sect_text in enumerate(corpus_sects_df.sect_raw):\n",
        "  if len(sect_text) > MIN_SECT_LEN:\n",
        "    sect_sents_ls = sent_tokenize(sect_text)\n",
        "    # Calc and save the sent_no that begins each Section\n",
        "    sect_first_sent = sect_sents_ls[0].strip()\n",
        "    # print(f'Searching for first sentence: {sect_first_sent}')\n",
        "    sect_start_sentnos_ls.append(int(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(sect_first_sent)]['sent_no']))\n",
        "    # Calc and save the sent_no in the middle of each Section\n",
        "    sect_sents_len = len(sect_sents_ls)\n",
        "    sect_mid_sentno = int(sect_sents_len/2) + sect_sentno_base\n",
        "    # print(f'Section #{i}: {len(sect_sents_ls)} Sentences, midpoint: {sect_mid_sentno}, cumulative midpoint: {sect_mid_sentno}')\n",
        "    sect_mid_sentnos_ls.append(sect_mid_sentno)\n",
        "    sect_sentno_base += sect_sents_len\n",
        "\n",
        "corpus_sects_df['sent_no_start'] = pd.Series(sect_start_sentnos_ls)\n",
        "corpus_sects_df['sent_no_mid'] = pd.Series(sect_mid_sentnos_ls)\n",
        "\n",
        "# Test \n",
        "print(f'First 2 Sections of {CORPUS_FULL}')\n",
        "# corpus_sects_df.head(2)\n",
        "corpus_sects_df.info()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "First 2 Sections of Machines Like Me by: Ian McEwan\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30 entries, 0 to 29\n",
            "Data columns (total 4 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   sect_no        30 non-null     int64 \n",
            " 1   sect_raw       30 non-null     object\n",
            " 2   sent_no_start  30 non-null     int64 \n",
            " 3   sent_no_mid    30 non-null     int64 \n",
            "dtypes: int64(3), object(1)\n",
            "memory usage: 1.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86zqJ3P-k1ze",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f40e9927-e887-4714-e5b9-e2d0ac669c0e"
      },
      "source": [
        "# Create Corpus Chapter DataFrame\n",
        "\n",
        "chap_no_ls = []\n",
        "chap_raw_ls = []\n",
        "\n",
        "# corpus_chaps_df = pd.DataFrame()\n",
        "\n",
        "for i, achap in enumerate(corpus_chaps_raw_ls):\n",
        "  chap_no_ls.append(i)\n",
        "  chap_raw_ls.append(achap)\n",
        "\n",
        "\n",
        "corpus_chaps_df = pd.DataFrame(\n",
        "    {'chap_no': chap_no_ls,\n",
        "     'chap_raw': chap_raw_ls,\n",
        "    })\n",
        "\n",
        "\n",
        "# Calculate the sentence number at the mid-point of each Chapter\n",
        "\n",
        "chap_mid_sentnos_ls = []\n",
        "chap_start_sentnos_ls = []\n",
        "chap_sentno_base = 0\n",
        "for i, chap_text in enumerate(corpus_chaps_df.chap_raw):\n",
        "  if len(chap_text) > MIN_CHAP_LEN:\n",
        "    chap_sents_ls = sent_tokenize(chap_text)\n",
        "    # Calc and save the sent_no that begins each Chapter\n",
        "    chap_first_sent = chap_sents_ls[0].strip()\n",
        "    # print(f'Searching for first sentence: {chap_first_sent}')\n",
        "    chap_start_sentnos_ls.append(int(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(chap_first_sent)]['sent_no']))\n",
        "    # Calc and save the sent_no in the middle of each Chapter\n",
        "    chap_sents_len = len(chap_sents_ls)\n",
        "    chap_mid_sentno = int(chap_sents_len/2) + chap_sentno_base\n",
        "    # print(f'Chapter #{i}: {len(chap_sents_ls)} Sentences, midpoint: {chap_mid_sentno}, cumulative midpoint: {chap_mid_sentno}')\n",
        "    chap_mid_sentnos_ls.append(chap_mid_sentno)\n",
        "    chap_sentno_base += chap_sents_len\n",
        "\n",
        "corpus_chaps_df['sent_no_start'] = pd.Series(chap_start_sentnos_ls)\n",
        "corpus_chaps_df['sent_no_mid'] = pd.Series(chap_mid_sentnos_ls)\n",
        "\n",
        "# Test \n",
        "print(f'First 2 Chapters of {CORPUS_FULL}')\n",
        "# corpus_chaps_df.head(2)\n",
        "corpus_chaps_df.info()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "First 2 Chapters of Machines Like Me by: Ian McEwan\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 4 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   chap_no        10 non-null     int64 \n",
            " 1   chap_raw       10 non-null     object\n",
            " 2   sent_no_start  10 non-null     int64 \n",
            " 3   sent_no_mid    10 non-null     int64 \n",
            "dtypes: int64(3), object(1)\n",
            "memory usage: 448.0+ bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5Q2wLFgVr-3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "28135b36-3610-406f-a3cb-39cb2f9edbd2"
      },
      "source": [
        "print(corpus_sents_df.iloc[237]['sent_raw'])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "At thirty-two, I was completely broke.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcejIS2vexaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6b48e893-ba2e-4bae-e956-13dcf8778da7"
      },
      "source": [
        "corpus_sects_df.head(2)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"\\n\\nIt was religious yearning granted hope, it was the holy grail of science. Our ambitions ran high and low--for a creation myth made real, for a monstrous act of self-love. As soon as it was feasible, we had no choice but to follow our desires and hang the consequences. In loftiest terms, we aimed to escape our mortality, confront or even replace the Godhead with a perfect self. More practically, we intended to devise an improved, more modern version of ourselves and exult in the joy of invention, the thrill of mastery. In the autumn of the twentieth century, it came about at last, the first step towards the fulfilment of an ancient dream, the beginning of the long lesson we would teach ourselves that however complicated we were, however faulty and difficult to describe in even our simplest actions and modes of being, we could be imitated and bettered. And I was there as a young man, an early and eager adopter in that chilly dawn.\\n\\nBut artificial humans were a cliche long before they arrived, so when they did, they seemed to some a disappointment. The imagination, fleeter than history, than technological advance, had already rehearsed this future in books, then films and TV dramas, as if human actors, walking with a certain glazed look, phony head movements, some stiffness in the lower back, could prepare us for life with our cousins from the future.\\n\\nI was among the optimists, blessed by unexpected funds following my mother's death and the sale of the family home, which turned out to be on a valuable development site. The first truly viable manufactured human with plausible intelligence and looks, believable motion and shifts of expression, went on sale the week before the Falklands Task Force set off on its hopeless mission. Adam cost PS86,000. I brought him home in a hired van to my unpleasant flat in north Clapham. I would made a reckless decision, but I was encouraged by reports that Sir Alan Turing, war hero and presiding genius of the digital age, had taken delivery of the same model. Fie probably wanted to have his lab take it apart to examine its workings fully.\\n\\nTwelve of this first edition were called Adam, thirteen were called Eve. Corny, everyone agreed, but commercial. Notions of biological race being scientifically discredited, the twenty-five were designed to cover a range of ethnicities. There were rumours, then complaints, that the Arab could not be told apart from the Jew. Random programming as well as life experience would grant to all complete latitude in sexual preference. By the end of the first week, all the Eves sold out. At a careless glance, I might have taken my Adam for a Turk or a Greek. He weighed 170 pounds, so I had to ask my upstairs neighbour, Miranda, to help me carry him in from the street on the disposable stretcher that came with the purchase. While his batteries began to charge, I made us coffee, then scrolled through the 470-page online handbook. Its language was mostly clear and precise. But Adam was created across different agencies and in places the instructions had the charm of a nonsense poem. \\\"Unreveal upside of B347k vest to gain carefree emoticon with motherboard output to attenuate mood-swing penumbra.\\\"\\n\\nAt last, with cardboard and polystyrene wrapping strewn around his ankles, he sat naked at my tiny dining table, eyes closed, a black power line trailing from the entry point in his umbilicus to a thirteen-amp socket in the wall. It would take sixteen hours to fire him up. Then sessions of download updates and personal preferences. I wanted him now, and so did Miranda. Like eager young parents, we were avid for his first words. There was no loudspeaker cheaply buried in his chest. We knew from the excited publicity that he formed sounds with breath, tongue, teeth and palate. Already his lifelike skin was warm to the touch and as smooth as a child's. Miranda claimed to see his eyelashes flicker. I was certain she was seeing vibrations from the Tube trains rolling a hundred feet below us, but I said nothing.\\n\\nAdam was not a sex toy. However, he was capable of sex and possessed functional mucous membranes, in the maintenance of which he consumed half a litre of water each day. While he sat at the table, I observed that he was uncircumcised, fairly well endowed, with copious dark pubic hair. This highly advanced model of artificial human was likely to reflect the appetites of its young creators of code. The Adams and Eves, it was thought, would be lively.\\n\\nHe was advertised as a companion, an intellectual sparring partner, friend and factotum who could wash dishes, make beds and \\\"think.\\\" Every moment of his existence, everything he heard and saw, he recorded and could retrieve. He could not drive as yet and was not allowed to swim or shower or go out in the rain without an umbrella, or operate a chainsaw unsupervised. As for range, thanks to breakthroughs in electrical storage, he could run seventeen kilometres in two hours without a charge or, its energy equivalent, converse non-stop for twelve days. He had a working life of twenty years. He was compactly built, square-shouldered, dark-skinned, with thick black hair swept back; narrow in the face, with a hint of hooked nose suggestive of fierce intelligence, pensively hooded eyes, tight lips that, even as we watched, were draining of their deathly yellowish-white tint and acquiring rich human colour, perhaps even relaxing a little at the corners. Miranda said he resembled \\\"a docker from the Bosphorus.\\\"\\n\\nBefore us sat the ultimate plaything, the dream of ages, the triumph of humanism--or its angel of death. Exciting beyond measure, but frustrating too. Sixteen hours was a long time to be waiting and watching. I thought that for the sum I would handed over after lunch, Adam should have been charged up and ready to go. It was a wintry late afternoon. I made toast and we drank more coffee. Miranda, a doctoral scholar of social history, said she wished the teenage Mary Shelley was here beside us, observing closely, not a monster like Frankenstein's, but this handsome dark-skinned young man coming to life. I said that what both creatures shared was a hunger for the animating force of electricity.\\n\\n\\\"We share it too.\\\" She spoke as though she was referring only to herself and me, rather than all of electrochemically charged humanity.\\n\\nShe was twenty-two, mature for her years and ten years younger than me. From a long perspective, there was not much between us. We were gloriously young. But I considered myself at a different stage of life. My formal education was far behind me. I would suffer a series of professional and financial and personal failures. I regarded myself as too hard-bitten, too cynical for a lovely young woman like Miranda. And though she was beautiful, with pale brown hair and a long thin face, and eyes that often appeared narrowed by suppressed mirth, and though in certain moods I looked at her in wonder, I would decide early on to confine her in the role of kind, neighbourly friend. We shared an entrance hall and her tiny apartment was right over mine. We saw each other for a coffee now and then to talk about relationships and politics and all the rest. With pitch-perfect neutrality she gave the impression of being at ease with the possibilities. To her, it seemed, an afternoon of intimate pleasure with me would have weighed equally with a chaste and companionable chat. She was relaxed in my company and I preferred to think that sex would ruin everything. We remained good chums. But there was something alluringly secretive or restrained about her. Perhaps, without knowing it, I had been in love with her for months. Without knowing it? What a flimsy formulation that was!\\n\\nReluctantly, we agreed to turn our backs on Adam and on each other for a while. Miranda had a seminar to attend north of the river, I had emails to write. By the early seventies, digital communication had discarded its air of convenience and become a daily chore. Likewise the 250 mph trains--crowded and dirty. Speech-recognition software, a fifties miracle, had long turned to drudge, with entire populations sacrificing hours each day to lonely soliloquising. Brain--machine interfacing, wild fruit of sixties optimism, could barely arouse the interest of a child. What people queued the entire weekend for became, six months later, as interesting as the socks on their feet. What happened to the cognition-enhancing helmets, the speaking fridges with a sense of smell? Gone the way of the mouse pad, the Filofax, the electric carving knife, the fondue set. The future kept arriving. Our bright new toys began to rust before we could get them home, and life went on much as before.\\n\\nWould Adam become a bore? It is not easy to dictate while trying to ward off a bout of buyer's remorse. Surely other people, other minds, must continue to fascinate us. As artificial people became more like us, then became us, then became more than us, we could never tire of them. They were bound to surprise us. They might fail us in ways that were beyond our imagining. Tragedy was a possibility, but not boredom. What was tedious was the prospect of the user's guide. Instructions. My prejudice was that any machine that could not tell you by its very functioning how it should be used was not worth its keep. On an old-fashioned impulse, I was printing out the manual, then looking for a folder. All the while, I continued to dictate emails.\\n\\nI could not think of myself as Adam's \\\"user.\\\" I would assume there was nothing to learn about him that he could not teach me himself. But the manual in my hands had fallen open at Chapter Fourteen. Here the English was plain: preferences; personality parameters. Then a set of headings--Agreeableness. Extraversion. Openness to experience. Conscientiousness. Emotional stability. The list was familiar to me. The Five Factor model. Educated as I was in the humanities, I was suspicious of such reductive categories, though I knew from a friend in psychology that each item had many subgroups. Glancing at the next page I saw that I was supposed to select various settings on a scale of one to ten.\\n\\nI had been expecting a friend. I was ready to treat Adam as a guest in my home, as an unknown I would come to know. I would have thought he would arrive optimally adjusted. Factory settings--a contemporary synonym for fate. My friends, family and acquaintances all had appeared in my life with fixed settings, with unalterable histories of genes and environment. I wanted my expensive new friend to do the same. Why leave it to me? But of course I knew the answer. Not many of us are optimally adjusted. Gentle Jesus? Humble Darwin? One every 1,800 years. Even if it knew the best, the least harmful parameters of personality, which it could not, a worldwide corporation with a precious reputation could not risk a mishap. Caveat emptor.\\n\\nGod had once delivered a fully formed companion for the benefit of the original Adam. I had to devise one for myself. Here was Extraversion and a graded set of childish statements. He loves to be the life and soul of the party and He knows how to entertain people and lead them. And at the bottom, He feels uncomfortable around other people and He prefers his own company. Here in the middle was, He likes a good party but he is always happy to come home. This was me. But should I be replicating myself? If I was to choose from the middle of each scale I might devise the soul of blandness. Extraversion appeared to include its antonym. There was a long adjectival list with boxes to tick: outgoing, shy, excitable, talkative, withdrawn, boastful, modest, bold, energetic, moody. I wanted none of them, not for him, not for myself.\\n\\nApart from my moments of crazed decisions, I passed most of my life, especially when alone, in a state of mood neutrality, with my personality, whatever that was, in suspension. Not bold, not withdrawn. Simply here, neither content nor morose, but carrying out tasks, thinking about dinner or sex, staring at the screen, taking a shower. Intermittent regrets about the past, occasional forebodings about the future, barely aware of the present, except in the obvious sensory realm. Psychology, once so interested in the trillion ways the mind goes awry, was now drawn to what it considered the common emotions, from grief to joy. But it had overlooked a vast domain of everyday existence: absent illness, famine, war or other stresses, a lot of life is lived in the neutral zone, a familiar garden, but a grey one, unremarkable, immediately forgotten, hard to describe. At the time, I was not to know that these graded options would have little effect on Adam. The real determinant was what was known as \\\"machine learning.\\\" The user's handbook merely granted an illusion of influence and control, the kind of illusion parents have in relation to their children's personalities. It was a way of binding me to my purchase and providing legal protection for the manufacturer. \\\"Take your time,\\\" the manual advised. \\\"Choose carefully. Allow yourself several weeks, if necessary.\\\"\\n\\nI let half an hour pass before I checked on him again. No change. Still at the table, arms pushed out straight before him, eyes closed. But I thought his hair, deepest black, was bulked out a little and had acquired a certain shine, as though he would just had a shower. Stepping closer, I saw to my delight that though he was not breathing, there was, by his left breast, a regular pulse, steady and calm, about one a second by my inexperienced guess. How reassuring. He had no blood to pump around, but this simulation had an effect. My doubts faded just a little. I felt protective towards Adam, even as I knew how absurd it was. I stretched out my hand and laid it over his heart and felt against my palm its calm, iambic tread. I sensed I was violating his private space. These vital signs were easy to believe in. The warmth of his skin, the firmness and yield of the muscle below it--my reason said plastic or some such, but my touch responded to flesh.\\n\\nIt was eerie, to be standing by this naked man, struggling between what I knew and what I felt. I walked behind him, partly to be out of range of eyes that could open at any moment and find me looming over him. He was muscular around his neck and spine. Dark hair grew along the line of his shoulders. His buttocks displayed muscular concavities. Below them, an athlete's knotted calves. I had not wanted a superman. I regretted once more that I would been too late for an Eve.\\n\\nOn my way out of the room I paused to look back and experienced one of those moments that can derange the emotional life: a startling realisation of the obvious, an absurd leap of understanding into what one already knows. I stood with one hand resting on the doorknob. It must have been Adam's nakedness and physical presence that prompted the insight, but I was not looking at him. It was the butter dish. Also, two plates and cups, two knives and two spoons scattered across the table. The remains of my long afternoon with Miranda. Two wooden chairs were pushed back from the table, turned companionably towards each other.\\n\\nWe had become closer this past month. We talked easily. I saw how precious she was to me and how carelessly I could lose her. I should have said something by now. I would taken her for granted. Some unfortunate event, some person, a fellow student, could get between us. Her face, her voice, her manner, both reticent and clear-headed, were sharply present. The feel of her hand in mine, that lost, preoccupied manner she had. Yes, we had become very close and I would have failed to notice it was happening. I was an idiot. I had to tell her.\\n\\nI went back into my office, which doubled as my bedroom. Between the desk and the bed there was enough space in which to walk up and down. That she knew nothing about my feelings was now an anxious matter. Describing them would be embarrassing, perilous. She was a neighbour, a friend, a kind of sister. I would be addressing a person I did not yet know. She would be obliged to step out from behind a screen, or remove a mask and speak to me in terms I had never heard from her. I am so sorry... I like you very much but, you see ... Or she would be horrified. Or, just possibly, overjoyed to hear the one thing she had longed for, or to say to herself but dreaded rejection. By chance, we were currently both free. She must have thought about it, about us. It was not an impossible fantasy. I would have to tell her face to face. Unbearable. Unavoidable. And so it went on, in tightening cycles. Restless, I went back next door. I saw no change in Adam as I brushed past to get to the fridge, where there was a half-full bottle of white Bordeaux. I sat facing him and raised my glass. To love. This time, I felt less tenderness. I saw Adam for what it was, an inanimate confection whose heartbeat was a regular electrical discharge, whose skin warmth was mere chemistry. When activated, some kind of microscopic balance-wheel device would prise open his eyes. He would seem to see me, but he would be blind. Not even blind. When it kicked in, another system would give a semblance of breath, but not of life. A man newly in love knows what life is.\\n\\nWith the inheritance, I could have bought a place somewhere north of the river, Notting Hill, or Chelsea. She might even have joined me. She would have had space for all the books that were boxed up in her father's garage in Salisbury. I saw a future without Adam, the future that was mine until yesterday: an urban garden, high ceilings with plaster mouldings, stainless-steel kitchen, old friends to dinner. Books everywhere. What to do? I could take him, or it, back, or sell it online and take a small loss. I gave it a hostile look. The hands were palms down on the table, the hawkish face remained angled towards the hands. My foolish infatuation with technology! Another fondue set. Best to get away from the table before I impoverished myself with a single swipe of my father's old claw hammer.\\n\\nI drank no more than half a glass, then I returned to the bedroom to distract myself with the Asian currency markets. All the while I listened out for footsteps in the flat above me. Late into the evening, I watched TV to catch up on the Task Force that would soon set off across 8,000 miles of ocean to recapture what we then called the Falkland Islands.\\n\\n\",\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 117,\n            'f': \"117\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n\"\\n\\nAt thirty-two, I was completely broke. Wasting my mother's inheritance on a gimmick was only one part of my problem--but typical of it. Whenever money came my way, I caused it to disappear, made a magic bonfire of it, stuffed it into a top hat and pulled out a turkey. Often, though not in this recent case, my intention was to conjure a far larger sum with minimal effort. I was a mug for schemes, semilegal ruses, cunning shortcuts. I was for grand and brilliant gestures. Others made them and flourished. They borrowed money, put it to interesting use and remained enriched even as they settled their debts. Or they had jobs, professions, as I once had, and enriched themselves more modestly, at a steady rate. I meanwhile leveraged or, rather, shorted myself into genteel ruin, into two damp ground-floor rooms in the dull no-man's land of Edwardian terraced streets between Stockwell and Clapham, south London.\\n\\nI grew up in a village near Stratford, Warwickshire, the only child of a musician father and community-nurse mother. Compared to Miranda's, my childhood was culturally undernourished. There was no time or space for books, or even music. I took a precocious interest in electronics but ended up with an anthropology degree from an unregarded college in the south Midlands; I did a conversion course to law and, once qualified, specialised in tax. A week after my twenty-ninth birthday I was struck off, and came close to a short spell in prison. My hundred hours of community service convinced me that I should never have a regular job again. I made some money out of a book I wrote at high speed on artificial intelligence: lost to a life-extension-pill scheme. I made a reasonable sum on a property deal: lost to a car-rental scheme. I was left some funds by a favourite uncle who had prosWhispered by way of a heat-pump patent: lost to a medical-insurance scheme.\\n\\nAt thirty-two, I was surviving by playing the stock and currency markets online. A scheme, just like the rest. For seven hours a day I bowed before my keyboard, buying, selling, hesitating, punching the air one moment, cursing the next, at least at the beginning. I read market reports, but I believed I was dealing in a random system and mostly relied on guesses. Sometimes I leaped ahead, sometimes I plunged, but on average through the year I made about as much as the postman. I paid my rent, which was low in those days, ate and dressed well enough and thought I was beginning to stabilise, learning to know myself. I was determined that my thirties would be a superior performance to my twenties.\\n\\nBut my parents' pleasant family home was sold just as the first convincing artificial person came on the market. 1982. Robots, androids, replicates were my passion, even more so after my research for the book. Prices were bound to fall, but I had to have one straight away, an Eve by preference, but an Adam would do.\\n\\nIt could have turned out differently. My previous girlfriend, Claire, was a sensible person who trained to be a dental nurse. She worked in a Harley Street practice and she would have talked me out of Adam. She was a woman of the world, of this one. She knew how to arrange a life. And not only her own. But I offended her with an act of undeniable disloyalty. She disowned me in a scene of regal fury, at the end of which she threw my clothes out into the street. Lime Grove. She never spoke to me again and belonged at the top of my list of errors and failures. She could have saved me from myself. But In the interests of balance, let that unsaved self speak up. I did not buy Adam to make money. On the contrary. My motives were pure. I handed over a fortune in the name of curiosity, that steadfast engine of science, of intellectual life, of life itself. This was no passing fad. There was a history, an account, a time-deposit, and I had a right to draw on it. Electronics and anthropology--distant cousins whom late modernity has drawn together and bound in marriage. The child of that coupling was Adam.\\n\\nSo, I appear before you, witness for the defence, after school, 5 p.m., typical specimen for my time--short trousers, scabby knees, freckles, short back and sides, eleven years old. I'm first in line, waiting for the lab to open and for \\\"Wiring Club to begin. Presiding is Mr. Cox, a gentle giant with carroty hair who teaches physics. My project is to build a radio. It's an act of faith, an extended prayer that has taken many weeks. I have a base of hardboard, six inches by nine, easily drilled. Colours are everything. Blue, red, yellow and white wires run their modest courses around the board, turning at right angles, disappearing below to emerge elsewhere and be interrupted by bright nodules, tiny vividly striped cylinders-- capacitors, resistors--then an induction coil I have wound myself, then an op-amp. I understand nothing. I follow a wiring diagram as a novice might murmur scripture. Mr. Cox gives softly spoken advice. I clumsily solder one piece, one wire or component, to another. The smoke and smell of solder is a drug I inhale deeply. I include in my circuit a toggle switch made of Bakelite which, I have persuaded myself, came out of a fighter plane, a Spitfire surely. The final connection, three months after my beginning, is from this piece of dark brown plastic to a nine-volt battery.\\n\\nIt is a cold, windy dusk in March. Other boys are hunched over their projects. We are twelve miles away from Shakespeare's hometown, in what will come to be known as a \\\"bogstandard\\\" comprehensive school. An excellent place, in fact. The fluorescent ceiling lights come on. Mr. Cox is on the far side of the lab with his back turned. I do not want to attract his attention in case of failure. I throw the switch and--miracle-- hear the sound of static. I jiggle the variable-tuning capacitor: music, terrible music, as I think, because violins are involved. Then comes the rapid voice of a woman, not speaking English. No one looks up, no one is interested. Building a radio is nothing special. But I am speechless, close to tears. No technology since will amaze me as much. Electricity, passing through pieces of metal carefully arranged by me, snatches from the air the voice of a foreign lady sitting somewhere far away. Her voice sounds kindly. She is not aware of me. I will never learn her name or understand her language, and never meet her, not knowingly. My radio, with its irregular blobs of solder on a board, appears no less a wonder than consciousness itself arising from matter.\\n\\nBrains and electronics were closely related, so I discovered through my teens as I built simple computers and programmed them myself. Then complicated computers. Electricity and bits of metal could add up numbers, make words, pictures, songs, remember things and even turn speech into writing.\\n\\nWhen I was seventeen, Peter Cox persuaded me to study physics at a local college. Within a month I was bored and looking to change. The subject was too abstract, the maths was beyond me. And by then I would read a book or two and was taking an interest in imaginary people. Heller's Catch-18, Fitzgerald's The High-Bouncing Lover, Orwell's The Last Man in Europe, lolstoy's All's Well That Ends Well--I did not get much further and yet I saw the point of art. It was a form of investigation. But I did not want to study literature--too intimidating, too intuitive. A single-sheet course summary I picked up in the college library announced anthropology as \\\"the science of people in their societies through space and time.\\\" Systematic study, with the human factor thrown in. I signed up.\\n\\nThe first thing to learn: my course was pitifully underfunded. No bunking off for a year to the Trobriand Islands, where, I read, it was taboo to eat in front of others. It was good manners to eat alone, with your back turned to friends and family. The islanders had spells to make ugly people beautiful. Children were actively encouraged to be sexual with each other. Yams were the viable currency. Women determined the status of men. How strange and bracing. My view of human nature had been shaped by the mostly white population crammed into the southern quarter of England. Now I was set free into bottomless relativism.\\n\\nAt the age of nineteen I wrote a wise essay on honour cultures entitled \\\"Mind-forged Manacles?\\\" Dispassionately, I gathered up my case studies. What did I know or care? There were places where rape was so common it did not have a name. A young father's throat was cut for failing in his duties to an ancient feud. Here was a family eager to kill a daughter for being seen holding hands with a lad from the wrong religious group. There, elderly women keenly assisted in the genital mutilation of their granddaughters. What of the instinctive parental impulses to love and protect? The cultural signal was louder. What of universal values? Upended. Nothing like this in Stratford-upon-Avon. It was all about the mind, the tradition, the religion--nothing but software, I now thought, and best regarded in value-free terms.\\n\\nAnthropologists did not pass judgement. They observed and reported on human variety. They celebrated difference. What was wicked in Warwickshire was unremarkable in Papua New Guinea. Locally, who was to say what was good or bad? Certainly not a colonial power. I derived from my studies some unfortunate conclusions about ethics which led me a few years later to the dock in a county court, accused of conspiring with others to mislead the tax authorities on a grand scale. I did not attempt to persuade His Honour that far from his court might be a coconut beach where such conspiracy was respected. Instead, I came to my senses just before I addressed the judge. Morals were real, they were true, good and bad inherited in the nature of things. Our actions must be judged on their terms. This was what I would assume before anthropology came along. In quavering, hesitant tones I apologised abjectly to the court and dodged a custodial sentence.\\n\\n\",\n{\n            'v': 237,\n            'f': \"237\",\n        },\n{\n            'v': 300,\n            'f': \"300\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sect_no\"], [\"string\", \"sect_raw\"], [\"number\", \"sent_no_start\"], [\"number\", \"sent_no_mid\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sect_no</th>\n",
              "      <th>sect_raw</th>\n",
              "      <th>sent_no_start</th>\n",
              "      <th>sent_no_mid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>\\n\\nIt was religious yearning granted hope, it...</td>\n",
              "      <td>3</td>\n",
              "      <td>117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nAt thirty-two, I was completely broke. Was...</td>\n",
              "      <td>237</td>\n",
              "      <td>300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sect_no  ... sent_no_mid\n",
              "0        0  ...         117\n",
              "1        1  ...         300\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqdYxCjhdMy8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6a167371-5dbc-45e2-c4db-3366fd427c73"
      },
      "source": [
        "# Raw Plot of Section Sentiments (Adjusted for (x-axis) mid-Section Sentence No and (y-axis) Sentiment weighted by Section length )\n",
        "# This function must be defined AFTER the corpus_sect_df DataFrame is created in the previous block\n",
        "\n",
        "# corpus_sects_df = pd.DataFrame()  # Create empty early as required by some utility functions\n",
        "\"\"\" \n",
        "# SECOND COPY\n",
        "\n",
        "def plot_stand_crux_sections(ts_df=corpus_sects_df, model_names_ls=['vader'], semantic_type='section', label_token_ct=0, title_xpos = 0.8, title_ypos=0.2, save2file=False):\n",
        "  '''\n",
        "  Given a Sections DataFrame, model_name and semantic type,\n",
        "  Return a Plot of the Cruxes\n",
        "  '''\n",
        "\n",
        "  crux_points_dt = {}\n",
        "  model_stand_names_ls = []\n",
        "  section_boundries_ls = []\n",
        "\n",
        "  for i, amodel in enumerate(model_names_ls):\n",
        "    amodel_stand_str = f'{amodel}_stand'\n",
        "    model_stand_names_ls.append(amodel_stand_str)\n",
        "    # sns.lineplot(data=ts_df, x='sent_no_mid', y=amodel_stand, markers=['o'], alpha=0.5, label=amodel_stand); # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment (Bing Lexicon)')\n",
        "\n",
        "  # Draw vertical lines at Section boundries\n",
        "  section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "  for i, sent_no in enumerate(section_boundries_ls):\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1);\n",
        "\n",
        "  # Get midpoints of each Section\n",
        "  section_midpoints_ls = list(corpus_sects_df['sent_no_mid']);\n",
        "\n",
        "  # How many sentiment time series are we plotting?\n",
        "  if len(model_stand_names_ls) == 1:\n",
        "    # Plotting only one model\n",
        "    model_names_str = model_names_ls[0]\n",
        "    sns.lineplot(data=ts_df, x='sent_no_mid', y=model_names_str, markers=['o'], alpha=0.5, label=model_stand_names_ls[0]) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "\n",
        "    # If plotting only one model, add labels\n",
        "    section_midpoints_sentiment_ls = list(corpus_sects_df[amodel_stand]);\n",
        "    for x,y in zip(section_midpoints_ls, section_midpoints_sentiment_ls):\n",
        "      label_token_int = int(label_token_ct)\n",
        "      if label_token_int < 0:\n",
        "        label = ''\n",
        "      elif label_token_int == 0:\n",
        "        label = f\"#{x}\"\n",
        "      else:\n",
        "        label = f\"#{x} {' '.join(corpus_sents_df.iloc[x]['sent_raw'].split()[:label_token_int])}\"; # \\nPolarity: {y:.2f}'\n",
        "\n",
        "      crux_full_str = ' '.join(corpus_sents_df.iloc[x]['sent_raw'].split())\n",
        "      crux_points_dt[x] = (y, crux_full_str)\n",
        "\n",
        "      plt.annotate(label,\n",
        "                   (x,y),\n",
        "                   textcoords='offset points',\n",
        "                   xytext=(0,10),\n",
        "                   ha='center',\n",
        "                   rotation=90)\n",
        "      \n",
        "    plt.title(f'{CORPUS_FULL} \\n Plot Section Sentiment (Standardized {model_names_ls[0].capitalize()})', x=title_xpos, y=title_ypos);\n",
        "    plt.plot(section_midpoints_ls, section_midpoints_sentiment_ls, marker=\"o\", ms=6) # , markevery=[0,1])\n",
        "\n",
        "  else:\n",
        "    # If plotting multiple models\n",
        "    model_names_str = 'multilex'\n",
        "    for i, amodel_stand_name in enumerate(model_stand_names_ls):\n",
        "      sns.lineplot(data=ts_df, x='sent_no_mid', y=amodel_stand_name, markers=['o'], alpha=0.5, label=amodel_stand_name) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} \\n Plot Section Sentiment (Standardized All Lex Models)', x=title_xpos, y=title_ypos)\n",
        "\n",
        "  plt.legend(loc='best');\n",
        "\n",
        "  if (save2file == True):\n",
        "    # Save graph to file.\n",
        "    plot_filename = f'plot_standnote_sects_{model_names_str}.png'\n",
        "    plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "    print(f'Plot saved: {plot_filename}');\n",
        "\n",
        "  return crux_points_dt\n",
        "\"\"\";"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNKoxe3kRV71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "51e699e1-a1a1-4516-8ba4-02f685ab298e"
      },
      "source": [
        "# TODO: More General Cleanup"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRUmgyLvRM1v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fcf62a08-04ac-49d5-9273-e532e314b152"
      },
      "source": [
        "# TODO: Normalize Paragraphs by Lengths (Smart Aggregate/Split)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqfhER7ylECP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6f9a1928-841d-4eec-b7a7-f77a28b65a39"
      },
      "source": [
        "'''\n",
        "# Generate full path and timestamp for new filepath/filename\n",
        "\n",
        "def gen_pathfiletime(file_str, subdir_str=''):\n",
        "\n",
        "  # Geenreate compressed author and title substrings\n",
        "  author_raw_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "  title_raw_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "\n",
        "  # Generate current/unique datetime string\n",
        "  datetime_str = str(datetime.now().strftime('%Y%m%d%H%M%S'))\n",
        "\n",
        "  # Built fullpath+filename string\n",
        "  file_base, file_ext = file_str.split('.')\n",
        "\n",
        "  author_str = re.sub('[^A-Za-z0-9]+', '', author_raw_str)\n",
        "  title_str = re.sub('[^A-Za-z0-9]+', '', title_raw_str)\n",
        "\n",
        "  full_filepath_str = f'{subdir_str}{file_base}_{author_str}_{title_str}_{datetime_str}.{file_ext}'\n",
        "\n",
        "  # print(f'Returning from gen_savepath() with full_filepath={full_filepath}')\n",
        "\n",
        "  return full_filepath_str\n",
        "\n",
        "# Test\n",
        "# pathfilename_str = gen_pathfiletime('hist_paraglen.png')\n",
        "# print(pathfilename_str)\n",
        "''';"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH65q_PIefyg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4f43b16e-089b-4a55-957e-d3861786987e"
      },
      "source": [
        "corpus_sects_df.columns"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sect_no', 'sect_raw', 'sent_no_start', 'sent_no_mid'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ZxHMuRe2wF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "76847400-619a-4741-c106-7630c774ef6b"
      },
      "source": [
        "# Calculate some char/token metrics and do some EDA on them\n",
        "\n",
        "# This code must be run AFTER corpus_sects_df is created below\n",
        "\n",
        "corpus_sents_df['char_len'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x))\n",
        "corpus_sents_df['token_len'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x.split())) \n",
        "\n",
        "corpus_parags_df['char_len'] = corpus_parags_df['parag_raw'].apply(lambda x: len(x))\n",
        "corpus_parags_df['token_len'] = corpus_parags_df['parag_raw'].apply(lambda x: len(x.split())) \n",
        "\n",
        "corpus_sects_df['char_len'] = corpus_sects_df['sect_raw'].apply(lambda x: len(x))\n",
        "corpus_sects_df['token_len'] = corpus_sects_df['sect_raw'].apply(lambda x: len(x.split())) \n",
        "\n",
        "corpus_chaps_df['char_len'] = corpus_chaps_df['chap_raw'].apply(lambda x: len(x))\n",
        "corpus_chaps_df['token_len'] = corpus_chaps_df['chap_raw'].apply(lambda x: len(x.split())) \n",
        "\n",
        "# corpus_sents_df.head()\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7au9Zy2kR0NK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d55a3e99-12dc-4d11-8343-f5cb09f8f3d6"
      },
      "source": [
        "# Default cleaned raw text\n",
        "\n",
        "# Sentences\n",
        "# Let's take a look at the updated text\n",
        "corpus_sents_df['sent_clean'] = corpus_sents_df['sent_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Text\n",
        "corpus_sents_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_sents_df.dropna(how='any', axis=0, subset=['sent_raw'], inplace=True)\n",
        "corpus_sents_df.dropna(how='any', axis=0, subset=['sent_clean'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Sentences:')\n",
        "print('--------------------------------------')\n",
        "corpus_sents_df.head(2)\n",
        "\n",
        "\n",
        "# Paragraphs\n",
        "# Let's take a look at the updated text\n",
        "corpus_parags_df['parag_clean'] = corpus_parags_df['parag_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Text\n",
        "corpus_parags_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_parags_df.dropna(how='any', axis=0, subset=['parag_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Paragraphs:')\n",
        "print('--------------------------------------')\n",
        "corpus_parags_df.head(2)\n",
        "\n",
        "\n",
        "# Sections\n",
        "# Let's take a look at the updated text\n",
        "corpus_sects_df['sect_clean'] = corpus_sects_df['sect_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Sentences with NaN or '' Raw Text\n",
        "corpus_sects_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_sects_df.dropna(how='any', axis=0, subset=['sect_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Sections:')\n",
        "print('--------------------------------------')\n",
        "# corpus_sects_df.head(2)\n",
        "\n",
        "\n",
        "# Chapters\n",
        "# Let's take a look at the updated text\n",
        "corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_raw'].apply(lambda x: text_clean(x))\n",
        "# Ensure to drop all Chapters with NaN or '' Raw Text\n",
        "corpus_chaps_df.replace(\"\", np.nan, regex=True, inplace=True)\n",
        "corpus_chaps_df.dropna(how='any', axis=0, subset=['chap_raw'], inplace=True)\n",
        "\n",
        "print('\\nCompare Raw and Cleaned Chapters:')\n",
        "print('--------------------------------------')\n",
        "# corpus_sects_df.head(2)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Compare Raw and Cleaned Sentences:\n",
            "--------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"But remember, please, the Law by which we live, We are not built to comprehend a lie ...\",\n{\n            'v': 88,\n            'f': \"88\",\n        },\n{\n            'v': 18,\n            'f': \"18\",\n        },\n\"but remember please the law by which we live we are not built to comprehend a lie \"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n\"--Rudyard Kipling, \\\"The Secret of the Machines\\\"\",\n{\n            'v': 47,\n            'f': \"47\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n\"  rudyard kipling the secret of the machines\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"number\", \"parag_no\"], [\"string\", \"sent_raw\"], [\"number\", \"char_len\"], [\"number\", \"token_len\"], [\"string\", \"sent_clean\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>sent_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>But remember, please, the Law by which we live...</td>\n",
              "      <td>88</td>\n",
              "      <td>18</td>\n",
              "      <td>but remember please the law by which we live w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>--Rudyard Kipling, \"The Secret of the Machines\"</td>\n",
              "      <td>47</td>\n",
              "      <td>7</td>\n",
              "      <td>rudyard kipling the secret of the machines</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sent_no  ...                                         sent_clean\n",
              "0        0  ...  but remember please the law by which we live w...\n",
              "1        1  ...         rudyard kipling the secret of the machines\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Compare Raw and Cleaned Paragraphs:\n",
            "--------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"But remember, please, the Law by which we live, We are not built to comprehend a lie ...\",\n{\n            'v': 88,\n            'f': \"88\",\n        },\n{\n            'v': 18,\n            'f': \"18\",\n        },\n\"but remember please the law by which we live we are not built to comprehend a lie \"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n\"--Rudyard Kipling, \\\"The Secret of the Machines\\\"\",\n{\n            'v': 47,\n            'f': \"47\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n\"  rudyard kipling the secret of the machines\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"parag_no\"], [\"string\", \"parag_raw\"], [\"number\", \"char_len\"], [\"number\", \"token_len\"], [\"string\", \"parag_clean\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parag_no</th>\n",
              "      <th>parag_raw</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>parag_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>But remember, please, the Law by which we live...</td>\n",
              "      <td>88</td>\n",
              "      <td>18</td>\n",
              "      <td>but remember please the law by which we live w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>--Rudyard Kipling, \"The Secret of the Machines\"</td>\n",
              "      <td>47</td>\n",
              "      <td>7</td>\n",
              "      <td>rudyard kipling the secret of the machines</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   parag_no  ...                                        parag_clean\n",
              "0         0  ...  but remember please the law by which we live w...\n",
              "1         1  ...         rudyard kipling the secret of the machines\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Compare Raw and Cleaned Sections:\n",
            "--------------------------------------\n",
            "\n",
            "Compare Raw and Cleaned Chapters:\n",
            "--------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf99apfwKPAO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "15ea2515-da6f-48a1-838d-56ee71c9e6c6"
      },
      "source": [
        "# Verify \n",
        "\n",
        "print(f'corpus_sents_df.shape: {corpus_sents_df.shape}')\n",
        "print(f'corpus_parags_df.shape: {corpus_parags_df.shape}')\n",
        "print(f'corpus_sects_df.shape: {corpus_sects_df.shape}')\n",
        "print(f'corpus_chaps_df.shape: {corpus_chaps_df.shape}')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "corpus_sents_df.shape: (7679, 6)\n",
            "corpus_parags_df.shape: (1428, 5)\n",
            "corpus_sects_df.shape: (30, 7)\n",
            "corpus_chaps_df.shape: (10, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLWZqZ9UzUyL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "111cc2f1-fd9c-4a4a-9a4c-0b1b33c55dae"
      },
      "source": [
        "corpus_sents_df[corpus_sents_df['sent_clean'].str.contains('destructionsmashed')]"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"number\", \"parag_no\"], [\"number\", \"sent_raw\"], [\"number\", \"char_len\"], [\"number\", \"token_len\"], [\"number\", \"sent_clean\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>sent_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [sent_no, parag_no, sent_raw, char_len, token_len, sent_clean]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH2-9i_xmWuP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "553c52e6-44e3-4bb9-fbb9-2f4d2b756bc4"
      },
      "source": [
        "corpus_sents_df[corpus_sents_df['sent_clean'].str.contains('summer')]"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 1400,\n            'f': \"1400\",\n        },\n{\n            'v': 1400,\n            'f': \"1400\",\n        },\n{\n            'v': 235,\n            'f': \"235\",\n        },\n\"I went into my bedroom, undressed, leaving my clothes in a heap on my desk, and lay down naked under the summer duvet.\",\n{\n            'v': 118,\n            'f': \"118\",\n        },\n{\n            'v': 23,\n            'f': \"23\",\n        },\n\"i went into my bedroom undressed leaving my clothes in a heap on my desk and lay down naked under the summer duvet\"],\n [{\n            'v': 1652,\n            'f': \"1652\",\n        },\n{\n            'v': 1652,\n            'f': \"1652\",\n        },\n{\n            'v': 272,\n            'f': \"272\",\n        },\n\"On the windowsill, where yellow gingham curtains hung still in our late-summer heat wave, a radio was playing the Beatles, recently regrouped after twelve years apart.\",\n{\n            'v': 167,\n            'f': \"167\",\n        },\n{\n            'v': 26,\n            'f': \"26\",\n        },\n\"on the windowsill where yellow gingham curtains hung still in our late summer heat wave a radio was playing the beatles recently regrouped after twelve years apart\"],\n [{\n            'v': 1792,\n            'f': \"1792\",\n        },\n{\n            'v': 1792,\n            'f': \"1792\",\n        },\n{\n            'v': 292,\n            'f': \"292\",\n        },\n\"For me, it was far too early for bed and it was hot, like a summer evening in Marrakech.\",\n{\n            'v': 88,\n            'f': \"88\",\n        },\n{\n            'v': 19,\n            'f': \"19\",\n        },\n\"for me it was far too early for bed and it was hot like a summer evening in marrakech\"],\n [{\n            'v': 2095,\n            'f': \"2095\",\n        },\n{\n            'v': 2095,\n            'f': \"2095\",\n        },\n{\n            'v': 344,\n            'f': \"344\",\n        },\n\"Here she was beside me, close enough for me to feel her summer-morning body warmth.\",\n{\n            'v': 83,\n            'f': \"83\",\n        },\n{\n            'v': 15,\n            'f': \"15\",\n        },\n\"here she was beside me close enough for me to feel her summer morning body warmth\"],\n [{\n            'v': 2613,\n            'f': \"2613\",\n        },\n{\n            'v': 2613,\n            'f': \"2613\",\n        },\n{\n            'v': 486,\n            'f': \"486\",\n        },\n\"The summer was hot and something was coming to the boil.\",\n{\n            'v': 56,\n            'f': \"56\",\n        },\n{\n            'v': 11,\n            'f': \"11\",\n        },\n\"the summer was hot and something was coming to the boil\"],\n [{\n            'v': 2952,\n            'f': \"2952\",\n        },\n{\n            'v': 2952,\n            'f': \"2952\",\n        },\n{\n            'v': 559,\n            'f': \"559\",\n        },\n\"If the court would suspend proceedings and serve an order on the British subsidiary of the phone company to release its copies of the texts, these disputed versions of a summer evening would be settled.\",\n{\n            'v': 202,\n            'f': \"202\",\n        },\n{\n            'v': 35,\n            'f': \"35\",\n        },\n\"if the court would suspend proceedings and serve an order on the british subsidiary of the phone company to release its copies of the texts these disputed versions of a summer evening would be settled\"],\n [{\n            'v': 3698,\n            'f': \"3698\",\n        },\n{\n            'v': 3698,\n            'f': \"3698\",\n        },\n{\n            'v': 714,\n            'f': \"714\",\n        },\n\"Our last summer at school came around.\",\n{\n            'v': 38,\n            'f': \"38\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n\"our last summer at school came around\"],\n [{\n            'v': 3866,\n            'f': \"3866\",\n        },\n{\n            'v': 3866,\n            'f': \"3866\",\n        },\n{\n            'v': 725,\n            'f': \"725\",\n        },\n\"I would had two boyfriends by that summer and I knew what to do.\",\n{\n            'v': 64,\n            'f': \"64\",\n        },\n{\n            'v': 14,\n            'f': \"14\",\n        },\n\"i would had two boyfriends by that summer and i knew what to do\"],\n [{\n            'v': 4410,\n            'f': \"4410\",\n        },\n{\n            'v': 4410,\n            'f': \"4410\",\n        },\n{\n            'v': 801,\n            'f': \"801\",\n        },\n\"Now she was writing a short essay, to be read aloud in a summer-course seminar, that argued against empathy as a means of historical exploration.\",\n{\n            'v': 145,\n            'f': \"145\",\n        },\n{\n            'v': 25,\n            'f': \"25\",\n        },\n\"now she was writing a short essay to be read aloud in a summer course seminar that argued against empathy as a means of historical exploration\"],\n [{\n            'v': 4676,\n            'f': \"4676\",\n        },\n{\n            'v': 4676,\n            'f': \"4676\",\n        },\n{\n            'v': 850,\n            'f': \"850\",\n        },\n\"I wanted to lie down on the worn-out grass of late summer and close my eyes.\",\n{\n            'v': 76,\n            'f': \"76\",\n        },\n{\n            'v': 16,\n            'f': \"16\",\n        },\n\"i wanted to lie down on the worn out grass of late summer and close my eyes\"],\n [{\n            'v': 6243,\n            'f': \"6243\",\n        },\n{\n            'v': 6243,\n            'f': \"6243\",\n        },\n{\n            'v': 1182,\n            'f': \"1182\",\n        },\n\"He was happy in a borrowed summer frock.\",\n{\n            'v': 40,\n            'f': \"40\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n\"he was happy in a borrowed summer frock\"],\n [{\n            'v': 7201,\n            'f': \"7201\",\n        },\n{\n            'v': 7201,\n            'f': \"7201\",\n        },\n{\n            'v': 1383,\n            'f': \"1383\",\n        },\n\"It was the time of an exceptional Indian summer.\",\n{\n            'v': 48,\n            'f': \"48\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        },\n\"it was the time of an exceptional indian summer\"],\n [{\n            'v': 7220,\n            'f': \"7220\",\n        },\n{\n            'v': 7220,\n            'f': \"7220\",\n        },\n{\n            'v': 1384,\n            'f': \"1384\",\n        },\n\"During the summer, I was drawn into the kind of labyrinthine bureaucracy I would have associated with the declining Ottoman Empire.\",\n{\n            'v': 131,\n            'f': \"131\",\n        },\n{\n            'v': 21,\n            'f': \"21\",\n        },\n\"during the summer i was drawn into the kind of labyrinthine bureaucracy i would have associated with the declining ottoman empire\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"number\", \"parag_no\"], [\"string\", \"sent_raw\"], [\"number\", \"char_len\"], [\"number\", \"token_len\"], [\"string\", \"sent_clean\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>sent_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1400</th>\n",
              "      <td>1400</td>\n",
              "      <td>235</td>\n",
              "      <td>I went into my bedroom, undressed, leaving my ...</td>\n",
              "      <td>118</td>\n",
              "      <td>23</td>\n",
              "      <td>i went into my bedroom undressed leaving my cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>1652</td>\n",
              "      <td>272</td>\n",
              "      <td>On the windowsill, where yellow gingham curtai...</td>\n",
              "      <td>167</td>\n",
              "      <td>26</td>\n",
              "      <td>on the windowsill where yellow gingham curtain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1792</th>\n",
              "      <td>1792</td>\n",
              "      <td>292</td>\n",
              "      <td>For me, it was far too early for bed and it wa...</td>\n",
              "      <td>88</td>\n",
              "      <td>19</td>\n",
              "      <td>for me it was far too early for bed and it was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2095</th>\n",
              "      <td>2095</td>\n",
              "      <td>344</td>\n",
              "      <td>Here she was beside me, close enough for me to...</td>\n",
              "      <td>83</td>\n",
              "      <td>15</td>\n",
              "      <td>here she was beside me close enough for me to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2613</th>\n",
              "      <td>2613</td>\n",
              "      <td>486</td>\n",
              "      <td>The summer was hot and something was coming to...</td>\n",
              "      <td>56</td>\n",
              "      <td>11</td>\n",
              "      <td>the summer was hot and something was coming to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2952</th>\n",
              "      <td>2952</td>\n",
              "      <td>559</td>\n",
              "      <td>If the court would suspend proceedings and ser...</td>\n",
              "      <td>202</td>\n",
              "      <td>35</td>\n",
              "      <td>if the court would suspend proceedings and ser...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3698</th>\n",
              "      <td>3698</td>\n",
              "      <td>714</td>\n",
              "      <td>Our last summer at school came around.</td>\n",
              "      <td>38</td>\n",
              "      <td>7</td>\n",
              "      <td>our last summer at school came around</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3866</th>\n",
              "      <td>3866</td>\n",
              "      <td>725</td>\n",
              "      <td>I would had two boyfriends by that summer and ...</td>\n",
              "      <td>64</td>\n",
              "      <td>14</td>\n",
              "      <td>i would had two boyfriends by that summer and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4410</th>\n",
              "      <td>4410</td>\n",
              "      <td>801</td>\n",
              "      <td>Now she was writing a short essay, to be read ...</td>\n",
              "      <td>145</td>\n",
              "      <td>25</td>\n",
              "      <td>now she was writing a short essay to be read a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4676</th>\n",
              "      <td>4676</td>\n",
              "      <td>850</td>\n",
              "      <td>I wanted to lie down on the worn-out grass of ...</td>\n",
              "      <td>76</td>\n",
              "      <td>16</td>\n",
              "      <td>i wanted to lie down on the worn out grass of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6243</th>\n",
              "      <td>6243</td>\n",
              "      <td>1182</td>\n",
              "      <td>He was happy in a borrowed summer frock.</td>\n",
              "      <td>40</td>\n",
              "      <td>8</td>\n",
              "      <td>he was happy in a borrowed summer frock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7201</th>\n",
              "      <td>7201</td>\n",
              "      <td>1383</td>\n",
              "      <td>It was the time of an exceptional Indian summer.</td>\n",
              "      <td>48</td>\n",
              "      <td>9</td>\n",
              "      <td>it was the time of an exceptional indian summer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7220</th>\n",
              "      <td>7220</td>\n",
              "      <td>1384</td>\n",
              "      <td>During the summer, I was drawn into the kind o...</td>\n",
              "      <td>131</td>\n",
              "      <td>21</td>\n",
              "      <td>during the summer i was drawn into the kind of...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      sent_no  ...                                         sent_clean\n",
              "1400     1400  ...  i went into my bedroom undressed leaving my cl...\n",
              "1652     1652  ...  on the windowsill where yellow gingham curtain...\n",
              "1792     1792  ...  for me it was far too early for bed and it was...\n",
              "2095     2095  ...  here she was beside me close enough for me to ...\n",
              "2613     2613  ...  the summer was hot and something was coming to...\n",
              "2952     2952  ...  if the court would suspend proceedings and ser...\n",
              "3698     3698  ...              our last summer at school came around\n",
              "3866     3866  ...  i would had two boyfriends by that summer and ...\n",
              "4410     4410  ...  now she was writing a short essay to be read a...\n",
              "4676     4676  ...  i wanted to lie down on the worn out grass of ...\n",
              "6243     6243  ...            he was happy in a borrowed summer frock\n",
              "7201     7201  ...    it was the time of an exceptional indian summer\n",
              "7220     7220  ...  during the summer i was drawn into the kind of...\n",
              "\n",
              "[13 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHscLkclSYqN"
      },
      "source": [
        "##**Save Preprocess Corpus DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEzo8eltSvWS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7e4e97e6-7e92-4ceb-8c5d-313f40890e09"
      },
      "source": [
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "# Sentences\n",
        "corpus_sents_filename = f'corpus_sents_clean_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_sents_filename}')\n",
        "\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "# Paragraphs\n",
        "corpus_parags_filename = f'corpus_parags_clean_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_parags_filename}')\n",
        "\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "# Sections\n",
        "corpus_sects_filename = f'corpus_sects_clean_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_sects_filename}')\n",
        "\n",
        "corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "# Chapters\n",
        "corpus_chaps_filename = f'corpus_chaps_clean_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_chaps_filename}')\n",
        "\n",
        "corpus_chaps_df.to_csv(corpus_chaps_filename)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving to file: corpus_sents_clean_ianmcewan_machineslikeme_20210708_1508.csv\n",
            "Saving to file: corpus_parags_clean_ianmcewan_machineslikeme_20210708_1508.csv\n",
            "Saving to file: corpus_sects_clean_ianmcewan_machineslikeme_20210708_1508.csv\n",
            "Saving to file: corpus_chaps_clean_ianmcewan_machineslikeme_20210708_1508.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8R_ANtfYG6"
      },
      "source": [
        "# (Optional) EDA Raw Text Features: Interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1njcRD-jgGJh"
      },
      "source": [
        "**(Optional) Can Skip Ahead to: 'EDA of Raw Text and Extracted Features'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ti9jQK7grxO"
      },
      "source": [
        "# Review Cleaned Up Sentences\n",
        "\n",
        "corpus_sents_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TN5ksy55kXy"
      },
      "source": [
        "# Summary Statistics\n",
        "\n",
        "corpus_sents_df.describe()\n",
        "corpus_sents_df['token_len'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxvxkbUGzkfy"
      },
      "source": [
        "# Create histogram of Paragraph lengths\n",
        "\n",
        "sns.histplot(data=corpus_sents_df['char_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Lengths');\n",
        "\n",
        "if (PLOT_OUTPUT == 'All'):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'hist_paraglen.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqUtGz8UjY2b"
      },
      "source": [
        "# Plot histogram of Sentence lengths\n",
        "\n",
        "sns.histplot(data=corpus_sents_df['token_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Lengths')\n",
        "\n",
        "if (PLOT_OUTPUT == 'All'):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'hist_sentlen.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OoBrucYR9Xc"
      },
      "source": [
        "# SELECT CORPUS TYPE\n",
        "# TODO: Customized Preprocessing (e.g. Tweets) by Corpus Type\n",
        "\n",
        "# Novel, Tweets, Chat Transcript\n",
        "\n",
        "# Processing Options\n",
        "\n",
        "# Apply first level cleaning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2tIua7tTSRz"
      },
      "source": [
        "# (Optional) Manually Create Sentiment Arc Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU11XOZIPalR"
      },
      "source": [
        "***Can skip to Section [Load Sentiment Polarities...] or [Calculate VADER...]***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cke3OowdWk6I"
      },
      "source": [
        "**Interactively Enter Cruxes and Edge Cases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv7Foe3oTMmz"
      },
      "source": [
        "# Setup data structures for endpoints of Sentiment Time Series\n",
        "\n",
        "#   [-1.0 to +1.0] = [v.neg, neg, neutral, pos, v.pos]\n",
        "\n",
        "corpus_man_crux_ols = []  # working datastructure to dynamically build ordered list of manually selected Crux Points\n",
        "corpus_man_cruxes_odt = OrderedDict() # Once all manual Crux points selected, this will be working data structure\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0] - 1\n",
        "\n",
        "corpus_parags_len = corpus_sents_df.parag_no.max() # make sure no omissions/repeats/skips\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-_ylgiAc6Aw"
      },
      "source": [
        "# <INPUT> Set the Begining and Ending Sentiment Values (Manual Versions)\n",
        "\n",
        "# Start of Corpus Sentiment Analysis Time Series\n",
        "Corpus_Starting_Sentiment = -0.1 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "# corpus_sa_begin = Corpus_Starting_Sentiment\n",
        "\n",
        "# End of Corpus Sentiment Analysis Time Series\n",
        "Corpus_Ending_Sentiment = -1 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "# corpus_sa_end = Corpus_Ending_Sentiment\n",
        "\n",
        "corpus_man_crux_ols = [tuple((0, Corpus_Starting_Sentiment)), tuple((corpus_sents_len, Corpus_Ending_Sentiment))]\n",
        "# corpus_man_cruxes_dt[0.] = corpus_sa_begin\n",
        "# corpus_man_cruxes_dt[float(corpus_sents_len)] = corpus_sa_end\n",
        "\n",
        "print(f'Manual Cruxes with Start/End: {corpus_man_crux_ols}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmcLrzyUw93d"
      },
      "source": [
        "**Seach for Key Words that suggest Min/Max Sentiment Crux**\n",
        "* Specific to the Novel: Introduction of Pivotal Character, Scene, Factual Reveal, McGuffin, etc...\n",
        "* General to Events/Themes: Death, Birth, Fight, Accident, Money, Sex, etc... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UESgwwIAY28T"
      },
      "source": [
        "# <INPUT> Search Corpus for Line No of Peaks/Valleys\n",
        "# TODO: Better Vis\n",
        "Search_String = \"Death\" #@param {type:\"string\"}\n",
        "if (Search_String == \"\"):\n",
        "  search_str = \"accident\"\n",
        "else:\n",
        "  search_str = Search_String.lower()\n",
        "\n",
        "# search the list of cleaned paragraphs\n",
        "# results_ls = [x for x in search_match_ls if re.search(subs, x)]\n",
        "\n",
        "# creating and passsing series to new column\n",
        "match_sents_ser = corpus_sents_df[\"sent_clean\"].str.find(search_str)\n",
        "\n",
        "# print(f'Found #{len(match_index>0)} Matches')\n",
        "match_sents_df = corpus_sents_df.loc[match_sents_ser > 0]\n",
        "print(f'Found #{match_sents_df.shape[0]} Matching Sentences')\n",
        "print('------------------------------------')\n",
        "# print(f'  {match_sents_df}')\n",
        "match_sents_df[['sent_no', 'parag_no', 'sent_raw', 'token_len']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oVYBfz6-EVo"
      },
      "source": [
        "**Get Context for Matched Sentence by Retrieving Surrounding Paragraph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AvYQqT5TsdV"
      },
      "source": [
        "# Extract Surrounding Paragraphs for context on matching Sentences\n",
        "\n",
        "def get_parag4sentno(asent_no):\n",
        "  '''\n",
        "  Return the original raw paragraph containing a \n",
        "  given sentence number.\n",
        "  '''\n",
        "  # parag_df = pd.DataFrame()\n",
        "  # print(f'Passed in sent_no: {asent_no}')\n",
        "  aparag_no = int(corpus_sents_df.loc[corpus_sents_df['sent_no'] == asent_no]['parag_no'])\n",
        "  # print(f'  This sent_no {asent_no} is in parag_no: {aparag_no}')\n",
        "  aparag_str = corpus_sents_df.loc[corpus_sents_df['parag_no'] == aparag_no]['sent_raw'].str.cat() # ['sent_clean']\n",
        "  # sentno_parag_df = corpus_sents_df[corpus_sents_df['sent_no']==asent_no]\n",
        "  # print(f'Sent #{asent_no} is in the paragraph: ')\n",
        "  # print(aparag)\n",
        "  # print(f'returning aparag_no: [{aparag_no}]: {aparag}')\n",
        "  return aparag_no, aparag_str\n",
        "\n",
        "'''\n",
        "# Testing\n",
        "asent_no = 7\n",
        "print(f'Searching for paragraph containing Sentence #{asent_no}')\n",
        "\n",
        "aparag_no, aparag_str = get_parag4sentno(asent_no)\n",
        "print(f'\\n  Found in Paragraph #{aparag_no} \\n\\n{aparag_str}')\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnHz08NyDroK"
      },
      "source": [
        "# Extract Surrounding Paragraphs for context on matching Sentences\n",
        "\n",
        "def get_parag_str(aparag_no):\n",
        "  '''\n",
        "  Return the original raw paragraph containing a \n",
        "  given sentence number.\n",
        "  '''\n",
        "  # parag_df = pd.DataFrame()\n",
        "  # print(f'Passed in sent_no: {asent_no}')\n",
        "  # aparag_no = int(corpus_sents_df.loc[corpus_sents_df['sent_no'] == asent_no]['parag_no'])\n",
        "  # print(f'  This sent_no {asent_no} is in parag_no: {aparag_no}')\n",
        "  aparag_str = corpus_sents_df.loc[corpus_sents_df['parag_no'] == aparag_no]['sent_raw'].str.cat() # ['sent_clean']\n",
        "  # sentno_parag_df = corpus_sents_df[corpus_sents_df['sent_no']==asent_no]\n",
        "  # print(f'Sent #{asent_no} is in the paragraph: ')\n",
        "  # print(aparag)\n",
        "  # print(f'returning aparag_no: [{aparag_no}]: {aparag}')\n",
        "  return aparag_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByKLYZDsK123"
      },
      "source": [
        "# Summarize current status of manually selected Crux Points\n",
        "# TODO:\n",
        "\n",
        "def crux_sum_short():\n",
        "  print(f'\\nOrdered list of all manually selected Crux Points')\n",
        "  print('---------------------------------------')\n",
        "  for i, acrux_tp in enumerate(corpus_man_crux_ols):\n",
        "    asent_no, asent_pol = acrux_tp\n",
        "    asent_str = corpus_sents_df[corpus_sents_df.sent_no==asent_no].sent_raw.str.cat()\n",
        "    # print(f'Type: {type(asent_str)}')\n",
        "    print(f'Sent No {asent_no:4d}: Polarity: {asent_pol}\\nText: {asent_str}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz0FLzt2UeSe"
      },
      "source": [
        "# Summarize current manually selected Crux Points\n",
        "\n",
        "def crux_summary():\n",
        "  print(f'\\nOrdered list of all manually selected Crux Points')\n",
        "  print('---------------------------------------\\n\\n')\n",
        "  for i, acrux_tp in enumerate(corpus_man_crux_ols):\n",
        "    asent_no, asent_pol = acrux_tp\n",
        "    asent_str = corpus_sents_df[corpus_sents_df.sent_no==asent_no].sent_raw.str.cat()\n",
        "    # print(f'Type: {type(asent_str)}')\n",
        "    print(f'Sent No {asent_no:4d}: Polarity: {asent_pol}')\n",
        "    print('------------------------------')\n",
        "    print(f'Text: {asent_str}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiUmseNw3CkN"
      },
      "source": [
        "# View the Paragraph containing your Matching Sentence:\n",
        "\n",
        "def get_nparags_context(crux_sent_no, parag_ct):\n",
        "\n",
        "  parag_win = int(parag_ct)\n",
        "  parag_crux_str = ''\n",
        "\n",
        "  parag_crux_no = 0\n",
        "\n",
        "  if (crux_sent_no < 0) | (crux_sent_no > corpus_sents_len):\n",
        "    print(f'ERROR: Pick a Sentence No between 0-{corpus_sents_len-1}')\n",
        "  else:\n",
        "    # get_sent_no = crux_sent_no\n",
        "    # print(f'Retrieving Sentence No: {get_sent_no}')\n",
        "    # print('----------')\n",
        "\n",
        "    parag_crux_no, aparag_str = get_parag4sentno(crux_sent_no)\n",
        "    if parag_win == 1:\n",
        "      print(f'Match #{i}: Sentence No. {asent_no} found in Paragraph No. {parag_crux_no}')\n",
        "      print('----------------------------')\n",
        "      print(f'Sentence:\\n')\n",
        "      # print(f'     {corpus_sents_df[corpus_sents_df.sent_no == crux_sent_no]}\\n\\n')\n",
        "      corpus_sents_df[corpus_sents_df.sent_no == crux_sent_no]\n",
        "      print('----------------------------')\n",
        "      print(f'Paragraph Context:\\n')\n",
        "      print(f'     {aparag_str}\\n\\n')\n",
        "    else:\n",
        "      parag_half_win = int((parag_win-1)/2)\n",
        "      parag_start = parag_crux_no - parag_half_win\n",
        "      parag_end = parag_crux_no + parag_half_win\n",
        "      print(f'Retrieving {parag_ct} Contextual Paragraphs Nos {parag_start} to {parag_end}')\n",
        "      print(f'  for Crux Point centered on Sentence No {crux_sent_no}')\n",
        "      for i in range(parag_start, parag_end + 1, 1):\n",
        "        if i == parag_crux_no:\n",
        "          print(f'\\n   ---------------------------------------------------------')\n",
        "          print(f'** Crux Point Paragraph #{i} with Sentence No. {crux_sent_no} **')\n",
        "          print(f'   ---------------------------------------------------------')\n",
        "          parag_crux_str = get_parag_str(i)\n",
        "          print(parag_crux_str)\n",
        "        else:\n",
        "          print(f'\\n   ----------------------')\n",
        "          print(f'   Regular Paragraph #{i}')\n",
        "          print(f'   ----------------------')\n",
        "          print(get_parag_str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjoXC8Fg3lBP"
      },
      "source": [
        "# Insert new crux point into ordered list: corpus_man_crux_ls\n",
        "\n",
        "# NOTE: For very long lists, use Python simple bisect library (at cost of additional dependency)\n",
        "\n",
        "\n",
        "def insert_ord_tp_list(crux_ord_ols, crux_tp):\n",
        "  '''\n",
        "  Insert new crux tuple: crux_tp = (sent_no, sentiment_polarity)\n",
        "  into ordered list of tuples while maintaining sent_no order\n",
        "  '''\n",
        "  sent_no, senti_pol = crux_tp\n",
        "\n",
        "  # Searching for the position\n",
        "  for i in range(len(crux_ord_ols)):\n",
        "    if crux_ord_ols[i][0] == sent_no:\n",
        "      # Attempting to insert duplicate\n",
        "      return crux_ord_ols\n",
        "    elif crux_ord_ols[i][0] < sent_no:\n",
        "      insert_idx = i\n",
        "    else:\n",
        "      break\n",
        "      \n",
        "  # Inserting n in the list\n",
        "  list = crux_ord_ols[:i] + [crux_tp] + crux_ord_ols[i:]\n",
        "  return list\n",
        "\n",
        "'''\n",
        "# Test\n",
        "crux_test_ls = [(1,0), (5,1), (10,-1)]\n",
        "crux_test_tp = (3,10)\n",
        "  \n",
        "print(insert_ord_tp_list(crux_test_ls, crux_test_tp))\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PPOLJDZ1wHc"
      },
      "source": [
        "**Start of Human in the Loop Manual Crux Point Identification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1MLexX57Guc"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "print('Enter a Sentence number based upon your search above to see the ')\n",
        "print('  surrounding Paragraph context.')\n",
        "print('----------------------------------------')\n",
        "print(f'(Enter an integer between 0 and {corpus_sents_len-1})\\n\\n')\n",
        "\n",
        "print('\\n')\n",
        "print('Enter an ODD NUMBER for the Number of surrounding Paragraphs ')\n",
        "print('  around the Sentence No to give Context.')\n",
        "print('----------------------------------------')\n",
        "print(f'(Enter an integer: 3, 5, 7\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exxWtaUPC30l"
      },
      "source": [
        "# Input your Context Retrieval Parameters\n",
        "\n",
        "Sentence_No =  2692#@param {type:\"integer\"}\n",
        "No_Paragraphs_Context = \"3\" #@param [\"1\", \"3\", \"5\"]\n",
        "\n",
        "get_nparags_context(Sentence_No, No_Paragraphs_Context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30fBikIrrgMe"
      },
      "source": [
        "**Add Crux to Manually Generated Sentiment Arc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgljeT6ypNbo"
      },
      "source": [
        "# Instructions to add current sentence as a Crux Point\n",
        "\n",
        "crux_summary()\n",
        "\n",
        "print('--------------------------------------------------------')\n",
        "print(f'INSTRUCTIONS To current Sentence No: {Sentence_No} as a Crux Point')\n",
        "print('--------------------------------------------------------')\n",
        "\n",
        "print(\"\\nCheck this box if you want to add the Sentence/Paragraph above \")\n",
        "print(\"  as a new Min/Max Crux Point with your approximation \")\n",
        "print(\"  for a Sentiment Polarity value between -1.0 to +1.0\\n\\n\")\n",
        "\n",
        "print(f\"Crux Sentence No: {Sentence_No} in Paragraph No: {parag_crux_no}\\n\")\n",
        "print(parag_crux_str)\n",
        "\n",
        "print(\"\\n\\nLeave Add_Sentence_Crux 'unchecked' to not add\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia1IXChRmyr3"
      },
      "source": [
        "# <INPUT> Option to add this Sentence/Paragraph as a Min/Max Crux Point\n",
        "\n",
        "Sentiment_Polarity = -0.4 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "Add_Sentence_Crux = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "# Add Crux if selected and give current summary status\n",
        "\n",
        "if Add_Sentence_Crux == True:\n",
        "  crux_new_tp = tuple((Sentence_No, Sentiment_Polarity))\n",
        "  corpus_man_crux_ols = insert_ord_tp_list(corpus_man_crux_ols, crux_new_tp)\n",
        "  if (corpus_man_crux_ols):\n",
        "    print(f'Successfully inserted new Crux = {crux_new_tp}')\n",
        "    print(f'Added Crux at Sentence No={Sentence_No} with Polarity={Sentiment_Polarity}')\n",
        "    # corpus_man_cruxes_dt[Sentence_No] = Sentiment_Polarity\n",
        "  else:\n",
        "    print(f'ERROR: Could not insert new Crux = {crux_new_tp}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lps67T-lUTi2"
      },
      "source": [
        "# Summary of current Crux Points after addition\n",
        "\n",
        "print('\\n------------------------------------------------------------')\n",
        "print(f'After addition of new Crux Point (Sentence No {Sentence_No})')\n",
        "print('------------------------------------------------------------\\n')\n",
        "\n",
        "crux_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjZT9_0CrzFk"
      },
      "source": [
        "**Delete Manually Selected Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTX30nokNPBp"
      },
      "source": [
        "len(corpus_man_crux_ols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxS-J_hPOfkI"
      },
      "source": [
        "crux_tp = (1, 2)\n",
        "a, b = crux_tp\n",
        "print(f'a is {a} and b is {b}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi7Cu1iErvzc"
      },
      "source": [
        "# Insert new crux point into ordered list: corpus_man_crux_ls\n",
        "\n",
        "# FIX: 20210616 and move to utility functions\n",
        "\n",
        "# NOTE: For very long lists, use Python simple bisect library (at cost of additional dependency)\n",
        "\n",
        "\n",
        "def del_ord_tp_list(acorpus_man_crux_ols, crux_tp):\n",
        "  '''\n",
        "  Insert new crux tuple: crux_tp = (sent_no, sentiment_polarity)\n",
        "  into ordered list of tuples while maintaining sent_no order\n",
        "  '''\n",
        "  crux_ct = len(acorpus_man_crux_ols)\n",
        "  sent_no = crux_tp[0]\n",
        "  print(f'Deleting sent_no: {sent_no} over crux_ls len={len(acorpus_man_crux_ols)}')\n",
        "\n",
        "  # Searching for the positionk\n",
        "  del_idx = -1\n",
        "  for i in range(len(acorpus_man_crux_ols)):\n",
        "    acrux_sent_no = acorpus_man_crux_ols[i][0]\n",
        "    print(f'Crux #{i} is sent_no={acrux_sent_no}')\n",
        "    if acrux_sent_no == sent_no:\n",
        "      print(f'Matching index at {i}')\n",
        "      del_idx = i\n",
        "      \n",
        "  # Delete n in the list\n",
        "  print(f'Deletion index = {del_idx}')\n",
        "  if del_idx == 0:\n",
        "    # Delete the first Crux\n",
        "    list = acorpus_man_crux_ols[1:]\n",
        "    return list\n",
        "  elif del_idx == crux_ct -1:\n",
        "    # Delete the last Crux\n",
        "    list = acorpus_man_crux_ols[:-1]\n",
        "    return list    \n",
        "  elif (del_idx > 0) & (del_idx < crux_ct):\n",
        "    # Delete an interior Crux\n",
        "    before_idx = i - 1\n",
        "    after_idx = i\n",
        "    list = acorpus_man_crux_ols[:before_idx] + acorpus_man_crux_ols[after_idx:]\n",
        "    print(f'Returning list: {list}')\n",
        "    return list\n",
        "  else:\n",
        "    print('No matching Crux tuple found')\n",
        "    return acorpus_man_crux_ols\n",
        "  \n",
        "\n",
        "# Test\n",
        "crux_test_ls = [(1,0), (5,1), (10,-1)]\n",
        "crux_test_tp = (5,5)\n",
        "  \n",
        "print(del_ord_tp_list(crux_test_ls, crux_test_tp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI5hZqmSLg5R"
      },
      "source": [
        "# Instructions to Delete a Crux Point\n",
        "\n",
        "crux_summary()\n",
        "\n",
        "print('--------------------------------------------------------')\n",
        "print('INSTRUCTIONS To Delete a Crux Point')\n",
        "print('--------------------------------------------------------')\n",
        "\n",
        "print(\"\\nEnter the Sentence No of a Crux you want to delete.\\n\")\n",
        "print(f'     Current Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n",
        "print(\" Skip this if you want to keep all manually selected Crux Points.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzowFY-I8U1_"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "print(\"\\nEnter the Sentence No of a Crux you want to delete.\\n\")\n",
        "print(f'     Current Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n",
        "print(\" Skip this if you want to keep all manually selected Crux Points.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ShFCfOsrvsv"
      },
      "source": [
        "Delete_Sent_No =  777#@param {type:\"integer\"}\n",
        "# Select a Crux to Delete\n",
        "# TODO: Drop down list\n",
        "\n",
        "# corpus_man_crux_ols\n",
        "corpus_man_crux_temp_ols = []\n",
        "\n",
        "crux_sent_set = set([x[0] for x in corpus_man_crux_ols])\n",
        "if not(Delete_Sent_No in crux_sent_set):\n",
        "  print(f'ERROR: {Delete_Sent_No} is not a Crux Point Sentence No')\n",
        "else:\n",
        "  # Keep the same tuple format for uniformity and future features\n",
        "  crux_del_tp = tuple((Delete_Sent_No, 'dummy_sentence'))\n",
        "  print(f'Selected {(crux_del_tp)} to delete')\n",
        "  # corpus_man_crux_temp_ols = \n",
        "  print(f'WTF: {del_ord_tp_list(corpus_man_crux_ols, crux_del_tp)}')\n",
        "  corpus_man_crux_old = del_ord_tp_list(corpus_man_crux_ols, crux_del_tp)\n",
        "  print(f\"corpus_man_crux_ols: {corpus_man_crux_ols}\")\n",
        "  # get_sent_no = Sentence_No\n",
        "  # print(f'Retrieving Sentence No: {get_sent_no}')\n",
        "  # print('----------')\n",
        "  print(f'Updated Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJdNu-oYrwHv"
      },
      "source": [
        "**Review Summary of all Manually Selected Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmpggAWuvMb1"
      },
      "source": [
        "# Generate Report Summary of All Manually Selected Cruxes\n",
        "\n",
        "f = io.StringIO()\n",
        "with contextlib.redirect_stdout(f):\n",
        "    crux_summary()\n",
        "crux_summary = f.getvalue()\n",
        "\n",
        "# Print Manual Crux Report Summary to Screen\n",
        "\n",
        "# print(crux_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ULFfHbxwDfu"
      },
      "source": [
        "# Save Manual Crux Summary Report\n",
        "\n",
        "plot_filename = 'man_cruxes.txt'\n",
        "plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "\n",
        "with open(plotpathfilename_str, 'a+') as outfp:\n",
        "  outfp.write(crux_summary)\n",
        "\n",
        "# Verify \n",
        "\n",
        "!ls -alt $plotpathfilename_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG_hGPgjxY7X"
      },
      "source": [
        "# Verify Report Content\n",
        "\n",
        "!cat man_cruxes_fscottfitzgerald_thegreatgatsby_20210616214050.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zPYRKCex8-U"
      },
      "source": [
        "**Clean and Organize Manual Crux Points into new Data Structures**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YICJNt3AXI2d"
      },
      "source": [
        "print(corpus_sents_df[corpus_sents_df['sent_no']==5]['sent_raw'].squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2bLtpweWQt4"
      },
      "source": [
        "# Convert and assemble all the Crux values in lists to save in a new Crux DataFrame\n",
        "\n",
        "\n",
        "pol_val_ls = [x[1] for x in corpus_man_crux_ols]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "parag_no_ls = [(get_parag4sentno(x))[0] for x in sent_no_ls]\n",
        "parag_str_ls = [get_parag_str(x) for x in parag_no_ls]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "sent_raw_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_raw'].squeeze() for x in sent_no_ls]\n",
        "sent_raw_ls\n",
        "sent_clean_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_clean'].squeeze() for x in sent_no_ls]\n",
        "sent_clean_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFEYg7Gfbj9i"
      },
      "source": [
        "# Create a Dict of Crux Points to Tuples (Polarity, Raw Sentence)\n",
        "\n",
        "# First Create the Tuples for each Sentence No (Sentiment Polarity, Raw Text)\n",
        "def merge(list1, list2):\n",
        "    merged_list = tuple(zip(list1, list2)) \n",
        "    return merged_list\n",
        "      \n",
        "crux_tp_ls = merge(pol_val_ls, sent_raw_ls)\n",
        "\n",
        "# Second, Create the Dictionary C\n",
        "corpus_man_cruxes_dt = {sent_no_ls[i]: crux_tp_ls[i] for i in range(len(crux_tp_ls))}\n",
        "\n",
        "# Verify\n",
        "corpus_man_cruxes_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtIHecpMdbXa"
      },
      "source": [
        "corpus_sents_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8qmpFRaTfu2"
      },
      "source": [
        "**Plot Interpolated Manual Sentiment Arc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ASg7uUZWf3"
      },
      "source": [
        "corpus_man_sa_df = pd.DataFrame({'sent_no':xn, 'sentiment':yn, 'sent_raw':corpus_sents_df.sent_raw.values})\n",
        "corpus_man_sa_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36VfGi4a47Yl"
      },
      "source": [
        "# Hermite Interpolation with SciPy\n",
        "\n",
        "pol_val_ls = [x[1] for x in corpus_man_crux_ols]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "parag_no_ls = [(get_parag4sentno(x))[0] for x in sent_no_ls]\n",
        "parag_str_ls = [get_parag_str(x) for x in parag_no_ls]\n",
        "sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n",
        "sent_raw_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_raw'].squeeze() for x in sent_no_ls]\n",
        "sent_raw_ls\n",
        "sent_clean_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_clean'].squeeze() for x in sent_no_ls]\n",
        "sent_clean_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElJKawWfZWbv"
      },
      "source": [
        "sent_no_ls\n",
        "pol_val_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xufA403s3lbF"
      },
      "source": [
        "corpus_man_crux_np = np.asarray(corpus_man_crux_ols)\n",
        "corpus_man_crux_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdgKbjpi63a9"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMkgU-JhwudG"
      },
      "source": [
        "x2 = np.array(sent_no_ls)\n",
        "y2 = np.array(pol_val_ls)\n",
        "\n",
        "xn = np.linspace(0, corpus_sents_len, corpus_sents_len)\n",
        "yn = interpolate.pchip_interpolate(x2, y2, xn)\n",
        "\n",
        "crux_man_df = pd.DataFrame(\n",
        "    {'sent_no': sent_no_ls,\n",
        "     'pol_val': pol_val_ls\n",
        "     }\n",
        ")\n",
        "\n",
        "# plt.plot(x2, y2, 'ok', label='True values')\n",
        "# plt.plot(xn, yn, label='Hermite Interpolation')\n",
        "\n",
        "# plt.plot(xn, yn4, label='Spline order 4')\n",
        "# plt.plot(xn, yn5, label='Spline order 5')\n",
        "# plt.plot(xn, yn6, label='Spline order 6')\n",
        "# plt.plot(xn, yn7, label='Spline order 7')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# sns.histplot(data=corpus_sents_df['char_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Lengths')\n",
        "sns.histplot(data=crux_man_df, x='sent_no', y='pol_val', kde=True).set_title(f'{CORPUS_FULL} \\n Manual Cruxes with Hermite Smoothing')\n",
        "\n",
        "\n",
        "# Save graph to file.\n",
        "plot_filename = 'man_crux_plot.png'\n",
        "plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "plt.savefig(plotpathfilename_str, format='png', dpi=300)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW-VkS2s_ogy"
      },
      "source": [
        "**Gaussian Process Regression**\n",
        "\n",
        "* https://blog.dominodatalab.com/fitting-gaussian-process-models-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLFFYDfYpPjn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0BukxX9ZKYA"
      },
      "source": [
        "# (Optional) Load Sentiment Polarities: Interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhPJ9e-V9NMu"
      },
      "source": [
        "***If you upload a file of Sentiment Values you don't have to Calculate them in the following sections***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpx7viqf9AAJ"
      },
      "source": [
        "!ls -altr *.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw8_mUHI9sdr"
      },
      "source": [
        "# Test\n",
        "\n",
        "files.download('sentiments_raw_all_virginiawoolf_tothelighthouse_20210618161224.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90axHClm9cGZ"
      },
      "source": [
        "# Upload your precomputed Sentiment Values\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02NH5nnto2AC"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL9JCyCI-Sic"
      },
      "source": [
        "# Verify the file was uploaded correctly\n",
        "\n",
        "newest_csvfile = get_recentfile().split('/')[-1]\n",
        "print(f'The most recently updated *.csv file is: {newest_csvfile}')\n",
        "\n",
        "!head -n 10 $newest_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTfBj8qdi_vZ"
      },
      "source": [
        "%whos DataFrame\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgjz-E-mYgzI"
      },
      "source": [
        "# Upload file into DataFrame\n",
        "\n",
        "corpus_test_df = pd.read_csv(newest_csvfile)\n",
        "corpus_test_df['sent_clean'] = corpus_test_df['sent_clean'].astype('string')\n",
        "corpus_test_df['sent_raw'] = corpus_test_df['sent_raw'].astype('string')\n",
        "corpus_test_df.head()\n",
        "corpus_test_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ajyb8Jpmzw"
      },
      "source": [
        "***Skip to Section <Calculate Median of All...> if SA Loaded***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGA6sQy6RPDC"
      },
      "source": [
        "corpus_lexicons_stats_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7CfH00OFkQV"
      },
      "source": [
        "# **Either (Load) and/or (Calculate) Sentiment Values**\n",
        "\n",
        "Sentiment Models\n",
        "\n",
        "* VADER [-1.0 to 1.0] zero peak\n",
        "* TextBlob [-1.0 to 1.0] zero peak\n",
        "* Stanza outliers [-1.0 to 199.0] pos, outliers(+peak)\n",
        "* AFINN [-14 (-8 to 8) 20] discrete\n",
        "* SentimentR 11,710 [-5.4 to 8.8] norm\n",
        "* Syuzhet [-5.4 to 8.8] norm\n",
        "* Bing [-100.0 (-20.0 to 20.0) 100] discrete, outliers\n",
        "* Pattern [-1.0 to 1.0] norm\n",
        "* SentiWord [-3.8 to 4.4] norm\n",
        "* SenticNet [-3.8 to 10] norm\n",
        "* NRC [-100.0 (-5.0 to 5.0) 100] zero, outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ueo8fTSqqaak"
      },
      "source": [
        "## **Read Verification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys6WgCgCDOQN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "d8f28211-fbbc-464f-fffc-67a4d7b0f8c1"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Variable           Type         Data/Info\n",
            "-----------------------------------------\n",
            "corpus_chaps_df    DataFrame       chap_no  ...          <...>\\n\\n[10 rows x 7 columns]\n",
            "corpus_parags_df   DataFrame          parag_no  ...      <...>\\n[1428 rows x 5 columns]\n",
            "corpus_sects_df    DataFrame        sect_no  ...         <...>\\n\\n[30 rows x 7 columns]\n",
            "corpus_sents_df    DataFrame          sent_no  ...       <...>\\n[7679 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d-AM_WvbqFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "34399851-a059-4016-9af9-90ce26fe42d9"
      },
      "source": [
        "# Verfiy there are no NaN or Empty strings that passed the cleaning process\n",
        "\n",
        "corpus_sents_df[corpus_sents_df['sent_clean'].isnull()]\n",
        "\n",
        "corpus_sents_df[corpus_sents_df['sent_clean'].apply(lambda x: len(str(x)) <= 0)]"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"number\", \"parag_no\"], [\"number\", \"sent_raw\"], [\"number\", \"char_len\"], [\"number\", \"token_len\"], [\"number\", \"sent_clean\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>sent_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [sent_no, parag_no, sent_raw, char_len, token_len, sent_clean]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"number\", \"parag_no\"], [\"number\", \"sent_raw\"], [\"number\", \"char_len\"], [\"number\", \"token_len\"], [\"number\", \"sent_clean\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>sent_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [sent_no, parag_no, sent_raw, char_len, token_len, sent_clean]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAAQ0SwfZynO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "132425aa-a519-4239-f1b8-3e9a2dc850ed"
      },
      "source": [
        "# Verify that hyphenated words are correctly handled (e.g. 'summer-mroning' -> 'summer morning')\n",
        "\n",
        "corpus_sents_df[corpus_sents_df['sent_clean'].str.contains('summer', na=False)]"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 1400,\n            'f': \"1400\",\n        },\n{\n            'v': 1400,\n            'f': \"1400\",\n        },\n{\n            'v': 235,\n            'f': \"235\",\n        },\n\"I went into my bedroom, undressed, leaving my clothes in a heap on my desk, and lay down naked under the summer duvet.\",\n{\n            'v': 118,\n            'f': \"118\",\n        },\n{\n            'v': 23,\n            'f': \"23\",\n        },\n\"i went into my bedroom undressed leaving my clothes in a heap on my desk and lay down naked under the summer duvet\"],\n [{\n            'v': 1652,\n            'f': \"1652\",\n        },\n{\n            'v': 1652,\n            'f': \"1652\",\n        },\n{\n            'v': 272,\n            'f': \"272\",\n        },\n\"On the windowsill, where yellow gingham curtains hung still in our late-summer heat wave, a radio was playing the Beatles, recently regrouped after twelve years apart.\",\n{\n            'v': 167,\n            'f': \"167\",\n        },\n{\n            'v': 26,\n            'f': \"26\",\n        },\n\"on the windowsill where yellow gingham curtains hung still in our late summer heat wave a radio was playing the beatles recently regrouped after twelve years apart\"],\n [{\n            'v': 1792,\n            'f': \"1792\",\n        },\n{\n            'v': 1792,\n            'f': \"1792\",\n        },\n{\n            'v': 292,\n            'f': \"292\",\n        },\n\"For me, it was far too early for bed and it was hot, like a summer evening in Marrakech.\",\n{\n            'v': 88,\n            'f': \"88\",\n        },\n{\n            'v': 19,\n            'f': \"19\",\n        },\n\"for me it was far too early for bed and it was hot like a summer evening in marrakech\"],\n [{\n            'v': 2095,\n            'f': \"2095\",\n        },\n{\n            'v': 2095,\n            'f': \"2095\",\n        },\n{\n            'v': 344,\n            'f': \"344\",\n        },\n\"Here she was beside me, close enough for me to feel her summer-morning body warmth.\",\n{\n            'v': 83,\n            'f': \"83\",\n        },\n{\n            'v': 15,\n            'f': \"15\",\n        },\n\"here she was beside me close enough for me to feel her summer morning body warmth\"],\n [{\n            'v': 2613,\n            'f': \"2613\",\n        },\n{\n            'v': 2613,\n            'f': \"2613\",\n        },\n{\n            'v': 486,\n            'f': \"486\",\n        },\n\"The summer was hot and something was coming to the boil.\",\n{\n            'v': 56,\n            'f': \"56\",\n        },\n{\n            'v': 11,\n            'f': \"11\",\n        },\n\"the summer was hot and something was coming to the boil\"],\n [{\n            'v': 2952,\n            'f': \"2952\",\n        },\n{\n            'v': 2952,\n            'f': \"2952\",\n        },\n{\n            'v': 559,\n            'f': \"559\",\n        },\n\"If the court would suspend proceedings and serve an order on the British subsidiary of the phone company to release its copies of the texts, these disputed versions of a summer evening would be settled.\",\n{\n            'v': 202,\n            'f': \"202\",\n        },\n{\n            'v': 35,\n            'f': \"35\",\n        },\n\"if the court would suspend proceedings and serve an order on the british subsidiary of the phone company to release its copies of the texts these disputed versions of a summer evening would be settled\"],\n [{\n            'v': 3698,\n            'f': \"3698\",\n        },\n{\n            'v': 3698,\n            'f': \"3698\",\n        },\n{\n            'v': 714,\n            'f': \"714\",\n        },\n\"Our last summer at school came around.\",\n{\n            'v': 38,\n            'f': \"38\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n\"our last summer at school came around\"],\n [{\n            'v': 3866,\n            'f': \"3866\",\n        },\n{\n            'v': 3866,\n            'f': \"3866\",\n        },\n{\n            'v': 725,\n            'f': \"725\",\n        },\n\"I would had two boyfriends by that summer and I knew what to do.\",\n{\n            'v': 64,\n            'f': \"64\",\n        },\n{\n            'v': 14,\n            'f': \"14\",\n        },\n\"i would had two boyfriends by that summer and i knew what to do\"],\n [{\n            'v': 4410,\n            'f': \"4410\",\n        },\n{\n            'v': 4410,\n            'f': \"4410\",\n        },\n{\n            'v': 801,\n            'f': \"801\",\n        },\n\"Now she was writing a short essay, to be read aloud in a summer-course seminar, that argued against empathy as a means of historical exploration.\",\n{\n            'v': 145,\n            'f': \"145\",\n        },\n{\n            'v': 25,\n            'f': \"25\",\n        },\n\"now she was writing a short essay to be read aloud in a summer course seminar that argued against empathy as a means of historical exploration\"],\n [{\n            'v': 4676,\n            'f': \"4676\",\n        },\n{\n            'v': 4676,\n            'f': \"4676\",\n        },\n{\n            'v': 850,\n            'f': \"850\",\n        },\n\"I wanted to lie down on the worn-out grass of late summer and close my eyes.\",\n{\n            'v': 76,\n            'f': \"76\",\n        },\n{\n            'v': 16,\n            'f': \"16\",\n        },\n\"i wanted to lie down on the worn out grass of late summer and close my eyes\"],\n [{\n            'v': 6243,\n            'f': \"6243\",\n        },\n{\n            'v': 6243,\n            'f': \"6243\",\n        },\n{\n            'v': 1182,\n            'f': \"1182\",\n        },\n\"He was happy in a borrowed summer frock.\",\n{\n            'v': 40,\n            'f': \"40\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n\"he was happy in a borrowed summer frock\"],\n [{\n            'v': 7201,\n            'f': \"7201\",\n        },\n{\n            'v': 7201,\n            'f': \"7201\",\n        },\n{\n            'v': 1383,\n            'f': \"1383\",\n        },\n\"It was the time of an exceptional Indian summer.\",\n{\n            'v': 48,\n            'f': \"48\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        },\n\"it was the time of an exceptional indian summer\"],\n [{\n            'v': 7220,\n            'f': \"7220\",\n        },\n{\n            'v': 7220,\n            'f': \"7220\",\n        },\n{\n            'v': 1384,\n            'f': \"1384\",\n        },\n\"During the summer, I was drawn into the kind of labyrinthine bureaucracy I would have associated with the declining Ottoman Empire.\",\n{\n            'v': 131,\n            'f': \"131\",\n        },\n{\n            'v': 21,\n            'f': \"21\",\n        },\n\"during the summer i was drawn into the kind of labyrinthine bureaucracy i would have associated with the declining ottoman empire\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"number\", \"parag_no\"], [\"string\", \"sent_raw\"], [\"number\", \"char_len\"], [\"number\", \"token_len\"], [\"string\", \"sent_clean\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>parag_no</th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>sent_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1400</th>\n",
              "      <td>1400</td>\n",
              "      <td>235</td>\n",
              "      <td>I went into my bedroom, undressed, leaving my ...</td>\n",
              "      <td>118</td>\n",
              "      <td>23</td>\n",
              "      <td>i went into my bedroom undressed leaving my cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>1652</td>\n",
              "      <td>272</td>\n",
              "      <td>On the windowsill, where yellow gingham curtai...</td>\n",
              "      <td>167</td>\n",
              "      <td>26</td>\n",
              "      <td>on the windowsill where yellow gingham curtain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1792</th>\n",
              "      <td>1792</td>\n",
              "      <td>292</td>\n",
              "      <td>For me, it was far too early for bed and it wa...</td>\n",
              "      <td>88</td>\n",
              "      <td>19</td>\n",
              "      <td>for me it was far too early for bed and it was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2095</th>\n",
              "      <td>2095</td>\n",
              "      <td>344</td>\n",
              "      <td>Here she was beside me, close enough for me to...</td>\n",
              "      <td>83</td>\n",
              "      <td>15</td>\n",
              "      <td>here she was beside me close enough for me to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2613</th>\n",
              "      <td>2613</td>\n",
              "      <td>486</td>\n",
              "      <td>The summer was hot and something was coming to...</td>\n",
              "      <td>56</td>\n",
              "      <td>11</td>\n",
              "      <td>the summer was hot and something was coming to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2952</th>\n",
              "      <td>2952</td>\n",
              "      <td>559</td>\n",
              "      <td>If the court would suspend proceedings and ser...</td>\n",
              "      <td>202</td>\n",
              "      <td>35</td>\n",
              "      <td>if the court would suspend proceedings and ser...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3698</th>\n",
              "      <td>3698</td>\n",
              "      <td>714</td>\n",
              "      <td>Our last summer at school came around.</td>\n",
              "      <td>38</td>\n",
              "      <td>7</td>\n",
              "      <td>our last summer at school came around</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3866</th>\n",
              "      <td>3866</td>\n",
              "      <td>725</td>\n",
              "      <td>I would had two boyfriends by that summer and ...</td>\n",
              "      <td>64</td>\n",
              "      <td>14</td>\n",
              "      <td>i would had two boyfriends by that summer and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4410</th>\n",
              "      <td>4410</td>\n",
              "      <td>801</td>\n",
              "      <td>Now she was writing a short essay, to be read ...</td>\n",
              "      <td>145</td>\n",
              "      <td>25</td>\n",
              "      <td>now she was writing a short essay to be read a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4676</th>\n",
              "      <td>4676</td>\n",
              "      <td>850</td>\n",
              "      <td>I wanted to lie down on the worn-out grass of ...</td>\n",
              "      <td>76</td>\n",
              "      <td>16</td>\n",
              "      <td>i wanted to lie down on the worn out grass of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6243</th>\n",
              "      <td>6243</td>\n",
              "      <td>1182</td>\n",
              "      <td>He was happy in a borrowed summer frock.</td>\n",
              "      <td>40</td>\n",
              "      <td>8</td>\n",
              "      <td>he was happy in a borrowed summer frock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7201</th>\n",
              "      <td>7201</td>\n",
              "      <td>1383</td>\n",
              "      <td>It was the time of an exceptional Indian summer.</td>\n",
              "      <td>48</td>\n",
              "      <td>9</td>\n",
              "      <td>it was the time of an exceptional indian summer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7220</th>\n",
              "      <td>7220</td>\n",
              "      <td>1384</td>\n",
              "      <td>During the summer, I was drawn into the kind o...</td>\n",
              "      <td>131</td>\n",
              "      <td>21</td>\n",
              "      <td>during the summer i was drawn into the kind of...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      sent_no  ...                                         sent_clean\n",
              "1400     1400  ...  i went into my bedroom undressed leaving my cl...\n",
              "1652     1652  ...  on the windowsill where yellow gingham curtain...\n",
              "1792     1792  ...  for me it was far too early for bed and it was...\n",
              "2095     2095  ...  here she was beside me close enough for me to ...\n",
              "2613     2613  ...  the summer was hot and something was coming to...\n",
              "2952     2952  ...  if the court would suspend proceedings and ser...\n",
              "3698     3698  ...              our last summer at school came around\n",
              "3866     3866  ...  i would had two boyfriends by that summer and ...\n",
              "4410     4410  ...  now she was writing a short essay to be read a...\n",
              "4676     4676  ...  i wanted to lie down on the worn out grass of ...\n",
              "6243     6243  ...            he was happy in a borrowed summer frock\n",
              "7201     7201  ...    it was the time of an exceptional indian summer\n",
              "7220     7220  ...  during the summer i was drawn into the kind of...\n",
              "\n",
              "[13 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnI6dd-kFe5E"
      },
      "source": [
        "## **(Optional) Load Raw Sentence Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiLWzvjFGRLy"
      },
      "source": [
        "### **Select Datafile to Read**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90gk7UOSwbHf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d7f2dc8b-56c1-4eae-cf7a-16f689cef7c8"
      },
      "source": [
        "# By convention, Summary Sentiment files are named with the RegEx template 'sum_sentiments_*.csv'\n",
        "\n",
        "!ls -altr sum_sentiments_*.csv"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 651742 Jul  8 13:44 sum_sentiments_syuzhetR_4models.csv\n",
            "-rw------- 1 root root 681263 Jul  8 14:48 sum_sentiments_sentimentR_7models.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id_VKOPiR7Mg"
      },
      "source": [
        "#### **Import SentimentR Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "c0Iw8urDwihD",
        "outputId": "10322c7e-b3f2-43ed-b505-401fb4118f9c"
      },
      "source": [
        "corpus_sentimentr_df.head(2)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 17.0,\n            'f': \"17.0\",\n        },\n{\n            'v': 0.16977493752543302,\n            'f': \"0.16977493752543302\",\n        },\n{\n            'v': 0.16977493752543302,\n            'f': \"0.16977493752543302\",\n        },\n{\n            'v': 0.24253562503633302,\n            'f': \"0.24253562503633302\",\n        },\n{\n            'v': 0.595849396808011,\n            'f': \"0.595849396808011\",\n        },\n{\n            'v': 0.151584765647708,\n            'f': \"0.151584765647708\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.24253562503633302,\n            'f': \"0.24253562503633302\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 7.0,\n            'f': \"7.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.226022754859518,\n            'f': \"0.226022754859518\",\n        },\n{\n            'v': 0.0944911182523068,\n            'f': \"0.0944911182523068\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"element_id\"], [\"number\", \"sentence_id\"], [\"number\", \"word_count\"], [\"number\", \"jockers_rinker\"], [\"number\", \"jockers\"], [\"number\", \"huliu\"], [\"number\", \"senticnet\"], [\"number\", \"sentiword\"], [\"number\", \"nrc\"], [\"number\", \"lmcd\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>element_id</th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>word_count</th>\n",
              "      <th>jockers_rinker</th>\n",
              "      <th>jockers</th>\n",
              "      <th>huliu</th>\n",
              "      <th>senticnet</th>\n",
              "      <th>sentiword</th>\n",
              "      <th>nrc</th>\n",
              "      <th>lmcd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.169775</td>\n",
              "      <td>0.169775</td>\n",
              "      <td>0.242536</td>\n",
              "      <td>0.595849</td>\n",
              "      <td>0.151585</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.242536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.226023</td>\n",
              "      <td>0.094491</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   element_id  sentence_id  word_count  ...  sentiword  nrc      lmcd\n",
              "0           1            1        17.0  ...   0.151585  0.0  0.242536\n",
              "1           2            1         7.0  ...   0.094491  0.0  0.000000\n",
              "\n",
              "[2 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Duhjc9DtyK-6",
        "outputId": "73d0797d-1f02-473b-af73-c7ced280cd86"
      },
      "source": [
        "corpus_sentimentr_df.shape"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7700, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "X05awke6vYdu",
        "outputId": "0afcd51e-658c-48e6-9ab9-0feda3df11cb"
      },
      "source": [
        "# Test\n",
        "\n",
        "!pip install kmeans1d"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting kmeans1d\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/c5/c89bd26504c57f0b0c5e0dff7514e5e64d55dc9cc04d85e5ae112a9239b6/kmeans1d-0.3.1-cp37-cp37m-manylinux2014_x86_64.whl (93kB)\n",
            "\r\u001b[K     |███▌                            | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 20kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 30kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 81kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: kmeans1d\n",
            "Successfully installed kmeans1d-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "SBWhXS9bxrtd",
        "outputId": "c1f7a9b6-af8b-4fbe-938a-f8505a19e2a8"
      },
      "source": [
        "import kmeans1d\n",
        "\n",
        "k = corpus_sentimentr_df.shape[0]//500  \n",
        "\n",
        "clusters, centroids = kmeans1d.cluster(np.array(corpus_sentimentr_df['jockers_rinker']), k)\n",
        "type(clusters)\n",
        "\n",
        "[[x,clusters.count(x)] for x in set(clusters)]\n",
        "centroids"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 10],\n",
              " [1, 92],\n",
              " [2, 215],\n",
              " [3, 324],\n",
              " [4, 437],\n",
              " [5, 559],\n",
              " [6, 626],\n",
              " [7, 2958],\n",
              " [8, 618],\n",
              " [9, 624],\n",
              " [10, 523],\n",
              " [11, 366],\n",
              " [12, 232],\n",
              " [13, 106],\n",
              " [14, 10]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1.368015009855796,\n",
              " -0.7910295756635374,\n",
              " -0.5451577889958344,\n",
              " -0.4022212257614058,\n",
              " -0.28858183852377894,\n",
              " -0.18794187257306585,\n",
              " -0.09116783093956264,\n",
              " 0.0003301146611772096,\n",
              " 0.09738225284515498,\n",
              " 0.18855992595452264,\n",
              " 0.2841837109825539,\n",
              " 0.40558874055845845,\n",
              " 0.5622555762714548,\n",
              " 0.8145846115190027,\n",
              " 1.500069915199759]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJp4DgcIwa7A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "outputId": "55efc268-732e-42e7-d745-208cd7b71729"
      },
      "source": [
        "# (Optional) Read Sentiment Series generated in RStudio by SentimentR into DataFrame: corpus_sentimentr_df\n",
        "#            SKIP if no SyuzhetR sentiment datafile to read in\n",
        "\n",
        "sum_sentiment_sentimentR_filename = 'sum_sentiments_sentimentR_7models.csv'\n",
        "corpus_sentimentr_df = pd.read_csv(sum_sentiment_sentimentR_filename)\n",
        "\n",
        "corpus_sentimentr_df.head(2)\n",
        "corpus_sentimentr_df.info()\n",
        "corpus_sentimentr_df.columns\n",
        "\n",
        "corpus_sentimentr_len = corpus_sentimentr_df.shape[0]\n",
        "\n",
        "# BUG FIX: SentimentR seems to create one additional row that must be deleted \n",
        "#          to enable it to be merged with other Sentiment Models on the same Corpus\n",
        "#\n",
        "#          Simplification, 1D cluster jockers_rinker column as proxy for full interrow distance features\n",
        "#                          and delete one row near the median from the largest cluster\n",
        "\n",
        "import kmeans1d\n",
        "\n",
        "# Approximate k cluster number as 1 cluster for every 500 sentences in Corpus\n",
        "k = corpus_sentimentr_df.shape[0]%500  \n",
        "clusters, centroids = kmeans1d.cluster(np.array(corpus_sentimentr_df['jockers_rinker']), k)\n",
        "\n",
        "def del_oneincluster(df, cluster_per=1):\n",
        "  '''\n",
        "  TODO: Skip for now and use kmeans1d instead\n",
        "  Given a DataFrame and a Cluster Percent to calculate a sliding window\n",
        "  Return DataFrame with one row removed within a sliding window cluster with most self-similiar rows\n",
        "  '''\n",
        "\n",
        "  # Compute sliding window for cluster size\n",
        "  win_cluster_len = int(cluster_per/100 * df.shape[0])\n",
        "  win_start = 0\n",
        "  win_stop = df.shape[0] - win_cluster_len\n",
        "\n",
        "  # Get numeric columns\n",
        "  numeric_df = df.select_dtypes(include=numerics)\n",
        "\n",
        "  most_selfsimilar_value = 0\n",
        "  most_selfsimilar_index = 0\n",
        "  for i in range(win_start, win_stop, 1):\n",
        "    selfsim_score = selfsim_metric(numeric_df.iloc[i:win_cluster_len+1])\n",
        "    if selfsim_score > most_selfsimilar_value:\n",
        "      most_selfsimilar_index = i\n",
        "\n",
        "  oneless_df = del_onerow(most_selfsimilar_index)\n",
        "\n",
        "  return oneless_df\n",
        "\n",
        "if corpus_sentimentr_len != corpus_sents_df.shape[0]:\n",
        "  print('\\n\\n\\n======================================================================\\n')\n",
        "  print(f'ERROR: sentence sentiment values read into corpus_syuzhetr (len={corpus_sentimentr_len})')\n",
        "  print(f'       is not the same length as corpus_sents_df (len={corpus_sents_df.shape[0]}) ')\n",
        "  print(f'\\nRECOMMENDATION: Use the preprocessed corpus output created by this notebook ')\n",
        "  print(f'                as input to SentimentR in RStudio to generate sentiment series')\n",
        "  print(f'                and then retry importing')\n",
        "  print('\\n======================================================================\\n')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 17.0,\n            'f': \"17.0\",\n        },\n{\n            'v': 0.16977493752543302,\n            'f': \"0.16977493752543302\",\n        },\n{\n            'v': 0.16977493752543302,\n            'f': \"0.16977493752543302\",\n        },\n{\n            'v': 0.24253562503633302,\n            'f': \"0.24253562503633302\",\n        },\n{\n            'v': 0.595849396808011,\n            'f': \"0.595849396808011\",\n        },\n{\n            'v': 0.151584765647708,\n            'f': \"0.151584765647708\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.24253562503633302,\n            'f': \"0.24253562503633302\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 7.0,\n            'f': \"7.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.226022754859518,\n            'f': \"0.226022754859518\",\n        },\n{\n            'v': 0.0944911182523068,\n            'f': \"0.0944911182523068\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"element_id\"], [\"number\", \"sentence_id\"], [\"number\", \"word_count\"], [\"number\", \"jockers_rinker\"], [\"number\", \"jockers\"], [\"number\", \"huliu\"], [\"number\", \"senticnet\"], [\"number\", \"sentiword\"], [\"number\", \"nrc\"], [\"number\", \"lmcd\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>element_id</th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>word_count</th>\n",
              "      <th>jockers_rinker</th>\n",
              "      <th>jockers</th>\n",
              "      <th>huliu</th>\n",
              "      <th>senticnet</th>\n",
              "      <th>sentiword</th>\n",
              "      <th>nrc</th>\n",
              "      <th>lmcd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.169775</td>\n",
              "      <td>0.169775</td>\n",
              "      <td>0.242536</td>\n",
              "      <td>0.595849</td>\n",
              "      <td>0.151585</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.242536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.226023</td>\n",
              "      <td>0.094491</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   element_id  sentence_id  word_count  ...  sentiword  nrc      lmcd\n",
              "0           1            1        17.0  ...   0.151585  0.0  0.242536\n",
              "1           2            1         7.0  ...   0.094491  0.0  0.000000\n",
              "\n",
              "[2 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7700 entries, 0 to 7699\n",
            "Data columns (total 10 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   element_id      7700 non-null   int64  \n",
            " 1   sentence_id     7700 non-null   int64  \n",
            " 2   word_count      7691 non-null   float64\n",
            " 3   jockers_rinker  7700 non-null   float64\n",
            " 4   jockers         7700 non-null   float64\n",
            " 5   huliu           7700 non-null   float64\n",
            " 6   senticnet       7700 non-null   float64\n",
            " 7   sentiword       7700 non-null   float64\n",
            " 8   nrc             7700 non-null   float64\n",
            " 9   lmcd            7700 non-null   float64\n",
            "dtypes: float64(8), int64(2)\n",
            "memory usage: 601.7 KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['element_id', 'sentence_id', 'word_count', 'jockers_rinker', 'jockers',\n",
              "       'huliu', 'senticnet', 'sentiword', 'nrc', 'lmcd'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "\n",
            "ERROR: sentence sentiment values read into corpus_syuzhetr (len=7700)\n",
            "       is not the same length as corpus_sents_df (len=7679) \n",
            "\n",
            "RECOMMENDATION: Use the preprocessed corpus output created by this notebook \n",
            "                as input to SentimentR in RStudio to generate sentiment series\n",
            "                and then retry importing\n",
            "\n",
            "======================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1d2Q3zqSES1"
      },
      "source": [
        "#### **Import SyuzhetR Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrxrUOf0w4o7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "59e9d1d5-524a-4c85-cd37-d82fabda7520"
      },
      "source": [
        "# (Optional) Read Sentiment Series generated in RStudio by SyuzhetR into DataFrame: corpus_syuzhetr_df\n",
        "#            SKIP if no SyuzhetR sentiment datafile to read in\n",
        "\n",
        "sum_sentiment_syuzhetR_filename = 'sum_sentiments_syuzhetR_4models.csv'\n",
        "corpus_syuzhetr_df = pd.read_csv(sum_sentiment_syuzhetR_filename)\n",
        "\n",
        "corpus_syuzhetr_df.head(2)\n",
        "corpus_syuzhetr_df.info()\n",
        "corpus_syuzhetr_df.columns\n",
        "\n",
        "corpus_syuzhetr_len = corpus_syuzhetr_df.shape[0]\n",
        "\n",
        "if corpus_syuzhetr_len != corpus_sents_df.shape[0]:\n",
        "  print('\\n\\n\\n======================================================================\\n')\n",
        "  print(f'ERROR: sentence sentiment values read into corpus_syuzhetr (len={corpus_syuzhetr_len})')\n",
        "  print(f'       is not the same length as corpus_sents_df (len={corpus_sents_df.shape[0]}) ')\n",
        "  print(f'\\nRECOMMENDATION: Use the preprocessed corpus output created by this notebook ')\n",
        "  print(f'                as input to SyuzhetR in RStudio to generate sentiment series')\n",
        "  print(f'                and then retry importing')\n",
        "  print('\\n======================================================================\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"But remember, please, the Law by which we live, We are not built to comprehend a lie ...\",\n{\n            'v': 1.3,\n            'f': \"1.3\",\n        },\n{\n            'v': -1,\n            'f': \"-1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n\"--Rudyard Kipling, \\\"The Secret of the Machines\\\"\",\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"string\", \"sent_raw\"], [\"number\", \"syuzhet\"], [\"number\", \"bing\"], [\"number\", \"afinn\"], [\"number\", \"nrc\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_no</th>\n",
              "      <th>sent_raw</th>\n",
              "      <th>syuzhet</th>\n",
              "      <th>bing</th>\n",
              "      <th>afinn</th>\n",
              "      <th>nrc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>But remember, please, the Law by which we live...</td>\n",
              "      <td>1.3</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>--Rudyard Kipling, \"The Secret of the Machines\"</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sent_no                                           sent_raw  ...  afinn  nrc\n",
              "0        0  But remember, please, the Law by which we live...  ...      1    0\n",
              "1        1    --Rudyard Kipling, \"The Secret of the Machines\"  ...      0    0\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7679 entries, 0 to 7678\n",
            "Data columns (total 6 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   sent_no   7679 non-null   int64  \n",
            " 1   sent_raw  7679 non-null   object \n",
            " 2   syuzhet   7679 non-null   float64\n",
            " 3   bing      7679 non-null   int64  \n",
            " 4   afinn     7679 non-null   int64  \n",
            " 5   nrc       7679 non-null   int64  \n",
            "dtypes: float64(1), int64(4), object(1)\n",
            "memory usage: 360.1+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sent_no', 'sent_raw', 'syuzhet', 'bing', 'afinn', 'nrc'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8wOUSyOSIVs"
      },
      "source": [
        "#### **Import Transformer Sentiment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96UWYvAyw4jA"
      },
      "source": [
        "# Read in Section DataFrame: corpus_sents_df\n",
        "\n",
        "corpus_sects_df = pd.read_csv('corpus_sections_lexrules_vatestsesyafbipasesenr_ianmcewan_machineslikeme_20210629_1749.csv')\n",
        "corpus_sects_df.head(2)\n",
        "corpus_sects_df.info()\n",
        "corpus_sects_df.columns\n",
        "\n",
        "\n",
        "print(f\\n\\n==================================================)\n",
        "print(f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0wVV0WHPxeO"
      },
      "source": [
        "# Verify "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k80YU4ZoHCs2"
      },
      "source": [
        "### **Calculate Lexical Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhthN28XHIVC"
      },
      "source": [
        "# models_ls = ['vader', 'textblob', 'stanza', 'afinn', 'sentimentr', 'syuzhet', 'bing', 'pattern', 'sentiword', 'senticnet', 'nrc']\n",
        "\n",
        "# corpus_lexicons_stats_dt = {}\n",
        "\n",
        "# for amodel in models_ls:\n",
        "#   get_lexstats(amodel)\n",
        "\n",
        "# Validate\n",
        "# corpus_lexicons_stats_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_CokfceWPtfD",
        "outputId": "8f1dfcac-ec3a-4cd1-fcff-8fb80faf070b"
      },
      "source": [
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7679, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsEbvCoCX7HY"
      },
      "source": [
        "## **(Optional) Calculate Sentiment Polarities (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZK1gA47xzkS"
      },
      "source": [
        "### **Select Sentiment Models (Manual)**\n",
        "\n",
        "NOTE:\n",
        "\n",
        "* Stanza (Stanford OpenNLP) can take upto 50 minutes to run\n",
        "\n",
        "* Listed in increasing order of (approx) run time\n",
        "\n",
        "* MPQA/SentiStrength not yet implemented (placeholders only for now)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0pStg1gJTZA"
      },
      "source": [
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "NCR_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = False #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0IACAN5JTZC"
      },
      "source": [
        "# Create and Verify custom list of Models to include\n",
        "\n",
        "MODELS_CUSTOM_LS = []\n",
        "\n",
        "if VADER_Arc:\n",
        "  MODELS_CUSTOM_LS.append('vader')\n",
        "if TextBlob_Arc:\n",
        "  MODELS_CUSTOM_LS.append('textblob')\n",
        "if Stanza_Arc:\n",
        "  MODELS_CUSTOM_LS.append('stanza')\n",
        "if SentimentR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentimentr')\n",
        "if Syuzhet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('syuzhet')\n",
        "if AFINN_Arc:\n",
        "  MODELS_CUSTOM_LS.append('afinn')\n",
        "if Bing_Arc:\n",
        "  MODELS_CUSTOM_LS.append('bing')\n",
        "if Pattern_Arc:\n",
        "  MODELS_CUSTOM_LS.append('pattern')\n",
        "if SentiWord_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentiword')\n",
        "if SenticNet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('senticnet')\n",
        "if NCR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('nrc')\n",
        "\n",
        "print(f'Here are the Models we are using to ensemble and save:\\n   {MODELS_CUSTOM_LS}')\n",
        "\n",
        "models_incl_ls = []\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  models_incl_ls.append(amodel[:2])\n",
        "models_incl_str = ''.join(models_incl_ls)\n",
        "\n",
        "print(f'Here is a custom name abbr: {models_incl_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw0JPNe6T2ap",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0509022a-5ac0-4700-b617-261b90002f6b"
      },
      "source": [
        "# Calculate (win_(x)1per) 1% of Corpus length for smallest (odd-valued) rolling window\n",
        "\n",
        "# Sentences\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "win_raw_s1per = int(corpus_sents_len * 0.01)\n",
        "# print(f'1% Rolling Window: {win_raw_s1per}')\n",
        "\n",
        "if win_raw_s1per % 2:\n",
        "  win_s1per = win_raw_s1per\n",
        "else:\n",
        "  win_s1per = win_raw_s1per + 1\n",
        "\n",
        "# Paragraphs\n",
        "\n",
        "corpus_parags_len = corpus_parags_df.shape[0]\n",
        "\n",
        "win_raw_p1per = int(corpus_parags_len * 0.01)\n",
        "# print(f'1% Rolling Window: {win_raw_1per}')\n",
        "\n",
        "if win_raw_p1per % 2:\n",
        "  win_p1per = win_raw_p1per\n",
        "else:\n",
        "  win_p1per = win_raw_p1per + 1\n",
        "\n",
        "\n",
        "# Sections\n",
        "\n",
        "# NO NEED FOR SLIDING WINDOW ON SECTIONS\n",
        "\n",
        "\n",
        "print(f'Sentence 1 Percent window: {win_s1per}')\n",
        "print(f'Paragraph 1 Percent window: {win_p1per}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence 1 Percent window: 77\n",
            "Paragraph 1 Percent window: 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGjfPK9u21HH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "1c6d074b-af3e-46d1-bc2a-9f3799ac0ef5"
      },
      "source": [
        "# Verify hash files for sentiment lexicons\n",
        "\n",
        "!pwd\n",
        "!ls hash*csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/research/2021/sa_book_code/books_sa/imcewan_machineslikeme\n",
            "hash_sentiment_bing.csv       hash_sentiment_sentimentr.csv\n",
            "hash_sentiment_nrc.csv\t      hash_sentiment_sentiword.csv\n",
            "hash_sentiment_senticnet.csv  hash_sentiment_syuzhet.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tAxuAxU7ueg"
      },
      "source": [
        "### **Calculate SentimentR (Jockers-Rinker) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foQeDPQpvyB3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "284a1f0d-600e-4edb-96fc-1399bcef75fb"
      },
      "source": [
        "model_base = 'sentimentr'\n",
        "model_name = 'sentimentr_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq7dImOJozm5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "e7a49eed-eedf-4be6-c929-858c683e9090"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "\n",
        "  lexicon_sentimentr_df = get_lexicon('hash_sentiment_sentimentr.csv')\n",
        "  lexicon_sentimentr_df['x'] = lexicon_sentimentr_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_sentimentr_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_sentimentr_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_sentimentr_df.head()\n",
        "    lexicon_sentimentr_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found hash_sentiment_sentimentr.csv in lexicon_directory)\n",
            "cp_cmd = copy ../sa_lexicons/hash_sentiment_sentimentr.csv ./\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11710 entries, 0 to 11709\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Unnamed: 0  11710 non-null  int64  \n",
            " 1   x           11710 non-null  object \n",
            " 2   y           11710 non-null  float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 274.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25_Zjja0o_Hx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "850ec7a6-a983-4f17-8bf8-b2fa2f0aa3db"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SentimentR_Arc == True:\n",
        "\n",
        "  id = lexicon_sentimentr_df.word.values\n",
        "  values = lexicon_sentimentr_df.polarity.values\n",
        "\n",
        "  lexicon_sentimentr_dt = dict(zip(id, values))\n",
        "  # lexicon_sentimentr_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_sentimentr_dt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxe15XjNwByS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "03c8b0fe-df1a-407f-d526-a5481e00120a"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_sentimentr(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_sentimentr_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SentimentR_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentimentr, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbGz2Yi5wByY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "f2d9708e-4465-49eb-bc68-3297320a3070"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentimentr_chaps': {'sentiment_max': 148.9999999999999,\n",
              "  'sentiment_min': 3.1000000000000116},\n",
              " 'sentimentr_parags': {'sentiment_max': 17.55, 'sentiment_min': -11.7},\n",
              " 'sentimentr_sects': {'sentiment_max': 96.14999999999986,\n",
              "  'sentiment_min': -17.149999999999977},\n",
              " 'sentimentr_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZjqwTvU76AR"
      },
      "source": [
        "### **Calculate Syuzhet (Jockers) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjOXGQX54lbX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f61a2b44-7066-4505-be0a-459755fbd6a8"
      },
      "source": [
        "model_base = 'syuzhet'\n",
        "model_name = 'syuzhet_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkhaGcuy4lbg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "166c1e70-128c-475a-8d78-d5446f05d4c7"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "\n",
        "  lexicon_syuzhet_df = get_lexicon('hash_sentiment_syuzhet.csv')\n",
        "  lexicon_syuzhet_df['word'] = lexicon_syuzhet_df['word'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_syuzhet_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_syuzhet_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_syuzhet_df.head()\n",
        "    lexicon_syuzhet_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found hash_sentiment_syuzhet.csv in lexicon_directory)\n",
            "cp_cmd = copy ../sa_lexicons/hash_sentiment_syuzhet.csv ./\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10748 entries, 0 to 10747\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Unnamed: 0  10748 non-null  int64  \n",
            " 1   word        10748 non-null  object \n",
            " 2   value       10748 non-null  float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 252.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9IuINuR4lbi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9d631ab1-9a8c-4ab5-fb32-4ef75ede4875"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "\n",
        "  id = lexicon_syuzhet_df.word.values\n",
        "  values = lexicon_syuzhet_df.value.values\n",
        "\n",
        "  lexicon_syuzhet_dt = dict(zip(id, values))\n",
        "  # lexicon_sentimentr_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_syuzhet_dt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO-txaFL4lbo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8bce2766-2c68-4f35-a40d-85b5b18309ac"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_syuzhet(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_syuzhet_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "\n",
        "if Syuzhet_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_syuzhet, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfsLZSo04lbp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "07f7e0df-b668-4fd3-e4e1-8e214cb2bcdc"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentimentr_chaps': {'sentiment_max': 148.9999999999999,\n",
              "  'sentiment_min': 3.1000000000000116},\n",
              " 'sentimentr_parags': {'sentiment_max': 17.55, 'sentiment_min': -11.7},\n",
              " 'sentimentr_sects': {'sentiment_max': 96.14999999999986,\n",
              "  'sentiment_min': -17.149999999999977},\n",
              " 'sentimentr_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'syuzhet_chaps': {'sentiment_max': 145.19999999999985,\n",
              "  'sentiment_min': -1.000000000000015},\n",
              " 'syuzhet_parags': {'sentiment_max': 18.05, 'sentiment_min': -11.7},\n",
              " 'syuzhet_sects': {'sentiment_max': 91.19999999999989,\n",
              "  'sentiment_min': -17.049999999999976},\n",
              " 'syuzhet_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA3dWsnF78mi"
      },
      "source": [
        "### **Calculate Bing (HuLiu) Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wvq1prK7n1k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d2a8ddd1-f87d-454e-bbef-1fda0aabb13f"
      },
      "source": [
        "model_base = 'bing'\n",
        "model_name = 'bing_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZELGxS8y9PiS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "da26d1a0-cdfa-4575-8232-079fa2c493f6"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if Bing_Arc == True:\n",
        "\n",
        "  lexicon_bing_df = get_lexicon('hash_sentiment_bing.csv')\n",
        "  lexicon_bing_df['x'] = lexicon_bing_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_bing_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_bing_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_bing_df.head()\n",
        "    lexicon_bing_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found hash_sentiment_bing.csv in lexicon_directory)\n",
            "cp_cmd = copy ../sa_lexicons/hash_sentiment_bing.csv ./\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6874 entries, 0 to 6873\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Unnamed: 0  6874 non-null   int64  \n",
            " 1   x           6874 non-null   object \n",
            " 2   y           6874 non-null   float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 161.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJQTaeue9VjI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "193f3fea-db42-4dbc-e244-bdba3a60f9e3"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if Bing_Arc == True:\n",
        "\n",
        "  id = lexicon_bing_df.word.values\n",
        "  values = lexicon_bing_df.polarity.values\n",
        "\n",
        "  lexicon_bing_dt = dict(zip(id, values))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Ffzu1m7n10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c627161f-e9d5-4c53-d5c1-1d6098272c54"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_bing(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_bing_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Bing_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_bing, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWITnL2a9rrx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "531cd622-5462-4dd6-9319-7bce3ca04f40"
      },
      "source": [
        "# Calculate Bing Sentiment [0,1,2]\n",
        "\n",
        "def bing_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += lex_discrete2continous_sentiment(str(aword), lexicon_bing_dt)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+1)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xpzm3hi7n1x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ca63e7aa-34b4-4bf4-bb3b-0d4249b97a16"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Bing_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(bing_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-47.650535804050435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UFEItnO-i7h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "bd5c54fa-8cab-4cf7-f026-04b2f2c13cb9"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Bing_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=bing_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4thQWz3-i7k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "f3bc685b-e0da-4755-b202-24c51406453c"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bing_chaps': {'sentiment_max': 591.1048063253189,\n",
              "  'sentiment_min': -665.5682824208717},\n",
              " 'bing_parags': {'sentiment_max': 233.62539105697772,\n",
              "  'sentiment_min': -231.75792169846113},\n",
              " 'bing_sects': {'sentiment_max': 721.8836270105633,\n",
              "  'sentiment_min': -635.8012840187225},\n",
              " 'bing_sents': {'sentiment_max': 143.60753220597746,\n",
              "  'sentiment_min': -159.0419852720603},\n",
              " 'sentimentr_chaps': {'sentiment_max': 148.9999999999999,\n",
              "  'sentiment_min': 3.1000000000000116},\n",
              " 'sentimentr_parags': {'sentiment_max': 17.55, 'sentiment_min': -11.7},\n",
              " 'sentimentr_sects': {'sentiment_max': 96.14999999999986,\n",
              "  'sentiment_min': -17.149999999999977},\n",
              " 'sentimentr_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'syuzhet_chaps': {'sentiment_max': 145.19999999999985,\n",
              "  'sentiment_min': -1.000000000000015},\n",
              " 'syuzhet_parags': {'sentiment_max': 18.05, 'sentiment_min': -11.7},\n",
              " 'syuzhet_sects': {'sentiment_max': 91.19999999999989,\n",
              "  'sentiment_min': -17.049999999999976},\n",
              " 'syuzhet_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkNZVk128jV9"
      },
      "source": [
        "### **Calculate SentiWord Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnL4ORNp8MgB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "bb3a1987-41f0-40f0-c1b8-84369d10504a"
      },
      "source": [
        "model_base = 'sentiword'\n",
        "model_name = 'sentiword_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpVmt0fK8xi_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "e83c0007-cffe-4935-df28-f75853da7fee"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "\n",
        "  lexicon_sentiword_df = get_lexicon('hash_sentiment_sentiword.csv')\n",
        "  lexicon_sentiword_df['x'] = lexicon_sentiword_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_sentiword_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_sentiword_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_sentiword_df.head()\n",
        "    lexicon_sentiword_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found hash_sentiment_sentiword.csv in lexicon_directory)\n",
            "cp_cmd = copy ../sa_lexicons/hash_sentiment_sentiword.csv ./\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20093 entries, 0 to 20092\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Unnamed: 0  20093 non-null  int64  \n",
            " 1   x           20092 non-null  object \n",
            " 2   y           20093 non-null  float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 471.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwtK2M9h879M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7a733ddd-702e-4529-cf92-8fec052935c7"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SentiWord_Arc == True:\n",
        "\n",
        "  id = lexicon_sentiword_df.word.values\n",
        "  values = lexicon_sentiword_df.polarity.values\n",
        "\n",
        "  lexicon_sentiword_dt = dict(zip(id, values))\n",
        "  # lexicon_sentiword_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(text2sentiment(sent_test, lexicon_sentiword_dt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I-QwB478MgP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b156ee82-db7e-43be-8c03-b36eaafa03ec"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_sentiword(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on sentimentr lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_sentiword_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SentiWord_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentiword, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbEF88568MgS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "12ff5426-555f-4591-8a45-854c3bb4ae03"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bing_chaps': {'sentiment_max': 591.1048063253189,\n",
              "  'sentiment_min': -665.5682824208717},\n",
              " 'bing_parags': {'sentiment_max': 233.62539105697772,\n",
              "  'sentiment_min': -231.75792169846113},\n",
              " 'bing_sects': {'sentiment_max': 721.8836270105633,\n",
              "  'sentiment_min': -635.8012840187225},\n",
              " 'bing_sents': {'sentiment_max': 143.60753220597746,\n",
              "  'sentiment_min': -159.0419852720603},\n",
              " 'sentimentr_chaps': {'sentiment_max': 148.9999999999999,\n",
              "  'sentiment_min': 3.1000000000000116},\n",
              " 'sentimentr_parags': {'sentiment_max': 17.55, 'sentiment_min': -11.7},\n",
              " 'sentimentr_sects': {'sentiment_max': 96.14999999999986,\n",
              "  'sentiment_min': -17.149999999999977},\n",
              " 'sentimentr_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'sentiword_chaps': {'sentiment_max': 42.542095634920614,\n",
              "  'sentiment_min': -1.5734063492063277},\n",
              " 'sentiword_parags': {'sentiment_max': 7.645183333333333,\n",
              "  'sentiment_min': -6.530059523809523},\n",
              " 'sentiword_sects': {'sentiment_max': 39.65655039682539,\n",
              "  'sentiment_min': -14.01572857142857},\n",
              " 'sentiword_sents': {'sentiment_max': 4.614583333333333,\n",
              "  'sentiment_min': -3.635416666666667},\n",
              " 'syuzhet_chaps': {'sentiment_max': 145.19999999999985,\n",
              "  'sentiment_min': -1.000000000000015},\n",
              " 'syuzhet_parags': {'sentiment_max': 18.05, 'sentiment_min': -11.7},\n",
              " 'syuzhet_sects': {'sentiment_max': 91.19999999999989,\n",
              "  'sentiment_min': -17.049999999999976},\n",
              " 'syuzhet_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUcANLM_8mtT"
      },
      "source": [
        "### **Calculate SenticNet Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://sentic.net/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OqhSberA1hZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "226e626f-d90b-412c-f713-2b998f035f26"
      },
      "source": [
        "model_base = 'senticnet'\n",
        "model_name = 'senticnet_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmMfvQvYBBoM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "b4eccfce-135c-4319-c3d3-5340a82a0e71"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "\n",
        "  lexicon_senticnet_df = get_lexicon('hash_sentiment_senticnet.csv')\n",
        "  lexicon_senticnet_df['x'] = lexicon_senticnet_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_senticnet_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_senticnet_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_senticnet_df.head()\n",
        "    lexicon_senticnet_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found hash_sentiment_senticnet.csv in lexicon_directory)\n",
            "cp_cmd = copy ../sa_lexicons/hash_sentiment_senticnet.csv ./\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 23626 entries, 0 to 23625\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Unnamed: 0  23626 non-null  int64  \n",
            " 1   x           23625 non-null  object \n",
            " 2   y           23626 non-null  float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 553.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2oS71STBIUS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "34948c18-4acb-4ee1-accb-84d4ce8724c5"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if SenticNet_Arc == True:\n",
        "\n",
        "  id = lexicon_senticnet_df.word.values\n",
        "  values = lexicon_senticnet_df.polarity.values\n",
        "\n",
        "  lexicon_senticnet_dt =dict(zip(id, values))\n",
        "  # lexicon_jockersrinker_dt\n",
        "\n",
        "  # Test\n",
        "  sent_test='I hate Mondays.'\n",
        "  text2sentiment(sent_test, lexicon_senticnet_dt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHQwXBl5BlRM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "82577e5a-efdd-422e-8c58-e4e36b3474c7"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Sentiment evaluation function\n",
        "def sentiment_senticnet(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return corresponding sentiment value based on senticnet lexicon\n",
        "  '''\n",
        "  \n",
        "  sentiment_val = text2sentiment(str(text_str), lexicon_senticnet_dt)\n",
        "\n",
        "  return sentiment_val \n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if SenticNet_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sentiment_senticnet, sentiment_type='lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raHUj3a4A1hs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "outputId": "68def2bc-96ce-4d25-c455-a790c4fc785e"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bing_chaps': {'sentiment_max': 591.1048063253189,\n",
              "  'sentiment_min': -665.5682824208717},\n",
              " 'bing_parags': {'sentiment_max': 233.62539105697772,\n",
              "  'sentiment_min': -231.75792169846113},\n",
              " 'bing_sects': {'sentiment_max': 721.8836270105633,\n",
              "  'sentiment_min': -635.8012840187225},\n",
              " 'bing_sents': {'sentiment_max': 143.60753220597746,\n",
              "  'sentiment_min': -159.0419852720603},\n",
              " 'senticnet_chaps': {'sentiment_max': 354.84699999999896,\n",
              "  'sentiment_min': 235.29899999999932},\n",
              " 'senticnet_parags': {'sentiment_max': 21.69200000000001,\n",
              "  'sentiment_min': -5.824},\n",
              " 'senticnet_sects': {'sentiment_max': 187.32800000000006,\n",
              "  'sentiment_min': 42.55199999999999},\n",
              " 'senticnet_sents': {'sentiment_max': 10.079999999999998,\n",
              "  'sentiment_min': -4.052},\n",
              " 'sentimentr_chaps': {'sentiment_max': 148.9999999999999,\n",
              "  'sentiment_min': 3.1000000000000116},\n",
              " 'sentimentr_parags': {'sentiment_max': 17.55, 'sentiment_min': -11.7},\n",
              " 'sentimentr_sects': {'sentiment_max': 96.14999999999986,\n",
              "  'sentiment_min': -17.149999999999977},\n",
              " 'sentimentr_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'sentiword_chaps': {'sentiment_max': 42.542095634920614,\n",
              "  'sentiment_min': -1.5734063492063277},\n",
              " 'sentiword_parags': {'sentiment_max': 7.645183333333333,\n",
              "  'sentiment_min': -6.530059523809523},\n",
              " 'sentiword_sects': {'sentiment_max': 39.65655039682539,\n",
              "  'sentiment_min': -14.01572857142857},\n",
              " 'sentiword_sents': {'sentiment_max': 4.614583333333333,\n",
              "  'sentiment_min': -3.635416666666667},\n",
              " 'syuzhet_chaps': {'sentiment_max': 145.19999999999985,\n",
              "  'sentiment_min': -1.000000000000015},\n",
              " 'syuzhet_parags': {'sentiment_max': 18.05, 'sentiment_min': -11.7},\n",
              " 'syuzhet_sects': {'sentiment_max': 91.19999999999989,\n",
              "  'sentiment_min': -17.049999999999976},\n",
              " 'syuzhet_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn4KQYpH3glK"
      },
      "source": [
        "### **Calculate NCR Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USx6LRmoCFV8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9d4d220d-6321-456a-f623-4a30182216c8"
      },
      "source": [
        "model_base = 'nrc'\n",
        "model_name = 'nrc_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFEszSc13glL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "a7630c93-7cba-4906-d2c6-83a8062fe69b"
      },
      "source": [
        "# Read Lexicon into DataFrame \n",
        "\n",
        "if NCR_Arc == True:\n",
        "\n",
        "  lexicon_nrc_df = get_lexicon('hash_sentiment_nrc.csv')\n",
        "  lexicon_nrc_df['x'] = lexicon_nrc_df['x'].astype('string')\n",
        "\n",
        "  # Clean/Reorg DataFrame\n",
        "  lexicon_nrc_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "  lexicon_nrc_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n",
        "\n",
        "  # Verify\n",
        "  if (PLOT_OUTPUT == 'All'):\n",
        "    lexicon_nrc_df.head()\n",
        "    lexicon_nrc_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found hash_sentiment_nrc.csv in lexicon_directory)\n",
            "cp_cmd = copy ../sa_lexicons/hash_sentiment_nrc.csv ./\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5468 entries, 0 to 5467\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  5468 non-null   int64 \n",
            " 1   x           5468 non-null   object\n",
            " 2   y           5468 non-null   int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 128.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nAkx4Mv3glL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d2cabb62-9e79-4cfa-a576-53b12253d601"
      },
      "source": [
        "# Convert DataFrame to Dict[word] = polarity\n",
        "\n",
        "if NCR_Arc == True:\n",
        "\n",
        "  id = lexicon_nrc_df.word.values\n",
        "  values = lexicon_nrc_df.polarity.values\n",
        "\n",
        "  lexicon_nrc_dt =dict(zip(id, values))\n",
        "  # lexicon_jockersrinker_dt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv1nVSATb7z4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "82e0ff12-16c3-44fb-bf17-12c23aca2a88"
      },
      "source": [
        "# Calculate NRC Sentiment [0,1,2]\n",
        "\n",
        "def nrc_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += lex_discrete2continous_sentiment(str(aword), lexicon_nrc_dt)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+10)\n",
        "\n",
        "  return text_sentiment_norm\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss3Xixo_CX2q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e4969dd4-8381-4087-b4d9-264f802b52f8"
      },
      "source": [
        "# Test\n",
        "\n",
        "if NCR_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(nrc_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-9.010135447483094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhqnljMSCX2w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "37325f24-b227-4ff7-c883-6cc5c0cf8196"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if NCR_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=nrc_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2q97Dm-CX2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "outputId": "a37602c4-30b6-42e4-956c-835372aab934"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bing_chaps': {'sentiment_max': 591.1048063253189,\n",
              "  'sentiment_min': -665.5682824208717},\n",
              " 'bing_parags': {'sentiment_max': 233.62539105697772,\n",
              "  'sentiment_min': -231.75792169846113},\n",
              " 'bing_sects': {'sentiment_max': 721.8836270105633,\n",
              "  'sentiment_min': -635.8012840187225},\n",
              " 'bing_sents': {'sentiment_max': 143.60753220597746,\n",
              "  'sentiment_min': -159.0419852720603},\n",
              " 'nrc_chaps': {'sentiment_max': 992.1420156684644,\n",
              "  'sentiment_min': 136.36421709165032},\n",
              " 'nrc_parags': {'sentiment_max': 133.15206794388564,\n",
              "  'sentiment_min': -88.59571651997537},\n",
              " 'nrc_sects': {'sentiment_max': 682.0582426859623,\n",
              "  'sentiment_min': -126.83645806124086},\n",
              " 'nrc_sents': {'sentiment_max': 92.90655652278458,\n",
              "  'sentiment_min': -61.110488553550724},\n",
              " 'senticnet_chaps': {'sentiment_max': 354.84699999999896,\n",
              "  'sentiment_min': 235.29899999999932},\n",
              " 'senticnet_parags': {'sentiment_max': 21.69200000000001,\n",
              "  'sentiment_min': -5.824},\n",
              " 'senticnet_sects': {'sentiment_max': 187.32800000000006,\n",
              "  'sentiment_min': 42.55199999999999},\n",
              " 'senticnet_sents': {'sentiment_max': 10.079999999999998,\n",
              "  'sentiment_min': -4.052},\n",
              " 'sentimentr_chaps': {'sentiment_max': 148.9999999999999,\n",
              "  'sentiment_min': 3.1000000000000116},\n",
              " 'sentimentr_parags': {'sentiment_max': 17.55, 'sentiment_min': -11.7},\n",
              " 'sentimentr_sects': {'sentiment_max': 96.14999999999986,\n",
              "  'sentiment_min': -17.149999999999977},\n",
              " 'sentimentr_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'sentiword_chaps': {'sentiment_max': 42.542095634920614,\n",
              "  'sentiment_min': -1.5734063492063277},\n",
              " 'sentiword_parags': {'sentiment_max': 7.645183333333333,\n",
              "  'sentiment_min': -6.530059523809523},\n",
              " 'sentiword_sects': {'sentiment_max': 39.65655039682539,\n",
              "  'sentiment_min': -14.01572857142857},\n",
              " 'sentiword_sents': {'sentiment_max': 4.614583333333333,\n",
              "  'sentiment_min': -3.635416666666667},\n",
              " 'syuzhet_chaps': {'sentiment_max': 145.19999999999985,\n",
              "  'sentiment_min': -1.000000000000015},\n",
              " 'syuzhet_parags': {'sentiment_max': 18.05, 'sentiment_min': -11.7},\n",
              " 'syuzhet_sects': {'sentiment_max': 91.19999999999989,\n",
              "  'sentiment_min': -17.049999999999976},\n",
              " 'syuzhet_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRTjCPLb8cbB"
      },
      "source": [
        "### **Calculate Afinn Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://github.com/fnielsen/afinn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcnxqnyzDCde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b5dcc7c4-2ab7-48f6-fdde-b2bf62ba7939"
      },
      "source": [
        "model_base = 'afinn'\n",
        "model_name = 'afinn_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDG2KxvNBdj6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "811bc267-e667-4a94-b009-121156cf99ac"
      },
      "source": [
        "!pip install afinn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting afinn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/e5/ffbb7ee3cca21ac6d310ac01944fb163c20030b45bda25421d725d8a859a/afinn-0.1.tar.gz (52kB)\n",
            "\r\u001b[K     |██████▎                         | 10kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-cp37-none-any.whl size=53452 sha256=7b521747ef9d87c87e10f4002f11ef976ca9e7a88cb49dcdd212f31146e967eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/1c/de/428301f3333ca509dcf20ff358690eb23a1388fbcbbde008b2\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evnkXWL58CcX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a8b5038a-9c1e-4cf6-803a-2914b0725ccb"
      },
      "source": [
        "# Install and configure for English\n",
        "\n",
        "from afinn import Afinn\n",
        "afinn = Afinn(language='en')\n",
        "\n",
        "# Test\n",
        "\n",
        "# afinn.score('I had the worst day.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKbeW_cMXhmI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "405e1849-9e9a-4d62-d4a5-c615fe8b1cb4"
      },
      "source": [
        "# Calculate AFINN Sentiment [0,1,2]\n",
        "\n",
        "def afinn_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += afinn.score(aword)\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+0.1)\n",
        "\n",
        "  return float(text_sentiment_norm)  # return float vs np.float64"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRCd4VpwDO0Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "16499004-99ba-4d37-bc30-4f37166c076b"
      },
      "source": [
        "# Test\n",
        "\n",
        "if AFINN_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(afinn_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-2.5028944124489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrP_wxB3DO0S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b393dbe5-79b2-4ea0-e665-2c287a1d8799"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if AFINN_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=afinn_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgZaMPYKDO0T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "outputId": "2a4cc350-e918-4cb9-fbae-669538017b18"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'afinn_chaps': {'sentiment_max': 23.566333953237162,\n",
              "  'sentiment_min': -13.963760054626414},\n",
              " 'afinn_parags': {'sentiment_max': 30.0, 'sentiment_min': -30.0},\n",
              " 'afinn_sects': {'sentiment_max': 16.284196895116033,\n",
              "  'sentiment_min': -15.574896102419311},\n",
              " 'afinn_sents': {'sentiment_max': 40.0, 'sentiment_min': -30.0},\n",
              " 'bing_chaps': {'sentiment_max': 591.1048063253189,\n",
              "  'sentiment_min': -665.5682824208717},\n",
              " 'bing_parags': {'sentiment_max': 233.62539105697772,\n",
              "  'sentiment_min': -231.75792169846113},\n",
              " 'bing_sects': {'sentiment_max': 721.8836270105633,\n",
              "  'sentiment_min': -635.8012840187225},\n",
              " 'bing_sents': {'sentiment_max': 143.60753220597746,\n",
              "  'sentiment_min': -159.0419852720603},\n",
              " 'nrc_chaps': {'sentiment_max': 992.1420156684644,\n",
              "  'sentiment_min': 136.36421709165032},\n",
              " 'nrc_parags': {'sentiment_max': 133.15206794388564,\n",
              "  'sentiment_min': -88.59571651997537},\n",
              " 'nrc_sects': {'sentiment_max': 682.0582426859623,\n",
              "  'sentiment_min': -126.83645806124086},\n",
              " 'nrc_sents': {'sentiment_max': 92.90655652278458,\n",
              "  'sentiment_min': -61.110488553550724},\n",
              " 'senticnet_chaps': {'sentiment_max': 354.84699999999896,\n",
              "  'sentiment_min': 235.29899999999932},\n",
              " 'senticnet_parags': {'sentiment_max': 21.69200000000001,\n",
              "  'sentiment_min': -5.824},\n",
              " 'senticnet_sects': {'sentiment_max': 187.32800000000006,\n",
              "  'sentiment_min': 42.55199999999999},\n",
              " 'senticnet_sents': {'sentiment_max': 10.079999999999998,\n",
              "  'sentiment_min': -4.052},\n",
              " 'sentimentr_chaps': {'sentiment_max': 148.9999999999999,\n",
              "  'sentiment_min': 3.1000000000000116},\n",
              " 'sentimentr_parags': {'sentiment_max': 17.55, 'sentiment_min': -11.7},\n",
              " 'sentimentr_sects': {'sentiment_max': 96.14999999999986,\n",
              "  'sentiment_min': -17.149999999999977},\n",
              " 'sentimentr_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'sentiword_chaps': {'sentiment_max': 42.542095634920614,\n",
              "  'sentiment_min': -1.5734063492063277},\n",
              " 'sentiword_parags': {'sentiment_max': 7.645183333333333,\n",
              "  'sentiment_min': -6.530059523809523},\n",
              " 'sentiword_sects': {'sentiment_max': 39.65655039682539,\n",
              "  'sentiment_min': -14.01572857142857},\n",
              " 'sentiword_sents': {'sentiment_max': 4.614583333333333,\n",
              "  'sentiment_min': -3.635416666666667},\n",
              " 'syuzhet_chaps': {'sentiment_max': 145.19999999999985,\n",
              "  'sentiment_min': -1.000000000000015},\n",
              " 'syuzhet_parags': {'sentiment_max': 18.05, 'sentiment_min': -11.7},\n",
              " 'syuzhet_sects': {'sentiment_max': 91.19999999999989,\n",
              "  'sentiment_min': -17.049999999999976},\n",
              " 'syuzhet_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAEiglIPDfFI"
      },
      "source": [
        "### **Calculate VADER Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgkvefzpDgx-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "00b08093-2456-4905-d597-3195a5e852e9"
      },
      "source": [
        "model_base = 'vader'\n",
        "model_name = 'vader_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wodGtjXhDmZN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "10aee195-812d-4300-ddfc-9e0dfec98faf"
      },
      "source": [
        "# Sentiment evaluation function\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Test\n",
        "sid.polarity_scores('hello world')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS8e25MkDmZP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "798173d9-872e-473a-baf8-2abe2d583e3f"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "if VADER_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=sid.polarity_scores, sentiment_type='compound')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Azyv2lDmZP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eebb4587-0c8a-48e8-ff77-692462d540be"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'afinn_chaps': {'sentiment_max': 23.566333953237162,\n",
              "  'sentiment_min': -13.963760054626414},\n",
              " 'afinn_parags': {'sentiment_max': 30.0, 'sentiment_min': -30.0},\n",
              " 'afinn_sects': {'sentiment_max': 16.284196895116033,\n",
              "  'sentiment_min': -15.574896102419311},\n",
              " 'afinn_sents': {'sentiment_max': 40.0, 'sentiment_min': -30.0},\n",
              " 'bing_chaps': {'sentiment_max': 591.1048063253189,\n",
              "  'sentiment_min': -665.5682824208717},\n",
              " 'bing_parags': {'sentiment_max': 233.62539105697772,\n",
              "  'sentiment_min': -231.75792169846113},\n",
              " 'bing_sects': {'sentiment_max': 721.8836270105633,\n",
              "  'sentiment_min': -635.8012840187225},\n",
              " 'bing_sents': {'sentiment_max': 143.60753220597746,\n",
              "  'sentiment_min': -159.0419852720603},\n",
              " 'nrc_chaps': {'sentiment_max': 992.1420156684644,\n",
              "  'sentiment_min': 136.36421709165032},\n",
              " 'nrc_parags': {'sentiment_max': 133.15206794388564,\n",
              "  'sentiment_min': -88.59571651997537},\n",
              " 'nrc_sects': {'sentiment_max': 682.0582426859623,\n",
              "  'sentiment_min': -126.83645806124086},\n",
              " 'nrc_sents': {'sentiment_max': 92.90655652278458,\n",
              "  'sentiment_min': -61.110488553550724},\n",
              " 'senticnet_chaps': {'sentiment_max': 354.84699999999896,\n",
              "  'sentiment_min': 235.29899999999932},\n",
              " 'senticnet_parags': {'sentiment_max': 21.69200000000001,\n",
              "  'sentiment_min': -5.824},\n",
              " 'senticnet_sects': {'sentiment_max': 187.32800000000006,\n",
              "  'sentiment_min': 42.55199999999999},\n",
              " 'senticnet_sents': {'sentiment_max': 10.079999999999998,\n",
              "  'sentiment_min': -4.052},\n",
              " 'sentimentr_chaps': {'sentiment_max': 148.9999999999999,\n",
              "  'sentiment_min': 3.1000000000000116},\n",
              " 'sentimentr_parags': {'sentiment_max': 17.55, 'sentiment_min': -11.7},\n",
              " 'sentimentr_sects': {'sentiment_max': 96.14999999999986,\n",
              "  'sentiment_min': -17.149999999999977},\n",
              " 'sentimentr_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'sentiword_chaps': {'sentiment_max': 42.542095634920614,\n",
              "  'sentiment_min': -1.5734063492063277},\n",
              " 'sentiword_parags': {'sentiment_max': 7.645183333333333,\n",
              "  'sentiment_min': -6.530059523809523},\n",
              " 'sentiword_sects': {'sentiment_max': 39.65655039682539,\n",
              "  'sentiment_min': -14.01572857142857},\n",
              " 'sentiword_sents': {'sentiment_max': 4.614583333333333,\n",
              "  'sentiment_min': -3.635416666666667},\n",
              " 'syuzhet_chaps': {'sentiment_max': 145.19999999999985,\n",
              "  'sentiment_min': -1.000000000000015},\n",
              " 'syuzhet_parags': {'sentiment_max': 18.05, 'sentiment_min': -11.7},\n",
              " 'syuzhet_sects': {'sentiment_max': 91.19999999999989,\n",
              "  'sentiment_min': -17.049999999999976},\n",
              " 'syuzhet_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'vader_chaps': {'sentiment_max': 1.0, 'sentiment_min': -0.9964},\n",
              " 'vader_parags': {'sentiment_max': 0.9952, 'sentiment_min': -0.995},\n",
              " 'vader_sects': {'sentiment_max': 0.9999, 'sentiment_min': -0.9998},\n",
              " 'vader_sents': {'sentiment_max': 0.9836, 'sentiment_min': -0.9723}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCN4c-G48e7-"
      },
      "source": [
        "### **Calculate TextBlob Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MfVWZ34Vg8U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b915d51f-745f-4f26-86bd-2793313b6ed5"
      },
      "source": [
        "model_base = 'textblob'\n",
        "model_name = 'textblob_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "118Blghk7fjp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1709621d-5544-40a5-c194-0e674b629a71"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhJsYxPoVhY4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7fd25d9b-6eb5-4df0-9e5e-85cc4a35eb6a"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "def textblob_sentiment(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return a sentiment value between -1.0 to +1.0 using TextBlob\n",
        "  '''\n",
        "  return TextBlob(text_str).sentiment.polarity\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if TextBlob_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=textblob_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlHRf2aFVhY7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7227aaf-89f3-4600-9c31-626f57726c90"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_name, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_name, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_name, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_name, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'afinn_chaps': {'sentiment_max': 23.566333953237162,\n",
              "  'sentiment_min': -13.963760054626414},\n",
              " 'afinn_parags': {'sentiment_max': 30.0, 'sentiment_min': -30.0},\n",
              " 'afinn_sects': {'sentiment_max': 16.284196895116033,\n",
              "  'sentiment_min': -15.574896102419311},\n",
              " 'afinn_sents': {'sentiment_max': 40.0, 'sentiment_min': -30.0},\n",
              " 'bing_chaps': {'sentiment_max': 591.1048063253189,\n",
              "  'sentiment_min': -665.5682824208717},\n",
              " 'bing_parags': {'sentiment_max': 233.62539105697772,\n",
              "  'sentiment_min': -231.75792169846113},\n",
              " 'bing_sects': {'sentiment_max': 721.8836270105633,\n",
              "  'sentiment_min': -635.8012840187225},\n",
              " 'bing_sents': {'sentiment_max': 143.60753220597746,\n",
              "  'sentiment_min': -159.0419852720603},\n",
              " 'nrc_chaps': {'sentiment_max': 992.1420156684644,\n",
              "  'sentiment_min': 136.36421709165032},\n",
              " 'nrc_parags': {'sentiment_max': 133.15206794388564,\n",
              "  'sentiment_min': -88.59571651997537},\n",
              " 'nrc_sects': {'sentiment_max': 682.0582426859623,\n",
              "  'sentiment_min': -126.83645806124086},\n",
              " 'nrc_sents': {'sentiment_max': 92.90655652278458,\n",
              "  'sentiment_min': -61.110488553550724},\n",
              " 'senticnet_chaps': {'sentiment_max': 354.84699999999896,\n",
              "  'sentiment_min': 235.29899999999932},\n",
              " 'senticnet_parags': {'sentiment_max': 21.69200000000001,\n",
              "  'sentiment_min': -5.824},\n",
              " 'senticnet_sects': {'sentiment_max': 187.32800000000006,\n",
              "  'sentiment_min': 42.55199999999999},\n",
              " 'senticnet_sents': {'sentiment_max': 10.079999999999998,\n",
              "  'sentiment_min': -4.052},\n",
              " 'sentimentr_chaps': {'sentiment_max': 148.9999999999999,\n",
              "  'sentiment_min': 3.1000000000000116},\n",
              " 'sentimentr_parags': {'sentiment_max': 17.55, 'sentiment_min': -11.7},\n",
              " 'sentimentr_sects': {'sentiment_max': 96.14999999999986,\n",
              "  'sentiment_min': -17.149999999999977},\n",
              " 'sentimentr_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'sentiword_chaps': {'sentiment_max': 42.542095634920614,\n",
              "  'sentiment_min': -1.5734063492063277},\n",
              " 'sentiword_parags': {'sentiment_max': 7.645183333333333,\n",
              "  'sentiment_min': -6.530059523809523},\n",
              " 'sentiword_sects': {'sentiment_max': 39.65655039682539,\n",
              "  'sentiment_min': -14.01572857142857},\n",
              " 'sentiword_sents': {'sentiment_max': 4.614583333333333,\n",
              "  'sentiment_min': -3.635416666666667},\n",
              " 'syuzhet_chaps': {'sentiment_max': 145.19999999999985,\n",
              "  'sentiment_min': -1.000000000000015},\n",
              " 'syuzhet_parags': {'sentiment_max': 18.05, 'sentiment_min': -11.7},\n",
              " 'syuzhet_sects': {'sentiment_max': 91.19999999999989,\n",
              "  'sentiment_min': -17.049999999999976},\n",
              " 'syuzhet_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'textblob_lnorm_medianiqr_chaps': {'sentiment_max': 0.878467267281498,\n",
              "  'sentiment_min': -0.9237636348048577},\n",
              " 'textblob_lnorm_medianiqr_parags': {'sentiment_max': 316.43867097222716,\n",
              "  'sentiment_min': -226.1374523178942},\n",
              " 'textblob_lnorm_medianiqr_sects': {'sentiment_max': 2.425006999921087,\n",
              "  'sentiment_min': -1.1672574524684922},\n",
              " 'textblob_lnorm_medianiqr_sents': {'sentiment_max': 115.09991311902692,\n",
              "  'sentiment_min': -92.07993049522155},\n",
              " 'vader_chaps': {'sentiment_max': 1.0, 'sentiment_min': -0.9964},\n",
              " 'vader_parags': {'sentiment_max': 0.9952, 'sentiment_min': -0.995},\n",
              " 'vader_sects': {'sentiment_max': 0.9999, 'sentiment_min': -0.9998},\n",
              " 'vader_sents': {'sentiment_max': 0.9836, 'sentiment_min': -0.9723}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2blGfVlKb_s"
      },
      "source": [
        "### **Calculate Pattern Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU60-nqpCsl7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "da85345e-0b6d-40a5-bb22-55617beedfc6"
      },
      "source": [
        "model_base = 'pattern'\n",
        "model_name = 'pattern_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KxnLfHoL3Fy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c053b98-bb09-4e28-d5b6-fba958ee4406"
      },
      "source": [
        "!pip install pattern"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting pattern\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/07/b0e61b6c818ed4b6145fe01d1c341223aa6cfbc3928538ad1f2b890924a3/Pattern-3.6.0.tar.gz (22.2MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 288kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/26/a6bd68f13e0f38fbb643d6e497fc3462be83a0b6c4d43425c78bb51a7291/backports.csv-1.0.7-py2.py3-none-any.whl\n",
            "Collecting mysqlclient\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/df/59cd2fa5e48d0804d213bdcb1acb4d08c403b61c7ff7ed4dd4a6a2deb3f7/mysqlclient-2.0.3.tar.gz (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.2.6)\n",
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/15bf6781a861bbc5dd801d467f26448fb322bfedcd30f2e62b148d104dfb/feedparser-6.0.8-py3-none-any.whl (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.2MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/f3/4fec7dabe8802ebec46141345bf714cd1fc7d93cb74ddde917e4b6d97d88/pdfminer.six-20201018-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.2.5)\n",
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/a0/52729ce4aa026f31b74cc877be1d11e4ddeaa361dc7aebec148171644b33/python-docx-0.8.11.tar.gz (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 35.6MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/89/e333e597c090d12d3a0d208f751366da7db8c44c8b392b467dd993366e53/CherryPy-18.6.1-py2.py3-none-any.whl (419kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (2.4.0)\n",
            "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (3.0.4)\n",
            "Collecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 29.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (1.15.0)\n",
            "Collecting cheroot>=8.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/95/86fe6480af78fea7b0e7e1bf02e6acd4cb9e561ea200bd6d6e1398fe5426/cheroot-8.5.2-py2.py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.7MB/s \n",
            "\u001b[?25hCollecting portend>=2.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/a1/fd29409cced540facdd29abb986d988cb1f22c8170d10022ea73af77fa55/portend-2.7.1-py3-none-any.whl\n",
            "Collecting jaraco.collections\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1a/a0d6861d2aca6df92643c755966c8a60e40353e4c5e7a5c2f4e5ed733817/jaraco.collections-3.3.0-py3-none-any.whl\n",
            "Collecting zc.lockfile\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/2a/268389776288f0f26c7272c70c36c96dcc0bdb88ab6216ea18e19df1fadd/zc.lockfile-2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2021.5.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->pattern) (1.14.5)\n",
            "Collecting jaraco.functools\n",
            "  Downloading https://files.pythonhosted.org/packages/b5/da/e51e7b58c8fe132990edd1e3ef25bcd9801eb7f91d0f642ac7f8d97e4a36/jaraco.functools-3.3.0-py3-none-any.whl\n",
            "Collecting tempora>=1.8\n",
            "  Downloading https://files.pythonhosted.org/packages/58/6e/928b4726ee2762efea4a84e8f5e73fb46b396636a5ec260d6274a1de24d5/tempora-4.1.1-py3-none-any.whl\n",
            "Collecting jaraco.text\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/74/2a3c4835c079df16db8a9c50263eebb0125849fee5b16de353a059b7545d/jaraco.text-3.5.0-py3-none-any.whl\n",
            "Collecting jaraco.classes\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/74/bee5fc11594974746535117546404678fc7b899476e769c3c55bc0cfaa02/jaraco.classes-3.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (57.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern) (2.20)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2018.9)\n",
            "Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-cp37-none-any.whl size=22332722 sha256=13c7d32c8d2075c92024c6d00bb7db87db45aa789172fa7751b5c2f5bd169747\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/9a/0e/5fb1a603ed4e3aa8722a88e9cf4a82da7d1b63e3d2cc34bee5\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.0.3-cp37-cp37m-linux_x86_64.whl size=100114 sha256=996cd4bf5c925c2a0fae68e2f254a935178b9cf9ea4f37fc9066c410f381bf8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/ca/e8/ad4e7ce3df18bcd91c7d84dd28c7c08db491a2a2360efed363\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-cp37-none-any.whl size=184508 sha256=f1310b9b79932adb98df191a25cae64dc25d679517f13cf78b7bd31e3ee5faab\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/f1/a7cb70b38633ae04e7fb963b1c70f63fd6fc01c075b8230adc\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=8f7cbd4fe98ceec4b18594c6bfa800dad198949c5291c90610a38a2adb6ebed6\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built pattern mysqlclient python-docx sgmllib3k\n",
            "Installing collected packages: backports.csv, mysqlclient, sgmllib3k, feedparser, cryptography, pdfminer.six, python-docx, jaraco.functools, cheroot, tempora, portend, jaraco.text, jaraco.classes, jaraco.collections, zc.lockfile, cherrypy, pattern\n",
            "Successfully installed backports.csv-1.0.7 cheroot-8.5.2 cherrypy-18.6.1 cryptography-3.4.7 feedparser-6.0.8 jaraco.classes-3.2.1 jaraco.collections-3.3.0 jaraco.functools-3.3.0 jaraco.text-3.5.0 mysqlclient-2.0.3 pattern-3.6 pdfminer.six-20201018 portend-2.7.1 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-4.1.1 zc.lockfile-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtwmIrSOKZRm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "94f753e1-6c97-47f3-d695-9046d0e366c4"
      },
      "source": [
        "from pattern.en import sentiment as pattern_sa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vwtm_jBKZM2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5887a9b8-110b-4be6-eafd-972308de6a82"
      },
      "source": [
        "# Test\n",
        "\n",
        "sent_test='I hate Mondays.'\n",
        "pattern_sa(sent_test)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXq-jDPxasVY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "268b57af-f3e7-4adc-835c-59efd586a5d4"
      },
      "source": [
        "# Calculate Pattern Sentiment [0,1,2]\n",
        "\n",
        "def pattern_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_total = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    text_sentiment_total += pattern_sa(str(aword))[0]\n",
        "  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+0.01)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-sXRBNWC08o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d2ee084d-3180-4b4b-9d4a-4f3e38e7abc6"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Pattern_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(pattern_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "-0.7216228867182435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db0hezLKC08p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cb3ab383-971f-44a4-bee1-c064ee1c560d"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Pattern_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=pattern_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGHixqpOC08q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "139ac332-904d-4fe3-a08b-469d46cde68c"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'afinn_chaps': {'sentiment_max': 23.566333953237162,\n",
              "  'sentiment_min': -13.963760054626414},\n",
              " 'afinn_parags': {'sentiment_max': 30.0, 'sentiment_min': -30.0},\n",
              " 'afinn_sects': {'sentiment_max': 16.284196895116033,\n",
              "  'sentiment_min': -15.574896102419311},\n",
              " 'afinn_sents': {'sentiment_max': 40.0, 'sentiment_min': -30.0},\n",
              " 'bing_chaps': {'sentiment_max': 591.1048063253189,\n",
              "  'sentiment_min': -665.5682824208717},\n",
              " 'bing_parags': {'sentiment_max': 233.62539105697772,\n",
              "  'sentiment_min': -231.75792169846113},\n",
              " 'bing_sects': {'sentiment_max': 721.8836270105633,\n",
              "  'sentiment_min': -635.8012840187225},\n",
              " 'bing_sents': {'sentiment_max': 143.60753220597746,\n",
              "  'sentiment_min': -159.0419852720603},\n",
              " 'nrc_chaps': {'sentiment_max': 992.1420156684644,\n",
              "  'sentiment_min': 136.36421709165032},\n",
              " 'nrc_parags': {'sentiment_max': 133.15206794388564,\n",
              "  'sentiment_min': -88.59571651997537},\n",
              " 'nrc_sects': {'sentiment_max': 682.0582426859623,\n",
              "  'sentiment_min': -126.83645806124086},\n",
              " 'nrc_sents': {'sentiment_max': 92.90655652278458,\n",
              "  'sentiment_min': -61.110488553550724},\n",
              " 'pattern_chaps': {'sentiment_max': 12.899986289573443,\n",
              "  'sentiment_min': 7.4713885446027914},\n",
              " 'pattern_parags': {'sentiment_max': 70.0, 'sentiment_min': -50.0},\n",
              " 'pattern_sects': {'sentiment_max': 7.981247233376556,\n",
              "  'sentiment_min': 1.5450422169498796},\n",
              " 'pattern_sents': {'sentiment_max': 100.0, 'sentiment_min': -80.0},\n",
              " 'senticnet_chaps': {'sentiment_max': 354.84699999999896,\n",
              "  'sentiment_min': 235.29899999999932},\n",
              " 'senticnet_parags': {'sentiment_max': 21.69200000000001,\n",
              "  'sentiment_min': -5.824},\n",
              " 'senticnet_sects': {'sentiment_max': 187.32800000000006,\n",
              "  'sentiment_min': 42.55199999999999},\n",
              " 'senticnet_sents': {'sentiment_max': 10.079999999999998,\n",
              "  'sentiment_min': -4.052},\n",
              " 'sentimentr_chaps': {'sentiment_max': 148.9999999999999,\n",
              "  'sentiment_min': 3.1000000000000116},\n",
              " 'sentimentr_parags': {'sentiment_max': 17.55, 'sentiment_min': -11.7},\n",
              " 'sentimentr_sects': {'sentiment_max': 96.14999999999986,\n",
              "  'sentiment_min': -17.149999999999977},\n",
              " 'sentimentr_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'sentiword_chaps': {'sentiment_max': 42.542095634920614,\n",
              "  'sentiment_min': -1.5734063492063277},\n",
              " 'sentiword_parags': {'sentiment_max': 7.645183333333333,\n",
              "  'sentiment_min': -6.530059523809523},\n",
              " 'sentiword_sects': {'sentiment_max': 39.65655039682539,\n",
              "  'sentiment_min': -14.01572857142857},\n",
              " 'sentiword_sents': {'sentiment_max': 4.614583333333333,\n",
              "  'sentiment_min': -3.635416666666667},\n",
              " 'syuzhet_chaps': {'sentiment_max': 145.19999999999985,\n",
              "  'sentiment_min': -1.000000000000015},\n",
              " 'syuzhet_parags': {'sentiment_max': 18.05, 'sentiment_min': -11.7},\n",
              " 'syuzhet_sects': {'sentiment_max': 91.19999999999989,\n",
              "  'sentiment_min': -17.049999999999976},\n",
              " 'syuzhet_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'textblob_lnorm_medianiqr_chaps': {'sentiment_max': 0.878467267281498,\n",
              "  'sentiment_min': -0.9237636348048577},\n",
              " 'textblob_lnorm_medianiqr_parags': {'sentiment_max': 316.43867097222716,\n",
              "  'sentiment_min': -226.1374523178942},\n",
              " 'textblob_lnorm_medianiqr_sects': {'sentiment_max': 2.425006999921087,\n",
              "  'sentiment_min': -1.1672574524684922},\n",
              " 'textblob_lnorm_medianiqr_sents': {'sentiment_max': 115.09991311902692,\n",
              "  'sentiment_min': -92.07993049522155},\n",
              " 'vader_chaps': {'sentiment_max': 1.0, 'sentiment_min': -0.9964},\n",
              " 'vader_parags': {'sentiment_max': 0.9952, 'sentiment_min': -0.995},\n",
              " 'vader_sects': {'sentiment_max': 0.9999, 'sentiment_min': -0.9998},\n",
              " 'vader_sents': {'sentiment_max': 0.9836, 'sentiment_min': -0.9723}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsaziON_Z263"
      },
      "source": [
        "### **Calculate Stanza/OpenNLP Sentiment Polarities (Optional: Auto)**\n",
        "\n",
        "* https://github.com/piyushpathak03/NLP-using-STANZA/blob/main/Stanza.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZgGfcCuFnmI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9b50b6c5-f0aa-4de2-b3ed-18a644d331e3"
      },
      "source": [
        "model_base = 'stanza'\n",
        "model_name = 'stanza_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoZUi2AwZ_7L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "e5eacb8a-8f85-42bf-a365-32c43904ec2f"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/f3/cd7eaacabcec195a1c6c07b08cf1587b9f3f8754feba5c87d28867d75671/stanza-1.2.1-py3-none-any.whl (334kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.9.0+cu102)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (57.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5txTb6aIZ2tN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193,
          "referenced_widgets": [
            "69b2558215214d0c863a7c35a8e96cc4",
            "d9d23dbb0a8a4397b5b39bd5eb1a5963",
            "ec76b73afff746958a46b5fd875a854f",
            "5256fcb8c519453dacb9133be40f37a9",
            "9add1f94ebd14ed383119c7f851fafb3",
            "607da4c564754163afadc841a2140c08",
            "34ec2c61e6f04ced8162c48c8764b130",
            "9b618404af54418bb0307fbe1f6a5b1d",
            "08b6afbf521b4395baf4e10b80d3e7dc",
            "72a9299f1a274944b96bbb0719ed41ed",
            "7d979efc197d48abb73d30b4a5a7ee44",
            "00a6ce99bfb942a087e56ff3093f863e",
            "3f98e668b0c44992947806a96e3b271a",
            "756e477b37ef403eb25926ce277bd12d",
            "5c0920806ac742c6bf1e0cb9e536e8e4",
            "4cb4558e33694ec2ab5f242ae991630e"
          ]
        },
        "outputId": "8af44cdc-3f54-4979-c180-ed8cb15e49bc"
      },
      "source": [
        "%time\n",
        "\n",
        "import stanza\n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  stanza.download('en')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 6.2 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69b2558215214d0c863a7c35a8e96cc4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading https://raw.githubusercontent.com/stanfordnlp…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-08 10:52:01 INFO: Downloading default packages for language: en (English)...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08b6afbf521b4395baf4e10b80d3e7dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading http://nlp.stanford.edu/software/stanza/1.2.1…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-08 10:53:26 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NORYbxsxZ2qg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "112eceba-a8df-45fb-b6a8-1e17c06d6a8e"
      },
      "source": [
        "if Stanza_Arc == True:\n",
        "  nlp = stanza.Pipeline('en', processors='tokenize,sentiment')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-08 10:53:26 INFO: Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "| sentiment | sstplus  |\n",
            "========================\n",
            "\n",
            "2021-07-08 10:53:27 INFO: Use device: cpu\n",
            "2021-07-08 10:53:27 INFO: Loading: tokenize\n",
            "2021-07-08 10:53:27 INFO: Loading: sentiment\n",
            "2021-07-08 10:53:27 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtOtBfYwZ2na",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a8a6d040-65c4-4edc-dea8-37311eb18f0c"
      },
      "source": [
        "# Test stanza directly\n",
        "\n",
        "# doc = nlp('Ram is a bad boy')\n",
        "# for i, sentence in enumerate(doc.sentences):\n",
        "#     print(i, sentence.sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKnox67kayod",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6161bbf5-d887-454c-a4d6-1b6d342ff90d"
      },
      "source": [
        "# Calculate Stanza Sentiment [0,1,2]\n",
        "\n",
        "def stanza_discrete2continous_sentiment(text):\n",
        "  '''\n",
        "  Given a plain text string, give it to\n",
        "    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n",
        "  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n",
        "    that approximates a normal distribution for all values\n",
        "    In order to get more fine grained measure of overall Sentence sentiment\n",
        "    Sentiment values will be Normalized/Standardized so absolute precision is not required\n",
        "  '''\n",
        "  text_sentiment_tot = 0.\n",
        "  text_ls = text.split()\n",
        "  text_len = len(text_ls)\n",
        "  for aword in text_ls:\n",
        "    adoc = nlp(aword)\n",
        "    for i, sentence in enumerate(adoc.sentences):\n",
        "      text_sentiment_tot += float(sentence.sentiment)\n",
        "  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.1)\n",
        "\n",
        "  return text_sentiment_norm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNngkBBAF26C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ad59ccff-1da2-4d22-b133-bb9f85b68643"
      },
      "source": [
        "# Test\n",
        "\n",
        "if Stanza_Arc == True:\n",
        "  sent_test='I hate Mondays.'\n",
        "  print(stanza_discrete2continous_sentiment(sent_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.6685962749659333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrY2mrhVF26D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "12930edd-068c-4867-890a-8e6e79c34d21"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# NOTE: requires about 50mins (20210708 at 0730) Colab Pro: GPU+RAM\n",
        "\n",
        "# Calculate all Sentiment values and variants\n",
        "if Stanza_Arc == True:\n",
        "  get_sentiments(model_base=model_base, sentiment_fn=stanza_discrete2continous_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSiYC73nF26D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a891213-92d8-4594-fae6-a42c7f276c79"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'afinn_chaps': {'sentiment_max': 23.566333953237162,\n",
              "  'sentiment_min': -13.963760054626414},\n",
              " 'afinn_parags': {'sentiment_max': 30.0, 'sentiment_min': -30.0},\n",
              " 'afinn_sects': {'sentiment_max': 16.284196895116033,\n",
              "  'sentiment_min': -15.574896102419311},\n",
              " 'afinn_sents': {'sentiment_max': 40.0, 'sentiment_min': -30.0},\n",
              " 'bing_chaps': {'sentiment_max': 591.1048063253189,\n",
              "  'sentiment_min': -665.5682824208717},\n",
              " 'bing_parags': {'sentiment_max': 233.62539105697772,\n",
              "  'sentiment_min': -231.75792169846113},\n",
              " 'bing_sects': {'sentiment_max': 721.8836270105633,\n",
              "  'sentiment_min': -635.8012840187225},\n",
              " 'bing_sents': {'sentiment_max': 143.60753220597746,\n",
              "  'sentiment_min': -159.0419852720603},\n",
              " 'nrc_chaps': {'sentiment_max': 992.1420156684644,\n",
              "  'sentiment_min': 136.36421709165032},\n",
              " 'nrc_parags': {'sentiment_max': 133.15206794388564,\n",
              "  'sentiment_min': -88.59571651997537},\n",
              " 'nrc_sects': {'sentiment_max': 682.0582426859623,\n",
              "  'sentiment_min': -126.83645806124086},\n",
              " 'nrc_sents': {'sentiment_max': 92.90655652278458,\n",
              "  'sentiment_min': -61.110488553550724},\n",
              " 'pattern_chaps': {'sentiment_max': 12.899986289573443,\n",
              "  'sentiment_min': 7.4713885446027914},\n",
              " 'pattern_parags': {'sentiment_max': 70.0, 'sentiment_min': -50.0},\n",
              " 'pattern_sects': {'sentiment_max': 7.981247233376556,\n",
              "  'sentiment_min': 1.5450422169498796},\n",
              " 'pattern_sents': {'sentiment_max': 100.0, 'sentiment_min': -80.0},\n",
              " 'senticnet_chaps': {'sentiment_max': 354.84699999999896,\n",
              "  'sentiment_min': 235.29899999999932},\n",
              " 'senticnet_parags': {'sentiment_max': 21.69200000000001,\n",
              "  'sentiment_min': -5.824},\n",
              " 'senticnet_sects': {'sentiment_max': 187.32800000000006,\n",
              "  'sentiment_min': 42.55199999999999},\n",
              " 'senticnet_sents': {'sentiment_max': 10.079999999999998,\n",
              "  'sentiment_min': -4.052},\n",
              " 'sentimentr_chaps': {'sentiment_max': 148.9999999999999,\n",
              "  'sentiment_min': 3.1000000000000116},\n",
              " 'sentimentr_parags': {'sentiment_max': 17.55, 'sentiment_min': -11.7},\n",
              " 'sentimentr_sects': {'sentiment_max': 96.14999999999986,\n",
              "  'sentiment_min': -17.149999999999977},\n",
              " 'sentimentr_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'sentiword_chaps': {'sentiment_max': 42.542095634920614,\n",
              "  'sentiment_min': -1.5734063492063277},\n",
              " 'sentiword_parags': {'sentiment_max': 7.645183333333333,\n",
              "  'sentiment_min': -6.530059523809523},\n",
              " 'sentiword_sects': {'sentiment_max': 39.65655039682539,\n",
              "  'sentiment_min': -14.01572857142857},\n",
              " 'sentiword_sents': {'sentiment_max': 4.614583333333333,\n",
              "  'sentiment_min': -3.635416666666667},\n",
              " 'stanza_chaps': {'sentiment_max': 1087.6785981297473,\n",
              "  'sentiment_min': 924.50498483062},\n",
              " 'stanza_parags': {'sentiment_max': 73.38332165182582, 'sentiment_min': 0.0},\n",
              " 'stanza_sects': {'sentiment_max': 715.6999671866258,\n",
              "  'sentiment_min': 164.45754600675522},\n",
              " 'stanza_sents': {'sentiment_max': 31.921659513272957, 'sentiment_min': 0.0},\n",
              " 'syuzhet_chaps': {'sentiment_max': 145.19999999999985,\n",
              "  'sentiment_min': -1.000000000000015},\n",
              " 'syuzhet_parags': {'sentiment_max': 18.05, 'sentiment_min': -11.7},\n",
              " 'syuzhet_sects': {'sentiment_max': 91.19999999999989,\n",
              "  'sentiment_min': -17.049999999999976},\n",
              " 'syuzhet_sents': {'sentiment_max': 8.9, 'sentiment_min': -6.4},\n",
              " 'textblob_lnorm_medianiqr_chaps': {'sentiment_max': 0.878467267281498,\n",
              "  'sentiment_min': -0.9237636348048577},\n",
              " 'textblob_lnorm_medianiqr_parags': {'sentiment_max': 316.43867097222716,\n",
              "  'sentiment_min': -226.1374523178942},\n",
              " 'textblob_lnorm_medianiqr_sects': {'sentiment_max': 2.425006999921087,\n",
              "  'sentiment_min': -1.1672574524684922},\n",
              " 'textblob_lnorm_medianiqr_sents': {'sentiment_max': 115.09991311902692,\n",
              "  'sentiment_min': -92.07993049522155},\n",
              " 'vader_chaps': {'sentiment_max': 1.0, 'sentiment_min': -0.9964},\n",
              " 'vader_parags': {'sentiment_max': 0.9952, 'sentiment_min': -0.995},\n",
              " 'vader_sects': {'sentiment_max': 0.9999, 'sentiment_min': -0.9998},\n",
              " 'vader_sents': {'sentiment_max': 0.9836, 'sentiment_min': -0.9723}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S5tUYZA1DExu",
        "outputId": "54479c35-7df7-449b-a649-f10ce7610c02"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 7679 entries, 0 to 7680\n",
            "Data columns (total 51 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   sent_no                     7679 non-null   int64  \n",
            " 1   parag_no                    7679 non-null   int64  \n",
            " 2   sent_raw                    7679 non-null   string \n",
            " 3   char_len                    7679 non-null   int64  \n",
            " 4   token_len                   7679 non-null   int64  \n",
            " 5   sent_clean                  7679 non-null   object \n",
            " 6   sentimentr                  7679 non-null   float64\n",
            " 7   sentimentr_lnorm_meanstd    7679 non-null   float64\n",
            " 8   sentimentr_medianiqr        7679 non-null   float64\n",
            " 9   sentimentr_lnorm_medianiqr  7679 non-null   float64\n",
            " 10  syuzhet                     7679 non-null   float64\n",
            " 11  syuzhet_lnorm_meanstd       7679 non-null   float64\n",
            " 12  syuzhet_medianiqr           7679 non-null   float64\n",
            " 13  syuzhet_lnorm_medianiqr     7679 non-null   float64\n",
            " 14  bing                        7679 non-null   float64\n",
            " 15  bing_lnorm_meanstd          7679 non-null   float64\n",
            " 16  bing_medianiqr              7679 non-null   float64\n",
            " 17  bing_lnorm_medianiqr        7679 non-null   float64\n",
            " 18  sentiword                   7679 non-null   float64\n",
            " 19  sentiword_lnorm_meanstd     7679 non-null   float64\n",
            " 20  sentiword_medianiqr         7679 non-null   float64\n",
            " 21  sentiword_lnorm_medianiqr   7679 non-null   float64\n",
            " 22  senticnet                   7679 non-null   float64\n",
            " 23  senticnet_lnorm_meanstd     7679 non-null   float64\n",
            " 24  senticnet_medianiqr         7679 non-null   float64\n",
            " 25  senticnet_lnorm_medianiqr   7679 non-null   float64\n",
            " 26  nrc                         7679 non-null   float64\n",
            " 27  nrc_lnorm_meanstd           7679 non-null   float64\n",
            " 28  nrc_medianiqr               7679 non-null   float64\n",
            " 29  nrc_lnorm_medianiqr         7679 non-null   float64\n",
            " 30  afinn                       7679 non-null   float64\n",
            " 31  afinn_lnorm_meanstd         7679 non-null   float64\n",
            " 32  afinn_medianiqr             7679 non-null   float64\n",
            " 33  afinn_lnorm_medianiqr       7679 non-null   float64\n",
            " 34  scores                      7679 non-null   object \n",
            " 35  vader                       7679 non-null   float64\n",
            " 36  vader_lnorm_meanstd         7679 non-null   float64\n",
            " 37  vader_medianiqr             7679 non-null   float64\n",
            " 38  vader_lnorm_medianiqr       7679 non-null   float64\n",
            " 39  textblob                    7679 non-null   float64\n",
            " 40  textblob_lnorm_meanstd      7679 non-null   float64\n",
            " 41  textblob_medianiqr          7679 non-null   float64\n",
            " 42  textblob_lnorm_medianiqr    7679 non-null   float64\n",
            " 43  pattern                     7679 non-null   float64\n",
            " 44  pattern_lnorm_meanstd       7679 non-null   float64\n",
            " 45  pattern_medianiqr           7679 non-null   float64\n",
            " 46  pattern_lnorm_medianiqr     7679 non-null   float64\n",
            " 47  stanza                      7679 non-null   float64\n",
            " 48  stanza_lnorm_meanstd        7679 non-null   float64\n",
            " 49  stanza_medianiqr            7679 non-null   float64\n",
            " 50  stanza_lnorm_medianiqr      7679 non-null   float64\n",
            "dtypes: float64(44), int64(4), object(2), string(1)\n",
            "memory usage: 3.0+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmnEr9UbX-aV"
      },
      "source": [
        "### **Calculate Huggingface BERT Default Sentiment Polarities (Optional: Auto)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mmopo4LX7QM"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jgjfJtXdrCGx",
        "outputId": "c59beb14-b18f-47ca-cd88-d068426ee852"
      },
      "source": [
        "MAX_LINE_LEN = 509\n",
        "\n",
        "model_base = 'hfbert'\n",
        "model_name = 'hfbert_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZSi0P7arbgs"
      },
      "source": [
        "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
        "\n",
        "# Sentiment analysis pipeline\n",
        "hf_sadef_clf = pipeline('sentiment-analysis')\n",
        "\n",
        "# Test\n",
        "hf_sadef_clf('Such a nice weather outside !')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VXgOI4nnd4Xh",
        "outputId": "650656fe-78e6-4346-ba24-fc5dd130c1b5"
      },
      "source": [
        "def hfbert_sentiment(text_str='Happy Happy Joy Joy'):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return a float sentiment value for this text string with the given model/sentiment_fn\n",
        "  '''\n",
        "\n",
        "  text_str_ls = text_str.split(' ')\n",
        "  if len(text_str_ls) > MAX_LINE_LEN:\n",
        "    text_str = string_trimlen(atext_str=text_str, max_len=MAX_LINE_LEN)\n",
        "\n",
        "  sentiment_res = hf_sadef_clf(text_str)\n",
        "\n",
        "  # print(f\"sentiment_res[0]: {sentiment_res[0]['label']}\")\n",
        "  if sentiment_res[0]['label'].lower() == 'positive':\n",
        "    # print('POSITIVE')\n",
        "    sign_multiplier = 1.0\n",
        "  else:\n",
        "    # print('NEGATIVE')\n",
        "    sign_multiplier = -1.0\n",
        "\n",
        "  sentiment_value = sign_multiplier * sentiment_res[0]['score']\n",
        "\n",
        "  return sentiment_value\n",
        "\n",
        "# Test\n",
        "# test_res = hfbert_sentiment(text_str='the it is')\n",
        "# print(test_res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHusp2-uyO8Z"
      },
      "source": [
        "# Test\n",
        "test_res = hfbert_sentiment(text_str='the it is')\n",
        "print(test_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rEo3jzE7ws_"
      },
      "source": [
        "[x.upper() for x in 'the cat sat'.split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "F9iSQKdlzbg6",
        "outputId": "55c3ef83-2b68-4aa8-82d1-81b4b17ff78f"
      },
      "source": [
        "def string_trimlen(atext_str, max_len):\n",
        "  '''\n",
        "  Given a text string of tokens\n",
        "  Return a truncated string of max_len tokens if necessary\n",
        "  e.g. Transformers can often only process 512 (509/510 with special tokens)\n",
        "  '''\n",
        "  \n",
        "  common_neutral_words_ls = \"the of and to a in for is on that by this with i you it or be are from at as your have an was we will us about page my has our information time they site he what which their use there his when here who these its x than had year day into two email n re b them t number then where m through she years r said de\".split(' ')\n",
        "  # common_neutral_words_ls = 'the of to and a an is it in you that he was are I his they be at this or had by word what some we were there when your said an each she which their if about then them these here thing him two has did number sound people who been place where day year'.split(' ')\n",
        "  # print(f'len common: {len(common_neutral_words_ls)}')\n",
        "  # text_str_ls = [x.lower for x in text_str.split()]\n",
        "  text_str_ls = atext_str.split()\n",
        "  # print(f'text_str_ls type: {type(text_str_ls[0])}')\n",
        "\n",
        "  if len(text_str_ls) <= max_len:\n",
        "    return text_str\n",
        "  \n",
        "  for i, aneutral_word in enumerate(common_neutral_words_ls):\n",
        "    string_current = str(\" \".join([str(x) for x in text_str_ls]))\n",
        "    # print(f\"top of loop with aword={aneutral_word} and text_str_ls: {str(string_current)}\")\n",
        "    aneutral_word_lower = aneutral_word.lower()\n",
        "    for aused_word in text_str_ls:\n",
        "      aused_word_lower = aused_word.lower()\n",
        "      # print(f'Comparing aused_word={aused_word_lower} with aneutral_word={aneutral_word_lower}')\n",
        "      if aused_word_lower == aneutral_word_lower:\n",
        "        # if aword_lower in text_str_ls:\n",
        "        # print(f'REMOVING aword=[{aused_word_lower}] from sentence')\n",
        "        # while aused_word_lower in text_str_ls: text_str_ls.remove(aused_word)\n",
        "        text_str_ls = list(filter((aused_word).__ne__, text_str_ls))\n",
        "        # text_str_ls.remove(aneutral_word, )\n",
        "        # print(f\"   new shorter list len = {len(text_str_ls)}, string={' '.join(text_str_ls)}\")\n",
        "    if len(text_str_ls) <= max_len:\n",
        "      # print('returning')\n",
        "      text_condensed_str = ' '.join([x for x in text_str_ls])\n",
        "      return text_condensed_str\n",
        "\n",
        "    # print(f'iteration #{i}')\n",
        "\n",
        "  text_condensed_str = ' '.join([x for x in text_str_ls])\n",
        "  return text_condensed_str\n",
        "\n",
        "# Test\n",
        "original_string = \"The rain in Spain falls mainly in the plain\"\n",
        "print(f'original string: {original_string}')\n",
        "trimmed_string = string_trimlen(atext_str=original_string, max_len=5)\n",
        "print(f'trimmed string: {trimmed_string}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "original string: The rain in Spain falls mainly in the plain\n",
            "trimmed string: rain Spain falls mainly plain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a_ysq28SYIZF",
        "outputId": "aacdbdba-d697-4ab3-959f-38bbbeec63a3"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 7679 entries, 0 to 7680\n",
            "Data columns (total 51 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   sent_no                     7679 non-null   int64  \n",
            " 1   parag_no                    7679 non-null   int64  \n",
            " 2   sent_raw                    7679 non-null   string \n",
            " 3   char_len                    7679 non-null   int64  \n",
            " 4   token_len                   7679 non-null   int64  \n",
            " 5   sent_clean                  7679 non-null   object \n",
            " 6   scores                      7679 non-null   object \n",
            " 7   vader                       7679 non-null   float64\n",
            " 8   vader_lnorm_meanstd         7679 non-null   float64\n",
            " 9   vader_medianiqr             7679 non-null   float64\n",
            " 10  vader_lnorm_medianiqr       7679 non-null   float64\n",
            " 11  textblob                    7679 non-null   float64\n",
            " 12  textblob_lnorm_meanstd      7679 non-null   float64\n",
            " 13  textblob_medianiqr          7679 non-null   float64\n",
            " 14  textblob_lnorm_medianiqr    7679 non-null   float64\n",
            " 15  afinn                       7679 non-null   float64\n",
            " 16  afinn_lnorm_meanstd         7679 non-null   float64\n",
            " 17  afinn_medianiqr             7679 non-null   float64\n",
            " 18  afinn_lnorm_medianiqr       7679 non-null   float64\n",
            " 19  sentimentr                  7679 non-null   float64\n",
            " 20  sentimentr_lnorm_meanstd    7679 non-null   float64\n",
            " 21  sentimentr_medianiqr        7679 non-null   float64\n",
            " 22  sentimentr_lnorm_medianiqr  7679 non-null   float64\n",
            " 23  syuzhet                     7679 non-null   float64\n",
            " 24  syuzhet_lnorm_meanstd       7679 non-null   float64\n",
            " 25  syuzhet_medianiqr           7679 non-null   float64\n",
            " 26  syuzhet_lnorm_medianiqr     7679 non-null   float64\n",
            " 27  bing                        7679 non-null   float64\n",
            " 28  bing_lnorm_meanstd          7679 non-null   float64\n",
            " 29  bing_medianiqr              7679 non-null   float64\n",
            " 30  bing_lnorm_medianiqr        7679 non-null   float64\n",
            " 31  pattern                     7679 non-null   float64\n",
            " 32  pattern_lnorm_meanstd       7679 non-null   float64\n",
            " 33  pattern_medianiqr           7679 non-null   float64\n",
            " 34  pattern_lnorm_medianiqr     7679 non-null   float64\n",
            " 35  sentiword                   7679 non-null   float64\n",
            " 36  sentiword_lnorm_meanstd     7679 non-null   float64\n",
            " 37  sentiword_medianiqr         7679 non-null   float64\n",
            " 38  sentiword_lnorm_medianiqr   7679 non-null   float64\n",
            " 39  senticnet                   7679 non-null   float64\n",
            " 40  senticnet_lnorm_meanstd     7679 non-null   float64\n",
            " 41  senticnet_medianiqr         7679 non-null   float64\n",
            " 42  senticnet_lnorm_medianiqr   7679 non-null   float64\n",
            " 43  nrc                         7679 non-null   float64\n",
            " 44  nrc_lnorm_meanstd           7679 non-null   float64\n",
            " 45  nrc_medianiqr               7679 non-null   float64\n",
            " 46  nrc_lnorm_medianiqr         7679 non-null   float64\n",
            " 47  stanza                      7679 non-null   float64\n",
            " 48  stanza_lnorm_meanstd        7679 non-null   float64\n",
            " 49  stanza_medianiqr            7679 non-null   float64\n",
            " 50  stanza_lnorm_medianiqr      7679 non-null   float64\n",
            "dtypes: float64(44), int64(4), object(2), string(1)\n",
            "memory usage: 3.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "tjEvi8nyW6xr",
        "outputId": "eb1bf07c-8053-4c0f-a73d-ffc02360ebca"
      },
      "source": [
        "list(corpus_sents_df.iloc[:20].groupby('parag_no').vader_lnorm_medianiqr.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6180915371329878,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 10.354832627539965,\n",
              " -2.187564766839378,\n",
              " 0.8156377571047255,\n",
              " 1.821243523316062]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJo55L3yykQ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f4caf815-421c-4e74-f365-db74237d3d1a"
      },
      "source": [
        "def get_transformer_sentiments(model_base, sentiment_fn, sentiment_type='value_fl'):\n",
        "  '''\n",
        "  Given a model_base name and sentiment evaluation function\n",
        "  Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "  '''\n",
        "\n",
        "  # Calculate Sentiment Polarities\n",
        "\n",
        "  if sentiment_type == 'value_fl':\n",
        "\n",
        "    corpus_sents_df[model_base] = corpus_sents_df['sent_raw'].apply(lambda text: hfbert_sentiment(str(text)))\n",
        "    corpus_parags_df[model_base] = corpus_sents_df.iloc[:20].groupby('parag_no').vader_lnorm_medianiqr.sum()\n",
        "    \n",
        "    # corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    # corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    # corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    # corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "  \n",
        "  elif sentiment_type == 'compound':\n",
        "    # VADER\n",
        "\n",
        "    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n",
        "    corpus_sents_df['scores'] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_parags_df['scores'] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_sects_df['scores'] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_chaps_df['scores'] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "\n",
        "    # Extract Compound Sentiment\n",
        "    corpus_sents_df[model_base]  = corpus_sents_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_parags_df[model_base]  = corpus_parags_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_sects_df[model_base]  = corpus_sects_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "    corpus_chaps_df[model_base]  = corpus_chaps_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "\n",
        "  elif sentiment_type == 'function':\n",
        "    # TextBlob\n",
        "\n",
        "    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n",
        "    corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "    corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: sentiment_type={sentiment_type} but must be one of (lexicon, compound, function)')\n",
        "    return\n",
        "\n",
        "\n",
        "  # Create new column names\n",
        "  col_meanstd = f'{model_base}_meanstd'\n",
        "  col_medianiqr = f'{model_base}_medianiqr'\n",
        "  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n",
        "  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "\n",
        "  return\n",
        "\n",
        "\"\"\"\n",
        "  # Get Chapter Standardization with MeanSTD and RobustStandardization with MedianIQRScaling\n",
        "  corpus_chaps_df[col_meanstd]  = mean_std_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Chapter Sentiment by dividing by Chapter Length\n",
        "  chaps_len_ls = list(corpus_chaps_df['token_len'])\n",
        "  chaps_sentiment_ls = list(corpus_chaps_df[model_base])\n",
        "  chaps_sentiment_norm_ls = [chaps_sentiment_ls[i]/chaps_len_ls[i] for i in range(len(chaps_len_ls))]\n",
        "  # RobustStandardize Chapter sentiment values\n",
        "  corpus_chaps_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n",
        "  corpus_chaps_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  # Get Section Standardization with MeanSTD and RobustStandardization with MedianIQRScaling\n",
        "  corpus_sects_df[col_meanstd]  = mean_std_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sects_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  # Normalize the Section Sentiment by dividing by Section Length\n",
        "  sects_len_ls = list(corpus_sects_df['token_len'])\n",
        "  sects_sentiment_ls = list(corpus_sects_df[model_base])\n",
        "  sects_sentiment_norm_ls = [sects_sentiment_ls[i]/sects_len_ls[i] for i in range(len(sects_len_ls))]\n",
        "  # RobustStandardize Section sentiment values\n",
        "  corpus_sects_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_sects_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sects_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  # Normalize the Paragraph Sentiment by dividing by Chapter Length\n",
        "  parags_len_ls = list(corpus_parags_df['token_len'])\n",
        "  parags_sentiment_ls = list(corpus_parags_df[model_base])\n",
        "  parags_sentiment_norm_ls = [parags_sentiment_ls[i]/parags_len_ls[i] for i in range(len(parags_len_ls))]\n",
        "  # RobustStandardize Paragraph sentiment values\n",
        "  corpus_parags_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_parags_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_parags_df[model_base]).reshape(-1, 1))\n",
        "  corpus_parags_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\n",
        "  # Normalize the Sentence Sentiment by dividing by Chapter Length\n",
        "  sents_len_ls = list(corpus_sents_df['token_len'])\n",
        "  sents_sentiment_ls = list(corpus_sents_df[model_base])\n",
        "  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/sents_len_ls[i] for i in range(len(sents_len_ls))]\n",
        "  # RobustStandardize Sentence sentiment values\n",
        "  corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "  corpus_sents_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n",
        "  corpus_sents_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pVAbbQrbY0sQ",
        "outputId": "4d9f40f9-a751-483d-ddb7-841bf8f37976"
      },
      "source": [
        "model_base"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hfbert'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vtLPvPBprOWb",
        "outputId": "93a3aa30-9349-420b-9d87-ed9ad34526e9"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "# NOTE: \n",
        "\n",
        "get_transformer_sentiments(model_base=model_base, sentiment_fn=hfbert_sentiment, sentiment_type='value_fl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIQpmPB5rOWc"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_r4gGab66DZ"
      },
      "source": [
        "### **NLPTown BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-rQDNkIxGYK"
      },
      "source": [
        "!pip3 install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxYk-DaaxGU4"
      },
      "source": [
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6am8is15xQD3"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBBZXJdAsqBj"
      },
      "source": [
        "model_base = 'nlptown'\n",
        "model_name = 'nlptown_lnorm_medianiqr'\n",
        "\n",
        "col_medianiqr = f'{model_base}_medianiqr'\n",
        "col_meanstd = f'{model_base}_meanstd'\n",
        "\n",
        "col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n",
        "col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br6qVCNSx04n"
      },
      "source": [
        "def nlptown_sentiment(text_str):\n",
        "  '''\n",
        "  Given a text string\n",
        "  Return the Sentiment Value computed by BERT NLPTown\n",
        "  '''\n",
        "  \n",
        "  tokens = tokenizer.encode(text_str, return_tensors='pt')\n",
        "\n",
        "  # Predict Tokens\n",
        "  result = model(tokens)\n",
        "  result\n",
        "\n",
        "  sentiment_base = int(torch.argmax(result.logits)) # Sentiment 0-4\n",
        "\n",
        "  # Normalize all 5 probabilities sum to 1\n",
        "  result_np = result.logits.detach().numpy()\n",
        "  result_sum_fl = np.absolute(result_np).sum()\n",
        "  result_norm_np = result_np.T/result_sum_fl\n",
        "  result_ls = list(result_norm_np)\n",
        "\n",
        "  # Adjusted Sentiment\n",
        "  sentiment_adj = result_ls[sentiment_base]\n",
        "  sentiment_final_np = sentiment_base + sentiment_adj\n",
        "\n",
        "  sentiment_final_float = float(sentiment_final_np)\n",
        "\n",
        "  return sentiment_final_float\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  # return predict_sentiment, result.logits.detach().numpy()\n",
        "\n",
        "\n",
        "# Test\n",
        "test_sentiment, result_np = get_sentiment_nlptown('I love and adore the wonderfully excellent things.')\n",
        "test_sentiment, result_np = get_sentiment_nlptown('I hate and despise the terribly rotten things.')\n",
        "test_sentiment, result_np = get_sentiment_nlptown('What it is.')\n",
        "test_text = 'I love and adore the wonderfully excellent things.'\n",
        "test_text = 'I hate and despise the terribly rotten things.'\n",
        "test_text = 'What is it'\n",
        "test_text = 'The sky is blue'\n",
        "\n",
        "test_sentiment, result_np = get_sentiment_nlptown(test_text)\n",
        "\n",
        "\n",
        "print(f'test_sentiment={test_sentiment}')\n",
        "print(f'result_np.shape = {result_np.shape}')\n",
        "result_sum_fl = np.absolute(result_np).sum()\n",
        "result_norm_np = result_np.T/result_sum_fl\n",
        "print(f'result_norm_np = {result_norm_np}')\n",
        "\n",
        "print(f'result_ls = {len(result_ls)}')\n",
        "adj_test_sentiment = result_ls[test_sentiment-1] #  + abs(result_norm_np[test_sentiment-1])\n",
        "print(f'adj test_sentiment={test_sentiment- 1 + adj_test_sentiment}')\n",
        "\"\"\";\n",
        "\n",
        "# Test\n",
        "# test_text = 'I love and adore the wonderfully excellent things.'\n",
        "test_text = 'I hate and despise the terribly rotten things.'\n",
        "# test_text = 'What is it'\n",
        "# test_text = 'The sky is blue'\n",
        "\n",
        "sentiment_text = get_sentiment_nlptown(test_text)\n",
        "print(f'sentiment_text: {sentiment_text}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slZ1QadstOn4"
      },
      "source": [
        "# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n",
        "\n",
        "get_sentiments(model_base=model_base, sentiment_fn=nlptown_sentiment, sentiment_type='function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq7xDiYdtOn6"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n",
        "get_lexstats(corpus_sects_df, model_base, text_unit='section')\n",
        "get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2LSH5K8ttR0"
      },
      "source": [
        "### **RoBERTa Large English Tuned on 15 SA Dataset**\n",
        "* **siebert/sentiment-roberta-large-english**\n",
        "\n",
        "This model is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019). It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.). Consequently, it outperforms models trained on only one type of text (e.g., movie reviews from the popular SST-2 benchmark) when used on new data as shown below.\n",
        "\n",
        "Jon Chun\n",
        "20 Jun 2021\n",
        "\n",
        "Reference:\n",
        "\n",
        "* https://huggingface.co/siebert/sentiment-roberta-large-english\n",
        "\n",
        "* https://huggingface.co/siebert/sentiment-roberta-large-english"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDz87pkm8mej"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmUf7BZQ8mbJ"
      },
      "source": [
        "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
        "\n",
        "# Test\n",
        "print(sentiment_analysis(\"I love this!\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00L7S_Oi_xwi"
      },
      "source": [
        "def robertalg15_sentiment(sentiment_fn, text_str='Happy Happy Joy Joy'):\n",
        "  '''\n",
        "  Given a sentiment function and text string\n",
        "  Return a float sentiment value for this text string with the given model/sentiment_fn\n",
        "  '''\n",
        "\n",
        "  sentiment_res = sentiment_fn(text_str)\n",
        "\n",
        "  # print(f\"sentiment_res[0]: {sentiment_res[0]['label']}\")\n",
        "  if sentiment_res[0]['label'].lower() == 'positive':\n",
        "    # print('POSITIVE')\n",
        "    sign_multiplier = 1.0\n",
        "  else:\n",
        "    # print('NEGATIVE')\n",
        "    sign_multiplier = -1.0\n",
        "\n",
        "  sentiment_value = sign_multiplier * sentiment_res[0]['score']\n",
        "\n",
        "  return sentiment_value\n",
        "\n",
        "# Test\n",
        "# test_res = get_sentiment_hfbert(hf_sadef_clf, text_str='the it is')\n",
        "# print(test_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1sorLMhur2d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lft10Lkuryv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_svvKFBv8mYg"
      },
      "source": [
        "%time\n",
        "\n",
        "# NOTE: upto 30-45mins\n",
        "\n",
        "# Calculate Sentiment Polarities\n",
        "\n",
        "# Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n",
        "# corpus_sents_df['nrc'] = corpus_sents_df['sent_clean'].apply(lambda text: lex_discrete2continous_sentiment(str(text), lexicon_nrc_dt))\n",
        "corpus_sents_df['robertalg15'] = corpus_sents_df['sent_clean'].apply(lambda text: get_labelscore2sentiment(sentiment_analysis, str(text)))\n",
        "corpus_parags_df['robertalg15'] = corpus_parags_df['parag_clean'].apply(lambda text: get_labelscore2sentiment(sentiment_analysis, str(text)))\n",
        "\n",
        "# Normalize by Paragraph Length\n",
        "# TODO:\n",
        "\n",
        "if (PLOT_OUTPUT == 'All'):\n",
        "  corpus_sents_df.head(2)\n",
        "  corpus_parags_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnGymJ1BC9-2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42555syfC900"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjlAtgGOC-Uy"
      },
      "source": [
        "parags_sents_median_ls = corpus_sents_df.groupby(['parag_no'])['robertalg15'].median()\n",
        "print(f'type(parags_sent_median_ls): {type(parags_sents_median_ls)}')\n",
        "print(f'len(parags_sent_median_ls): {len(parags_sents_median_ls)}')\n",
        "print(f'parags_sent_median_ls: {parags_sents_median_ls}')\n",
        "\n",
        "# corpus_parags_df['nlptown_sentsmedian'] = corpus_parags_df['parag_clean'].apply(lambda text: get_sentiment_nlptown(str(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaw3s_bFC-Uz"
      },
      "source": [
        "corpus_parags_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56xoj8NlC-Uz"
      },
      "source": [
        "corpus_parags_df['robertalg15_sentsmedian'] = pd.Series(parags_sents_median_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzCs3jOUC9uv"
      },
      "source": [
        "corpus_parags_df[['robertalg15', 'robertalg15_sentsmedian']].rolling(70).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLZA4sLh8mVj"
      },
      "source": [
        "# Get/Set Sentiment Statistics\n",
        "\n",
        "get_lexstats(corpus_sents_df, 'robertalg15', text_unit='sentence')\n",
        "get_lexstats(corpus_parags_df, 'robertalg15', text_unit='paragraph')\n",
        "\n",
        "# Validate\n",
        "corpus_lexicons_stats_dt\n",
        "\n",
        "# corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg6gZSEt8vc2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWrxFA82xGR3"
      },
      "source": [
        "from transformers import pipeline\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJO9xk0exGOo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZp7C4aEhLlW"
      },
      "source": [
        "# Setup for RoBERTa Large English 15datasets: siebert/sentiment-roberta-large-english\n",
        "\n",
        "sa_model = 'robertalg15'\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
        "\n",
        "# Create class for data preparation\n",
        "class SimpleDataset:\n",
        "    def __init__(self, tokenized_texts):\n",
        "        self.tokenized_texts = tokenized_texts\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_texts[\"input_ids\"])\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n",
        "\n",
        "\n",
        "# Set model\n",
        "sa_model = 'robertalg15'\n",
        "model_name = \"siebert/sentiment-roberta-large-english\"\n",
        "\n",
        "# Load tokenizer and model, create trainer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "trainer = Trainer(model=model);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8NfWjCQiPox"
      },
      "source": [
        "# Prepare Text from DataFrame\n",
        "\n",
        "sents_pred_df = corpus_sents_df.copy()\n",
        "sents_pred_texts = sents_pred_df['sent_raw'].astype('str').tolist() # Want to catch NaN, .dropna().astype('str').tolist()\n",
        "\n",
        "# Tokenize texts and create prediction data set\n",
        "\n",
        "sents_tokenized_texts = tokenizer(sents_pred_texts,truncation=True,padding=True)\n",
        "sents_pred_dataset = SimpleDataset(sents_tokenized_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a99JxQASwGRp"
      },
      "source": [
        "def polprob2sentiment(pol_str, prob_fl):\n",
        "  '''\n",
        "  Given a Polarity string (Negative or Positive) and a Probability float (0.0-1.0)\n",
        "  Return a Sentiment float value (-1.0 to 1.0)\n",
        "  '''\n",
        "  sign_fl = 1.0\n",
        "  if pol_str.lower().startswith('neg'):\n",
        "    # print(f'pol_str: {pol_str} is Negative')\n",
        "    sign_fl = -1.0\n",
        "  elif pol_str.lower().startswith('pos'):\n",
        "    # print(f'pol_str: {pol_str} is Positive')\n",
        "    pass\n",
        "  else:\n",
        "    print(f'ERROR: pol_str: {pol_str} is neither Negative nor Positive')\n",
        "    sign_fl = 0.0\n",
        "\n",
        "  return sign_fl * prob_fl\n",
        "\n",
        "# Test\n",
        "polprob2sentiment('Positive', 0.91)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQz2AS56iP44"
      },
      "source": [
        "temp_ser = temp_sentiment_df.apply(lambda x: polprob2sentiment(str(x.label), float(x.score)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5unQZepFiPwS"
      },
      "source": [
        "# Transform predictions to labels\n",
        "\n",
        "# sents_preds = sents_predictions.predictions.argmax(-1)\n",
        "sents_preds = sents_pred_dataset.predictions.argmax(-1)\n",
        "\n",
        "\n",
        "sents_labels = pd.Series(sents_preds).map(model.config.id2label)\n",
        "sents_scores = (np.exp(sents_predictions[0])/np.exp(sents_predictions[0]).sum(-1,keepdims=True)).max(1)\n",
        "\n",
        "# Create DataFrame with texts, predictions, labels, and scores\n",
        "\n",
        "temp_sentiment_df = pd.DataFrame(list(zip(sents_pred_texts,sents_preds,sents_labels,sents_scores)), columns=['text','pred','label','score'])\n",
        "# temp_sentiment_df.head()\n",
        "\n",
        "# Convert label (Neg/Pos) and score (Prob) to a +/-Sentiment Float Value\n",
        "\n",
        "corpus_sents_df[sa_model] = temp_sentiment_df.apply(lambda x: polprob2sentiment(x.label,x.score), axis=1)\n",
        "corpus_sents_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V5jMKj0iPtp"
      },
      "source": [
        "# Aggregate Sentence Sentiments to populate Paragraph Sentiment DataFrame\n",
        "\n",
        "# def sentiment_sents2parags(ts_df, model_name='roberta_lg15'):\n",
        "parags_sentiment_ls = sentiment_sents2parags(corpus_sents_df, sa_model)\n",
        "corpus_parags_df[sa_model] = pd.Series(parags_sentiment_ls)\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.tail(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j7zdx-Qifob"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9cZmIkx7Too"
      },
      "source": [
        "# **EDA (Repeat for each Sentiment Model)** (Auto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJRV2n0M_xN6"
      },
      "source": [
        "#### **Histograms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJTcj9faMh61"
      },
      "source": [
        "**Sentiment Histogram Plots**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Histograms are provided for the (a) Length-Normed and (b) Scaled (Median Interquartile Range) Sentiment values for Sentences, Paragraphs and Sections. \n",
        "\n",
        "* There we used extensively early on to compare which Sentiment Time series preprocessing techniques worked best with our various Novel corpora according to two criteria: \n",
        "\n",
        "* (a) Vertical Scaling Method with the ability to transform histograms of sentiment values to well-behaved near-gaussian distributions and clipping outliers. After experimenting with various techniques including: mean/STD, median/MAD, and various two stage outlier/normalization methods median/IQR proved best (define).\n",
        "\n",
        "* (b) Horizontal Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPEv0DsBRhhQ"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU46yaCPPdSm"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYs2V2ILENaZ"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8b4CjTYwlgP"
      },
      "source": [
        "# Plot Histogram of Sentence lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuwVO3Q4Rmu4"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MhJ4T18PjTM"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SajUdBthwlgR"
      },
      "source": [
        "# Plot Histogram of Paragraph lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nuOQ7cZRrue"
      },
      "source": [
        "plot_histogram(model_name=model_name, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfhjLMdjRwZX"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_histogram(model_name=col_medianiqr, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBFb-SLywlgS"
      },
      "source": [
        "# Plot Histogram of Section lengths\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_histogram(model_name=col_lnorm_medianiqr, text_unit='section', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43YL_Iuf0JHZ"
      },
      "source": [
        "#### **Raw Sentiment Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AHHijpONvpt"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GorGKbFbR28W"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN27M4WlwlgT"
      },
      "source": [
        "# Plot Raw Sentence Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='sentence', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0QCx2OMN_jm"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxL5FryPR-ln"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d8uXvYJwlgU"
      },
      "source": [
        "# Plot Raw Paragraph Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "plot_raw_sentiments(model_name=model_name, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp7xjR7GvxkT"
      },
      "source": [
        "# TODO: Add Section Crux Nos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2hFkRQHSLVE"
      },
      "source": [
        "plot_raw_sentiments(model_name=model_name, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adhoCpB5SFQv"
      },
      "source": [
        "# col_medianiqr = f'{model_name}_medianiqr'\n",
        "# col_meanstd = f'{model_name}_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_medianiqr, semantic_type='section', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_meanstd, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjZZqY6cwlgW"
      },
      "source": [
        "# Plot Raw Standardized Section Sentiments\n",
        "\n",
        "# NOTE: Compared with Length-Normed, the Raw Standardizations lose most SATS features\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "# col_lnorm_meanstd = f'{model_name}_lnorm_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='section', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_lnorm_meanstd, semantic_type='section', save2file=False)\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDF_-tZhr44H"
      },
      "source": [
        "# Plot Raw Standardized Chapter Sentiments\n",
        "\n",
        "# col_lnorm_medianiqr = f'{model_name}_lnorm_medianiqr'\n",
        "# col_lnorm_meanstd = f'{model_name}_lnorm_meanstd'\n",
        "\n",
        "plot_raw_sentiments(model_name=col_lnorm_medianiqr, semantic_type='chapter', save2file=False)\n",
        "plot_raw_sentiments(model_name=col_lnorm_meanstd, semantic_type='chapter', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1tIJZepmvLu"
      },
      "source": [
        "#### **Crux Points and Surrounding Contexts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FBWrNEJyit6"
      },
      "source": [
        "# Veify all the model sentiment variations\n",
        "\n",
        "[x for x in corpus_sects_df.columns if x.startswith(model_base)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKazAFV_qpCn"
      },
      "source": [
        "**Compare Chapters vs Sections Crux Points**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At the highest level, the Corpus is divided into Chapters which may then futher subdivided into Sections (e.g. extra spaces, punctuation like '* * *' or special printer glyph/fleuron).  Horizonal dark blue lines indicate Chapter divisions while Section boundries lie at both dark and light blue vertical lines.\n",
        "\n",
        "* Since each Chapter may contain multiple Sections, the Section Sentiment plot is more detailed/jagged than the Chapter Sentiment plots. By plotting both together, the smoother Chapter Sentiment plot gives a more general sense of the Corpus Sentiment Arc while the next, more-detailed Section Sentiment plot enables a more detailed investigation/localization of Crux Point neighborhoods.\n",
        "\n",
        "* At this early stage both the Chapter and Section Sentiment plots are too general to provide accurate/fixed Crux localization. As such, only aggregrate Sentiment values for each Chapter/Section are assigned to the mid-point of each Chapter/Section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sme-1HVRZOGs"
      },
      "source": [
        "col_lnorm_medianiqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08xDu5WgkGAg"
      },
      "source": [
        "# col_lnorm_medianiqr = 'vader_lnorm_medianiqr'\n",
        "\n",
        "# corpus_chaps_df.drop(columns=['textblob_lnorm_medianiqr_lnorm_medianiqr', 'textblob_lnorm_medianiqr_medianiqr', 'textblob_lnorm_medianiqr_lnorm_meanstd'], inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06m1xDRzmWbq"
      },
      "source": [
        "corpus_chaps_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tojSdIGkPDz"
      },
      "source": [
        "corpus_chaps_df.columns\n",
        "# corpus_chaps_df.iloc[:20][['chap_no','sent_no_start','sent_no_mid','char_len','token_len','vader', 'vader_lnorm_medianiqr', 'textblob', 'textblob_lnorm_medianiqr']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu6F5kWerAPZ"
      },
      "source": [
        "corpus_chaps_df.vader_lnorm_medianiqr.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jku9ZQ9v9Wg"
      },
      "source": [
        "# Plot Annotated Section Cruxes of Standardized Sentiment Time Series\n",
        "\n",
        "sec_y_ht = -20\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='chapter', label_token_ct=3, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=-1, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxA0sDvlcdLx"
      },
      "source": [
        "**Sections Crux Points in Detail**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtj76Wtprj3E"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2tKXTD8v9RH"
      },
      "source": [
        "# Plot Annotated Section Cruxes of Standardized Sentiment Time Series\n",
        "sec_y_ht = -1.2\n",
        "\n",
        "corpus_cruxes_dt[col_lnorm_medianiqr] = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, sec_y_height=sec_y_ht, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSw-wARBF2fW"
      },
      "source": [
        "**Verify Crux Point Sentence Number and Text Match**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At [Crux_Sentence_Text] enter the first few words that uniquely identify the Crux Sentence and confirm the Sentence No matches the information in the plot above. (NOTE: Search is for an exact match including case and puncutation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU6l4UaPdmAF"
      },
      "source": [
        "Crux_Sentence_Text = \"I was impatient to\" #@param {type:\"string\"}\n",
        "\n",
        "# Verify individual Crux Sentence Numbers matches Content\n",
        "\n",
        "crux_sent_no = int(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(Crux_Sentence_Text)]['sent_no'])\n",
        "\n",
        "print(f'The Sentence:\\n\\n    {Crux_Sentence_Text}\\n\\nMatches Sentence #{crux_sent_no}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Q_weR3H5Qu"
      },
      "source": [
        "**Review Context Around Any Crux Point**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Enter [Crux_Sentence_No] that matches a Crux point/Sentence No you want to explore\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYAqkxi2FSw1"
      },
      "source": [
        "# Select details about the Crux Point Context to Retrieve\n",
        "\n",
        "# print(f'Last Sentence No: {corpus_sents_df.shape[0]}')\n",
        "Crux_Sentence_No =  1051#@param {type:\"number\"}\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "\n",
        "# if (Crux_Sentence_No >= No_Paragraphs_on_Each_Side) & (Crux_Sentence_No+No_Paragraphs_on_Each_Side <= corpus_parag_len):\n",
        "# get_sentnocontext_report()\n",
        "# try:\n",
        "get_sentnocontext_report(the_sent_no=Crux_Sentence_No, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "# except:\n",
        "#   print('ERROR')\n",
        "# else:\n",
        "#   print(f'ERROR: The combination of your [Crux_Sentence_No] and [No_Pargraphs_on_Each_Side]\\n       results in a window outside the range of the Corpus Paragraphs.\\n\\n       Try again with different values.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7-PoXAcq4S4"
      },
      "source": [
        "**Compare Paragraph vs Section Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzgfs-U9QZeq"
      },
      "source": [
        "# Verify the valid ranges for Sentences and Paragraphs\n",
        "\n",
        "corpus_sents_len = corpus_sents_df.shape[0]\n",
        "print(f'There are {corpus_sents_len} Sentences in the Corpus')\n",
        "corpus_parags_len = corpus_parags_df.shape[0]\n",
        "print(f'There are {corpus_parags_len} Paragraphs in the Corpus')\n",
        "\n",
        "# Create a new Corpus Paragraph DataFrame (corpus_parags_zoom_df) that is streteched out to have as many sample points as there are Sentences\n",
        "#   That is, go from an original corpus_parags_df of #Paragraph datapoints to an expanded corpus_parags_zoom_df of #Sentences datapoints using scipy.ndimage.interpolation.zoom\n",
        "\n",
        "corpus_parags_zoom_df = pd.DataFrame()\n",
        "\n",
        "resample_ratio = corpus_sents_df.shape[0]/corpus_parags_df.shape[0]   # ratio = no_sents/no_parags\n",
        "\n",
        "corpus_parags_df_numcols_ls = corpus_parags_df.select_dtypes(include=['number']).columns\n",
        "\n",
        "for acol in corpus_parags_df_numcols_ls:\n",
        "  parags_zoom_temp_np = zoom(np.array(corpus_parags_df[acol]), resample_ratio)\n",
        "  corpus_parags_zoom_df[acol] = pd.Series(parags_zoom_temp_np)\n",
        "\n",
        "print('\\n')\n",
        "print(f'New expanded corpus_parags_zoom_df.shape = {corpus_parags_zoom_df.shape}')\n",
        "print(f'           matches corpus_sents_df.shape = {corpus_sents_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wskHXVNSmjfQ"
      },
      "source": [
        "**Adjust the Paragraph Sentiment Plot to compare it with the Section Sentiment Plot for this Corpus**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Adjust [Scale_Vertical_Rolling_Paragraph] until the vertical min/max spans of the two plots are approximately equal\n",
        "\n",
        "* Adjust [Set_Paragraph_Rolling_Window_Percent] to set horizonal smoothness of Rolling Paragraph (typically 5,10 or 20%)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9P9k98ibh4n"
      },
      "source": [
        "col_lnorm_medianiqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgaBEAVCTMRt"
      },
      "source": [
        "Scale_Vertical_Rolling_Paragraph = 2.7 #@param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "Set_Paragraph_Rolling_Window_Percent = 3 #@param {type:\"slider\", min:0, max:30, step:1}\n",
        "\n",
        "# Compare Section Midpoints vs Sentence SMA Sentiment Values\n",
        "\n",
        "scale_sma_paragraph = Scale_Vertical_Rolling_Paragraph\n",
        "sentence_count = corpus_sents_df.shape[0]\n",
        "if Set_Paragraph_Rolling_Window_Percent == 0:\n",
        "  sma_parag_win = 1\n",
        "else:\n",
        "  sma_parag_win = int((Set_Paragraph_Rolling_Window_Percent/100)*sentence_count)\n",
        "\n",
        "\n",
        "# corpus_parags_zoom_df['vader_lnorm_medianiqr'].rolling(window=sma_parag_win, center=True).mean().apply(lambda x: x*scale_sma_paragraph).plot(label=f'{model_base} Paragraphs', alpha=0.3)\n",
        "corpus_parags_zoom_df[col_lnorm_medianiqr].rolling(window=sma_parag_win, center=True).mean().apply(lambda x: x*scale_sma_paragraph).plot(label=f'{model_base} Paragraphs', alpha=0.3)\n",
        "\n",
        "_ = plot_crux_sections(model_names_ls=[col_lnorm_medianiqr], semantic_type='section', label_token_ct=0, title_xpos=0.5, title_ypos=1.0, sec_y_height=8, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} \\n SMA vs midpoint Paragraph MedianIQR Sentiment with Crux Points via SciPy.peaks')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUyZF6tE1rL0"
      },
      "source": [
        "#### **Zoom into a Section**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* At [Select_Section_No] pick a Section of the Corpus to zoom into\n",
        "\n",
        "* Adjust [Scale_Vertical_Rolling_Sentences] until the vertical min/max spans of the two plots are approximately equal \n",
        "\n",
        "* Adjust [Set_Sentence_Rolling_Window_Percent] to set horizonal smoothness of Rolling Paragraph (typically 5,10 or 20%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu7DYRELQBoH"
      },
      "source": [
        "# Explore a Corpus Section Up-Close\n",
        "\n",
        "section_count = corpus_sects_df.shape[0]\n",
        "print(f'There are {section_count} Sections in this corpus,\\n  pick one numbered between 0 and {section_count-1}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Q2a854Zxl1"
      },
      "source": [
        "Select_Section_No =  15#@param {type:\"integer\"}\n",
        "Scale_Vertical_Rolling_Sentences = 0.2 #@param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "Set_Sentence_Rolling_Window_Percent = 3 #@param {type:\"slider\", min:0, max:30, step:1}\n",
        "\n",
        "# Make Copies instead of just using References / Only Reference, not copy()\n",
        "# section_sents_df = pd.DataFrame()\n",
        "# section_parags_df = pd.DataFrame()\n",
        "\n",
        "section_sents_df, section_parags_df = get_section_timeseries(Select_Section_No)\n",
        "\n",
        "# section_sents_df.head()\n",
        "\n",
        "print(f'section_sents_df.shape: {section_sents_df.shape}')\n",
        "print(f'section_parags_df.shape: {section_parags_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MbSPmp2dN9Z"
      },
      "source": [
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE-CDUmvFDOe"
      },
      "source": [
        "# Add expanded Paragraph sentiment to corpus_sents_df\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr_parag'] = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df')\n",
        "parags_midpoint_ls = []\n",
        "col_name_parag = f'{model_name}_parag'\n",
        "section_sents_df[col_name_parag], parags_midpoint_ls = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df', model_name=model_name)\n",
        "\n",
        "# Verify Sentences and Expanded Paragraph lengths match\n",
        "print(f'\\nIn Section #{Select_Section_No}\\n')\n",
        "print(f'            Sentence Count: {section_sents_df.shape[0]}')\n",
        "print(f\"(expanded) Paragraph Count: {str(section_sents_df['parag_no'].unique().shape[0])}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOBqRxQz3gbh"
      },
      "source": [
        "##### **Section Histograms**\n",
        "\n",
        "EDA Unbalanced Paragraph and Sentence Features within selected Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MoMM3l8Px2R"
      },
      "source": [
        "# Verify the non-uniform distribution of Paragraph lengths within selected Section (thus necessity for noralizing Paragraph Sentiment by Paragraph length)\n",
        "\n",
        "section_sents_df['parag_no'].value_counts().hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Number of Sentences per Paragraph in Section #{Select_Section_No}')\n",
        "plt.xlabel('Number of Sentences per Paragraph')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5eC0i5KO9cm"
      },
      "source": [
        "section_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoVdCaReDJvl"
      },
      "source": [
        "section_sents_df[model_name].hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.xlabel('Sentence Sentiment')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSrXWnnEEm00"
      },
      "source": [
        "# Histogram of Paragraph Sentiments within selected Section\n",
        " \n",
        "# create a unified Section DataFrame with equal length Sentences and (expanded) Paragraphs Sentiment Series\n",
        "# section_sents_parags_df = section_sents_df.copy()\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr_parag_approx'] = parag_sentiment_expanded_ls\n",
        "\n",
        "\n",
        "section_sents_df[col_name_parag].hist(bins=30)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nHistogram of Length-Normed Paragraph Sentiment in Section #{Select_Section_No}')\n",
        "plt.xlabel('Length-Normed Sentiment of Paragraph')\n",
        "plt.ylabel('Frequency');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcao3RNuyCr_"
      },
      "source": [
        "**Naive Raw and LOWESS Smoothed Paragraph Sentiment plots within selected Section**\n",
        "\n",
        "NOTE: Horizonal x-axis narrative time axis not adjusted for variable paragraph lengths - simply used midpoints assuming equal length Paragraphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag47-b4EEO6k"
      },
      "source": [
        "**Raw and SMA Sentence Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJUleS6xGAej"
      },
      "source": [
        "section_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkpQiVCoZTJ0"
      },
      "source": [
        "# section_crux_sents_dt\n",
        "\n",
        "# type(section_crux_sents_dt['vader_lnorm_medianiqr_roll50_frac14_win10'][0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB4fEZzJQPkg"
      },
      "source": [
        "##### **Section SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx40oRnWjgoj"
      },
      "source": [
        "sec_y_ht = 0.02\n",
        "\n",
        "plot_smas(section_view=False, model_name='textblob_lnorm_medianiqr', text_unit='sentence', wins_ls=[5,10,15,20], alpha=0.5, y_height=sec_y_ht, subtitle_str=f'(Model: {model_base.capitalize()})', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAIAX2OhW2k0"
      },
      "source": [
        "# SMA with Raw Sentiment values over entire Corpus\n",
        "\n",
        "sec_y_ht = -0.06\n",
        "\n",
        "plot_smas(section_view=False, model_name=model_base, text_unit='sentence', wins_ls=[5,10,15,20], alpha=0.5, y_height=sec_y_ht, subtitle_str=f'(Model: {model_base.capitalize()})', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8xWHqNAYV_p"
      },
      "source": [
        "# SMA with Length-Normed MedianIQR Sentiment values over entire Corpus\n",
        "\n",
        "sec_y_ht = -0.11\n",
        "\n",
        "plot_smas(section_view=False, model_name=model_name, text_unit='sentence', wins_ls=[5,10,15,20], alpha=0.5, y_height=sec_y_ht, subtitle_str=f'(Model: {model_base.capitalize()})', save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nA285mHvM4T"
      },
      "source": [
        "sec_y_ht = 0.65\n",
        "\n",
        "# At Section boundries draw blue vertical lines \n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "  # 'BigNews1', xy=(sent_no, 0.5), xytext=(-10, 25), textcoords='offset points',                   rotation=90, va='bottom', ha='center', annotation_clip=True)\n",
        "\n",
        "  # plt.text(sent_no, -.5, 'goodbye',rotation=90, zorder=0)\n",
        "      \n",
        "for win_size in range(50,250,25):\n",
        "  section_sents_df[model_name].rolling(win_size, center=True).mean().plot(alpha=0.5, label=f'SMA win={win_size}')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base.capitalize()})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfZiEeDhWJJM"
      },
      "source": [
        "# Plot Sentence by Sentence Sentiment within selected Section\n",
        "\n",
        "sec_y_ht = -0.5\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "awins_ls = [10]\n",
        "get_smas(section_sents_df, model_name=model_name, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=col_medianiqr, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=1.8, subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=model_base, text_unit='sentence', wins_ls=awins_ls, alpha=0.5, scale_factor=8., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False);\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqlsQS-RYT7x"
      },
      "source": [
        "# Plot Paragraph by Sentence Sentiment within selected Section\n",
        "\n",
        "get_smas(section_sents_df, model_name=model_name, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=col_medianiqr, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=1.8, subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_sents_df, model_name=model_base, text_unit='paragraph', wins_ls=[5], alpha=0.5, scale_factor=8., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Sentence Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTHlWyjzYf7C"
      },
      "source": [
        "# Plot Paragraph by Paragraph Sentiment within selected Section\n",
        "\n",
        "sec_y_ht = -15\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(aparag_no, sec_y_ht, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(aparag_no, color='blue', alpha=0.1)\n",
        "\n",
        "awins_ls = [5]\n",
        "get_smas(section_parags_df, model_name=model_name, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=1., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_parags_df, model_name=col_medianiqr, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=8., subtitle_str='', mean_adj=0., do_plot=True, save2file=False)\n",
        "get_smas(section_parags_df, model_name=model_base, text_unit='paragraph', wins_ls=awins_ls, alpha=0.5, scale_factor=10., subtitle_str=f'Section #{Select_Section_No}', mean_adj=0., do_plot=True, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nSMA Length-Normed Paragraph Sentiment in Section #{Select_Section_No} (win={awins_ls[0]}%)')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhIM7zwuQUBq"
      },
      "source": [
        "##### **Raw Sentiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp0JvXFrV-B4"
      },
      "source": [
        "# Plot Raw vs MedianIQR Sentence Sentiment within selected Section\n",
        "\n",
        "sec_y_ht = -4.0\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_base, data=section_sents_df, alpha=0.3, label=f'Raw Sentence Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_sents_df, alpha=0.5, label=f'MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw vs MedianIQR Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzoXo8mVZMq_"
      },
      "source": [
        "# MedianIQR vs Length-Normed MedianIQR Sentence Sentiment within selected Section\n",
        "\n",
        "sec_y_ht = -17\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(sent_no, sec_y_ht, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_name, data=section_sents_df, alpha=0.3, label=f'Length-Normed MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_sents_df, alpha=0.3, label=f'MedianIQR Sentence Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLength-Normed vs non-Normed Sentence Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tag3LC8Y9Is"
      },
      "source": [
        "# Plot Paragraph by Paragraph Sentiment within selected Section\n",
        "\n",
        "sec_y_ht = -15\n",
        "\n",
        "paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "  if i%5 == 0:\n",
        "    # Plot every 5th paragraph\n",
        "    sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "    plt.text(aparag_no, sec_y_ht, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "    plt.axvline(aparag_no, color='blue', alpha=0.1)\n",
        "\n",
        "plt.plot(model_name, data=section_parags_df, alpha=0.3, label=f'Length-Normed MedianIQR Paragraph Sentiment ({model_base})')\n",
        "plt.plot(col_medianiqr, data=section_parags_df, alpha=0.3, label=f'MedianIQR Paragraph Sentiment ({model_base})')\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLength-Normed vs non-Normed Paragraph Sentiment in Section #{Select_Section_No}')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJmDFQeH6gI"
      },
      "source": [
        "##### **LOWESS Smoothed**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhYnx-MgH_G0"
      },
      "source": [
        "model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0bylmqBf7Js"
      },
      "source": [
        "# MedianIQR vs Length-Normed MedianIQR Sentence Sentiment within selected Section\n",
        "\n",
        "win_lowess_no = 10\n",
        "sec_y_ht = -17\n",
        "\n",
        "section_crux_ls = get_crux_points(ts_df=section_sents_df, col_series=model_name, text_type='sentence', win_lowess=win_lowess_no, y_height=sec_y_ht, do_plot=True, save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DQ4zGtRizlM"
      },
      "source": [
        "# TODO: Only printing sentiment for first crux point\n",
        "\n",
        "section_sents_df.iloc[62]['sent_raw']\n",
        "print('\\n')\n",
        "section_sents_df.iloc[62]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B0vm5RjgCLS"
      },
      "source": [
        "Get_Peak_Cruxes = True #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "\n",
        "\n",
        "crux_sortsents_report(section_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa124qbuKzzH"
      },
      "source": [
        "# LOWESS Smoothed Sentences within chosen Selection No\n",
        "\n",
        "my_afrac = 1./12   # 1./12 ~ 0.08\n",
        "\n",
        "temp_df = get_lowess(section_sents_df, [model_name], plot_subtitle='LOWESS Smoothed MedianIRQ Sentence Sentiment', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                afrac=my_afrac, ait=7, alpha=0.8, do_plot=True, save2file=False)\n",
        "temp_df.columns\n",
        "col_lowess = f'{model_name}_{my_afrac:2.3f}lowess'\n",
        "col_lowess_clean = col_lowess.replace('_0.','_')\n",
        "section_sents_df[col_lowess_clean] = temp_df['median']\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Sentence Sentiments with selected Section #{Select_Section_No} (LOWESS frac={my_afrac:.2f})')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend('',frameon=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLeyWMtMaEqZ"
      },
      "source": [
        "# LOWESS Smoothed Paragraphs within chosen Selection No\n",
        "\n",
        "my_afrac = 1./8 # 1./8 ~ 0.125\n",
        "\n",
        "temp_df = get_lowess(section_parags_df, [model_name], plot_subtitle='LOWESS Smoothed Mean Rolling Sentence Sentiment', alabel=f'LOWESS Smoothed (afrac={my_afrac})', \n",
        "                afrac=my_afrac, ait=7, alpha=0.8, do_plot=True, save2file=False)\n",
        "\n",
        "section_parags_df[f'{model_name}_{my_afrac:.2f}_lowess'] = temp_df['median']\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Sentence Sentiments with selected Section #{Select_Section_No} (LOWESS frac={my_afrac:.2f})')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend('',frameon=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGDccDTMfMT4"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10, 1./14, 1./16]\n",
        "# grid_fracs = [1./10, 1./14, 1./16]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.3]\n",
        "win_lowess=9\n",
        "\n",
        "\n",
        "# section_sents_df['vader_lnorm_medianiqr'].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot('vader_lnorm_medianiqr', data=section_sents_df)\n",
        "plt.title(f'LOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "section_crux_sents_dt = {}\n",
        "\n",
        "for afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  #   _ = get_lowess(section_sents_df, ['vader_lnorm_medianiqr'], plot_subtitle='Naive Raw + MedianIQR Midpoints', alabel=f'LOWESS Smoothed (afrac={my_afrac})', \n",
        "  #                afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # afrac = 1./7\n",
        "  # model_name = 'vader_lnorm_medianiqr' # model_name\n",
        "  sm_x, sm_y = sm_lowess(endog=section_sents_df[model_name].values, exog=section_sents_df.index.values, frac=afrac, it=3, return_sorted = True).T\n",
        "  # _ = get_lowess(ts_df='section_sents_df', models_ls=[col_roll_str], text_unit='sentence', plot_subtitle='', alabel='', afrac=1./10, ait=5, alpha=0.5, do_plot=True, save2file=False)\n",
        "  section_crux_ls = list(get_crux_points(section_sents_df, col_series=model_name, win_lowess=win_lowess, do_plot=False))\n",
        "  col_lowess_str = f'{model_name}_frac{int((afrac-int(afrac))*100)}_win{win_lowess}'\n",
        "  # print(f\"col_lowess_str: {col_lowess_str}\")\n",
        "  section_crux_sents_dt[col_lowess_str] = section_crux_ls # list(zip(sm_x, sm_y))\n",
        "  # x, y = zip(*data)\n",
        "\n",
        "  # Set vertical y-axis magnification\n",
        "  y_mag = 30\n",
        "  plt.plot(sm_x, y_mag*sm_y)\n",
        "  plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nDifferent LOWESS Smoothed SMA Sentence Sentiments and Crux Points within selected Section #{Select_Section_No}')\n",
        "\n",
        "\n",
        "for key,value in section_crux_sents_dt.items():\n",
        "  model_name = key\n",
        "  crux_points_ls = value\n",
        "  plt.scatter(*zip(*crux_points_ls))\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nDifferent SMA Windows of Raw Sentence Sentiments and Crux Points within selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "\n",
        "# Plot the mean of all SMA MedianIQR Sentiment Time Series\n",
        "# col_meanroll = f'{model_name}_rollmean'\n",
        "# section_sents_df[col_meanroll] = section_sents_df[col_rolls_ls].mean(axis=1)\n",
        "# section_sents_df[col_meanroll].plot(label='mean', color='black', linewidth=3)\n",
        "\n",
        "# Plot corresponding Crux Points\n",
        "# model_name = 'vader_lnorm_medianiqr'\n",
        "section_crux_ls = get_crux_points(section_sents_df, col_series=model_name, win_lowess=10) # 'vader_lnorm_medianiqr_0.07_lowess')\n",
        "section_sents_df.shape[0]\n",
        "print('\\n');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVCq0YUXBpX8"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2]\n",
        "\n",
        "# section_sents_df['vader_lnorm_medianiqr'].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot('vader_lnorm_medianiqr', data=section_sents_df)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "for my_afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  _ = get_lowess(section_sents_df, [model_name], plot_subtitle='Naive Raw + MedianIQR Midpoints', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                 afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # corpus_cruxes_dt['vader_lnorm_medianiqr'] = plot_crux_sections(model_names_ls=['vader_lnorm_medianiqr'], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL}\\n LOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpzOnRqZS3jZ"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "# grid_fracs = [1./3, 1./4, 1./6, 1./10]\n",
        "# grid_fracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.12, 0.14, 0.16, 0.18, 0.2]\n",
        "\n",
        "# section_sents_df[model_name].plot(label=f'Raw Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "# plt.plot(model_name, data=section_sents_df)\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n",
        "\n",
        "for my_afrac in grid_fracs:\n",
        "  # print(f'type(my_afrac) = {type(my_afrac)}, value = {my_afrac}')\n",
        "  _ = get_lowess(section_parags_df, [model_name], plot_subtitle='MedianIQR Midpoints', alabel=f'LOWESS (afrac={my_afrac})', \n",
        "                 afrac=my_afrac, ait=7, alpha=my_afrac, do_plot=True, save2file=False);\n",
        "\n",
        "  # corpus_cruxes_dt[model_name] = plot_crux_sections(model_names_ls=[model_name], semantic_type='section', label_token_ct=5, title_xpos=0.8, title_ypos=1.05, save2file=False)\n",
        "plt.title(f'{CORPUS_FULL}\\n LOWESS Smoothed Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O85J51__Ka-z"
      },
      "source": [
        "section_sents_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48u_zr6-KkH-"
      },
      "source": [
        "# SMA Sentences\n",
        "\n",
        "# grid_afracs = [0.14, 0.16, 0.18, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "grid_fracs = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.3]\n",
        "# grid_fracs = [1./6, 1./7, 1./8, 1./9, 1./10, 1./15, 1./20]\n",
        "scale_roll = 1.\n",
        "win_lowess_per = 30\n",
        "win_lowess = int(win_lowess/100 * section_sents_df.shape[0])\n",
        "\n",
        "col_meanroll_lowess_ls = []\n",
        "\n",
        "col_meanroll = f'{model_name}_mean_roll050'\n",
        "for afrac in grid_fracs:\n",
        "  lowess_smooth_df = get_lowess(section_sents_df, [col_meanroll], plot_subtitle='SMA Mean of MedianIQR ', alabel=f'LOWESS afrac={afrac:.3f}', \n",
        "                afrac=afrac, ait=7, alpha=0.3, do_plot=True, save2file=False)\n",
        "  # print(f'type: {lowess_smooth_df.columns}')\n",
        "\n",
        "# col_lowess_str = f'{col_mean_roll}_lowess_frac{10*win_lowess}'\n",
        "col_meanroll_lowess_str = f'{col_meanroll}_frac{int((afrac-int(afrac))*100)}_win{win_lowess}'\n",
        "col_meanroll_lowess_ls.append(col_meanroll_lowess_str)\n",
        "section_sents_df[col_meanroll_lowess_str] = section_sents_df[model_name].apply(lambda x: x*scale_roll).rolling(win_lowess, center=True).mean()\n",
        "section_sents_df[col_meanroll_lowess_str].plot(alpha=0.7)\n",
        "get_lowess(section_sents_df, [col_meanroll_lowess_str], plot_subtitle='SMA Mean of MedianIQR ', alabel=f'LOWESS (afrac={my_afrac:.3f})', \n",
        "                afrac=afrac, ait=7, alpha=1.0, do_plot=True, save2file=False);\n",
        "\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nLOWESS Smoothed Sentence Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KlWvNkcMI-M"
      },
      "source": [
        "**Raw Paragraph Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhRcTp4ALYUQ"
      },
      "source": [
        "# TODO: Convert to input widgets\n",
        "win_sents_ls = [5,10,15,20,25]\n",
        "scale_roll = 6\n",
        "\n",
        "plt.plot(model_name, data=section_parags_df, alpha=0.3, label=f'Raw Paragraph Sentiment within selected Segment #{Select_Section_No}')\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_base})\\nRaw Paragraph Sentiments with selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5TcmNEsMMiE"
      },
      "source": [
        "**Length-Normed Paragraph Sentiment Plot within selected Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BemEIw0Tknk"
      },
      "source": [
        "corpus_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyRTfn6HI50X"
      },
      "source": [
        "# Plot and Compare Naive Raw and LOWESS Smoothed Paragraph Sentiment Time Series within selected Section\n",
        "\n",
        "section_parag_lowess_df = pd.DataFrame()\n",
        "section_parag_lowess_df['parag_no'] = section_parags_df['parag_no'].copy()\n",
        "\n",
        "parags_midpoint_sentiment_ls = []\n",
        "for parags_midpoint_idx in parags_midpoint_ls:\n",
        "  parags_midpoint_sentiment_ls.append(float(corpus_parags_df[corpus_parags_df['parag_no'] == parags_midpoint_idx][model_name]))\n",
        "\n",
        "col_midapprox = f'{model_name}_midapprox'\n",
        "section_parag_lowess_df[col_midapprox] = parags_midpoint_sentiment_ls\n",
        "\n",
        "\n",
        "section_parag_lowess_df[col_midapprox].plot(label='Raw Midpoints')\n",
        "plt.xlabel(f'Niave Paragraph No within selected Section #{Select_Section_No}')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "\n",
        "_ = get_lowess(section_parag_lowess_df, [col_midapprox], plot_subtitle=f'{model_base.capitalize()} Naive Raw + MedianIQR Midpoints', alabel='LOWESS Midpoints', afrac=1./4, ait=7, do_plot=True, save2file=False);\n",
        "\n",
        "# section_lowess_parags_df = get_lowess(section_sents_parags_df, ['vader_lnorm_medianiqr'], plot_subtitle='Approximate Paragraph MedianIQR', afrac=1./4, ait=7, do_plot=True, save2file=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24M7uauXygFc"
      },
      "source": [
        "**Length-Noramlized Raw and LOWESS Smoothed Paragraph Sentiment plots within selected Section**\n",
        "\n",
        "NOTE: Horizonal x-axis narrative time axis adjusted for variable paragraph lengths - used midpoints of unequal length Paragraphs to more accurately visualize Sentiment Arc and precisely localize Crux Points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOsXfELh-tt_"
      },
      "source": [
        "# Verify details on currently selected Section\n",
        "print(f'Details on Section #{Select_Section_No}')\n",
        "print('------------------------')\n",
        "print(f' Paragraph Count: {section_parags_df.shape[0]}')\n",
        "print(f' Sentence Count:  {section_sents_df.shape[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfQS_qAo-Qdh"
      },
      "source": [
        "# %load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq2JjzKIHEdO"
      },
      "source": [
        "# Plot Raw and Rolling Sentence Sentiments within selected Section\n",
        "\n",
        "win_per = 5  # Rolling Window size in percentage of total Corpus length\n",
        "\n",
        "section_sents_parags_df.plot(x='sent_no', y='vader_lnorm_medianiqr')\n",
        "\n",
        "plt.title(f'Raw and Rolling Sentence Sentiments within selected Section #{Select_Section_No}')\n",
        "plt.xlabel(f'Sentence No within selected Section #{Select_Section_No} (Length-Normalized in terms of Paragraphs)')\n",
        "plt.ylabel(f'Sentiment Value')\n",
        "\n",
        "section_sents_parags_df['vader_lnorm_medianiqr'].rolling(int((win_per/100)*section_sents_parags_df.shape[0])).mean().plot(label=\"Approx Paragraph VADER SMA (win=5%)\");\n",
        "\n",
        "# section_sents_parags_df.plot(x='sent_no', y='vader_lnorm_medianiqr', label='Sentence VADER MedianIQR')\n",
        "# section_sents_parags_df['vader_lnorm_medianiqr'].rolling(int(0.05*section_sents_parags_df.shape[0])).mean().plot(label=\"Sentence VADER SMA (win=5%)\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laYb3dm101Qa"
      },
      "source": [
        "**Get Crux Points within selected Section**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peaks] to search for Peaks (unselect to search for Valley)\n",
        "\n",
        "* Pick [Crux_Rank] (1-5) to get the 1st to 5th biggest Peak or Valley Crux Paragraph\n",
        "\n",
        "* Pick [Context_Paragraphs_Each_Side] (0-5) to get n paragraphs before and n paragraphs after the selected Crux Paragraph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyekwnvX4wkj"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# ARCHIVED\n",
        "\n",
        "def get_sentnocontext(ts_df, model_name='vader', get_peaks=True, crux_rank=1, n_sideparags=1):\n",
        "  # get_cruxparags_section()\n",
        "  '''\n",
        "  Given a Section DataFrame with model_name sentiment column and crux peak/valley, rank and side paragraphs context\n",
        "  Return a list with the appropriate Crux Paragraph within this Section, and context\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "  Given a sentence number in the Corpus\n",
        "  Return the containing paragraph and n-paragraphs on either side\n",
        "  (e.g. if n=2, return 2+1+2=5 paragraphs)\n",
        "  '''\n",
        "\n",
        "  crux_parags_context_ls = []\n",
        "\n",
        "  if get_peaks == True:\n",
        "    sort_asc_flag=False\n",
        "  else:\n",
        "    sort_asc_flag=True\n",
        "\n",
        "  crux_parag_no = ts_df.sort_values(by=[model_name], ascending=sort_asc_flag).iloc[crux_rank-1]['parag_no']\n",
        "\n",
        "  print(f'crux_parag_no: {crux_parag_no}')\n",
        "\n",
        "  if n_sideparags == 0:\n",
        "    crux_parags_context_ls = list(corpus_parags_df[corpus_parags_df['parag_no'] == crux_parag_no]['parag_raw'])\n",
        "\n",
        "  else:\n",
        "    parag_start = crux_parag_no - n_sideparags\n",
        "    parag_end = crux_parag_no + n_sideparags + 1\n",
        "    crux_parags_context_ls = list(corpus_parags_df.iloc[parag_start:parag_end]['parag_raw'])\n",
        "\n",
        "  return crux_parags_context_ls\n",
        "\n",
        "# Test\n",
        "parags_context_ls = get_sentnocontext(ts_df=section_parags_df, model_name='vader_lnorm_medianiqr', get_peaks=True, crux_rank=1, n_sideparags=1)\n",
        "parags_context_ls\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxeabYXf2iqB"
      },
      "source": [
        "# def get_crux_parags_report(ts_df, model_name='vader', get_peaks=True, crux_rank=1, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence):\n",
        "\n",
        "# def get_sentnocontext_report(ts_df, model_name='vader', get_peaks=True, crux_rank=1, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence):\n",
        "'''\n",
        "Given a DataFrame with model_name sentiment column and crux peak/valley, rank and side paragraphs context\n",
        "Return a list with the appropriate Crux Paragraph, and context\n",
        "'''\n",
        "\"\"\"\n",
        "\n",
        "def get_sentnocontext_report(the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n",
        "  '''\n",
        "  Wrapper function around  get_sentnocontext()\n",
        "  Prints a nicely formatted context report\n",
        "  '''\n",
        "\n",
        "  context_noparags = the_n_sideparags*2+1\n",
        "\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f'The {context_noparags} Paragraph(s) Context around the Sentence #{Crux_Sentence_No} Crux Point:')\n",
        "  print('-------------------------------------------------------------')\n",
        "  print(f\"\\nCrux Sentence Raw Text: -------------------------------\\n\\n    {corpus_sents_df[corpus_sents_df['sent_no'] == the_sent_no]['sent_raw']}\") # iloc[the_sent_no]['sent_raw']}\")\n",
        "\n",
        "  print(f\"\\n{context_noparags} Paragraph(s) Context: ------------------------------\")\n",
        "  # context_parags_ls = get_sentnocontext(sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_parags_ls = get_sentnocontext(sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n",
        "  context_len = len(context_parags_ls)\n",
        "  context_mid = context_len//2\n",
        "  for i, aparag in enumerate(context_parags_ls):\n",
        "    if i==context_mid:\n",
        "      # print(f'\\n>>> Paragraph #{i}: <<< Crux Point Sentence CAPITALIZED within this Paragraph\\n\\n    {aparag}')\n",
        "      print(f'\\n<*> {aparag}')\n",
        "    else:\n",
        "      # print(f'\\n    Paragraph #{i}:\\n\\n    {aparag}')\n",
        "      print(f'\\n    {aparag}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Test\n",
        "# get_sentnocontext_report(sent_no=1051, n_sideparags=1, sent_highlight=True)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxQGzYhgHEUD"
      },
      "source": [
        "# Get_Peaks = True #@param {type:\"boolean\"}\n",
        "Crux_Rank = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "No_Paragraphs_on_Each_Side = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Crux_Sentence = True #@param {type:\"boolean\"}\n",
        "\n",
        "# try:\n",
        "  \n",
        "# get_sentnocontext_report(ts_df=section_sents_df, model_name=model_name, get_peaks=Get_Peaks, crux_rank=Crux_Rank, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "get_sentnocontext_report(corpus_sents_df, the_sent_no=Crux_Rank, the_n_sideparags=No_Paragraphs_on_Each_Side, the_sent_highlight=Highlight_Crux_Sentence)\n",
        "\n",
        "# except:\n",
        "#   print('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cPnJ7-QAHBp"
      },
      "source": [
        "section_parags_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga65SGcg5sM9"
      },
      "source": [
        "print(section_parags_df.sort_values(by=['vader_lnorm_medianiqr'], ascending=False).iloc[0]) # ['parag_no'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGuFadpM8QoB"
      },
      "source": [
        "parags_context_ls = get_cruxparag_context(ts_df=section_sents_parags_df, model_name='vader_lnorm_medianiqr', get_peaks=True, crux_rank=1, n_sideparags=2, sent_highlight=True)\n",
        "parags_context_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPE_ODwK2imT"
      },
      "source": [
        "section_sents_parags_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0A0M6TPu_Ml"
      },
      "source": [
        "## **Compare Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpX60Cld_B7z"
      },
      "source": [
        "def drop_dupcols(df):\n",
        "  '''\n",
        "  Given a DataFrame\n",
        "  Drop repeatitive columns\n",
        "  '''\n",
        "\n",
        "  col_drop_ls = []\n",
        "\n",
        "  col_ls = list(df.columns)\n",
        "  print(f'BEFORE: Columns #{len(df.columns)}')\n",
        "\n",
        "  for i, acol in enumerate(col_ls):\n",
        "    acol_word_ls = acol.split('_')\n",
        "    # print(f'acol_word_ls: {acol_word_ls}')\n",
        "    if (len(acol_word_ls)) == len(set(acol_word_ls)):\n",
        "      continue\n",
        "    else:\n",
        "      col_drop_ls.append(acol)\n",
        "\n",
        "  df.drop(columns=col_drop_ls, inplace=True, axis=1)\n",
        "\n",
        "  print(f'AFTER: Columns #{len(df.columns)}')\n",
        "\n",
        "  return col_drop_ls\n",
        "\n",
        "dropped_cols_ls = drop_dupcols(corpus_parags_df)\n",
        "print(f'dropped: {dropped_cols_ls}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIOFkFzYinfd"
      },
      "source": [
        "corpus_parags_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lpvN8PR9VRk"
      },
      "source": [
        "# List of Tuples (Model, Scaling Factor) to plot together with same size for comparison\n",
        "\n",
        "models_sma_ls = [('vader_lnorm_medianiqr',1),\n",
        "                 ('textblob_lnorm_medianiqr',20),\n",
        "                 ('afinn_lnorm_medianiqr',4),\n",
        "                 ('sentimentr_lnorm_medianiqr',1),\n",
        "                 ('syuzhet_lnorm_medianiqr',1),\n",
        "                 ('bing_lnorm_medianiqr',0.1),\n",
        "                 ('sentiword_lnorm_medianiqr',10),\n",
        "                 ('senticnet_lnorm_medianiqr',.5),\n",
        "                 ('nrc_lnorm_medianiqr',0.2),\n",
        "                 ('pattern_lnorm_medianiqr',20),\n",
        "                 ('stanza_lnorm_medianiqr',0.5),\n",
        "                 ('hfbert_lnorm_medianiqr',5),\n",
        "                 ('nlptown_lnorm_medianiqr',5),\n",
        "                 ('robertalg15_lnorm_medianiqr',5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1pjqF9nTQg-"
      },
      "source": [
        "# List of Tuples (Model, Scaling Factor) to plot together with same size for comparison\n",
        "\n",
        "models_sma_ls = [('vader',1),\n",
        "                 ('textblob',20),\n",
        "                 ('afinn',4),\n",
        "                 ('sentimentr',1),\n",
        "                 ('syuzhet',1),\n",
        "                 ('bing',0.1),\n",
        "                 ('sentiword',10),\n",
        "                 ('senticnet',.5),\n",
        "                 ('nrc',0.2),\n",
        "                 ('pattern',20),\n",
        "                 ('stanza',0.5),\n",
        "                 ('hfbert',5),\n",
        "                 ('nlptown',5),\n",
        "                 ('robertalg15',5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RihyT4wZFP5k"
      },
      "source": [
        "corpus_sents_df[['stanza_lnorm_medianiqr','stanza']].rolling(500,center=True).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaxzZkVlBcNf"
      },
      "source": [
        "def plot_autoscaled_ts(ts_df=corpus_sents_df, ts_ls=['vader_lnorm_medianiqr', 'textblob_lnorm_medianiqr', \n",
        "                                                     'syuzhet_lnorm_medianiqr', 'sentimentr_lnorm_medianiqr',\n",
        "                                                     'bing_lnorm_medianiqr', 'afinn_lnorm_medianiqr',\n",
        "                                                     'pattern_lnorm_medianiqr', 'stanza_lnorm_medianiqr']):\n",
        "  '''\n",
        "  Given a DataFrame and list of Columns/Time Series\n",
        "  Automatically scale all to the same range and plot together\n",
        "  '''\n",
        "\n",
        "  ts_spans_ls = []\n",
        "\n",
        "  current_min = ts_df[ts_ls[0]].min()\n",
        "  current_max = ts_df[ts_ls[0]].max()\n",
        "  ts_spans_ls.append(float(current_max - current_min))\n",
        "\n",
        "  for ats in ts_ls[1:]:\n",
        "    current_min = ts_df[ats].min()\n",
        "    current_max = ts_df[ats].max()\n",
        "    ts_spans_ls.append(float(current_max - current_min))\n",
        "\n",
        "  # find index of maximum span\n",
        "  max_index = ts_spans_ls.index(max(ts_spans_ls))\n",
        "  max_span_value = ts_spans_ls[max_index]\n",
        "  max_span_model = ts_ls[max_index]\n",
        "  print(f'max span is: {max_span_value} from {max_span_model}')\n",
        "\n",
        "  for i, ats in enumerate(ts_ls):\n",
        "    y_scaling_factor = max_span_value/ts_spans_ls[i]\n",
        "    print(f'ats={ats} with scaling={y_scaling_factor}')\n",
        "    ts_y_scaled_ser = ts_df[ats].apply(lambda x: x*y_scaling_factor)\n",
        "    # plt.plot()\n",
        "    plot_df = pd.DataFrame()\n",
        "    plot_df['x_value'] = ts_df.index\n",
        "    plot_df['y_scaled'] = ts_y_scaled_ser\n",
        "    plot_df['y_scaled_roll050'] = plot_df['y_scaled'].rolling(350, center=True).mean()\n",
        "    plot_df['y_scaled_roll050'].plot(label=f'{ats}')\n",
        "    plt.legend(loc='best')\n",
        "    # sns.lineplot(data=plot_df, x='x_value', y='y_scaled', alpha=0.5, label=f'{ats}')\n",
        "\n",
        "  return ts_spans_ls, ts_ls\n",
        "\n",
        "# Test\n",
        "ts_spans_ls, ts_ls = plot_autoscaled_ts()\n",
        "zip_ls = zip(ts_spans_ls, ts_ls)\n",
        "for aspan, amodel in zip_ls:\n",
        "  print(f'model: {amodel} with span: {aspan}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emkaDWvDILDc"
      },
      "source": [
        "ts_df=corpus_sents_df, ts_ls=['vader_lnorm_medianiqr', 'textblob_lnorm_medianiqr', \n",
        "                                                     'syuzhet_lnorm_medianiqr', 'sentimentr_lnorm_medianiqr',\n",
        "                                                     'bing_lnorm_medianiqr', 'afinn_lnorm_medianiqr',\n",
        "                                                     'pattern_lnorm_medianiqr', 'stanza_lnorm_medianiqr']):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T8pf6DsIGmq"
      },
      "source": [
        "# Test\n",
        "ts_spans_ls, ts_ls = plot_autoscaled_ts(ts_df=corpus_parags_df)\n",
        "zip_ls = zip(ts_spans_ls, ts_ls)\n",
        "for aspan, amodel in zip_ls:\n",
        "  print(f'model: {amodel} with span: {aspan}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWGdzLS4Ibr-"
      },
      "source": [
        "corpus_sents_df['pattern_lnorm_medianiqr'].hist(bins=100) # rolling(100, center=True).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42RrUksBOxfo"
      },
      "source": [
        "models_sma_ls = [('vader_lnorm_medianiqr',1),\n",
        "                 ('textblob_lnorm_medianiqr',20),\n",
        "                 ('sentimentr_lnorm_medianiqr',1),\n",
        "                 ('syuzhet_lnorm_medianiqr',1),\n",
        "                 ('stanza_lnorm_medianiqr',0.2)]\n",
        "\n",
        "win_per = 5  # 5=5% of full corpus length\n",
        "win_roll = int(corpus_sents_df.shape[0]* win_per/100)\n",
        "\n",
        "for amodel, amag in models_sma_ls:\n",
        "  corpus_parags_df[amodel].rolling(win_roll, center=True).mean().apply(lambda x: amag*x).plot(linewidth=4, label=amodel)\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNCGf1KZEpld"
      },
      "source": [
        "# **Save Raw Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REL23tcN9NS-"
      },
      "source": [
        "## **Sentence and Paragraph DataFrames (incl raw/clean text)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNkAJzWhEpld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "289d8c49-2aa0-4489-90ce-e736f86b918e"
      },
      "source": [
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "\n",
        "# Save the original Raw and Cleaned Corpus text at both Sentence and Paragraph Levels\n",
        "\n",
        "corpus_text_sentences_raw_filename = f'corpus_text_sentences_raw_{author_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text raw sentences to file: {corpus_text_sentences_raw_filename}')\n",
        "corpus_sents_df['sent_raw'].to_csv(corpus_text_sentences_raw_filename)\n",
        "\n",
        "corpus_text_sentences_clean_filename = f'corpus_text_sentences_clean_{author_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text clean sentences to file: {corpus_text_sentences_clean_filename}')\n",
        "corpus_sents_df['sent_clean'].to_csv(corpus_text_sentences_clean_filename)\n",
        "\n",
        "\n",
        "corpus_text_paragraphs_raw_filename = f'corpus_text_paragraphs_raw_{author_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text raw paragraphs to file: {corpus_text_paragraphs_raw_filename}')\n",
        "corpus_parags_df['parag_raw'].to_csv(corpus_text_paragraphs_raw_filename)\n",
        "\n",
        "corpus_text_paragraphs_clean_filename = f'corpus_text_sentences_clean_{author_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus text clean sentences to file: {corpus_text_paragraphs_clean_filename}')\n",
        "corpus_parags_df['parag_clean'].to_csv(corpus_text_paragraphs_clean_filename)\n",
        "\n",
        "\n",
        "\n",
        "# Save the Sentiment Values at both the Sentence and Paragraph Levels\n",
        "\n",
        "corpus_sents_filename = f'corpus_sentences_lexrules_{author_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sentences to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "corpus_parags_filename = f'corpus_paragraphs_lexrules_{author_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Paragraphs to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Corpus text raw sentences to file: corpus_text_sentences_raw_ianmcewan_machineslikeme.csv\n",
            "Saving Corpus text clean sentences to file: corpus_text_sentences_clean_ianmcewan_machineslikeme.csv\n",
            "Saving Corpus text raw paragraphs to file: corpus_text_paragraphs_raw_ianmcewan_machineslikeme.csv\n",
            "Saving Corpus text clean sentences to file: corpus_text_sentences_clean_ianmcewan_machineslikeme.csv\n",
            "Saving Corpus Sentences to file: corpus_sentences_lexrules_ianmcewan_machineslikeme.csv\n",
            "Saving Corpus Paragraphs to file: corpus_paragraphs_lexrules_ianmcewan_machineslikeme.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysa0Yp9yXWAe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "07dd09b7-0456-4828-b936-9839da93904f"
      },
      "source": [
        "# Verify exported Sentence Sentiments file\n",
        "\n",
        "!head -n 5 $corpus_sents_filename"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            ",sent_no,parag_no,sent_raw,char_len,token_len,sent_clean,sentimentr,sentimentr_lnorm_meanstd,sentimentr_medianiqr,sentimentr_lnorm_medianiqr,syuzhet,syuzhet_lnorm_meanstd,syuzhet_medianiqr,syuzhet_lnorm_medianiqr,bing,bing_lnorm_meanstd,bing_medianiqr,bing_lnorm_medianiqr,sentiword,sentiword_lnorm_meanstd,sentiword_medianiqr,sentiword_lnorm_medianiqr,senticnet,senticnet_lnorm_meanstd,senticnet_medianiqr,senticnet_lnorm_medianiqr,nrc,nrc_lnorm_meanstd,nrc_medianiqr,nrc_lnorm_medianiqr,afinn,afinn_lnorm_meanstd,afinn_medianiqr,afinn_lnorm_medianiqr,scores,vader,vader_lnorm_meanstd,vader_medianiqr,vader_lnorm_medianiqr,textblob,textblob_lnorm_meanstd,textblob_medianiqr,textblob_lnorm_medianiqr,pattern,pattern_lnorm_meanstd,pattern_medianiqr,pattern_lnorm_medianiqr,stanza,stanza_lnorm_meanstd,stanza_medianiqr,stanza_lnorm_medianiqr\n",
            "0,0,0,\"But remember, please, the Law by which we live, We are not built to comprehend a lie ...\",88,18,but remember please the law by which we live we are not built to comprehend a lie ,1.3,0.5502366922209025,1.7333333333333334,1.2505557576492463,1.3,0.5702091752474734,1.7333333333333334,1.3333333333333335,-26.087773109487912,-0.19325208879604308,-26.087773109487912,-1.4493207283048841,0.16666666666666696,0.07260881161101837,0.29629629629629683,0.17777777777777812,1.979,0.6432336626884361,1.4987080103359174,0.7773844153554587,0.0,-0.06361253816756895,0.0,0.0,0.34092303651433087,-0.016936937442353944,1.4089621024860528,4.289846646559382,\"{'neg': 0.0, 'neu': 0.836, 'pos': 0.164, 'compound': 0.4497}\",0.4497,0.373827751631761,1.9871851524524966,1.6180915371329878,0.13636363636363635,0.02123658164189527,1.0,0.8719690387805069,0.04796109889140285,-0.02440320109239325,0.46903800040770177,0.40379330567105476,5.113845547714964,-0.2560576805167806,0.29397078855938874,-0.731982631857708\n",
            "1,1,1,\"--Rudyard Kipling, \"\"The Secret of the Machines\"\"\",47,7,  rudyard kipling the secret of the machines,0.0,-0.095230889689469,0.0,0.0,0.0,-0.09005396687724992,0.0,0.0,0.0,-0.014697590979302346,0.0,0.0,0.25,0.4881281801541171,0.4444444444444444,0.6857142857142857,0.598,0.4505366371904925,0.30921619293712316,0.556148353827846,0.0,-0.06361253816756895,0.0,0.0,0.0,-0.03031367389577884,0.0,0.0,\"{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\",0.0,-0.07491102743835892,0.0,0.0,-0.4,-1.2742624762195274,-2.9333333333333336,-6.577137892515824,-0.20450837181513495,-0.03545688598408564,-2.0,-4.427469169882173,3.421460127773553,-0.12061473756734395,-0.4020199918128499,0.4020251515278962\n",
            "2,2,2,CHAPTER 1,9,2,chapter ,0.0,-0.095230889689469,0.0,0.0,0.0,-0.09005396687724992,0.0,0.0,0.0,-0.014697590979302346,0.0,0.0,0.0,-0.07282296737906656,0.0,0.0,0.105,0.19171502540966778,-0.11541774332472002,0.25899444664815546,0.0,-0.06361253816756895,0.0,0.0,0.0,-0.03031367389577884,0.0,0.0,\"{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\",0.0,-0.07491102743835892,0.0,0.0,0.0,-0.1304104652649602,0.0,0.0,0.0,-0.02532705981148542,0.0,0.0,10.0,2.8646325268666226,2.3033939437073934,25.39626670800747\n",
            "3,3,3,\"It was religious yearning granted hope, it was the holy grail of science.\",73,13,it was religious yearning granted hope it was the holy grail of science,1.25,0.7641194412562918,1.6666666666666667,1.6649410974620142,1.25,0.7889945951231212,1.6666666666666667,1.7751479289940828,28.050889359956056,0.2511358261817865,28.050889359956056,2.15776071999662,0.25,0.22922765052341698,0.4444444444444444,0.3692307692307693,3.472,1.8783094530546782,2.784668389319552,2.1953788179293934,23.875941833529858,1.6427461245367818,3.125609681566532,5.302952463382378,1.500966608915281,0.05123070074043227,6.203174448640517,26.150837560185938,\"{'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compound': 0.6597}\",0.6597,0.8365673095634109,2.915156871409633,3.2866679952172175,0.0,-0.1304104652649602,0.0,0.0,0.0,-0.02532705981148542,0.0,0.0,5.253383131203484,-0.17664652144873272,0.3513553952093289,-0.06710581723875071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NH8ltVX9SvD"
      },
      "source": [
        "## **Section, Chapter Crux DataFrames (summary stats/sentiments only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ovjd7uvr2hS8",
        "outputId": "11d4635d-3321-40eb-c921-2a525e412f83"
      },
      "source": [
        "corpus_chaps_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 10 entries, 0 to 9\n",
            "Data columns (total 63 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   chap_no                     10 non-null     int64  \n",
            " 1   chap_raw                    10 non-null     object \n",
            " 2   sent_no_start               10 non-null     int64  \n",
            " 3   sent_no_mid                 10 non-null     int64  \n",
            " 4   char_len                    10 non-null     int64  \n",
            " 5   token_len                   10 non-null     int64  \n",
            " 6   chap_clean                  10 non-null     object \n",
            " 7   sentimentr                  10 non-null     float64\n",
            " 8   sentimentr_meanstd          10 non-null     float64\n",
            " 9   sentimentr_medianiqr        10 non-null     float64\n",
            " 10  sentimentr_lnorm_meanstd    10 non-null     float64\n",
            " 11  sentimentr_lnorm_medianiqr  10 non-null     float64\n",
            " 12  syuzhet                     10 non-null     float64\n",
            " 13  syuzhet_meanstd             10 non-null     float64\n",
            " 14  syuzhet_medianiqr           10 non-null     float64\n",
            " 15  syuzhet_lnorm_meanstd       10 non-null     float64\n",
            " 16  syuzhet_lnorm_medianiqr     10 non-null     float64\n",
            " 17  bing                        10 non-null     float64\n",
            " 18  bing_meanstd                10 non-null     float64\n",
            " 19  bing_medianiqr              10 non-null     float64\n",
            " 20  bing_lnorm_meanstd          10 non-null     float64\n",
            " 21  bing_lnorm_medianiqr        10 non-null     float64\n",
            " 22  sentiword                   10 non-null     float64\n",
            " 23  sentiword_meanstd           10 non-null     float64\n",
            " 24  sentiword_medianiqr         10 non-null     float64\n",
            " 25  sentiword_lnorm_meanstd     10 non-null     float64\n",
            " 26  sentiword_lnorm_medianiqr   10 non-null     float64\n",
            " 27  senticnet                   10 non-null     float64\n",
            " 28  senticnet_meanstd           10 non-null     float64\n",
            " 29  senticnet_medianiqr         10 non-null     float64\n",
            " 30  senticnet_lnorm_meanstd     10 non-null     float64\n",
            " 31  senticnet_lnorm_medianiqr   10 non-null     float64\n",
            " 32  nrc                         10 non-null     float64\n",
            " 33  nrc_meanstd                 10 non-null     float64\n",
            " 34  nrc_medianiqr               10 non-null     float64\n",
            " 35  nrc_lnorm_meanstd           10 non-null     float64\n",
            " 36  nrc_lnorm_medianiqr         10 non-null     float64\n",
            " 37  afinn                       10 non-null     float64\n",
            " 38  afinn_meanstd               10 non-null     float64\n",
            " 39  afinn_medianiqr             10 non-null     float64\n",
            " 40  afinn_lnorm_meanstd         10 non-null     float64\n",
            " 41  afinn_lnorm_medianiqr       10 non-null     float64\n",
            " 42  scores                      10 non-null     object \n",
            " 43  vader                       10 non-null     float64\n",
            " 44  vader_meanstd               10 non-null     float64\n",
            " 45  vader_medianiqr             10 non-null     float64\n",
            " 46  vader_lnorm_meanstd         10 non-null     float64\n",
            " 47  vader_lnorm_medianiqr       10 non-null     float64\n",
            " 48  textblob                    10 non-null     float64\n",
            " 49  textblob_meanstd            10 non-null     float64\n",
            " 50  textblob_medianiqr          10 non-null     float64\n",
            " 51  textblob_lnorm_meanstd      10 non-null     float64\n",
            " 52  textblob_lnorm_medianiqr    10 non-null     float64\n",
            " 53  pattern                     10 non-null     float64\n",
            " 54  pattern_meanstd             10 non-null     float64\n",
            " 55  pattern_medianiqr           10 non-null     float64\n",
            " 56  pattern_lnorm_meanstd       10 non-null     float64\n",
            " 57  pattern_lnorm_medianiqr     10 non-null     float64\n",
            " 58  stanza                      10 non-null     float64\n",
            " 59  stanza_meanstd              10 non-null     float64\n",
            " 60  stanza_medianiqr            10 non-null     float64\n",
            " 61  stanza_lnorm_meanstd        10 non-null     float64\n",
            " 62  stanza_lnorm_medianiqr      10 non-null     float64\n",
            "dtypes: float64(55), int64(5), object(3)\n",
            "memory usage: 5.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BLb7mQgV7_JK",
        "outputId": "0fc37e69-ef08-4b55-abab-f32a3d33c05e"
      },
      "source": [
        "# Create a Chapter Summary DataFrame extracting only key information (no text)\n",
        "\n",
        "corpus_chaps_summary_df = corpus_chaps_df[['chap_no','sent_no_start','sent_no_mid','char_len','token_len',\n",
        "                 'sentimentr','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'syuzhet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'bing','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'sentiword','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'senticnet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'nrc','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'afinn','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'vader','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'textblob','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'pattern','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'stanza','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 ]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xSvXSwp-2ePp",
        "outputId": "c0eb372b-3c35-40c2-816b-60aedba7694b"
      },
      "source": [
        "corpus_sects_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 30 entries, 0 to 29\n",
            "Data columns (total 63 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   sect_no                     30 non-null     int64  \n",
            " 1   sect_raw                    30 non-null     object \n",
            " 2   sent_no_start               30 non-null     int64  \n",
            " 3   sent_no_mid                 30 non-null     int64  \n",
            " 4   char_len                    30 non-null     int64  \n",
            " 5   token_len                   30 non-null     int64  \n",
            " 6   sect_clean                  30 non-null     object \n",
            " 7   sentimentr                  30 non-null     float64\n",
            " 8   sentimentr_meanstd          30 non-null     float64\n",
            " 9   sentimentr_medianiqr        30 non-null     float64\n",
            " 10  sentimentr_lnorm_meanstd    30 non-null     float64\n",
            " 11  sentimentr_lnorm_medianiqr  30 non-null     float64\n",
            " 12  syuzhet                     30 non-null     float64\n",
            " 13  syuzhet_meanstd             30 non-null     float64\n",
            " 14  syuzhet_medianiqr           30 non-null     float64\n",
            " 15  syuzhet_lnorm_meanstd       30 non-null     float64\n",
            " 16  syuzhet_lnorm_medianiqr     30 non-null     float64\n",
            " 17  bing                        30 non-null     float64\n",
            " 18  bing_meanstd                30 non-null     float64\n",
            " 19  bing_medianiqr              30 non-null     float64\n",
            " 20  bing_lnorm_meanstd          30 non-null     float64\n",
            " 21  bing_lnorm_medianiqr        30 non-null     float64\n",
            " 22  sentiword                   30 non-null     float64\n",
            " 23  sentiword_meanstd           30 non-null     float64\n",
            " 24  sentiword_medianiqr         30 non-null     float64\n",
            " 25  sentiword_lnorm_meanstd     30 non-null     float64\n",
            " 26  sentiword_lnorm_medianiqr   30 non-null     float64\n",
            " 27  senticnet                   30 non-null     float64\n",
            " 28  senticnet_meanstd           30 non-null     float64\n",
            " 29  senticnet_medianiqr         30 non-null     float64\n",
            " 30  senticnet_lnorm_meanstd     30 non-null     float64\n",
            " 31  senticnet_lnorm_medianiqr   30 non-null     float64\n",
            " 32  nrc                         30 non-null     float64\n",
            " 33  nrc_meanstd                 30 non-null     float64\n",
            " 34  nrc_medianiqr               30 non-null     float64\n",
            " 35  nrc_lnorm_meanstd           30 non-null     float64\n",
            " 36  nrc_lnorm_medianiqr         30 non-null     float64\n",
            " 37  afinn                       30 non-null     float64\n",
            " 38  afinn_meanstd               30 non-null     float64\n",
            " 39  afinn_medianiqr             30 non-null     float64\n",
            " 40  afinn_lnorm_meanstd         30 non-null     float64\n",
            " 41  afinn_lnorm_medianiqr       30 non-null     float64\n",
            " 42  scores                      30 non-null     object \n",
            " 43  vader                       30 non-null     float64\n",
            " 44  vader_meanstd               30 non-null     float64\n",
            " 45  vader_medianiqr             30 non-null     float64\n",
            " 46  vader_lnorm_meanstd         30 non-null     float64\n",
            " 47  vader_lnorm_medianiqr       30 non-null     float64\n",
            " 48  textblob                    30 non-null     float64\n",
            " 49  textblob_meanstd            30 non-null     float64\n",
            " 50  textblob_medianiqr          30 non-null     float64\n",
            " 51  textblob_lnorm_meanstd      30 non-null     float64\n",
            " 52  textblob_lnorm_medianiqr    30 non-null     float64\n",
            " 53  pattern                     30 non-null     float64\n",
            " 54  pattern_meanstd             30 non-null     float64\n",
            " 55  pattern_medianiqr           30 non-null     float64\n",
            " 56  pattern_lnorm_meanstd       30 non-null     float64\n",
            " 57  pattern_lnorm_medianiqr     30 non-null     float64\n",
            " 58  stanza                      30 non-null     float64\n",
            " 59  stanza_meanstd              30 non-null     float64\n",
            " 60  stanza_medianiqr            30 non-null     float64\n",
            " 61  stanza_lnorm_meanstd        30 non-null     float64\n",
            " 62  stanza_lnorm_medianiqr      30 non-null     float64\n",
            "dtypes: float64(55), int64(5), object(3)\n",
            "memory usage: 15.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N5s8R38P81M7",
        "outputId": "5e8e7f22-ac8a-4a46-84e8-6bc7989d5181"
      },
      "source": [
        "# Create a Section Summary DataFrame extracting only key information (no text)\n",
        "\n",
        "corpus_sects_summary_df = corpus_sects_df[['sect_no','sent_no_start','sent_no_mid','char_len','token_len',\n",
        "                 'sentimentr','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'syuzhet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'bing','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'sentiword','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'senticnet','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'nrc','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'afinn','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'vader','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'textblob','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'pattern','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 'stanza','sentimentr_medianiqr','sentimentr_lnorm_medianiqr',\n",
        "                 ]]\n",
        "\n",
        "corpus_sects_summary_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sect_no</th>\n",
              "      <th>sent_no_start</th>\n",
              "      <th>sent_no_mid</th>\n",
              "      <th>char_len</th>\n",
              "      <th>token_len</th>\n",
              "      <th>sentimentr</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>syuzhet</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>bing</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>sentiword</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>senticnet</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>nrc</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>afinn</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>vader</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>textblob</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>pattern</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "      <th>stanza</th>\n",
              "      <th>sentimentr_medianiqr</th>\n",
              "      <th>sentimentr_lnorm_medianiqr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>117</td>\n",
              "      <td>18434</td>\n",
              "      <td>3230</td>\n",
              "      <td>48.70</td>\n",
              "      <td>1.071705</td>\n",
              "      <td>0.652573</td>\n",
              "      <td>53.40</td>\n",
              "      <td>1.071705</td>\n",
              "      <td>0.652573</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.071705</td>\n",
              "      <td>0.652573</td>\n",
              "      <td>2.667242</td>\n",
              "      <td>1.071705</td>\n",
              "      <td>0.652573</td>\n",
              "      <td>141.684</td>\n",
              "      <td>1.071705</td>\n",
              "      <td>0.652573</td>\n",
              "      <td>436.710452</td>\n",
              "      <td>1.071705</td>\n",
              "      <td>0.652573</td>\n",
              "      <td>8.791435</td>\n",
              "      <td>1.071705</td>\n",
              "      <td>0.652573</td>\n",
              "      <td>0.9998</td>\n",
              "      <td>1.071705</td>\n",
              "      <td>0.652573</td>\n",
              "      <td>0.054853</td>\n",
              "      <td>1.071705</td>\n",
              "      <td>0.652573</td>\n",
              "      <td>2.815865</td>\n",
              "      <td>1.071705</td>\n",
              "      <td>0.652573</td>\n",
              "      <td>404.283911</td>\n",
              "      <td>1.071705</td>\n",
              "      <td>0.652573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>237</td>\n",
              "      <td>300</td>\n",
              "      <td>10014</td>\n",
              "      <td>1739</td>\n",
              "      <td>38.10</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>1.270280</td>\n",
              "      <td>32.85</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>1.270280</td>\n",
              "      <td>153.243256</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>1.270280</td>\n",
              "      <td>6.761631</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>1.270280</td>\n",
              "      <td>71.176</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>1.270280</td>\n",
              "      <td>285.988108</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>1.270280</td>\n",
              "      <td>2.637394</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>1.270280</td>\n",
              "      <td>0.9982</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>1.270280</td>\n",
              "      <td>0.123126</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>1.270280</td>\n",
              "      <td>2.762543</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>1.270280</td>\n",
              "      <td>236.046724</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>1.270280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>370</td>\n",
              "      <td>418</td>\n",
              "      <td>6530</td>\n",
              "      <td>1156</td>\n",
              "      <td>9.90</td>\n",
              "      <td>-0.261056</td>\n",
              "      <td>0.063651</td>\n",
              "      <td>8.90</td>\n",
              "      <td>-0.261056</td>\n",
              "      <td>0.063651</td>\n",
              "      <td>185.896102</td>\n",
              "      <td>-0.261056</td>\n",
              "      <td>0.063651</td>\n",
              "      <td>7.185714</td>\n",
              "      <td>-0.261056</td>\n",
              "      <td>0.063651</td>\n",
              "      <td>52.263</td>\n",
              "      <td>-0.261056</td>\n",
              "      <td>0.063651</td>\n",
              "      <td>64.444226</td>\n",
              "      <td>-0.261056</td>\n",
              "      <td>0.063651</td>\n",
              "      <td>2.650291</td>\n",
              "      <td>-0.261056</td>\n",
              "      <td>0.063651</td>\n",
              "      <td>0.9923</td>\n",
              "      <td>-0.261056</td>\n",
              "      <td>0.063651</td>\n",
              "      <td>0.101188</td>\n",
              "      <td>-0.261056</td>\n",
              "      <td>0.063651</td>\n",
              "      <td>1.615229</td>\n",
              "      <td>-0.261056</td>\n",
              "      <td>0.063651</td>\n",
              "      <td>164.457546</td>\n",
              "      <td>-0.261056</td>\n",
              "      <td>0.063651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>472</td>\n",
              "      <td>602</td>\n",
              "      <td>17520</td>\n",
              "      <td>3181</td>\n",
              "      <td>52.30</td>\n",
              "      <td>1.195363</td>\n",
              "      <td>0.775899</td>\n",
              "      <td>50.05</td>\n",
              "      <td>1.195363</td>\n",
              "      <td>0.775899</td>\n",
              "      <td>352.715515</td>\n",
              "      <td>1.195363</td>\n",
              "      <td>0.775899</td>\n",
              "      <td>6.860745</td>\n",
              "      <td>1.195363</td>\n",
              "      <td>0.775899</td>\n",
              "      <td>79.014</td>\n",
              "      <td>1.195363</td>\n",
              "      <td>0.775899</td>\n",
              "      <td>276.663938</td>\n",
              "      <td>1.195363</td>\n",
              "      <td>0.775899</td>\n",
              "      <td>13.092741</td>\n",
              "      <td>1.195363</td>\n",
              "      <td>0.775899</td>\n",
              "      <td>0.9997</td>\n",
              "      <td>1.195363</td>\n",
              "      <td>0.775899</td>\n",
              "      <td>0.083555</td>\n",
              "      <td>1.195363</td>\n",
              "      <td>0.775899</td>\n",
              "      <td>3.371887</td>\n",
              "      <td>1.195363</td>\n",
              "      <td>0.775899</td>\n",
              "      <td>399.389787</td>\n",
              "      <td>1.195363</td>\n",
              "      <td>0.775899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>737</td>\n",
              "      <td>822</td>\n",
              "      <td>16185</td>\n",
              "      <td>2809</td>\n",
              "      <td>64.60</td>\n",
              "      <td>1.617862</td>\n",
              "      <td>1.368686</td>\n",
              "      <td>63.60</td>\n",
              "      <td>1.617862</td>\n",
              "      <td>1.368686</td>\n",
              "      <td>580.630883</td>\n",
              "      <td>1.617862</td>\n",
              "      <td>1.368686</td>\n",
              "      <td>12.280833</td>\n",
              "      <td>1.617862</td>\n",
              "      <td>1.368686</td>\n",
              "      <td>128.686</td>\n",
              "      <td>1.617862</td>\n",
              "      <td>1.368686</td>\n",
              "      <td>400.985181</td>\n",
              "      <td>1.617862</td>\n",
              "      <td>1.368686</td>\n",
              "      <td>14.151337</td>\n",
              "      <td>1.617862</td>\n",
              "      <td>1.368686</td>\n",
              "      <td>0.9997</td>\n",
              "      <td>1.617862</td>\n",
              "      <td>1.368686</td>\n",
              "      <td>0.121595</td>\n",
              "      <td>1.617862</td>\n",
              "      <td>1.368686</td>\n",
              "      <td>4.507794</td>\n",
              "      <td>1.617862</td>\n",
              "      <td>1.368686</td>\n",
              "      <td>361.231504</td>\n",
              "      <td>1.617862</td>\n",
              "      <td>1.368686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>911</td>\n",
              "      <td>1051</td>\n",
              "      <td>18778</td>\n",
              "      <td>3362</td>\n",
              "      <td>-15.20</td>\n",
              "      <td>-1.123229</td>\n",
              "      <td>-1.119469</td>\n",
              "      <td>-17.05</td>\n",
              "      <td>-1.123229</td>\n",
              "      <td>-1.119469</td>\n",
              "      <td>-492.687024</td>\n",
              "      <td>-1.123229</td>\n",
              "      <td>-1.119469</td>\n",
              "      <td>-14.015729</td>\n",
              "      <td>-1.123229</td>\n",
              "      <td>-1.119469</td>\n",
              "      <td>65.540</td>\n",
              "      <td>-1.123229</td>\n",
              "      <td>-1.119469</td>\n",
              "      <td>-126.836458</td>\n",
              "      <td>-1.123229</td>\n",
              "      <td>-1.119469</td>\n",
              "      <td>-7.165771</td>\n",
              "      <td>-1.123229</td>\n",
              "      <td>-1.119469</td>\n",
              "      <td>-0.9327</td>\n",
              "      <td>-1.123229</td>\n",
              "      <td>-1.119469</td>\n",
              "      <td>0.051037</td>\n",
              "      <td>-1.123229</td>\n",
              "      <td>-1.119469</td>\n",
              "      <td>2.011444</td>\n",
              "      <td>-1.123229</td>\n",
              "      <td>-1.119469</td>\n",
              "      <td>411.728185</td>\n",
              "      <td>-1.123229</td>\n",
              "      <td>-1.119469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>1196</td>\n",
              "      <td>1301</td>\n",
              "      <td>15142</td>\n",
              "      <td>2659</td>\n",
              "      <td>10.00</td>\n",
              "      <td>-0.257621</td>\n",
              "      <td>-0.370640</td>\n",
              "      <td>3.00</td>\n",
              "      <td>-0.257621</td>\n",
              "      <td>-0.370640</td>\n",
              "      <td>-112.412493</td>\n",
              "      <td>-0.257621</td>\n",
              "      <td>-0.370640</td>\n",
              "      <td>0.161490</td>\n",
              "      <td>-0.257621</td>\n",
              "      <td>-0.370640</td>\n",
              "      <td>79.253</td>\n",
              "      <td>-0.257621</td>\n",
              "      <td>-0.370640</td>\n",
              "      <td>162.049120</td>\n",
              "      <td>-0.257621</td>\n",
              "      <td>-0.370640</td>\n",
              "      <td>-1.125590</td>\n",
              "      <td>-0.257621</td>\n",
              "      <td>-0.370640</td>\n",
              "      <td>0.9972</td>\n",
              "      <td>-0.257621</td>\n",
              "      <td>-0.370640</td>\n",
              "      <td>0.088099</td>\n",
              "      <td>-0.257621</td>\n",
              "      <td>-0.370640</td>\n",
              "      <td>3.378399</td>\n",
              "      <td>-0.257621</td>\n",
              "      <td>-0.370640</td>\n",
              "      <td>334.675354</td>\n",
              "      <td>-0.257621</td>\n",
              "      <td>-0.370640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>1408</td>\n",
              "      <td>1506</td>\n",
              "      <td>15847</td>\n",
              "      <td>2791</td>\n",
              "      <td>43.50</td>\n",
              "      <td>0.893087</td>\n",
              "      <td>0.698542</td>\n",
              "      <td>46.45</td>\n",
              "      <td>0.893087</td>\n",
              "      <td>0.698542</td>\n",
              "      <td>312.955065</td>\n",
              "      <td>0.893087</td>\n",
              "      <td>0.698542</td>\n",
              "      <td>7.294118</td>\n",
              "      <td>0.893087</td>\n",
              "      <td>0.698542</td>\n",
              "      <td>88.815</td>\n",
              "      <td>0.893087</td>\n",
              "      <td>0.698542</td>\n",
              "      <td>373.322042</td>\n",
              "      <td>0.893087</td>\n",
              "      <td>0.698542</td>\n",
              "      <td>6.959140</td>\n",
              "      <td>0.893087</td>\n",
              "      <td>0.698542</td>\n",
              "      <td>0.9996</td>\n",
              "      <td>0.893087</td>\n",
              "      <td>0.698542</td>\n",
              "      <td>0.119275</td>\n",
              "      <td>0.893087</td>\n",
              "      <td>0.698542</td>\n",
              "      <td>3.771970</td>\n",
              "      <td>0.893087</td>\n",
              "      <td>0.698542</td>\n",
              "      <td>356.034572</td>\n",
              "      <td>0.893087</td>\n",
              "      <td>0.698542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>1603</td>\n",
              "      <td>1699</td>\n",
              "      <td>15673</td>\n",
              "      <td>2799</td>\n",
              "      <td>13.75</td>\n",
              "      <td>-0.128811</td>\n",
              "      <td>-0.266511</td>\n",
              "      <td>18.35</td>\n",
              "      <td>-0.128811</td>\n",
              "      <td>-0.266511</td>\n",
              "      <td>78.229492</td>\n",
              "      <td>-0.128811</td>\n",
              "      <td>-0.266511</td>\n",
              "      <td>14.053226</td>\n",
              "      <td>-0.128811</td>\n",
              "      <td>-0.266511</td>\n",
              "      <td>98.259</td>\n",
              "      <td>-0.128811</td>\n",
              "      <td>-0.266511</td>\n",
              "      <td>150.434321</td>\n",
              "      <td>-0.128811</td>\n",
              "      <td>-0.266511</td>\n",
              "      <td>5.218667</td>\n",
              "      <td>-0.128811</td>\n",
              "      <td>-0.266511</td>\n",
              "      <td>0.9879</td>\n",
              "      <td>-0.128811</td>\n",
              "      <td>-0.266511</td>\n",
              "      <td>0.094227</td>\n",
              "      <td>-0.128811</td>\n",
              "      <td>-0.266511</td>\n",
              "      <td>3.257644</td>\n",
              "      <td>-0.128811</td>\n",
              "      <td>-0.266511</td>\n",
              "      <td>354.123841</td>\n",
              "      <td>-0.128811</td>\n",
              "      <td>-0.266511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>1794</td>\n",
              "      <td>1847</td>\n",
              "      <td>9149</td>\n",
              "      <td>1648</td>\n",
              "      <td>15.35</td>\n",
              "      <td>-0.073851</td>\n",
              "      <td>0.131492</td>\n",
              "      <td>13.85</td>\n",
              "      <td>-0.073851</td>\n",
              "      <td>0.131492</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.073851</td>\n",
              "      <td>0.131492</td>\n",
              "      <td>-8.061710</td>\n",
              "      <td>-0.073851</td>\n",
              "      <td>0.131492</td>\n",
              "      <td>42.552</td>\n",
              "      <td>-0.073851</td>\n",
              "      <td>0.131492</td>\n",
              "      <td>74.603920</td>\n",
              "      <td>-0.073851</td>\n",
              "      <td>0.131492</td>\n",
              "      <td>3.454985</td>\n",
              "      <td>-0.073851</td>\n",
              "      <td>0.131492</td>\n",
              "      <td>0.9989</td>\n",
              "      <td>-0.073851</td>\n",
              "      <td>0.131492</td>\n",
              "      <td>0.103047</td>\n",
              "      <td>-0.073851</td>\n",
              "      <td>0.131492</td>\n",
              "      <td>2.652499</td>\n",
              "      <td>-0.073851</td>\n",
              "      <td>0.131492</td>\n",
              "      <td>227.231718</td>\n",
              "      <td>-0.073851</td>\n",
              "      <td>0.131492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>1900</td>\n",
              "      <td>1960</td>\n",
              "      <td>9997</td>\n",
              "      <td>1724</td>\n",
              "      <td>21.00</td>\n",
              "      <td>0.120223</td>\n",
              "      <td>0.390687</td>\n",
              "      <td>22.00</td>\n",
              "      <td>0.120223</td>\n",
              "      <td>0.390687</td>\n",
              "      <td>200.716578</td>\n",
              "      <td>0.120223</td>\n",
              "      <td>0.390687</td>\n",
              "      <td>3.608710</td>\n",
              "      <td>0.120223</td>\n",
              "      <td>0.390687</td>\n",
              "      <td>53.809</td>\n",
              "      <td>0.120223</td>\n",
              "      <td>0.390687</td>\n",
              "      <td>143.105294</td>\n",
              "      <td>0.120223</td>\n",
              "      <td>0.390687</td>\n",
              "      <td>2.642129</td>\n",
              "      <td>0.120223</td>\n",
              "      <td>0.390687</td>\n",
              "      <td>0.9986</td>\n",
              "      <td>0.120223</td>\n",
              "      <td>0.390687</td>\n",
              "      <td>0.113616</td>\n",
              "      <td>0.120223</td>\n",
              "      <td>0.390687</td>\n",
              "      <td>3.415996</td>\n",
              "      <td>0.120223</td>\n",
              "      <td>0.390687</td>\n",
              "      <td>235.413661</td>\n",
              "      <td>0.120223</td>\n",
              "      <td>0.390687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>2020</td>\n",
              "      <td>2203</td>\n",
              "      <td>17365</td>\n",
              "      <td>3153</td>\n",
              "      <td>0.55</td>\n",
              "      <td>-0.582224</td>\n",
              "      <td>-0.694910</td>\n",
              "      <td>0.75</td>\n",
              "      <td>-0.582224</td>\n",
              "      <td>-0.694910</td>\n",
              "      <td>-341.834493</td>\n",
              "      <td>-0.582224</td>\n",
              "      <td>-0.694910</td>\n",
              "      <td>5.259664</td>\n",
              "      <td>-0.582224</td>\n",
              "      <td>-0.694910</td>\n",
              "      <td>94.692</td>\n",
              "      <td>-0.582224</td>\n",
              "      <td>-0.694910</td>\n",
              "      <td>-38.740993</td>\n",
              "      <td>-0.582224</td>\n",
              "      <td>-0.694910</td>\n",
              "      <td>-4.651894</td>\n",
              "      <td>-0.582224</td>\n",
              "      <td>-0.694910</td>\n",
              "      <td>0.9987</td>\n",
              "      <td>-0.582224</td>\n",
              "      <td>-0.694910</td>\n",
              "      <td>0.097417</td>\n",
              "      <td>-0.582224</td>\n",
              "      <td>-0.694910</td>\n",
              "      <td>3.157608</td>\n",
              "      <td>-0.582224</td>\n",
              "      <td>-0.694910</td>\n",
              "      <td>391.003914</td>\n",
              "      <td>-0.582224</td>\n",
              "      <td>-0.694910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>2382</td>\n",
              "      <td>2495</td>\n",
              "      <td>14968</td>\n",
              "      <td>2704</td>\n",
              "      <td>19.35</td>\n",
              "      <td>0.063547</td>\n",
              "      <td>-0.063651</td>\n",
              "      <td>15.75</td>\n",
              "      <td>0.063547</td>\n",
              "      <td>-0.063651</td>\n",
              "      <td>-22.435982</td>\n",
              "      <td>0.063547</td>\n",
              "      <td>-0.063651</td>\n",
              "      <td>-2.427987</td>\n",
              "      <td>0.063547</td>\n",
              "      <td>-0.063651</td>\n",
              "      <td>56.491</td>\n",
              "      <td>0.063547</td>\n",
              "      <td>-0.063651</td>\n",
              "      <td>44.657181</td>\n",
              "      <td>0.063547</td>\n",
              "      <td>-0.063651</td>\n",
              "      <td>2.495554</td>\n",
              "      <td>0.063547</td>\n",
              "      <td>-0.063651</td>\n",
              "      <td>0.9948</td>\n",
              "      <td>0.063547</td>\n",
              "      <td>-0.063651</td>\n",
              "      <td>0.113821</td>\n",
              "      <td>0.063547</td>\n",
              "      <td>-0.063651</td>\n",
              "      <td>3.204694</td>\n",
              "      <td>0.063547</td>\n",
              "      <td>-0.063651</td>\n",
              "      <td>342.639576</td>\n",
              "      <td>0.063547</td>\n",
              "      <td>-0.063651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>2605</td>\n",
              "      <td>2726</td>\n",
              "      <td>14141</td>\n",
              "      <td>2472</td>\n",
              "      <td>23.25</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.139722</td>\n",
              "      <td>20.30</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.139722</td>\n",
              "      <td>45.320418</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.139722</td>\n",
              "      <td>-0.202633</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.139722</td>\n",
              "      <td>84.116</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.139722</td>\n",
              "      <td>162.683319</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.139722</td>\n",
              "      <td>3.911157</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.139722</td>\n",
              "      <td>0.9988</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.139722</td>\n",
              "      <td>0.132375</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.139722</td>\n",
              "      <td>3.304640</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.139722</td>\n",
              "      <td>316.677525</td>\n",
              "      <td>0.197510</td>\n",
              "      <td>0.139722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>2847</td>\n",
              "      <td>2968</td>\n",
              "      <td>18754</td>\n",
              "      <td>3437</td>\n",
              "      <td>16.05</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.288455</td>\n",
              "      <td>11.65</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.288455</td>\n",
              "      <td>-262.166143</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.288455</td>\n",
              "      <td>2.856193</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.288455</td>\n",
              "      <td>99.856</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.288455</td>\n",
              "      <td>55.082761</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.288455</td>\n",
              "      <td>-5.815010</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.288455</td>\n",
              "      <td>0.9769</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.288455</td>\n",
              "      <td>0.098508</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.288455</td>\n",
              "      <td>3.936685</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.288455</td>\n",
              "      <td>422.678530</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.288455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>3090</td>\n",
              "      <td>3329</td>\n",
              "      <td>30220</td>\n",
              "      <td>5459</td>\n",
              "      <td>96.15</td>\n",
              "      <td>2.701589</td>\n",
              "      <td>0.881845</td>\n",
              "      <td>91.20</td>\n",
              "      <td>2.701589</td>\n",
              "      <td>0.881845</td>\n",
              "      <td>509.349647</td>\n",
              "      <td>2.701589</td>\n",
              "      <td>0.881845</td>\n",
              "      <td>32.671090</td>\n",
              "      <td>2.701589</td>\n",
              "      <td>0.881845</td>\n",
              "      <td>187.328</td>\n",
              "      <td>2.701589</td>\n",
              "      <td>0.881845</td>\n",
              "      <td>682.058243</td>\n",
              "      <td>2.701589</td>\n",
              "      <td>0.881845</td>\n",
              "      <td>16.284197</td>\n",
              "      <td>2.701589</td>\n",
              "      <td>0.881845</td>\n",
              "      <td>0.9999</td>\n",
              "      <td>2.701589</td>\n",
              "      <td>0.881845</td>\n",
              "      <td>0.125300</td>\n",
              "      <td>2.701589</td>\n",
              "      <td>0.881845</td>\n",
              "      <td>7.524768</td>\n",
              "      <td>2.701589</td>\n",
              "      <td>0.881845</td>\n",
              "      <td>642.308358</td>\n",
              "      <td>2.701589</td>\n",
              "      <td>0.881845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>3564</td>\n",
              "      <td>3738</td>\n",
              "      <td>20609</td>\n",
              "      <td>3918</td>\n",
              "      <td>-9.30</td>\n",
              "      <td>-0.920567</td>\n",
              "      <td>-0.925302</td>\n",
              "      <td>-6.05</td>\n",
              "      <td>-0.920567</td>\n",
              "      <td>-0.925302</td>\n",
              "      <td>-344.707182</td>\n",
              "      <td>-0.920567</td>\n",
              "      <td>-0.925302</td>\n",
              "      <td>-4.308424</td>\n",
              "      <td>-0.920567</td>\n",
              "      <td>-0.925302</td>\n",
              "      <td>118.900</td>\n",
              "      <td>-0.920567</td>\n",
              "      <td>-0.925302</td>\n",
              "      <td>65.633876</td>\n",
              "      <td>-0.920567</td>\n",
              "      <td>-0.925302</td>\n",
              "      <td>-9.423563</td>\n",
              "      <td>-0.920567</td>\n",
              "      <td>-0.925302</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.920567</td>\n",
              "      <td>-0.925302</td>\n",
              "      <td>0.057939</td>\n",
              "      <td>-0.920567</td>\n",
              "      <td>-0.925302</td>\n",
              "      <td>2.917354</td>\n",
              "      <td>-0.920567</td>\n",
              "      <td>-0.925302</td>\n",
              "      <td>473.086697</td>\n",
              "      <td>-0.920567</td>\n",
              "      <td>-0.925302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>3903</td>\n",
              "      <td>3977</td>\n",
              "      <td>8410</td>\n",
              "      <td>1529</td>\n",
              "      <td>9.15</td>\n",
              "      <td>-0.286818</td>\n",
              "      <td>-0.169599</td>\n",
              "      <td>10.40</td>\n",
              "      <td>-0.286818</td>\n",
              "      <td>-0.169599</td>\n",
              "      <td>-107.860995</td>\n",
              "      <td>-0.286818</td>\n",
              "      <td>-0.169599</td>\n",
              "      <td>6.030193</td>\n",
              "      <td>-0.286818</td>\n",
              "      <td>-0.169599</td>\n",
              "      <td>49.995</td>\n",
              "      <td>-0.286818</td>\n",
              "      <td>-0.169599</td>\n",
              "      <td>74.953560</td>\n",
              "      <td>-0.286818</td>\n",
              "      <td>-0.169599</td>\n",
              "      <td>3.224042</td>\n",
              "      <td>-0.286818</td>\n",
              "      <td>-0.169599</td>\n",
              "      <td>0.9784</td>\n",
              "      <td>-0.286818</td>\n",
              "      <td>-0.169599</td>\n",
              "      <td>0.111385</td>\n",
              "      <td>-0.286818</td>\n",
              "      <td>-0.169599</td>\n",
              "      <td>2.581030</td>\n",
              "      <td>-0.286818</td>\n",
              "      <td>-0.169599</td>\n",
              "      <td>209.697034</td>\n",
              "      <td>-0.286818</td>\n",
              "      <td>-0.169599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>4039</td>\n",
              "      <td>4227</td>\n",
              "      <td>25726</td>\n",
              "      <td>4468</td>\n",
              "      <td>88.25</td>\n",
              "      <td>2.430228</td>\n",
              "      <td>1.075198</td>\n",
              "      <td>84.35</td>\n",
              "      <td>2.430228</td>\n",
              "      <td>1.075198</td>\n",
              "      <td>721.883627</td>\n",
              "      <td>2.430228</td>\n",
              "      <td>1.075198</td>\n",
              "      <td>32.980474</td>\n",
              "      <td>2.430228</td>\n",
              "      <td>1.075198</td>\n",
              "      <td>185.952</td>\n",
              "      <td>2.430228</td>\n",
              "      <td>1.075198</td>\n",
              "      <td>548.322975</td>\n",
              "      <td>2.430228</td>\n",
              "      <td>1.075198</td>\n",
              "      <td>13.732715</td>\n",
              "      <td>2.430228</td>\n",
              "      <td>1.075198</td>\n",
              "      <td>0.9999</td>\n",
              "      <td>2.430228</td>\n",
              "      <td>1.075198</td>\n",
              "      <td>0.140953</td>\n",
              "      <td>2.430228</td>\n",
              "      <td>1.075198</td>\n",
              "      <td>7.981247</td>\n",
              "      <td>2.430228</td>\n",
              "      <td>1.075198</td>\n",
              "      <td>540.388218</td>\n",
              "      <td>2.430228</td>\n",
              "      <td>1.075198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>4392</td>\n",
              "      <td>4532</td>\n",
              "      <td>17848</td>\n",
              "      <td>3189</td>\n",
              "      <td>48.25</td>\n",
              "      <td>1.056247</td>\n",
              "      <td>0.657341</td>\n",
              "      <td>44.05</td>\n",
              "      <td>1.056247</td>\n",
              "      <td>0.657341</td>\n",
              "      <td>429.753943</td>\n",
              "      <td>1.056247</td>\n",
              "      <td>0.657341</td>\n",
              "      <td>15.255822</td>\n",
              "      <td>1.056247</td>\n",
              "      <td>0.657341</td>\n",
              "      <td>125.771</td>\n",
              "      <td>1.056247</td>\n",
              "      <td>0.657341</td>\n",
              "      <td>271.093264</td>\n",
              "      <td>1.056247</td>\n",
              "      <td>0.657341</td>\n",
              "      <td>4.159042</td>\n",
              "      <td>1.056247</td>\n",
              "      <td>0.657341</td>\n",
              "      <td>0.9966</td>\n",
              "      <td>1.056247</td>\n",
              "      <td>0.657341</td>\n",
              "      <td>0.138328</td>\n",
              "      <td>1.056247</td>\n",
              "      <td>0.657341</td>\n",
              "      <td>4.922616</td>\n",
              "      <td>1.056247</td>\n",
              "      <td>0.657341</td>\n",
              "      <td>396.943821</td>\n",
              "      <td>1.056247</td>\n",
              "      <td>0.657341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>4637</td>\n",
              "      <td>4803</td>\n",
              "      <td>17989</td>\n",
              "      <td>3246</td>\n",
              "      <td>31.70</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.172319</td>\n",
              "      <td>37.55</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.172319</td>\n",
              "      <td>-87.922264</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.172319</td>\n",
              "      <td>17.862915</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.172319</td>\n",
              "      <td>104.523</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.172319</td>\n",
              "      <td>375.712471</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.172319</td>\n",
              "      <td>5.732444</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.172319</td>\n",
              "      <td>0.9990</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.172319</td>\n",
              "      <td>0.133457</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.172319</td>\n",
              "      <td>5.219200</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.172319</td>\n",
              "      <td>404.320226</td>\n",
              "      <td>0.487763</td>\n",
              "      <td>0.172319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>4928</td>\n",
              "      <td>5082</td>\n",
              "      <td>17966</td>\n",
              "      <td>3248</td>\n",
              "      <td>18.95</td>\n",
              "      <td>0.049807</td>\n",
              "      <td>-0.183156</td>\n",
              "      <td>19.55</td>\n",
              "      <td>0.049807</td>\n",
              "      <td>-0.183156</td>\n",
              "      <td>-98.935700</td>\n",
              "      <td>0.049807</td>\n",
              "      <td>-0.183156</td>\n",
              "      <td>-0.788812</td>\n",
              "      <td>0.049807</td>\n",
              "      <td>-0.183156</td>\n",
              "      <td>109.305</td>\n",
              "      <td>0.049807</td>\n",
              "      <td>-0.183156</td>\n",
              "      <td>320.498342</td>\n",
              "      <td>0.049807</td>\n",
              "      <td>-0.183156</td>\n",
              "      <td>3.415960</td>\n",
              "      <td>0.049807</td>\n",
              "      <td>-0.183156</td>\n",
              "      <td>0.9987</td>\n",
              "      <td>0.049807</td>\n",
              "      <td>-0.183156</td>\n",
              "      <td>0.112357</td>\n",
              "      <td>0.049807</td>\n",
              "      <td>-0.183156</td>\n",
              "      <td>4.507869</td>\n",
              "      <td>0.049807</td>\n",
              "      <td>-0.183156</td>\n",
              "      <td>400.033307</td>\n",
              "      <td>0.049807</td>\n",
              "      <td>-0.183156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>5189</td>\n",
              "      <td>5386</td>\n",
              "      <td>20877</td>\n",
              "      <td>3779</td>\n",
              "      <td>39.35</td>\n",
              "      <td>0.750537</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>43.65</td>\n",
              "      <td>0.750537</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>216.213931</td>\n",
              "      <td>0.750537</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>39.656550</td>\n",
              "      <td>0.750537</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>129.115</td>\n",
              "      <td>0.750537</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>339.724208</td>\n",
              "      <td>0.750537</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>10.538799</td>\n",
              "      <td>0.750537</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>0.9995</td>\n",
              "      <td>0.750537</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>0.138638</td>\n",
              "      <td>0.750537</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>5.451137</td>\n",
              "      <td>0.750537</td>\n",
              "      <td>0.230814</td>\n",
              "      <td>462.509556</td>\n",
              "      <td>0.750537</td>\n",
              "      <td>0.230814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>5527</td>\n",
              "      <td>5678</td>\n",
              "      <td>11840</td>\n",
              "      <td>2215</td>\n",
              "      <td>19.85</td>\n",
              "      <td>0.080721</td>\n",
              "      <td>0.099602</td>\n",
              "      <td>16.85</td>\n",
              "      <td>0.080721</td>\n",
              "      <td>0.099602</td>\n",
              "      <td>68.920261</td>\n",
              "      <td>0.080721</td>\n",
              "      <td>0.099602</td>\n",
              "      <td>2.554550</td>\n",
              "      <td>0.080721</td>\n",
              "      <td>0.099602</td>\n",
              "      <td>54.973</td>\n",
              "      <td>0.080721</td>\n",
              "      <td>0.099602</td>\n",
              "      <td>180.732627</td>\n",
              "      <td>0.080721</td>\n",
              "      <td>0.099602</td>\n",
              "      <td>1.281113</td>\n",
              "      <td>0.080721</td>\n",
              "      <td>0.099602</td>\n",
              "      <td>0.9136</td>\n",
              "      <td>0.080721</td>\n",
              "      <td>0.099602</td>\n",
              "      <td>0.081025</td>\n",
              "      <td>0.080721</td>\n",
              "      <td>0.099602</td>\n",
              "      <td>2.400540</td>\n",
              "      <td>0.080721</td>\n",
              "      <td>0.099602</td>\n",
              "      <td>284.022747</td>\n",
              "      <td>0.080721</td>\n",
              "      <td>0.099602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>5762</td>\n",
              "      <td>5978</td>\n",
              "      <td>18957</td>\n",
              "      <td>3548</td>\n",
              "      <td>-17.15</td>\n",
              "      <td>-1.190210</td>\n",
              "      <td>-1.147732</td>\n",
              "      <td>-15.10</td>\n",
              "      <td>-1.190210</td>\n",
              "      <td>-1.147732</td>\n",
              "      <td>-500.949666</td>\n",
              "      <td>-1.190210</td>\n",
              "      <td>-1.147732</td>\n",
              "      <td>0.330995</td>\n",
              "      <td>-1.190210</td>\n",
              "      <td>-1.147732</td>\n",
              "      <td>93.875</td>\n",
              "      <td>-1.190210</td>\n",
              "      <td>-1.147732</td>\n",
              "      <td>-5.499776</td>\n",
              "      <td>-1.190210</td>\n",
              "      <td>-1.147732</td>\n",
              "      <td>-15.574896</td>\n",
              "      <td>-1.190210</td>\n",
              "      <td>-1.147732</td>\n",
              "      <td>-0.9998</td>\n",
              "      <td>-1.190210</td>\n",
              "      <td>-1.147732</td>\n",
              "      <td>0.037546</td>\n",
              "      <td>-1.190210</td>\n",
              "      <td>-1.147732</td>\n",
              "      <td>1.545042</td>\n",
              "      <td>-1.190210</td>\n",
              "      <td>-1.147732</td>\n",
              "      <td>429.818838</td>\n",
              "      <td>-1.190210</td>\n",
              "      <td>-1.147732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>6117</td>\n",
              "      <td>6281</td>\n",
              "      <td>14181</td>\n",
              "      <td>2569</td>\n",
              "      <td>3.95</td>\n",
              "      <td>-0.465436</td>\n",
              "      <td>-0.571660</td>\n",
              "      <td>-0.80</td>\n",
              "      <td>-0.465436</td>\n",
              "      <td>-0.571660</td>\n",
              "      <td>-236.943242</td>\n",
              "      <td>-0.465436</td>\n",
              "      <td>-0.571660</td>\n",
              "      <td>-1.752528</td>\n",
              "      <td>-0.465436</td>\n",
              "      <td>-0.571660</td>\n",
              "      <td>71.548</td>\n",
              "      <td>-0.465436</td>\n",
              "      <td>-0.571660</td>\n",
              "      <td>123.160417</td>\n",
              "      <td>-0.465436</td>\n",
              "      <td>-0.571660</td>\n",
              "      <td>0.251165</td>\n",
              "      <td>-0.465436</td>\n",
              "      <td>-0.571660</td>\n",
              "      <td>0.9981</td>\n",
              "      <td>-0.465436</td>\n",
              "      <td>-0.571660</td>\n",
              "      <td>0.117689</td>\n",
              "      <td>-0.465436</td>\n",
              "      <td>-0.571660</td>\n",
              "      <td>3.148206</td>\n",
              "      <td>-0.465436</td>\n",
              "      <td>-0.571660</td>\n",
              "      <td>325.887033</td>\n",
              "      <td>-0.465436</td>\n",
              "      <td>-0.571660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>6367</td>\n",
              "      <td>6556</td>\n",
              "      <td>21804</td>\n",
              "      <td>3894</td>\n",
              "      <td>11.15</td>\n",
              "      <td>-0.218119</td>\n",
              "      <td>-0.451784</td>\n",
              "      <td>13.75</td>\n",
              "      <td>-0.218119</td>\n",
              "      <td>-0.451784</td>\n",
              "      <td>-204.877222</td>\n",
              "      <td>-0.218119</td>\n",
              "      <td>-0.451784</td>\n",
              "      <td>25.857234</td>\n",
              "      <td>-0.218119</td>\n",
              "      <td>-0.451784</td>\n",
              "      <td>107.965</td>\n",
              "      <td>-0.218119</td>\n",
              "      <td>-0.451784</td>\n",
              "      <td>169.641347</td>\n",
              "      <td>-0.218119</td>\n",
              "      <td>-0.451784</td>\n",
              "      <td>-0.716517</td>\n",
              "      <td>-0.218119</td>\n",
              "      <td>-0.451784</td>\n",
              "      <td>0.9987</td>\n",
              "      <td>-0.218119</td>\n",
              "      <td>-0.451784</td>\n",
              "      <td>0.069982</td>\n",
              "      <td>-0.218119</td>\n",
              "      <td>-0.451784</td>\n",
              "      <td>3.415675</td>\n",
              "      <td>-0.218119</td>\n",
              "      <td>-0.451784</td>\n",
              "      <td>468.721250</td>\n",
              "      <td>-0.218119</td>\n",
              "      <td>-0.451784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>6663</td>\n",
              "      <td>6892</td>\n",
              "      <td>18669</td>\n",
              "      <td>3458</td>\n",
              "      <td>10.85</td>\n",
              "      <td>-0.228424</td>\n",
              "      <td>-0.426985</td>\n",
              "      <td>4.85</td>\n",
              "      <td>-0.228424</td>\n",
              "      <td>-0.426985</td>\n",
              "      <td>-229.583503</td>\n",
              "      <td>-0.228424</td>\n",
              "      <td>-0.426985</td>\n",
              "      <td>1.266145</td>\n",
              "      <td>-0.228424</td>\n",
              "      <td>-0.426985</td>\n",
              "      <td>98.957</td>\n",
              "      <td>-0.228424</td>\n",
              "      <td>-0.426985</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.228424</td>\n",
              "      <td>-0.426985</td>\n",
              "      <td>-3.516431</td>\n",
              "      <td>-0.228424</td>\n",
              "      <td>-0.426985</td>\n",
              "      <td>0.9537</td>\n",
              "      <td>-0.228424</td>\n",
              "      <td>-0.426985</td>\n",
              "      <td>0.050377</td>\n",
              "      <td>-0.228424</td>\n",
              "      <td>-0.426985</td>\n",
              "      <td>2.488082</td>\n",
              "      <td>-0.228424</td>\n",
              "      <td>-0.426985</td>\n",
              "      <td>422.941743</td>\n",
              "      <td>-0.228424</td>\n",
              "      <td>-0.426985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>7015</td>\n",
              "      <td>7157</td>\n",
              "      <td>12965</td>\n",
              "      <td>2309</td>\n",
              "      <td>16.05</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.082187</td>\n",
              "      <td>14.15</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.082187</td>\n",
              "      <td>-57.088486</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.082187</td>\n",
              "      <td>18.622564</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.082187</td>\n",
              "      <td>94.165</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.082187</td>\n",
              "      <td>242.139839</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.082187</td>\n",
              "      <td>-5.853658</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.082187</td>\n",
              "      <td>0.7841</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.082187</td>\n",
              "      <td>0.116369</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.082187</td>\n",
              "      <td>3.059165</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.082187</td>\n",
              "      <td>298.791087</td>\n",
              "      <td>-0.049807</td>\n",
              "      <td>-0.082187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>7171</td>\n",
              "      <td>7491</td>\n",
              "      <td>34397</td>\n",
              "      <td>6246</td>\n",
              "      <td>-12.95</td>\n",
              "      <td>-1.045942</td>\n",
              "      <td>-0.898146</td>\n",
              "      <td>-15.15</td>\n",
              "      <td>-1.045942</td>\n",
              "      <td>-0.898146</td>\n",
              "      <td>-635.801284</td>\n",
              "      <td>-1.045942</td>\n",
              "      <td>-0.898146</td>\n",
              "      <td>6.586879</td>\n",
              "      <td>-1.045942</td>\n",
              "      <td>-0.898146</td>\n",
              "      <td>154.689</td>\n",
              "      <td>-1.045942</td>\n",
              "      <td>-0.898146</td>\n",
              "      <td>-90.659537</td>\n",
              "      <td>-1.045942</td>\n",
              "      <td>-0.898146</td>\n",
              "      <td>-9.263993</td>\n",
              "      <td>-1.045942</td>\n",
              "      <td>-0.898146</td>\n",
              "      <td>0.9605</td>\n",
              "      <td>-1.045942</td>\n",
              "      <td>-0.898146</td>\n",
              "      <td>0.078501</td>\n",
              "      <td>-1.045942</td>\n",
              "      <td>-0.898146</td>\n",
              "      <td>5.027702</td>\n",
              "      <td>-1.045942</td>\n",
              "      <td>-0.898146</td>\n",
              "      <td>715.699967</td>\n",
              "      <td>-1.045942</td>\n",
              "      <td>-0.898146</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sect_no  sent_no_start  ...  sentimentr_medianiqr  sentimentr_lnorm_medianiqr\n",
              "0         0              3  ...              1.071705                    0.652573\n",
              "1         1            237  ...              0.707600                    1.270280\n",
              "2         2            370  ...             -0.261056                    0.063651\n",
              "3         3            472  ...              1.195363                    0.775899\n",
              "4         4            737  ...              1.617862                    1.368686\n",
              "5         5            911  ...             -1.123229                   -1.119469\n",
              "6         6           1196  ...             -0.257621                   -0.370640\n",
              "7         7           1408  ...              0.893087                    0.698542\n",
              "8         8           1603  ...             -0.128811                   -0.266511\n",
              "9         9           1794  ...             -0.073851                    0.131492\n",
              "10       10           1900  ...              0.120223                    0.390687\n",
              "11       11           2020  ...             -0.582224                   -0.694910\n",
              "12       12           2382  ...              0.063547                   -0.063651\n",
              "13       13           2605  ...              0.197510                    0.139722\n",
              "14       14           2847  ...             -0.049807                   -0.288455\n",
              "15       15           3090  ...              2.701589                    0.881845\n",
              "16       16           3564  ...             -0.920567                   -0.925302\n",
              "17       17           3903  ...             -0.286818                   -0.169599\n",
              "18       18           4039  ...              2.430228                    1.075198\n",
              "19       19           4392  ...              1.056247                    0.657341\n",
              "20       20           4637  ...              0.487763                    0.172319\n",
              "21       21           4928  ...              0.049807                   -0.183156\n",
              "22       22           5189  ...              0.750537                    0.230814\n",
              "23       23           5527  ...              0.080721                    0.099602\n",
              "24       24           5762  ...             -1.190210                   -1.147732\n",
              "25       25           6117  ...             -0.465436                   -0.571660\n",
              "26       26           6367  ...             -0.218119                   -0.451784\n",
              "27       27           6663  ...             -0.228424                   -0.426985\n",
              "28       28           7015  ...             -0.049807                   -0.082187\n",
              "29       29           7171  ...             -1.045942                   -0.898146\n",
              "\n",
              "[30 rows x 38 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "x2Op-IgH9iP2",
        "outputId": "b26563b8-8090-47d0-ba3c-54052aaac319"
      },
      "source": [
        "# Save the original Corpus text at 4 levels of semantic grouping: sentences, paragraphs, sections and chapters\n",
        "\n",
        "# Save Section and Chapter DataFrames Metainformation (e.g. sent_no_start) and Sentiment Values\n",
        "corpus_sects_summary_filename = f'corpus_section_summary_lexrules_{author_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Section Summary to file: {corpus_sects_summary_filename}')\n",
        "corpus_sects_summary_df.to_csv(corpus_sects_summary_filename)\n",
        "\n",
        "corpus_chaps_summary_filename = f'corpus_chapter_summary_lexrules_{author_str}_{title_str}.csv' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Chapters Summary to file: {corpus_chaps_summary_filename}')\n",
        "corpus_chaps_summary_df.to_csv(corpus_chaps_summary_filename)\n",
        "\n",
        "# Save Corpus Cruxes Dictionary is saved to a JSON file\n",
        "corpus_cruxes_summary_filename = f'corpus_cruxes_summary_lexrules_{author_str}_{title_str}.json' # _{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes Summary to file: {corpus_cruxes_summary_filename}')\n",
        "with open(corpus_cruxes_summary_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Corpus Section Summary to file: corpus_section_summary_lexrules_ianmcewan_machineslikeme.csv\n",
            "Saving Corpus Chapters Summary to file: corpus_chapter_summary_lexrules_ianmcewan_machineslikeme.csv\n",
            "Saving Corpus Cruxes Summary to file: corpus_cruxes_summary_lexrules_ianmcewan_machineslikeme.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSO0vT7oXOXO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "fb31d43f-6929-45ff-a3d4-e9c4e7469f50"
      },
      "source": [
        "# Verify exported Section Summary file\n",
        "\n",
        "!head -n 5 $corpus_sects_summary_filename\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            ",sect_no,sent_no_start,sent_no_mid,char_len,token_len,sentimentr,sentimentr_medianiqr,sentimentr_lnorm_medianiqr,syuzhet,sentimentr_medianiqr,sentimentr_lnorm_medianiqr,bing,sentimentr_medianiqr,sentimentr_lnorm_medianiqr,sentiword,sentimentr_medianiqr,sentimentr_lnorm_medianiqr,senticnet,sentimentr_medianiqr,sentimentr_lnorm_medianiqr,nrc,sentimentr_medianiqr,sentimentr_lnorm_medianiqr,afinn,sentimentr_medianiqr,sentimentr_lnorm_medianiqr,vader,sentimentr_medianiqr,sentimentr_lnorm_medianiqr,textblob,sentimentr_medianiqr,sentimentr_lnorm_medianiqr,pattern,sentimentr_medianiqr,sentimentr_lnorm_medianiqr,stanza,sentimentr_medianiqr,sentimentr_lnorm_medianiqr\n",
            "0,0,3,117,18434,3230,48.69999999999996,1.0717045942464558,0.6525729670629598,53.399999999999956,1.0717045942464558,0.6525729670629598,0.0,1.0717045942464558,0.6525729670629598,2.6672416666666656,1.0717045942464558,0.6525729670629598,141.68399999999946,1.0717045942464558,0.6525729670629598,436.71045158298784,1.0717045942464558,0.6525729670629598,8.791435097358383,1.0717045942464558,0.6525729670629598,0.9998,1.0717045942464558,0.6525729670629598,0.05485273929042925,1.0717045942464558,0.6525729670629598,2.815865248825103,1.0717045942464558,0.6525729670629598,404.28391121324455,1.0717045942464558,0.6525729670629598\n",
            "1,1,237,300,10014,1739,38.099999999999994,0.7075998282524683,1.2702796322258891,32.85000000000001,0.7075998282524683,1.2702796322258891,153.24325649945803,0.7075998282524683,1.2702796322258891,6.761631349206352,0.7075998282524683,1.2702796322258891,71.17600000000002,0.7075998282524683,1.2702796322258891,285.9881084430227,0.7075998282524683,1.2702796322258891,2.637393561732882,0.7075998282524683,1.2702796322258891,0.9982,0.7075998282524683,1.2702796322258891,0.12312647268343473,0.7075998282524683,1.2702796322258891,2.76254317969484,0.7075998282524683,1.2702796322258891,236.04672377509294,0.7075998282524683,1.2702796322258891\n",
            "2,2,370,418,6530,1156,9.899999999999997,-0.2610562473164453,0.06365134240947672,8.899999999999999,-0.2610562473164453,0.06365134240947672,185.89610175863479,-0.2610562473164453,0.06365134240947672,7.185714285714289,-0.2610562473164453,0.06365134240947672,52.262999999999984,-0.2610562473164453,0.06365134240947672,64.44422580656752,-0.2610562473164453,0.06365134240947672,2.6502912418391427,-0.2610562473164453,0.06365134240947672,0.9923,-0.2610562473164453,0.06365134240947672,0.1011880179800533,-0.2610562473164453,0.06365134240947672,1.6152292968569704,-0.2610562473164453,0.06365134240947672,164.45754600675522,-0.2610562473164453,0.06365134240947672\n",
            "3,3,472,602,17520,3181,52.29999999999998,1.1953628166595092,0.7758993330418654,50.05,1.1953628166595092,0.7758993330418654,352.71551494459464,1.1953628166595092,0.7758993330418654,6.86074523809524,1.1953628166595092,0.7758993330418654,79.01400000000005,1.1953628166595092,0.7758993330418654,276.6639384547143,1.1953628166595092,0.7758993330418654,13.092741171562222,1.1953628166595092,0.7758993330418654,0.9997,1.1953628166595092,0.7758993330418654,0.08355453780950475,1.1953628166595092,0.7758993330418654,3.371887091248851,1.1953628166595092,0.7758993330418654,399.3897867661597,1.1953628166595092,0.7758993330418654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C3RIJOdsI_Ud",
        "outputId": "e269784b-6450-435f-eff9-4906be11edbf"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Variable                                Type                          Data/Info\n",
            "-------------------------------------------------------------------------------\n",
            "Afinn                                   type                          <class 'afinn.afinn.Afinn'>\n",
            "CORPUS_AUTHOR                           str                           Ian McEwan\n",
            "CORPUS_FILENAME                         str                           mlm_final_handclean_sections.txt\n",
            "CORPUS_FULL                             str                           Machines Like Me by: Ian McEwan\n",
            "CORPUS_SUBDIR                           str                           ./research/2021/sa_book_c<...>sa/imcewan_machineslikeme\n",
            "CORPUS_TITLE                            str                           Machines Like Me\n",
            "CubicSpline                             type                          <class 'scipy.interpolate._cubic.CubicSpline'>\n",
            "FILE_OUTPUT                             str                           Major\n",
            "HTML                                    type                          <class 'IPython.core.display.HTML'>\n",
            "InteractiveShell                        MetaHasTraits                 <class 'IPython.core.inte<...>eshell.InteractiveShell'>\n",
            "MIN_CHAP_LEN                            int                           5000\n",
            "MIN_PARAG_LEN                           int                           2\n",
            "MIN_SECT_LEN                            int                           5000\n",
            "MIN_SENT_LEN                            int                           2\n",
            "MODELS_LS                               list                          n=11\n",
            "MinMaxScaler                            type                          <class 'sklearn.preproces<...>sing._data.MinMaxScaler'>\n",
            "OrderedDict                             type                          <class 'collections.OrderedDict'>\n",
            "PLOT_OUTPUT                             str                           Major\n",
            "RobustScaler                            type                          <class 'sklearn.preproces<...>sing._data.RobustScaler'>\n",
            "SentimentIntensityAnalyzer              type                          <class 'nltk.sentiment.va<...>timentIntensityAnalyzer'>\n",
            "StandardScaler                          type                          <class 'sklearn.preproces<...>ng._data.StandardScaler'>\n",
            "TextBlob                                type                          <class 'textblob.blob.TextBlob'>\n",
            "achap                                   str                           \\n\\nOur immediate duty wa<...> troubled home.\\n\\n\\n\\n\\n\n",
            "afinn                                   Afinn                         <afinn.afinn.Afinn object at 0x7fcbf90b2a10>\n",
            "afinn_discrete2continous_sentiment      function                      <function afinn_discrete2<...>timent at 0x7fcbf9533f80>\n",
            "aparag                                  str                           I stood by Adam's side, a<...>towards my troubled home.\n",
            "argrelextrema                           function                      <function argrelextrema at 0x7fcbfb844950>\n",
            "asect                                   str                           \\n\\nMiranda served six mo<...> troubled home.\\n\\n\\n\\n\\n\n",
            "asent                                   str                           \"Yes.\"\n",
            "author_str                              str                           ianmcewan\n",
            "bing_discrete2continous_sentiment       function                      <function bing_discrete2c<...>timent at 0x7fcbf93d1e60>\n",
            "chap_first_sent                         str                           Our immediate duty was to<...>ng to marry his daughter.\n",
            "chap_mid_sentno                         int                           7413\n",
            "chap_mid_sentnos_ls                     list                          n=10\n",
            "chap_no_ls                              list                          n=10\n",
            "chap_raw_ls                             list                          n=10\n",
            "chap_sentno_base                        int                           7748\n",
            "chap_sents_len                          int                           669\n",
            "chap_sents_ls                           list                          n=669\n",
            "chap_start_sentnos_ls                   list                          n=10\n",
            "chap_text                               str                           \\n\\nOur immediate duty wa<...> troubled home.\\n\\n\\n\\n\\n\n",
            "clip_outliers                           function                      <function clip_outliers at 0x7fcbfac027a0>\n",
            "col_lnorm_meanstd                       str                           stanza_lnorm_meanstd\n",
            "col_lnorm_medianiqr                     str                           stanza_lnorm_medianiqr\n",
            "col_meanstd                             str                           stanza_meanstd\n",
            "col_medianiqr                           str                           stanza_medianiqr\n",
            "collections                             module                        <module 'collections' fro<...>collections/__init__.py'>\n",
            "contextlib                              module                        <module 'contextlib' from<...>python3.7/contextlib.py'>\n",
            "contractions                            module                        <module 'contractions' fr<...>ontractions/__init__.py'>\n",
            "convert_file                            TextIOWrapper                 <_io.TextIOWrapper name='<...>ode='w' encoding='UTF-8'>\n",
            "corpus2chaps                            function                      <function corpus2chaps at 0x7fcbfac54830>\n",
            "corpus2parags                           function                      <function corpus2parags at 0x7fcbfac31710>\n",
            "corpus2sects                            function                      <function corpus2sects at 0x7fcbfac8cb90>\n",
            "corpus_chaps_df                         DataFrame                        chap_no  ... stanza_ln<...>n\\n[10 rows x 63 columns]\n",
            "corpus_chaps_filename                   str                           corpus_chaps_clean_ianmce<...>slikeme_20210708_1050.csv\n",
            "corpus_chaps_preraw_ls                  list                          n=11\n",
            "corpus_chaps_raw_ls                     list                          n=10\n",
            "corpus_chaps_summary_df                 DataFrame                        chap_no  sent_no_start<...>n\\n[10 rows x 38 columns]\n",
            "corpus_chaps_summary_filename           str                           corpus_chapter_summary_le<...>mcewan_machineslikeme.csv\n",
            "corpus_cruxes_dt                        dict                          n=0\n",
            "corpus_cruxes_summary_filename          str                           corpus_cruxes_summary_lex<...>cewan_machineslikeme.json\n",
            "corpus_filename                         str                           \n",
            "corpus_lexicons_stats_dt                dict                          n=44\n",
            "corpus_parags_df                        DataFrame                           parag_no  ... stanz<...>n[1428 rows x 50 columns]\n",
            "corpus_parags_filename                  str                           corpus_paragraphs_lexrule<...>mcewan_machineslikeme.csv\n",
            "corpus_parags_len                       int                           1428\n",
            "corpus_parags_raw_ls                    list                          n=1428\n",
            "corpus_sects_df                         DataFrame                         sect_no  ... stanza_l<...>n\\n[30 rows x 63 columns]\n",
            "corpus_sects_filename                   str                           corpus_sects_clean_ianmce<...>slikeme_20210708_1050.csv\n",
            "corpus_sects_preraw_ls                  list                          n=31\n",
            "corpus_sects_raw_ls                     list                          n=30\n",
            "corpus_sects_summary_df                 DataFrame                         sect_no  sent_no_star<...>n\\n[30 rows x 38 columns]\n",
            "corpus_sects_summary_filename           str                           corpus_section_summary_le<...>mcewan_machineslikeme.csv\n",
            "corpus_sents_df                         DataFrame                           sent_no  parag_no  <...>n[7679 rows x 51 columns]\n",
            "corpus_sents_filename                   str                           corpus_sentences_lexrules<...>mcewan_machineslikeme.csv\n",
            "corpus_sents_len                        int                           7679\n",
            "corpus_sents_row_ls                     list                          n=7681\n",
            "corpus_text_paragraphs_clean_filename   str                           corpus_text_sentences_cle<...>mcewan_machineslikeme.csv\n",
            "corpus_text_paragraphs_raw_filename     str                           corpus_text_paragraphs_ra<...>mcewan_machineslikeme.csv\n",
            "corpus_text_sentences_clean_filename    str                           corpus_text_sentences_cle<...>mcewan_machineslikeme.csv\n",
            "corpus_text_sentences_raw_filename      str                           corpus_text_sentences_raw<...>mcewan_machineslikeme.csv\n",
            "crux_sortsents                          function                      <function crux_sortsents at 0x7fcbfaba4320>\n",
            "crux_sortsents_report                   function                      <function crux_sortsents_<...>report at 0x7fcbfaba4dd0>\n",
            "datetime                                type                          <class 'datetime.datetime'>\n",
            "datetime_now                            str                           20210708_1200\n",
            "display                                 function                      <function display at 0x7fcc4a3433b0>\n",
            "drive                                   module                        <module 'google.colab.dri<...>s/google/colab/drive.py'>\n",
            "expand_parags2sents                     function                      <function expand_parags2sents at 0x7fcbfabbf200>\n",
            "files                                   module                        <module 'google.colab.fil<...>s/google/colab/files.py'>\n",
            "gdrive_subdir                           str                           ./research/2021/sa_book_c<...>sa/imcewan_machineslikeme\n",
            "gen_pathfiletime                        function                      <function gen_pathfiletime at 0x7fcbfac31e60>\n",
            "get_crux_points                         function                      <function get_crux_points at 0x7fcbfb1160e0>\n",
            "get_lexicon                             function                      <function get_lexicon at 0x7fcbfabfc050>\n",
            "get_lexstats                            function                      <function get_lexstats at 0x7fcbfabfcd40>\n",
            "get_lowess                              function                      <function get_lowess at 0x7fcbfabf4d40>\n",
            "get_recentfile                          function                      <function get_recentfile at 0x7fcbfac8c830>\n",
            "get_section_timeseries                  function                      <function get_section_tim<...>series at 0x7fcbfabf3950>\n",
            "get_sent2dets                           function                      <function get_sent2dets at 0x7fcbfabf47a0>\n",
            "get_sentiments                          function                      <function get_sentiments at 0x7fcbfc142200>\n",
            "get_sentnocontext                       function                      <function get_sentnocontext at 0x7fcbfabf4a70>\n",
            "get_sentnocontext_report                function                      <function get_sentnoconte<...>report at 0x7fcbfabf3cb0>\n",
            "get_smas                                function                      <function get_smas at 0x7fcbfac31f80>\n",
            "glob                                    module                        <module 'glob' from '/usr/lib/python3.7/glob.py'>\n",
            "gmtime                                  builtin_function_or_method    <built-in function gmtime>\n",
            "i                                       int                           9\n",
            "id                                      StringArray                   <StringArray>\\n[    'aban<...>ngth: 5468, dtype: string\n",
            "interactive                             MetaHasTraits                 <class 'ipywidgets.widget<...>interaction.interactive'>\n",
            "interpolate                             module                        <module 'scipy.interpolat<...>interpolate/__init__.py'>\n",
            "io                                      module                        <module 'io' from '/usr/lib/python3.7/io.py'>\n",
            "json                                    module                        <module 'json' from '/usr<...>hon3.7/json/__init__.py'>\n",
            "lex_discrete2continous_sentiment        function                      <function lex_discrete2co<...>timent at 0x7fcbfac02710>\n",
            "lexicon_bing_df                         DataFrame                                 word  polarit<...>\\n[6874 rows x 2 columns]\n",
            "lexicon_bing_dt                         dict                          n=6874\n",
            "lexicon_nrc_df                          DataFrame                                  word  polari<...>\\n[5468 rows x 2 columns]\n",
            "lexicon_nrc_dt                          dict                          n=5468\n",
            "lexicon_senticnet_df                    DataFrame                                   word  polar<...>n[23626 rows x 2 columns]\n",
            "lexicon_senticnet_dt                    dict                          n=23626\n",
            "lexicon_sentimentr_df                   DataFrame                                   word  polar<...>n[11710 rows x 2 columns]\n",
            "lexicon_sentimentr_dt                   dict                          n=11710\n",
            "lexicon_sentiword_df                    DataFrame                                   word  polar<...>n[20093 rows x 2 columns]\n",
            "lexicon_sentiword_dt                    dict                          n=20093\n",
            "lexicon_syuzhet_df                      DataFrame                                   word  value<...>n[10748 rows x 2 columns]\n",
            "lexicon_syuzhet_dt                      dict                          n=10747\n",
            "logging                                 module                        <module 'logging' from '/<...>3.7/logging/__init__.py'>\n",
            "mean_std_scaler                         StandardScaler                StandardScaler(copy=True,<...>mean=True, with_std=True)\n",
            "median_iqr_scaler                       RobustScaler                  RobustScaler(copy=True, q<...>       with_scaling=True)\n",
            "model_base                              str                           stanza\n",
            "model_name                              str                           stanza_lnorm_medianiqr\n",
            "n_shortest                              int                           10\n",
            "nlp                                     Pipeline                      <stanza.pipeline.core.Pip<...>object at 0x7fcb980f4250>\n",
            "nltk                                    module                        <module 'nltk' from '/usr<...>ckages/nltk/__init__.py'>\n",
            "norm2negpos1                            function                      <function norm2negpos1 at 0x7fcbfabf5830>\n",
            "np                                      module                        <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n",
            "nrc_discrete2continous_sentiment        function                      <function nrc_discrete2co<...>timent at 0x7fcbf93d1cb0>\n",
            "os                                      module                        <module 'os' from '/usr/lib/python3.7/os.py'>\n",
            "parag2sents                             function                      <function parag2sents at 0x7fcbfac8c320>\n",
            "parag_no_ls                             list                          n=1428\n",
            "parag_raw_ls                            list                          n=1428\n",
            "pattern_discrete2continous_sentiment    function                      <function pattern_discret<...>timent at 0x7fcbf39ea0e0>\n",
            "pattern_sa                              Sentiment                     {'13th': {'JJ': [0.0, 0.0<...>, None: (0.4, 0.8, 1.0)}}\n",
            "pd                                      module                        <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n",
            "plot_crux_sections                      function                      <function plot_crux_sections at 0x7fcbfac3ab90>\n",
            "plot_histogram                          function                      <function plot_histogram at 0x7fcbfac3a5f0>\n",
            "plot_raw_sections                       function                      <function plot_raw_sections at 0x7fcbfabf5ef0>\n",
            "plot_raw_sentiments                     function                      <function plot_raw_sentiments at 0x7fcbfabf44d0>\n",
            "plot_smas                               function                      <function plot_smas at 0x7fcbfabfcc20>\n",
            "plt                                     module                        <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
            "re                                      module                        <module 're' from '/usr/lib/python3.7/re.py'>\n",
            "rename_cols                             function                      <function rename_cols at 0x7fcbfabf5200>\n",
            "robust                                  module                        <module 'statsmodels.robu<...>dels/robust/__init__.py'>\n",
            "sect_first_sent                         str                           Miranda served six months<...> prison north of Ipswich.\n",
            "sect_mid_sentno                         int                           7491\n",
            "sect_mid_sentnos_ls                     list                          n=30\n",
            "sect_no_ls                              list                          n=30\n",
            "sect_raw_ls                             list                          n=30\n",
            "sect_sentno_base                        int                           7748\n",
            "sect_sents_len                          int                           513\n",
            "sect_sents_ls                           list                          n=513\n",
            "sect_start_sentnos_ls                   list                          n=30\n",
            "sect_text                               str                           \\n\\nMiranda served six mo<...> troubled home.\\n\\n\\n\\n\\n\n",
            "sections_ls                             list                          n=31\n",
            "sent_test                               str                           I hate Mondays.\n",
            "sent_tokenize                           function                      <function sent_tokenize at 0x7fcbfce13b90>\n",
            "sentiment_bing                          function                      <function sentiment_bing at 0x7fcbfac445f0>\n",
            "sentiment_senticnet                     function                      <function sentiment_senticnet at 0x7fcbfac44050>\n",
            "sentiment_sentimentr                    function                      <function sentiment_sentimentr at 0x7fcbfabf4680>\n",
            "sentiment_sentiword                     function                      <function sentiment_sentiword at 0x7fcbf93d1950>\n",
            "sentiment_syuzhet                       function                      <function sentiment_syuzhet at 0x7fcc039e09e0>\n",
            "sentiment_textblob                      function                      <function sentiment_textblob at 0x7fcbfa826ef0>\n",
            "set_css                                 function                      <function set_css at 0x7fcbfac8c5f0>\n",
            "sid                                     SentimentIntensityAnalyzer    <nltk.sentiment.vader.Sen<...>object at 0x7fcbf8f141d0>\n",
            "signal                                  module                        <module 'scipy.signal' fr<...>cipy/signal/__init__.py'>\n",
            "sm_lowess                               function                      <function lowess at 0x7fcbfc142b90>\n",
            "sns                                     module                        <module 'seaborn' from '/<...>ges/seaborn/__init__.py'>\n",
            "standardize_ts                          function                      <function standardize_ts at 0x7fcbfabf5170>\n",
            "stanza                                  module                        <module 'stanza' from '/u<...>ages/stanza/__init__.py'>\n",
            "stanza_discrete2continous_sentiment     function                      <function stanza_discrete<...>timent at 0x7fcb978e77a0>\n",
            "strftime                                builtin_function_or_method    <built-in function strftime>\n",
            "string                                  module                        <module 'string' from '/u<...>lib/python3.7/string.py'>\n",
            "sys                                     module                        <module 'sys' (built-in)>\n",
            "target_string                           str                           My name is maximums and m<...>luck numbers are 12 45 78\n",
            "temp_ls                                 list                          n=1428\n",
            "text2sentiment                          function                      <function text2sentiment at 0x7fcbfabfc440>\n",
            "text_clean                              function                      <function text_clean at 0x7fcbfac8c440>\n",
            "textblob_sentiment                      function                      <function textblob_sentiment at 0x7fcbf877bdd0>\n",
            "time                                    module                        <module 'time' (built-in)>\n",
            "title_str                               str                           machineslikeme\n",
            "values                                  ndarray                       5468: 5468 elems, type `int64`, 43744 bytes\n",
            "warnings                                module                        <module 'warnings' from '<...>b/python3.7/warnings.py'>\n",
            "widgets                                 module                        <module 'ipywidgets.widge<...>ets/widgets/__init__.py'>\n",
            "win_p1per                               int                           15\n",
            "win_raw_p1per                           int                           14\n",
            "win_raw_s1per                           int                           76\n",
            "win_s1per                               int                           77\n",
            "word_list                               list                          n=12\n",
            "zoom                                    function                      <function zoom at 0x7fcc08a2f170>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQgdwsJGZN_"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpOCp1-88rrF"
      },
      "source": [
        "# **Standardize and Remove Outliers (Auto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mul5MSZrgKsw"
      },
      "source": [
        "## **Remove Outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07hyJuT1c5rJ"
      },
      "source": [
        "### **Before Removing Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKX8f0Q1I53M"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  print(f'Plotting {model_name}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_name, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzUoTpEWeyCy"
      },
      "source": [
        "# sns.lineplot(data=corpus_sents_df, x='sent_no', y='y_scaled', legend='brief', label='y_scaled')\n",
        "      \n",
        "# plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nSMA Smoothed Sentence Sentiment Plot (windows={win_ls})')\n",
        "# plt.legend(loc='best')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr1k-3Rsc5XL"
      },
      "source": [
        "# Plot all Raw Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_LS:\n",
        "  print(f'Plotting {model_name}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_name, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FrNFec7tNij"
      },
      "source": [
        "### **Remove Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYXa7qoevyuo"
      },
      "source": [
        "# Trim outliers to max of 3*Median Abs Variance in Standardized Sentiment Time Series\n",
        "#   and overwrite results in model_name column\n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\n",
        "# Sentence\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Sentence: {col_noouts_str} --------------------')\n",
        "  corpus_sents_df[col_noouts_str] = clip_outliers(corpus_sents_df[amodel_str])\n",
        "  \n",
        "  print(f'  old Standardized max: {corpus_sents_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sents_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_sents_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_sents_df[col_noouts_str].min()}')\n",
        "  \n",
        "# col_rename_dt = rename_cols(corpus_sents_df, models_ls) # ERROR: created 1 new col with col_rename_dt dictionary name instead of mapping correctly\n",
        "# col_rename_dt\n",
        "# _ = corpus_sents_df.rename(columns=col_rename_dt, inplace=True, errors='raise');\n",
        "\n",
        "# Paragraph\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Paragraph: {col_noouts_str} --------------------')\n",
        "  corpus_parags_df[col_noouts_str] = clip_outliers(corpus_parags_df[amodel_str])\n",
        "\n",
        "  print(f'  old Standardized max: {corpus_parags_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_parags_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_parags_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_parags_df[col_noouts_str].min()}')\n",
        "\n",
        "# Section\n",
        "for amodel_str in MODELS_LS:\n",
        "  col_noouts_str = amodel_str + '_noouts'\n",
        "  print(f'Section: {col_noouts_str} --------------------')\n",
        "  corpus_sects_df[col_noouts_str] = clip_outliers(corpus_sects_df[amodel_str])\n",
        "\n",
        "  print(f'  old Standardized max: {corpus_sects_df[amodel_str].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sects_df[amodel_str].min()}')\n",
        "  print(f'  new max: {corpus_sects_df[col_noouts_str].max()}')\n",
        "  print(f'  new min: {corpus_sects_df[col_noouts_str].min()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyPsLI9E4tbB"
      },
      "source": [
        "# Trim outliers to max of 3*Median Abs Variance in Standardized Sentiment Time Series\n",
        "#   and overwrite results in model_name column\n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\"\"\"\n",
        "# Sentences\n",
        "for amodel in MODELS_LS:\n",
        "  col_stand = amodel + '_stand'\n",
        "  col_standout = amodel + '_standout'\n",
        "  print(f'Sentences: {col_stand} --------------------')\n",
        "  # corpus_sents_df[amodel] = corpus_sents_df[col_stand]\n",
        "  corpus_sents_df[col_standout] = clip_outliers(corpus_sents_df[col_stand])\n",
        "  \n",
        "  print(f'  old Standardized max: {corpus_sents_df[col_stand].max()}')\n",
        "  print(f'  old Standardized min: {corpus_sents_df[col_stand].min()}')\n",
        "  print(f'  new max: {corpus_sents_df[col_standout].max()}')\n",
        "  print(f'  new min: {corpus_sents_df[col_standout].min()}')\n",
        "  \n",
        "# col_rename_dt = rename_cols(corpus_sents_df, models_ls) # ERROR: created 1 new col with col_rename_dt dictionary name instead of mapping correctly\n",
        "# col_rename_dt\n",
        "# _ = corpus_sents_df.rename(columns=col_rename_dt, inplace=True, errors='raise');\n",
        "\n",
        "# Paragraphs\n",
        "for amodel in MODELS_LS:\n",
        "  col_stand = amodel + '_stand'\n",
        "  col_standout = amodel + '_standout'\n",
        "  print(f'Paragraphs: {col_stand} --------------------')\n",
        "  # corpus_parags_df[amodel] = corpus_parags_df[col_stand]\n",
        "  corpus_parags_df[col_standout] = clip_outliers(corpus_parags_df[col_stand])\n",
        "  print(f'  old Standardized max: {corpus_parags_df[col_stand].max()}')\n",
        "  print(f'  old Standardized min: {corpus_parags_df[col_stand].min()}')\n",
        "  print(f'  new max: {corpus_parags_df[col_standout].max()}')\n",
        "  print(f'  new min: {corpus_parags_df[col_standout].min()}')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPKABrvstSIo"
      },
      "source": [
        "### **After Removing Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ccxOjqghbgP"
      },
      "source": [
        "# Plot all Raw Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "# Exlopre to find which Sentiment Time Series still have outliers after initial 2.5*Median Abs Dev Clipping\n",
        "MODELS_SENTS_EXCLUDE_LS = ['nrc','bing','afinn','stanza']  # Likely these TS are not normal or heavy tailed so 2.5*MedAbsDev did not clip well\n",
        "MODELS_SENTS_CUSTOM_LS = [x for x in MODELS_LS if x not in MODELS_SENTS_EXCLUDE_LS] \n",
        "\n",
        "for model_name in MODELS_SENTS_CUSTOM_LS:\n",
        "  model_noouts = f'{model_name}_noouts'\n",
        "  print(f'Plotting {model_noouts}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_noouts, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Sentence Sentiment w/Trimmed Outliers Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJHXLvayIhOC"
      },
      "source": [
        "# Plot all Raw Paragraph Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_LS:\n",
        "  model_standout = f'{model_name}_standout'\n",
        "  print(f'Plotting {model_standout}')\n",
        "  sns.lineplot(data=corpus_parags_df, x='parag_no', y=model_standout, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nRaw Paragraph Sentiment Plot')\n",
        "# plt.legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNp9zwBfmgpI"
      },
      "source": [
        "## **Standardize Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ_wcgiyx-NJ"
      },
      "source": [
        "### **Before Standardizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEUbDF4hx-NN"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  model_noouts_str = f'{model_name}_noouts'\n",
        "  print(f'Plotting {model_noouts_str}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_noouts_str, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_noouts_str}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IrnbsMByTmX"
      },
      "source": [
        "### **Standardized the NoOutliers Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhkW7faSmPWh"
      },
      "source": [
        "# Standardize Sentence and Paragraphs Sentiment Time Series (Section TS was Standardized above)\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "# orig_cols_ls = list(set(corpus_all_df.columns) - set(['sent_no','parag_no','sent_raw','sent_clean']))\n",
        "# cols_ls = []\n",
        "\n",
        "# Sentences\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_sents_df[acol].values.reshape(-1,1)))\n",
        "    corpus_sents_df[acol_new] = pd.Series(temp_np.squeeze())\n",
        "\n",
        "# Paragraphs\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_parags_df[acol].values.reshape(-1,1)))\n",
        "    corpus_parags_df[acol_new] = pd.Series(temp_np.squeeze())\n",
        "\n",
        "# Paragraphs\n",
        "for acol in MODELS_LS:\n",
        "    acol_new = acol + '_standouts'\n",
        "    temp_np = std_scaler.fit_transform(np.array(corpus_sects_df[acol].values.reshape(-1,1)))\n",
        "    corpus_sects_df[acol_new] = pd.Series(temp_np.squeeze())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn6EWXYaybhj"
      },
      "source": [
        "### **After Standardizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcGzZNZJO7fd"
      },
      "source": [
        "for model_name in MODELS_LS:\n",
        "  model_standouts_str = f'{model_name}_standouts'\n",
        "  print(f'Plotting {model_standouts_str}')\n",
        "  sns.lineplot(data=corpus_sents_df, x='sent_no', y=model_standouts_str, alpha=0.3, legend='brief', label=model_name)\n",
        "      \n",
        "plt.title(f'{CORPUS_FULL} (Model: {model_standouts_str}) \\nRaw Sentence Sentiment Plot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37_ztlEpzYHx"
      },
      "source": [
        "MODELS_LS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Be4bSszK-A"
      },
      "source": [
        "## **Deselect Poorly Behaved Sentiment Time Series Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY_HdLarzK-D"
      },
      "source": [
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "NCR_Arc = True #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6z5qYX3zm9n"
      },
      "source": [
        "# Create and Verify custom list of Models to include\n",
        "\n",
        "MODELS_CUSTOM_LS = []\n",
        "\n",
        "if VADER_Arc:\n",
        "  MODELS_CUSTOM_LS.append('vader')\n",
        "if TextBlob_Arc:\n",
        "  MODELS_CUSTOM_LS.append('textblob')\n",
        "if Stanza_Arc:\n",
        "  MODELS_CUSTOM_LS.append('stanza')\n",
        "if SentimentR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentimentr')\n",
        "if Syuzhet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('syuzhet')\n",
        "if AFINN_Arc:\n",
        "  MODELS_CUSTOM_LS.append('afinn')\n",
        "if Bing_Arc:\n",
        "  MODELS_CUSTOM_LS.append('bing')\n",
        "if Pattern_Arc:\n",
        "  MODELS_CUSTOM_LS.append('pattern')\n",
        "if SentiWord_Arc:\n",
        "  MODELS_CUSTOM_LS.append('sentiword')\n",
        "if SenticNet_Arc:\n",
        "  MODELS_CUSTOM_LS.append('senticnet')\n",
        "if NCR_Arc:\n",
        "  MODELS_CUSTOM_LS.append('nrc')\n",
        "\n",
        "print(f'Here are the Models we are using to ensemble and save:\\n   {MODELS_CUSTOM_LS}')\n",
        "\n",
        "models_incl_ls = []\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  models_incl_ls.append(amodel[:2])\n",
        "models_incl_str = ''.join(models_incl_ls)\n",
        "\n",
        "print(f'Here is a custom name abbr: {models_incl_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfiyOjMBBraW"
      },
      "source": [
        "## **Calculate Median of All (Trimmed Outliers then Standardized) Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1tX6BmT4U5h"
      },
      "source": [
        "# Create a list of models with Outliers trimmed and Standardized Time Series \n",
        "\n",
        "MODELS_CUSTOM_STANDOUTS_LS = []\n",
        "\n",
        "for amodel in MODELS_CUSTOM_LS:\n",
        "  model_standout_str = f'{amodel}_standouts'\n",
        "  MODELS_CUSTOM_STANDOUTS_LS.append(model_standout_str)\n",
        "\n",
        "print(f'List of NoOutliers/Standardized Models to compute Median on: {MODELS_CUSTOM_STANDOUTS_LS}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx3-Qug9Y6h0"
      },
      "source": [
        "# Disregard poorly behaved time series identified and stored in MODELS_SENTS_EXCLUDE_LS \n",
        "\n",
        "# TODO: Add widget to select which models to include\n",
        "\n",
        "corpus_sents_df['median_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_sents_df.head(2)\n",
        "\n",
        "corpus_sents_df['std_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_sents_df.head(2)\n",
        "\n",
        "corpus_sents_df['mean_standouts_custom_lex'] = corpus_sents_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_sents_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2qunU4VfJ4J"
      },
      "source": [
        "# (For now) Paragraph Sentiment TS are better behaved and don't require excluding any Model\n",
        "\n",
        "corpus_parags_df['median_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_parags_df['std_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_parags_df['mean_standouts_custom_lex'] = corpus_parags_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_parags_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtTY4t4k1QBc"
      },
      "source": [
        "# (For now) Paragraph Sentiment TS are better behaved and don't require excluding any Model\n",
        "\n",
        "corpus_sects_df['median_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].median(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_sects_df['std_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].std(axis=1)\n",
        "# corpus_parags_df.head(2)\n",
        "\n",
        "corpus_sects_df['mean_standouts_custom_lex'] = corpus_sects_df[MODELS_CUSTOM_STANDOUTS_LS].mean(axis=1)\n",
        "# corpus_parags_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp_gYLYbkjxd"
      },
      "source": [
        "## **Save Processed Sentiment Time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmyasWR1kjxh"
      },
      "source": [
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "\n",
        "author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "corpus_sents_filename = f'corpus_sentences_lexrules_{models_incl_str}_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sentences to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "corpus_parags_filename = f'corpus_paragraphs_lexrules_{models_incl_str}_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Paragraphs to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "corpus_sects_filename = f'corpus_sections_lexrules_{models_incl_str}_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Sections to file: {corpus_sects_filename}')\n",
        "corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "corpus_cruxes_filename = f'corpus_cruxes_lexrules_{models_incl_str}_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes to file: {corpus_cruxes_filename}')\n",
        "with open(corpus_cruxes_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUnbmOFckt2t"
      },
      "source": [
        "# **EDA Visualizations and Comparisons**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww1a9l8Lkjxk"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_sents_df.head(2)\n",
        "corpus_sents_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiHul8A1kjxn"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_parags_df.head(2)\n",
        "corpus_parags_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbY7D82jkjxp"
      },
      "source": [
        "# Verify\n",
        "\n",
        "corpus_sects_df.head(2)\n",
        "corpus_sects_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvvfzj7xkjxp"
      },
      "source": [
        "corpus_cruxes_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDiTp97_zlUm"
      },
      "source": [
        "# norm_cols_ls = ['distbertsst_norm', 'nlptown_norm','xlnet_sst5_norm','bert_imdb_norm', 'bertuc_googapps_norm', 'roberta_lg15_norm']\n",
        "\"\"\"\n",
        "\n",
        "# ARCHIVE\n",
        "\n",
        "cols_norm_ls = []\n",
        "cols_stand_ls = []\n",
        "\n",
        "for acol in corpus_all_df.columns:\n",
        "  if acol.endswith('_norm'):\n",
        "    print(f'Adding {acol} to norm_cols_ls')\n",
        "    cols_norm_ls.append(acol)\n",
        "  elif acol.endswith('_stand'):\n",
        "    print(f'Adding {acol} to stand_cols_ls')\n",
        "    cols_stand_ls.append(acol)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "print(f'\\nNormalized Columns: {cols_norm_ls}')\n",
        "\n",
        "print(f'\\nStandardized Columns: {cols_stand_ls}')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O3l61be6_GQ"
      },
      "source": [
        "### **LOWESS Smoothed Single Plot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUjsXyRG81zT"
      },
      "source": [
        "**Normalized Sentiment Smoothed with LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5MYEwGuZ2vh"
      },
      "source": [
        "MODELS_STAND_LS = []\n",
        "for amodel in MODELS_LS:\n",
        "  MODELS_STAND_LS.append(f'{amodel}_stand')\n",
        "\n",
        "print(f'MODELS_STAND_LS: {MODELS_STAND_LS}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBT-Qd4o8919"
      },
      "source": [
        "**Standardized Sentiment Smoothed with LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ-Oe8icVpHn"
      },
      "source": [
        "# Plot and Compare all LOWESS Smoothed *Standardized* Sentiment Time Series\n",
        "\n",
        "corpus_lowess_stand_df = get_lowess(corpus_all_df, cols_stand_ls, plot_subtitle='Standardized', afrac=1./10, ait=5, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JizqEQE6X3oQ"
      },
      "source": [
        "### **High Level: Section View**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIwEIY4cd2PB"
      },
      "source": [
        "##### **Raw Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryWst950gpYx"
      },
      "source": [
        "# Plot Raw Section Sentiments\n",
        "\"\"\"\n",
        "\n",
        "# ARCHIVED\n",
        "\n",
        "for amodel in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  amodel_stand_str = f'{amodel}'\n",
        "  plot_raw_sentiments(model_name=amodel_stand_str, semantic_type='section', save2file=False)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx4mryQwVz7h"
      },
      "source": [
        "# Raw Standardized Section Sentiment Time Series\n",
        "\n",
        "_ = plot_stand_crux_sections(ts_df=corpus_sects_df, model_names_ls=MODELS_CUSTOM_STANDOUTS_LS, semantic_type='section', label_token_ct=5, title_xpos=0.5, title_ypos=1, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0HK0lbzjXXF"
      },
      "source": [
        "# Plot the Median of the Customized Set of NoOutliers/Standardized Section Sentiment Time Series\n",
        "\n",
        "corpus_sects_df['median_standouts_custom_lex'].plot()\n",
        "plt.title(f'{CORPUS_FULL}\\n Median of NoOutliers/Standardized Section Sentiment Time Series');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKz34IzZdxIg"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeOWF6frEm0S"
      },
      "source": [
        "Window_Width = 4 #@param {type:\"slider\", min:2, max:20, step:1}\n",
        "\n",
        "# DEFAULT 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1lngzR7XM5l"
      },
      "source": [
        "# SMA of Custom Set of NoOutliers/Standardized Section Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/Adjust the win_ls to explore different window_size by hand to see EDA agreement among plots\n",
        "\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  # print(f'Plotting {acol_name}')\n",
        "  get_smas(corpus_sects_df, model_name, text_unit='section', subtitle_str='(NOTE: different x-scale)', win_ls=[Window_Width])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5Qw6frQdz1C"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAuz19B5ntxk"
      },
      "source": [
        "# Standardized Section LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sects_df, models_ls=MODELS_STAND_LS, text_unit='section', afrac=1./4, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-u2ScQZD8Lv"
      },
      "source": [
        "LOWESS_fraction = 0.15 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 (approx 0.17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L82SbpnkO_3"
      },
      "source": [
        "# Standardized Median Section LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sects_df, models_ls=['median'], text_unit='section', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6Ipaa-lWVwP"
      },
      "source": [
        "### **Middle Level: Paragraph Views**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDYYpJYRfrZ-"
      },
      "source": [
        "##### **Raw Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxyJD-fjf4ev"
      },
      "source": [
        "# Plot Custom Set of NoOutliers/Standardized Paragraph Sentiments Time Series \n",
        "\n",
        "for amodel in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  plot_raw_sentiments(model_name=amodel, semantic_type='paragraph', save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwD9lNuJkCfo"
      },
      "source": [
        "# Plot the Median for the Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series\n",
        "\n",
        "corpus_parags_df['median_standouts_custom_lex'].plot()\n",
        "plt.title(f'{CORPUS_FULL}\\n Median of Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jii7hmhleyHo"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJn79SwJDKHt"
      },
      "source": [
        "Window_Width = 10 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "490cQGkgIBK2"
      },
      "source": [
        "# SMA Standardized Paragraph Sentiment Arcs\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots (5-20 defaults)\n",
        "\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  get_smas(corpus_parags_df, model_name, text_unit='paragraph', win_ls=[Window_Width])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDo7OHyHCtUJ"
      },
      "source": [
        "Window_Percentage = 5 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT: 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mahzCm5W9-QL"
      },
      "source": [
        "# SMA Plot the Median for the Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots\n",
        "\n",
        "win_percentage = Window_Percentage  # 5 means rolling window size is 5% of corpus length (5-20 default)\n",
        "\n",
        "win_size = int(corpus_parags_df.shape[0]*(win_percentage*0.01))\n",
        "\n",
        "plot_title = f'{CORPUS_FULL}\\n Median of Custom Set of NoOutlier/Standardized Paragraph Sentiment Time Series (win={win_percentage}%)'\n",
        "corpus_parags_df['median_standouts_custom_lex'].rolling(win_size).mean().plot(title=plot_title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2CIR3U9e4WC"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIzHfrCdCC9d"
      },
      "source": [
        "LOWESS_fraction = 0.17 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 (approx 0.17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2337wydGe4WC"
      },
      "source": [
        "# Standardized Paragraph LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "# NOTE: EDA/adjust the win_size below by hand to see EDA agreement among plots (0.10-0.20 default)\n",
        "\n",
        "_ = get_lowess(corpus_parags_df, models_ls=MODELS_CUSTOM_STANDOUTS_LS, text_unit='paragraph', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0emSLGZCh6-Y"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vq8s_aShoEJ"
      },
      "source": [
        "# Plot it\n",
        "\n",
        "_ = get_lowess(corpus_parags_df, models_ls=['median_standouts_custom_lex'], text_unit='paragraph', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzcDX02pZGbr"
      },
      "source": [
        "### **Low Level: Sentence Views**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV6spOW1fSKf"
      },
      "source": [
        "##### **SMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE_-_JJ9FeVj"
      },
      "source": [
        "Window_Percentage = 10 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# DEFAULT: 5-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gYKbdptopin"
      },
      "source": [
        "# Plot all Standardized Sentence Sentiment Arcs\n",
        "#   Adjust scale_factor and mean_adj by hand to see EDA agreement among plots\n",
        "\n",
        "models_stand_ls = []\n",
        "for model_name in MODELS_CUSTOM_STANDOUTS_LS:\n",
        "  get_smas(corpus_sents_df, model_name, text_unit='sentence', alpha=0.3, win_ls=[Window_Percentage])\n",
        "\n",
        "# print(f'models_stand_ls: {models_stand_ls}')\n",
        "corpus_sents_df['median_stand'] = corpus_sents_df[models_stand_ls].median()\n",
        "plt.plot(corpus_sents_df.sent_no, corpus_sents_df.median_stand, color='black', label='Median')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0fL5RXKfZG6"
      },
      "source": [
        "##### **LOWESS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-NGw8yEFnJD"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDlGY9CMfUj9"
      },
      "source": [
        "# Standardized Paragraph LOWESS Smoothed Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sents_df, models_ls=MODELS_CUSTOM_STANDOUTS_LS, text_unit='sentence', afrac=LOWESS_fraction, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9SQeAvgGFb6"
      },
      "source": [
        "LOWESS_fraction = 0.12 #@param {type:\"slider\", min:0.1, max:0.3, step:0.01}\n",
        "\n",
        "# Default: 1./6 to 1./8 (approx 0.17 to 0.12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-GtGa5mlEhZ"
      },
      "source": [
        "# Median of Custom Set of NoOutlier/Standardized Sentiment Time Series\n",
        "\n",
        "_ = get_lowess(corpus_sents_df, models_ls=['median_standouts_custom_lex'], text_unit='paragraph', afrac=1./8, do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1VhYgLai3RK"
      },
      "source": [
        "## **Save Standardized-NoOutliers Sentiment Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3zmDWROOEvM"
      },
      "source": [
        "# Save all Processed DataFrames\n",
        "\n",
        "author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n",
        "title_str = ''.join(CORPUS_TITLE.split()).lower()\n",
        "datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Sentences DataFrame\n",
        "corpus_sents_filename = f'corpus_sentences_only_lexrules_standouts_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_sents_filename}')\n",
        "corpus_sents_df.to_csv(corpus_sents_filename)\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Paragraphs DataFrame\n",
        "corpus_parags_filename = f'corpus_paragraphs_only_lexrules_standouts_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_parags_filename}')\n",
        "corpus_parags_df.to_csv(corpus_parags_filename)\n",
        "\n",
        "\n",
        "# Save Preprocessed Corpus Section DataFrame\n",
        "corpus_sects_filename = f'corpus_sections_only_lexrules_standouts_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving to file: {corpus_sects_filename}')\n",
        "corpus_sects_df.to_csv(corpus_sects_filename)\n",
        "\n",
        "\n",
        "# Save Cruxes\n",
        "corpus_cruxes_filename = f'corpus_cruxes_lexrules_{models_incl_str}_{author_str}_{title_str}_{datetime_now}.csv'\n",
        "print(f'Saving Corpus Cruxes to file: {corpus_cruxes_filename}')\n",
        "with open(corpus_cruxes_filename, 'w') as convert_file:\n",
        "  convert_file.write(json.dumps(corpus_cruxes_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vUzh39HHJXz"
      },
      "source": [
        "# **Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJI_13B0HMnU"
      },
      "source": [
        "## **Sentiment Stability**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R6SMWtSHJLK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9sEH8_IG6Qq"
      },
      "source": [
        "## **Crux Point Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRjKHnQaOTu2"
      },
      "source": [
        "### **Gather (n) Highest/Lowest Sentiment Values for Each Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG6Y0vPFOkzn"
      },
      "source": [
        "def getn_cruxes(crux_dt, model_name='vader', get_n=6):\n",
        "  '''\n",
        "  Given a Crux Dictionary, a Model item/dict within, and and integer n\n",
        "  Return the n highest and n lowest sentiment values\n",
        "  NOTE: if get_n == 0, return all Crux Points for all Models\n",
        "  '''\n",
        "\n",
        "  cruxes_all_df = pd.DataFrame\n",
        "  cruxes_n_top_df = pd.DataFrame()\n",
        "\n",
        "  cruxes_all_df = pd.DataFrame.from_dict(crux_dt[model_name])\n",
        "\n",
        "  cruxes_all_df = cruxes_all_df.transpose().reset_index().rename(columns={'index':'var'})\n",
        "\n",
        "  cruxes_all_df.rename(columns={'var':'sent_no',0:model_name,1:'sent_raw'}, inplace=True)\n",
        "  cruxes_all_df.drop(columns=['sent_raw'], inplace=True)\n",
        "  cruxes_all_df.rename(columns={model_name:'sentiment'}, inplace=True)\n",
        "  cruxes_all_df['sentiment'] = cruxes_all_df['sentiment'].astype('float')\n",
        "  cruxes_all_df['model_name'] = model_name\n",
        "  cruxes_all_df = cruxes_all_df[['sent_no','model_name','sentiment']]\n",
        "\n",
        "  if get_n > 0:\n",
        "    cruxes_n_top_df = cruxes_all_df.nlargest(get_n, 'sentiment')\n",
        "    cruxes_n_top_df = cruxes_n_top_df.append(cruxes_all_df.nsmallest(get_n, 'sentiment'))\n",
        "  elif get_n ==0:\n",
        "    cruxes_n_top_df = cruxes_all_df\n",
        "  else:\n",
        "    print(f'ERROR: argument get_n must be either 0 (return all Cruxes) or greater than 0')\n",
        "    \n",
        "  return cruxes_n_top_df\n",
        "\n",
        "# Test\n",
        "\n",
        "cruxes_n_top_df = getn_cruxes(corpus_cruxes_dt, model_name='vader', get_n=3)\n",
        "cruxes_n_top_df.head(6)\n",
        "cruxes_n_top_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H67tOfiaSaHB"
      },
      "source": [
        "# Acculumate all the Crux Points from All Models into one cruxes_n_top_all_df DataFrame\n",
        "\n",
        "cruxes_n_top_all_df = pd.DataFrame()\n",
        "\n",
        "for amodel in MODELS_LS:\n",
        "  print(f'Appending Cruxes from {amodel}')\n",
        "  cruxes_n_top_df = getn_cruxes(corpus_cruxes_dt, model_name=amodel, get_n=12)\n",
        "  cruxes_n_top_all_df = cruxes_n_top_all_df.append(cruxes_n_top_df, ignore_index=True)\n",
        "\n",
        "for amodel in MODELS_LS:\n",
        "  crux_ct = len(cruxes_n_top_all_df[cruxes_n_top_all_df['model_name'] == amodel])\n",
        "  print(f'{amodel.capitalize()} has {crux_ct} Cruxes')\n",
        "\n",
        "print(f'There are a total of {cruxes_n_top_all_df.shape[0]} in all Models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQDWi6YLYVn0"
      },
      "source": [
        "# Plot Crux Points in 2D Space: Scatterplot\n",
        "\n",
        "sns.lmplot('sent_no', 'sentiment', data=cruxes_n_top_all_df, fit_reg=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab2oHmU1Yqcl"
      },
      "source": [
        "# Plot Crux Points in 1D Space: Histogram\n",
        "\n",
        "sns.distplot(cruxes_n_top_all_df.sent_no, bins=2000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtdL2OjAJm55"
      },
      "source": [
        "sns.clustermap(cruxes_n_top_all_df.sent_no)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiTZC_X4G6EC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE8EJC1wHCEn"
      },
      "source": [
        "## **Sentiment Arc Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFKeAFHlG-vm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TfBnxjYG5_u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wjBEXGyvU4x"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAjtTbABsw7R"
      },
      "source": [
        "# Verify the *.csv file was created\n",
        "\n",
        "latest_file = get_recentfile('csv')\n",
        "print(f'Most recenst *.csv file: {latest_file}')\n",
        "\n",
        "!ls -altr $latest_file\n",
        "print('\\n')\n",
        "!head -n 2 $latest_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moVcxPHoaYN5"
      },
      "source": [
        "# Plot Raw Sentiment Values\n",
        "\n",
        "plt.plot(corpus_sents_df['vader'], alpha=0.3, label='VADER')\n",
        "plt.plot(corpus_sents_df['textblob'], alpha=0.3, label='TextBlob')\n",
        "plt.plot(corpus_sents_df['stanza'], alpha=0.3, label='Stanza')\n",
        "plt.plot(corpus_sents_df['afinn'], alpha=0.3, label='AFINN')\n",
        "plt.plot(corpus_sents_df['sentimentr'], alpha=0.3, label='SentimentR')\n",
        "plt.plot(corpus_sents_df['syuzhet'], alpha=0.3, label='Syuzhet')\n",
        "plt.plot(corpus_sents_df['bing'], alpha=0.3, label='Bing')\n",
        "plt.plot(corpus_sents_df['pattern'], alpha=0.3, label='Bing')\n",
        "plt.plot(corpus_sents_df['sentiword'], alpha=0.3, label='SentiWord')\n",
        "plt.plot(corpus_sents_df['senticnet'], alpha=0.3, label='SenticNet')\n",
        "plt.plot(corpus_sents_df['nrc'], alpha=0.3, label='NRC')\n",
        "plt.plot(corpus_sents_df['median_lex'], label='Median')\n",
        "plt.plot(corpus_sents_df['mean_lex'], label='Mean')\n",
        "plt.legend()\n",
        "plt.title(f'{CORPUS_FULL} \\n Raw Sentence Sentiment (All)')\n",
        "\n",
        "if (PLOT_OUTPUT in ['All']):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'raw_sent_sa_all.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH-02cI2a9oE"
      },
      "source": [
        "# Plot 5% Median SMA for Raw Sentiment Values\n",
        "\n",
        "plt.plot(corpus_sents_df['vader'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='VADER')\n",
        "plt.plot(corpus_sents_df['textblob'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='TextBlob')\n",
        "plt.plot(corpus_sents_df['stanza'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='Stanza')\n",
        "plt.plot(corpus_sents_df['afinn'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='AFINN')\n",
        "plt.plot(corpus_sents_df['sentimentr'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='SentimentR')\n",
        "plt.plot(corpus_sents_df['syuzhet'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='Syuzhet')\n",
        "plt.plot(corpus_sents_df['bing'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='Bing')\n",
        "plt.plot(corpus_sents_df['pattern'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='Pattern')\n",
        "plt.plot(corpus_sents_df['sentiword'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='SentiWord')\n",
        "plt.plot(corpus_sents_df['senticnet'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='SenticNet')\n",
        "plt.plot(corpus_sents_df['nrc'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='NRC')\n",
        "plt.plot(corpus_sents_df['median_lex'].rolling(5*win_1per, center=True).mean(), alpha=0.9, label='Median', color='black')\n",
        "plt.plot(corpus_sents_df['std_lex'].rolling(5*win_1per, center=True).mean(), alpha=0.9, label='STD', color='red')\n",
        "plt.plot(corpus_sents_df['mean_lex'].rolling(5*win_1per, center=True).mean(), alpha=0.9, label='Mean', color='grey')\n",
        "plt.legend()\n",
        "plt.title(f'{CORPUS_FULL} \\n 5% Median SMA for Raw Sentence Sentiment (All)')\n",
        "\n",
        "if (PLOT_OUTPUT in ['All','Major']):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'plot_sent_sa_050100mean_all.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0XB65Q6jsl4"
      },
      "source": [
        "**Calculate Median of All Normed Sentiment Polarities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3y0UMO8lcAn"
      },
      "source": [
        "# DELETE? Create DataFrame with just raw Sentiment Values\n",
        "\n",
        "sentiment_sentno_cols_ls = sentiment_only_cols_ls.copy()\n",
        "sentiment_sentno_cols_ls.insert(0, 'sent_no')\n",
        "\n",
        "corpus_sents_sa_raw_df = corpus_sents_df[sentiment_sentno_cols_ls].copy()\n",
        "\n",
        "corpus_sents_sa_raw_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMYlFRclk-Nh"
      },
      "source": [
        "# Review Sentiment Value Statistics for all Models\n",
        "\n",
        "corpus_lexicons_stats_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTIXIrCpnFgX"
      },
      "source": [
        "# Normalize all Sentiment Values to same Sentiment Range -1.0 to +1.0 using sklearn.MinMaxScaler()\n",
        "\n",
        "min_max_scaler = MinMaxScaler(feature_range=(-1.0,1.0))\n",
        "corpus_sents_norm_np = min_max_scaler.fit_transform(corpus_sents_df[sentiment_only_cols_ls])\n",
        "corpus_sents_norm_df = pd.DataFrame(corpus_sents_norm_np)\n",
        "corpus_sents_norm_df.columns = sentiment_only_cols_ls\n",
        "corpus_sents_norm_df.insert(0, 'sent_no', corpus_sents_df['sent_no'])\n",
        "\n",
        "corpus_sents_norm_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfq25a3Nh1n_"
      },
      "source": [
        "corpus_sents_norm_df['median'] = corpus_sents_norm_df[sentiment_only_cols_ls].median(axis=1)\n",
        "corpus_sents_norm_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8KWKKGUh1oB"
      },
      "source": [
        "corpus_sents_norm_df['std'] = corpus_sents_norm_df[sentiment_only_cols_ls].std(axis=1)\n",
        "corpus_sents_norm_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlxjC1gMh1oB"
      },
      "source": [
        "corpus_sents_norm_df['mean'] = corpus_sents_norm_df[sentiment_only_cols_ls].mean(axis=1)\n",
        "corpus_sents_norm_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40oiiFcQMe-m"
      },
      "source": [
        "# Verify all Sentiment within -1.0 to +1.0\n",
        "\n",
        "corpus_sents_norm_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYYPfd4yjHUh"
      },
      "source": [
        "**Save all Normed Sentiment Value to File**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkH5U2sXov9_"
      },
      "source": [
        "# Save all the calculated Norm Sentiment Values\n",
        "\n",
        "sats_norm_filename_str = f'corpus_sa_norm_range_all.csv'\n",
        "print(sats_norm_filename_str)\n",
        "\n",
        "sentiment_pathfilename_str = gen_pathfiletime(sats_norm_filename_str)\n",
        "\n",
        "print(f'Saving all Norm Sentiment Values from All Models to file: {sentiment_pathfilename_str}')\n",
        "corpus_sents_norm_df.to_csv(sentiment_pathfilename_str, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N41jC6chgWoR"
      },
      "source": [
        "# Verify the *.csv file was created\n",
        "\n",
        "latest_file = get_recentfile('csv')\n",
        "\n",
        "!ls -altr $latest_file\n",
        "print('\\n')\n",
        "!head -n 5 $latest_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T80CNI9vS5wT"
      },
      "source": [
        "# Plot Normed Sentiment Values\n",
        "\n",
        "plt.plot(corpus_sents_norm_df['vader'], alpha=0.3, label='VADER')\n",
        "plt.plot(corpus_sents_norm_df['textblob'], alpha=0.3, label='TextBlob')\n",
        "plt.plot(corpus_sents_norm_df['stanza'], alpha=0.3, label='Stanza')\n",
        "plt.plot(corpus_sents_norm_df['afinn'], alpha=0.3, label='AFINN')\n",
        "plt.plot(corpus_sents_norm_df['sentimentr'], alpha=0.3, label='SentimentR')\n",
        "plt.plot(corpus_sents_norm_df['syuzhet'], alpha=0.3, label='Syuzhet')\n",
        "plt.plot(corpus_sents_norm_df['bing'], alpha=0.3, label='Bing')\n",
        "plt.plot(corpus_sents_norm_df['sentiword'], alpha=0.3, label='SentiWord')\n",
        "plt.plot(corpus_sents_norm_df['senticnet'], alpha=0.3, label='SenticNet')\n",
        "plt.plot(corpus_sents_norm_df['nrc'], alpha=0.3, label='NRC')\n",
        "plt.plot(corpus_sents_norm_df['mean'], label='Mean', color='grey')\n",
        "plt.plot(corpus_sents_norm_df['median'], label='Median', color='black')\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} \\n Sentence Sentiment - Normed Range (All)')\n",
        "plt.legend()\n",
        "\n",
        "if (PLOT_OUTPUT in ['All']):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'raw_norm_range_sent_sa_all.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV49DFhXhDYa"
      },
      "source": [
        "# Plot 5% SMA Normed Sentiment Values\n",
        "\n",
        "plt.plot(corpus_sents_norm_df['vader'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='VADER')\n",
        "plt.plot(corpus_sents_norm_df['textblob'].rolling(5*win_1per, center=True).mean(),  alpha=0.3, label='TextBlob')\n",
        "plt.plot(corpus_sents_norm_df['stanza'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='Stanza')\n",
        "plt.plot(corpus_sents_norm_df['afinn'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='AFINN')\n",
        "plt.plot(corpus_sents_norm_df['sentimentr'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='SentimentR')\n",
        "plt.plot(corpus_sents_norm_df['syuzhet'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='Syuzhet')\n",
        "plt.plot(corpus_sents_norm_df['bing'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='Bing')\n",
        "plt.plot(corpus_sents_norm_df['sentiword'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='SentiWord')\n",
        "plt.plot(corpus_sents_norm_df['senticnet'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='SenticNet')\n",
        "plt.plot(corpus_sents_norm_df['nrc'].rolling(5*win_1per, center=True).mean(), alpha=0.3, label='NRC')\n",
        "plt.plot(corpus_sents_norm_df['median'].rolling(5*win_1per, center=True).mean(), alpha=0.8, label='Median', color='black')\n",
        "plt.plot(corpus_sents_norm_df['mean'].rolling(5*win_1per, center=True).mean(), alpha=0.6, label='Mean', color='grey')\n",
        "plt.plot(corpus_sents_norm_df['std'].rolling(5*win_1per, center=True).mean(), alpha=0.8, label='STD', color='red')\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} \\n 5% SMA Sentence Sentiment - Normed Range -1.0 to +1.0 (All)')\n",
        "plt.legend()\n",
        "\n",
        "if (PLOT_OUTPUT in ['All','Major']):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'plot_5sma_norm_range_sent_sa_all.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrA93wx5lMRF"
      },
      "source": [
        "**[ALL] Sentiment Arcs with Normed Range and Mean**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iva-FHIgwSSA"
      },
      "source": [
        "corpus_sents_norm_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyxNvdEWmnJn"
      },
      "source": [
        "# Recenter [ALL] Time Series with normed Range -1.0 to 1.0 and Mean = 0\n",
        "\n",
        "vader_mean = corpus_sents_norm_df['vader'].mean()\n",
        "corpus_sents_norm_df['vader_2norm'] = corpus_sents_norm_df['vader'].apply(lambda x: x-vader_mean)\n",
        "\n",
        "textblob_mean = corpus_sents_norm_df['textblob'].mean()\n",
        "corpus_sents_norm_df['textblob_2norm'] = corpus_sents_norm_df['vader'].apply(lambda x: x-textblob_mean)\n",
        "\n",
        "stanza_mean = corpus_sents_norm_df['stanza'].mean()\n",
        "corpus_sents_norm_df['stanza_2norm'] = corpus_sents_norm_df['stanza'].apply(lambda x: x-stanza_mean)\n",
        "\n",
        "afinn_mean = corpus_sents_norm_df['afinn'].mean()\n",
        "corpus_sents_norm_df['afinn_2norm'] = corpus_sents_norm_df['afinn'].apply(lambda x: x-afinn_mean)\n",
        "\n",
        "sentimentr_mean = corpus_sents_norm_df['sentimentr'].mean()\n",
        "corpus_sents_norm_df['sentimentr_2norm'] = corpus_sents_norm_df['sentimentr'].apply(lambda x: x-sentimentr_mean)\n",
        "\n",
        "syuzhet_mean = corpus_sents_norm_df['syuzhet'].mean()\n",
        "corpus_sents_norm_df['syuzhet_2norm'] = corpus_sents_norm_df['syuzhet'].apply(lambda x: x-syuzhet_mean)\n",
        "\n",
        "bing_mean = corpus_sents_norm_df['bing'].mean()\n",
        "corpus_sents_norm_df['bing_2norm'] = corpus_sents_norm_df['bing'].apply(lambda x: x-bing_mean)\n",
        "\n",
        "sentiword_mean = corpus_sents_norm_df['sentiword'].mean()\n",
        "corpus_sents_norm_df['sentiword_2norm'] = corpus_sents_norm_df['sentiword'].apply(lambda x: x-sentiword_mean)\n",
        "\n",
        "senticnet_mean = corpus_sents_norm_df['senticnet'].mean()\n",
        "corpus_sents_norm_df['senticnet_2norm'] = corpus_sents_norm_df['senticnet'].apply(lambda x: x-senticnet_mean)\n",
        "\n",
        "nrc_mean = corpus_sents_norm_df['nrc'].mean()\n",
        "corpus_sents_norm_df['nrc_2norm'] = corpus_sents_norm_df['nrc'].apply(lambda x: x-senticnet_mean)\n",
        "\n",
        "median_mean = corpus_sents_norm_df['median'].mean()\n",
        "corpus_sents_norm_df['median_2norm'] = corpus_sents_norm_df['median'].apply(lambda x: x-median_mean)\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['vader'].rolling(5*win_1per).mean(), alpha=0.3, label='VADER')\n",
        "plt.plot(corpus_sents_norm_df['vader_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='VADER-2n')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['textblob'].rolling(5*win_1per).mean(), alpha=0.3, label='TextBlob')\n",
        "plt.plot(corpus_sents_norm_df['textblob_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='TextBlob-2n')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['stanza'].rolling(5*win_1per).mean(), alpha=0.3, label='Stanza')\n",
        "plt.plot(corpus_sents_norm_df['stanza_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='Stanza-2n')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['afinn'].rolling(5*win_1per).mean(), alpha=0.3, label='AFINN')\n",
        "plt.plot(corpus_sents_norm_df['afinn_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='AFINN-2n')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['sentimentr'].rolling(5*win_1per).mean(), alpha=0.3, label='SentimentR')\n",
        "plt.plot(corpus_sents_norm_df['sentimentr_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='SentimentR-2n')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['syuzhet'].rolling(5*win_1per).mean(), alpha=0.3, label='Syuzhet')\n",
        "plt.plot(corpus_sents_norm_df['syuzhet_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='Syuzhet-2n')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['bing'].rolling(5*win_1per).mean(), alpha=0.3, label='Bing')\n",
        "plt.plot(corpus_sents_norm_df['bing_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='Bing-2n')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['sentiword'].rolling(5*win_1per).mean(), alpha=0.3, label='SentiWord')\n",
        "plt.plot(corpus_sents_norm_df['sentiword_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='SentiWord-2n')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['senticnet'].rolling(5*win_1per).mean(), alpha=0.3, label='SenticNet')\n",
        "plt.plot(corpus_sents_norm_df['senticnet_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='SenticNet-2n')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['nrc'].rolling(5*win_1per).mean(), alpha=0.3, label='NRC')\n",
        "plt.plot(corpus_sents_norm_df['nrc_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='NRC-2n')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['median'].rolling(5*win_1per).mean(), alpha=0.3, label='Median')\n",
        "plt.plot(corpus_sents_norm_df['median_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='Median-2n');\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} \\n 5% SMA Sentence Sentiment - Normed Range + Mean (All)')\n",
        "plt.legend()\n",
        "\n",
        "if (PLOT_OUTPUT in ['All','Major']):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'plot_050100sma_norm_rm_sent_sa_all.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otf-OWkjtplb"
      },
      "source": [
        "# Save Sentiment Values Normedx2 (Range and Mean)\n",
        "\n",
        "sats_norm_rm_str = f'corpus_sa_norm_rangemean_all.csv'\n",
        "print(sats_norm_rm_str)\n",
        "\n",
        "sentiment_norm_pathfilename_str = gen_pathfiletime(sats_norm_rm_str)\n",
        "\n",
        "print(f'Saving all Norm Sentiment Values from All Models to file: {sentiment_norm_pathfilename_str}')\n",
        "corpus_sents_norm_df.to_csv(sentiment_norm_pathfilename_str, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cx_AI4Xtplc"
      },
      "source": [
        "# Verify the *.csv file was created\n",
        "\n",
        "latest_file = get_recentfile('csv')\n",
        "\n",
        "!ls -altr $latest_file\n",
        "print('\\n')\n",
        "!head -n 5 $latest_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iWFYbF1dknh"
      },
      "source": [
        "# **(Optional) Remove Model from Corpus Sentiment Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS2bZ0k4lWme"
      },
      "source": [
        "**[OUTLIERS DROPPED] Sentiment Arcs with Normed Range and Mean**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVxu1pjcliLY"
      },
      "source": [
        "# TODO: Add AutoML to provide <outlier metrics> on each Sentiment Arc\n",
        "\n",
        "sentiment_only_cols_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlowdDz1yRbN"
      },
      "source": [
        "**Deselect Outlier Sentiment Arc then Run the next Code Cell (to get a more Accurate Ensemble)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhLxDOg3xY74"
      },
      "source": [
        "VADER_Arc = True #@param {type:\"boolean\"}\n",
        "TextBlob_Arc = True #@param {type:\"boolean\"}\n",
        "Stanza_Arc = True #@param {type:\"boolean\"}\n",
        "SentimentR_Arc = True #@param {type:\"boolean\"}\n",
        "Syuzhet_Arc = True #@param {type:\"boolean\"}\n",
        "AFINN_Arc = True #@param {type:\"boolean\"}\n",
        "Bing_Arc = True #@param {type:\"boolean\"}\n",
        "Pattern_Arc = True #@param {type:\"boolean\"}\n",
        "SentiWord_Arc = True #@param {type:\"boolean\"}\n",
        "SenticNet_Arc = True #@param {type:\"boolean\"}\n",
        "NCR_Arc = True #@param {type:\"boolean\"}\n",
        "MPQA_Arc = False #@param {type:\"boolean\"}\n",
        "SentiStrength_Arc = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7_wWwYCkUZb"
      },
      "source": [
        "for amodel in models_ls:\n",
        "  print(amodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvzuenf-lUx8"
      },
      "source": [
        "win_1per"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkYBEKjjrVZa"
      },
      "source": [
        "# Recenter Similar Normed Time Series with Mean set to Zero (Dropping Outliers)\n",
        "\n",
        "for model_name:\n",
        "sentiment_noout_cols_ls = []\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['vader'].rolling(5*win_1per).mean(), alpha=0.3, label='VADER')\n",
        "if VADER_Arc:\n",
        "  plt.plot(corpus_sents_norm_df['vader_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='VADER-rm')\n",
        "  sentiment_noout_cols_ls.append('vader_2norm')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['textblob'].rolling(5*win_1per).mean(), alpha=0.3, label='TextBlob')\n",
        "if TextBlob_Arc:\n",
        "  plt.plot(corpus_sents_norm_df['textblob_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='TextBlob-rm')\n",
        "  sentiment_noout_cols_ls.append('textblob_2norm')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['stanza'].rolling(5*win_1per).mean(), alpha=0.3, label='Stanza')\n",
        "if Stanza_Arc:\n",
        "  plt.plot(corpus_sents_norm_df['stanza_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='Stanza-rm')\n",
        "  sentiment_noout_cols_ls.append('stanza_2norm')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['afinn'].rolling(5*win_1per).mean(), alpha=0.3, label='AFINN')\n",
        "if AFINN_Arc:\n",
        "  plt.plot(corpus_sents_norm_df['afinn_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='AFINN-rm')\n",
        "  sentiment_noout_cols_ls.append('afinn_2norm')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['sentimentr'].rolling(5*win_1per).mean(), alpha=0.3, label='SentimentR')\n",
        "if SentimentR_Arc:\n",
        "  plt.plot(corpus_sents_norm_df['sentimentr_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='SentimentR-rm')\n",
        "  sentiment_noout_cols_ls.append('sentimentr_2norm')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['syuzhet'].rolling(5*win_1per).mean(), alpha=0.3, label='Syuzhet')\n",
        "if Syuzhet_Arc:\n",
        "  plt.plot(corpus_sents_norm_df['syuzhet_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='Syuzhet-rm')\n",
        "  sentiment_noout_cols_ls.append('syuzhet_2norm')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['bing'].rolling(5*win_1per).mean(), alpha=0.3, label='Bing')\n",
        "if Bing_Arc:\n",
        "  plt.plot(corpus_sents_norm_df['bing_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='Bing-rm')\n",
        "  sentiment_noout_cols_ls.append('bing_2norm')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['bing'].rolling(5*win_1per).mean(), alpha=0.3, label='Bing')\n",
        "if Pattern_Arc:\n",
        "  plt.plot(corpus_sents_norm_df['bing_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='Bing-rm')\n",
        "  sentiment_noout_cols_ls.append('bing_2norm')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['sentiword'].rolling(5*win_1per).mean(), alpha=0.3, label='SentiWord')\n",
        "if SentiWord_Arc:\n",
        "  plt.plot(corpus_sents_norm_df['sentiword_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='SentiWord-rm')\n",
        "  sentiment_noout_cols_ls.append('sentiword_2norm')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['senticnet'].rolling(5*win_1per).mean(), alpha=0.3, label='SenticNet')\n",
        "if SenticNet_Arc:\n",
        "  plt.plot(corpus_sents_norm_df['senticnet_2norm'].rolling(5*win_1per).mean(), alpha=0.3, label='SenticNet-rm')\n",
        "  sentiment_noout_cols_ls.append('senticnet_2norm')\n",
        "\n",
        "# plt.plot(corpus_sents_norm_df['median'].rolling(5*win_1per).mean(), alpha=0.3, label='Median')\n",
        "plt.plot(corpus_sents_norm_df['median_2norm'].rolling(5*win_1per).mean(), alpha=0.6, label='Median-rm', color='black');\n",
        "\n",
        "# Recalculated Median with Outlier(s) Removed\n",
        "corpus_sents_norm_df['median_2norm_noout'] = corpus_sents_norm_df[sentiment_noout_cols_ls].median(axis=1)\n",
        "plt.plot(corpus_sents_norm_df['median_2norm_noout'].rolling(5*win_1per).mean(), alpha=0.6, label='Median-rm-NoOut', color='red');\n",
        "\n",
        "\n",
        "\n",
        "plt.title(f'{CORPUS_FULL} \\n 5% SMA Sentence Sentiment - Normed Range + Mean=0 (Outliers Dropped)')\n",
        "plt.legend()\n",
        "\n",
        "if (PLOT_OUTPUT in ['All','Major']):\n",
        "  # Save graph to file.\n",
        "  plot_filename = 'plot_050100sma_norm_rm_sent_sa_noout.png'\n",
        "  plotpathfilename_str = gen_pathfiletime(plot_filename)\n",
        "  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n",
        "  print(f'Plot saved: {plot_filename}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPZESYkgjtx4"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na7bzhxQj3EI"
      },
      "source": [
        "corpus_sents_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQyIY7SkjIgi"
      },
      "source": [
        "# Save all Sentiment Time Series\n",
        "\n",
        "corpus_sents_df.to_csv('sentiment_lexruleembed_imcewan_machineslikeme_20210621.csv', index=False)\n",
        "corpus_sents_norm_df.to_csv('sentiment_norm_lexruleembed_imcewan_machineslikeme_20210621.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mceEwwMvbdsa"
      },
      "source": [
        "***Summary of Sentiment Datafiles Produced***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keNpGHQ7bjnl"
      },
      "source": [
        "%whos DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya-mbO1PbdXo"
      },
      "source": [
        "print('SUMMARY OF SENTIMENT ANALYSIS DATAFILE PRODUCED:')\n",
        "print('------------------------------------------------')\n",
        "print(f'Raw Sentiment (+5%/10% SMA) in corpus_sents_df: {sentiment_raw_pathfilename_str}')\n",
        "print('\\n')\n",
        "print(f'Normed Sentiment (Range/Range+Mean) in corpus_sents_norm_df: {sentiment_norm_pathfilename_str}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhPYjyAGqJth"
      },
      "source": [
        "!ls -altr *.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYEdzgEnqJof"
      },
      "source": [
        "sats_raw_filename_str\n",
        "sats_norm1_str\n",
        "sats_norm2_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZPDCvYBCdPY"
      },
      "source": [
        "# View Combined Sentiment Scores (Redundant?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG9RkkW3qF6G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM65b4hK8xY6"
      },
      "source": [
        "**Calculate MedianNorm (-Outliers) Sentiment Polarities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g81eYIl8wuu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRo_Tt35VW8"
      },
      "source": [
        "**Bi/Tri-Polarity Lexicons**\n",
        "\n",
        "NRC: \n",
        "* https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
        "\n",
        "MPQA (Upitt):\n",
        "* https://mpqa.cs.pitt.edu/lexicons/effect_lexicon/\n",
        "* https://github.com/nlpcl-lab/mpqa2.0-preprocessing\n",
        "* https://github.com/kvangundy/basic-sentiment-analyzer/blob/master/sentimentDict.csv\n",
        "\n",
        "SentimentAnalysis.R (20210217 124s):\n",
        "* QDAP \n",
        "* DictionaryGI: Harvard-IV dictionary asused in the General Inquirer software (2005-/1637+)\n",
        "* DictionaryLM: Loughran-McDonald Financial dictionary (2355-/354+/297?)\n",
        "* DictionaryHE: Henry's Financial Dictionary (85-/105+)\n",
        "\n",
        "Custom Lexicons:\n",
        "* https://nealcaren.org/lessons/wordlists/ \n",
        "\n",
        "Lexicons\n",
        "* https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMr3786u5TMF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZhkzD3u7fdw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJiJ2tUBYEPu"
      },
      "source": [
        "# EDA of Raw Sentiment Polarities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmRPRSy7Oegz"
      },
      "source": [
        "corpus_sents_df['vader_nltk'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcOrHkuydk12"
      },
      "source": [
        "**Character/Token Length Histograms**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFqAqzYDkRD7"
      },
      "source": [
        "corpus_sents_df.head()\n",
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn2koUCXby-H"
      },
      "source": [
        "# Distribution of Paragraph Lengths\n",
        "\n",
        "sns.histplot(corpus_sents_df['sent_clean'].str.len(), kde=True).set_title(f'{BOOK_TITLE_FULL} \\n Paragraph Length Histogram ({sa_model})')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nG2M4UdbCU3"
      },
      "source": [
        "# Distribution of Paragraph Sentiment Scores\n",
        "\n",
        "# sns.histplot(corpus_parags_df['vader_nltk'].rolling(5).median(), kde=True).set_title(f'{BOOK_TITLE_FULL} \\n Paragraph Sentiment Histogram Rolling Win=501 ({sa_model})')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5AijWGudhbf"
      },
      "source": [
        "**Sentiment Histograms**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSI2mRBkkojx"
      },
      "source": [
        "corpus_sentiments_df.head()\n",
        "corpus_sentiments_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGUpziQIhkWA"
      },
      "source": [
        "# Distribution of Paragraph Sentiment Scores\n",
        "\n",
        "sns.histplot(corpus_sents_df['vader_nltk'], kde=True).set_title(f'{BOOK_TITLE_FULL} \\n Paragraph Sentiment Histogram Rolling Win=501 ({sa_model})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eplnf4Zga3FH"
      },
      "source": [
        "# corpus_parags_df['vader_nltk'].lineplot()\n",
        "# sns.lineplot(data=corpus_parags_df['vader_nltk'].rolling(50).median())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzlIk8ONh7VE"
      },
      "source": [
        "# corpus_parags_df['vader_nltk'].lineplot()\n",
        "sns.lineplot(data=corpus_sents_df['vader_nltk'].rolling(50).median())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVnMw9e3Cxlr"
      },
      "source": [
        "**Histograms of Sentiment Values by Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX8-Ab4JkSzj"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdUbwxqIkRAy"
      },
      "source": [
        "# Histograms of Sentiment Values\n",
        "\n",
        "# set a grey background (use sns.set_theme() if seaborn version 0.11.0 or above) \n",
        "sns.set(style=\"darkgrid\")\n",
        "# df = sns.load_dataset(\"iris\")\n",
        "\n",
        "fig, axs = plt.subplots(4, 2, figsize=(12, 18))\n",
        "\n",
        "sns.histplot(data=corpus_sents_df, x=\"median\", kde=True, color=\"skyblue\", ax=axs[0, 0])\n",
        "sns.histplot(data=corpus_sents_df, x=\"vader_mean_roll050\", kde=True, color=\"skyblue\", ax=axs[0, 1])\n",
        "sns.histplot(data=corpus_sents_df, x=\"sentimentr_mean_roll050\", kde=True, color=\"olive\", ax=axs[1, 0])\n",
        "sns.histplot(data=corpus_sents_df, x=\"syuzhet_mean_roll050\", kde=True, color=\"olive\", ax=axs[1, 1])\n",
        "sns.histplot(data=corpus_sents_df, x=\"bing_mean_roll050\", kde=True, color=\"gold\", ax=axs[2, 0])\n",
        "sns.histplot(data=corpus_sents_df, x=\"textblob_mean_roll050\", kde=True, color=\"gold\", ax=axs[2, 1])\n",
        "sns.histplot(data=corpus_sents_df, x=\"sentiword_mean_roll050\", kde=True, color=\"teal\", ax=axs[3, 0])\n",
        "sns.histplot(data=corpus_sents_df, x=\"senticnet_mean_roll050\", kde=True, color=\"teal\", ax=axs[3, 1])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsOKzM-RYHf5"
      },
      "source": [
        "# Smooth Raw Sentiment Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWKU0OzT_9pe"
      },
      "source": [
        "**Simple Moving Average (SMA) by Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGluyBsFkQ7C"
      },
      "source": [
        "# Line Plots of Sentiment Values\n",
        "\n",
        "# set a grey background (use sns.set_theme() if seaborn version 0.11.0 or above) \n",
        "sns.set(style=\"darkgrid\")\n",
        "# df = sns.load_dataset(\"iris\")\n",
        "\n",
        "fig, axs = plt.subplots(4, 2, figsize=(12, 18))\n",
        "\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"median\", color=\"skyblue\", ax=axs[0, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"vader_mean_roll050\", color=\"skyblue\", ax=axs[0, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"sentimentr_mean_roll050\", color=\"olive\", ax=axs[1, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"syuzhet_mean_roll050\", color=\"olive\", ax=axs[1, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"bing_mean_roll050\", color=\"gold\", ax=axs[2, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"textblob_mean_roll050\", color=\"gold\", ax=axs[2, 1])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"sentiword_mean_roll050\", color=\"teal\", ax=axs[3, 0])\n",
        "sns.lineplot(data=corpus_sents_df, x=\"sent_no\", y=\"senticnet_mean_roll050\", color=\"teal\", ax=axs[3, 1])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySuYAv2YxCO0"
      },
      "source": [
        "corpus_sents_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iX1nCK6ljY8"
      },
      "source": [
        "%whos dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC3r9eWywyXB"
      },
      "source": [
        "\n",
        "# g=sns.pointplot(x=0, y=1, data=df, dodge=True,plot_kws=dict(alpha=0.3))\n",
        "# plt.setp(g.collections, alpha=.3) #for the markers\n",
        "# plt.setp(g.lines, alpha=.3)       #for the lines\n",
        "\n",
        "for i, sa_model in enumerate(corpus_sents_df.columns):\n",
        "  if (sa_model.endswith('_roll050')):\n",
        "    # if (sa_model != 'sent_no'):\n",
        "    if (sa_model == 'median'):\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, color='black')\n",
        "    else:\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, alpha=0.3)\n",
        "\n",
        "# print(f'{i}: {sa_model}')\n",
        "\n",
        "'''\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='median')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='textb')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "sns.lineplot(data=corpus_sentiments_df, x='sent_no', y='vader_roll500')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTKI4Fu2ACKP"
      },
      "source": [
        "**Exponential Moving Average**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvVMrD58AL97"
      },
      "source": [
        "# Not Necessary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wEab5F8_3Wl"
      },
      "source": [
        "**LOWESS and LOESS Smoothing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPOo6VAMmWh_"
      },
      "source": [
        "sa_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlPwfjS8mKpY"
      },
      "source": [
        "def plot_lowess(df, df_cols_ls, aplot=True, afrac=1./10, ait=5):\n",
        "  '''\n",
        "  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n",
        "  Return a DataFrame with LOWESS values\n",
        "  If 'plot=True', also output plot\n",
        "  '''\n",
        "\n",
        "  # global corpus_sents_norm_df\n",
        "\n",
        "  lowess_df = pd.DataFrame()\n",
        "\n",
        "  for i,acol in enumerate(df_cols_ls):\n",
        "    sm_x, sm_y = sm_lowess(endog=df[acol].values, exog=df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n",
        "    col_new = f'{acol}_lowess'\n",
        "    lowess_df[col_new] = pd.Series(sm_x)\n",
        "    if aplot:\n",
        "      plt.plot(sm_x, sm_y, label=acol, alpha=0.5, linewidth=2)\n",
        "\n",
        "      frac_str = str(round(100*afrac))\n",
        "      plt.title(f'{CORPUS_FULL} \\n LOWESS (frac={frac_str} Sentence Sentiment (Model: {sa_model})')\n",
        "      plt.legend(title='Sentiment Series')\n",
        "\n",
        "  return lowess_df\n",
        "\n",
        "# Test\n",
        "new_lowess_col = f'{sa_model}_lowess'\n",
        "my_frac = 1./10\n",
        "my_frac_per = round(100*my_frac)\n",
        "new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n",
        "corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], afrac=my_frac)\n",
        "corpus_sents_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qRUhRfmmyUA"
      },
      "source": [
        "corpus_sents_norm_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxbZdPVbnL6w"
      },
      "source": [
        "corpus_sents_norm_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25CDCoDJmTcI"
      },
      "source": [
        "norm_cols_ls = []\n",
        "for acol in corpus_sents_norm_df.columns:\n",
        "  if acol.endswith('_2norm'):\n",
        "    norm_cols_ls.append(acol)\n",
        "\n",
        "print(f'All norm_cols_ls')\n",
        "\n",
        "temp_cols_ls = list(set(norm_cols_ls) - set(['stanza_2norm','afinn_2norm']))\n",
        "\n",
        "print(f'Trimmed temp_cols_ls:')\n",
        "print(temp_cols_ls)\n",
        "\n",
        "\n",
        "plot_lowess(corpus_sents_norm_df, temp_cols_ls, 'Normed')\n",
        "\n",
        "'''\n",
        "for i, sa_model in enumerate(corpus_sents_df.columns):\n",
        "  if (sa_model.endswith('_roll050')):\n",
        "    # if (sa_model != 'sent_no'):\n",
        "    if (sa_model == 'median'):\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, color='black')\n",
        "    else:\n",
        "      sns.lineplot(data=corpus_sents_df, x='sent_no', y=sa_model, alpha=0.3)\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7hs9FIxpugn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq8fWCKCpuaW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvKW23iwAGnk"
      },
      "source": [
        "%time\n",
        "\n",
        "plt.title(f'{BOOK_TITLE_FULL} \\n {sa_model} with statsmodels LOWESS (frac=0.1/iter=5)')\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['median'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "plt.plot(sm_x, sm_y, label='Median', color='black', linewidth=3)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['vader_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='VADER', color='royalblue', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['jockers_rinker_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='Jockers-Rinker', color='red', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['syuzhet_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='Syuzhet', color='tomato', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['huliu_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='HuLiu', color='teal', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['textblob_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='TextBlob', color='lime', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['sentiword_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='SentiWord', color='goldenrod', alpha=0.5)\n",
        "\n",
        "sm_x, sm_y = sm_lowess(endog=corpus_sentiments_df['senticnet_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n",
        "                       it=5, return_sorted = True).T\n",
        "sns.lineplot(sm_x, sm_y, label='SenticNet', color='forestgreen', alpha=0.5)\n",
        "\n",
        "# y_upper = corpus_sentiments_df['norm_score'].max() + 0.01\n",
        "# y_lower = corpus_sentiments_df['norm_score'].min() - 0.01\n",
        "# y_range = y_upper - y_lower + 0.02\n",
        "# plt.ylim([0.71, 0.74])\n",
        "# plt.ylim([y_lower, y_upper])\n",
        "# plt.plot(x, y, 'k.');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iORvQhmNFrw"
      },
      "source": [
        "**Discrete Cosine Transform (DCT)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917t5E9t_Pez"
      },
      "source": [
        "Libraries:\n",
        "\n",
        "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html#scipy.fft.dct\n",
        "* https://github.com/search?q=discrete+cosine \n",
        "\n",
        "Tutorial\n",
        "\n",
        "* https://realpython.com/python-scipy-fft/#the-discrete-cosine-and-sine-transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI-UtGpXwsJ6"
      },
      "source": [
        "from sktime.transformations.series.cos import CosineTransformer\n",
        "from sktime.datasets import load_airline\n",
        "y = load_airline()\n",
        "transformer = CosineTransformer()\n",
        "y_hat = transformer.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HUsqY_c3wsGA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YfHqTtxwsC4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKYGvaNxwr_m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noayFmY5v6Wc"
      },
      "source": [
        "!pip install pyts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-DLnLsjVWB-"
      },
      "source": [
        "**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n",
        "\n",
        "* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n",
        "* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n",
        "* http://futuredata.stanford.edu/asap/ \n",
        "* https://www.datadoghq.com/blog/auto-smoother-asap/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsz35wSyWlvA"
      },
      "source": [
        "import scipy.stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLFb9ZitVV3E"
      },
      "source": [
        "# ASAP Simple (Brute Force)\n",
        "def moving_average(data, _range):\n",
        "    ret = np.cumsum(data, dtype=float)\n",
        "    ret[_range:] = ret[_range:] - ret[:-_range]\n",
        "    return ret[_range - 1:] / _range\n",
        "\n",
        "def SMA(data, _range, slide):\n",
        "    ret = moving_average(data, _range)[::slide]\n",
        "    return list(ret)\n",
        "\n",
        "def kurtosis(values):\n",
        "    return scipy.stats.kurtosis(values)\n",
        "\n",
        "def roughness(vals):\n",
        "    return np.std(np.diff(vals))\n",
        "\n",
        "def smooth_simple(data, max_window=5, resolution=None):\n",
        "    data = np.array(data)\n",
        "    # Preaggregate according to resolution\n",
        "    window_size = 1\n",
        "    slide_size = 1\n",
        "    if resolution:\n",
        "        slide_size = int(len(data) / resolution)\n",
        "        if slide_size > 1:\n",
        "            data = SMA(data, slide_size, slide_size)\n",
        "    orig_kurt   = kurtosis(data)\n",
        "    min_obj     = roughness(data)\n",
        "    for w in range(2, int(len(data) / max_window + 1)):\n",
        "        smoothed = SMA(data, w, 1)\n",
        "        if kurtosis(smoothed) >= orig_kurt:\n",
        "            r = roughness(smoothed)\n",
        "            if r < min_obj:\n",
        "                min_obj = r\n",
        "                window_size = w\n",
        "    return window_size, slide_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHdCbShgVVrR"
      },
      "source": [
        "# Plot time series before and after smoothing\n",
        "def plot(data, window_size, slide_size, plot_title):\n",
        "    plt.clf()\n",
        "    plt.figure()\n",
        "    data = SMA(data, slide_size, slide_size)\n",
        "    method_names = [\"Original\", \"ASAP Smoothed\"]\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
        "    smoothed = SMA(data, window_size, 1)\n",
        "    smoothed_range = range(int(window_size/2), int(window_size/2) + len(smoothed))\n",
        "    ax1.set_xlim(0, len(data))\n",
        "    ax1.plot(data, linestyle='-', linewidth=1.5)\n",
        "    ax2.plot(smoothed_range, smoothed, linestyle='-', linewidth=1.5)\n",
        "    axes = [ax1, ax2]\n",
        "    for i in range(2):\n",
        "        axes[i].get_xaxis().set_visible(False)\n",
        "        axes[i].text(0.02, 0.8, \"%s\" %(method_names[i]),\n",
        "            verticalalignment='center', horizontalalignment='left',\n",
        "            transform=axes[i].transAxes, fontsize=25)\n",
        "\n",
        "    fig.set_size_inches(16, 12)\n",
        "    plt.tight_layout(w_pad=1)\n",
        "    plt.title(plot_title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpSktA4gWV3J"
      },
      "source": [
        "corpus_sentiments_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "710ghW5qVVoF"
      },
      "source": [
        "# Taxi\n",
        "# raw_data = load_csv('Taxi.csv')\n",
        "# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n",
        "# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n",
        "window_size, slide_size = smooth_simple(list(corpus_sentiments_df['median']), resolution=1000)\n",
        "print(\"Window Size: \", window_size)\n",
        "plot_title = f'{BOOK_TITLE_FULL} \\n Median Sentiment Smoothed with ASAP from Stanford (res=1000)'\n",
        "plot(list(corpus_sentiments_df['median']), window_size, slide_size, plot_title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wkkx80YYLCg"
      },
      "source": [
        "# Calculate Error Metrics based on Distance from Median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0K9P4jA_bDj"
      },
      "source": [
        "Libraries:\n",
        "\n",
        "* https://github.com/wannesm/dtaidistance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmSSemdcLQ9x"
      },
      "source": [
        "# sentiment_lowess_df['median']\n",
        "sentiment_lowess_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tvyv-LtJiqJ"
      },
      "source": [
        "# Rank each Sentiment Model by Error/Distance Metrics from Median\n",
        "\n",
        "sentiment_lowess_df['min'] = sentiment_lowess_df[['vader','jockers-rinker','syuzhet','huliu','textblob','sentiword','senticnet']].min(axis=1)\n",
        "sentiment_lowess_df['max'] = sentiment_lowess_df[['vader','jockers-rinker','syuzhet','huliu','textblob','sentiword','senticnet']].max(axis=1)\n",
        "sentiment_lowess_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18y52tS9JTeO"
      },
      "source": [
        "# LOWESS Smoothed Median Curve with Min/Max Confidence Intervals\n",
        "\n",
        "plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Min/Max Confidence Intervals')\n",
        "\n",
        "sns.lineplot(data=sentiment_lowess_df, x='x_value', y='median', linewidth=3, color='black')\n",
        "plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cQqCyABTkzT"
      },
      "source": [
        "# Error Metrics of each Model relative to the Median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BBTjp7nTkhK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHVeb7Jy9zaa"
      },
      "source": [
        "# Group and Classify Sentiment Arcs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xeV-709-_M2"
      },
      "source": [
        "Libraries\n",
        "\n",
        "* https://github.com/alan-turing-institute/sktime\n",
        "\n",
        "* https://github.com/johannfaouzi/pyts \n",
        "\n",
        "Code\n",
        "\n",
        "* https://colab.research.google.com/drive/1oEFfK5KTJyFQGs2xunc1cDW2OcbkZKCY\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFEtVAt29zMP"
      },
      "source": [
        "# https://colab.research.google.com/drive/1oEFfK5KTJyFQGs2xunc1cDW2OcbkZKCY\n",
        "\n",
        "# https://github.com/johannfaouzi/pyts \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJbFlIpAYNYO"
      },
      "source": [
        "# Automatically Extract Cruxes and Key Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdi7IXlZIjpS"
      },
      "source": [
        "**Find Peaks and Valleys**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnVYfyoBZcCb"
      },
      "source": [
        "# Pick a Sentiment Time Series (default Median)\n",
        "\n",
        "crux_models_ls = ['median', 'jockers-rinker', 'syuzhet', 'huliu', 'textblob', 'sentiword', 'senticnet']\n",
        "\n",
        "crux_model = crux_models_ls[0] # Edit\n",
        "\n",
        "print(f'Working with Sentiment Time Series from model: {crux_model}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn2BAHmhZ46B"
      },
      "source": [
        "# Verify\n",
        "\n",
        "sentiment_lowess_df.head()\n",
        "sm_x = sentiment_lowess_df['x_value'].values\n",
        "sm_y = sentiment_lowess_df[crux_model].values\n",
        "sm_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HdZ-dM0c0G9"
      },
      "source": [
        "sentiment_lowess_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcHMPlcfe6fI"
      },
      "source": [
        "print(peak_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6krPalOihS9"
      },
      "source": [
        "!pip install adjustText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO8Sxs_7kYA_"
      },
      "source": [
        "from adjustText import adjust_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO1saxDmZb71"
      },
      "source": [
        "# Find peaks(max).\n",
        "\n",
        "peak_indexes = signal.argrelextrema(sm_y, np.greater)\n",
        "peak_indexes = peak_indexes[0]\n",
        " \n",
        "# Find valleys(min).\n",
        "valley_indexes = signal.argrelextrema(sm_y, np.less)\n",
        "valley_indexes = valley_indexes[0]\n",
        " \n",
        "# Plot main graph.\n",
        "(fig, ax) = plt.subplots()\n",
        "ax.plot(sm_x, sm_y)\n",
        "\n",
        "win_half = 0 # 2500\n",
        "\n",
        "# Plot peaks.\n",
        "peak_x = peak_indexes\n",
        "peak_y = sm_y[peak_indexes]\n",
        "# ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "ax.scatter(peak_x, peak_y)\n",
        "for i, txt in enumerate(list(peak_x)):\n",
        "    ax.annotate(f'Sent No.\\n   {txt}', (peak_x[i], peak_y[i]))\n",
        "\n",
        "\n",
        "# Plot valleys.\n",
        "valley_x = valley_indexes\n",
        "valley_y = sm_y[valley_indexes]\n",
        "# ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "ax.scatter(valley_x, valley_y)\n",
        "# for i, txt in enumerate(list(valley_x)):\n",
        "#     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x[i], valley_y[i]))\n",
        "\n",
        "# plt.plot(x, y, 'bo')\n",
        "texts = [plt.text(valley_x[i], valley_y[i], 'Sent No.\\n   %s' %valley_x[i], ha='right', va='top') for i in range(len(valley_x))]\n",
        "adjust_text(texts)\n",
        "\n",
        "# Confidence Interval (Min/Max Range)\n",
        "plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        " \n",
        " \n",
        "# Save graph to file.\n",
        "plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('argrelextrema.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9b8exeloEF9"
      },
      "source": [
        "type(peaks_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc4xI_TAwyGW"
      },
      "source": [
        "# Extract Surrounding Paragraphs for context on matching Sentences\n",
        "\n",
        "def get_parag4sentno(asent_no):\n",
        "  '''\n",
        "  Return the original raw paragraph containing a \n",
        "  given sentence number.\n",
        "  '''\n",
        "  # parag_df = pd.DataFrame()\n",
        "  # print(f'Passed in sent_no: {asent_no}')\n",
        "  aparag_no = int(corpus_sents_df.loc[corpus_sents_df['sent_no'] == asent_no]['parag_no'])\n",
        "  # print(f'  This sent_no {asent_no} is in parag_no: {aparag_no}')\n",
        "  aparag_str = corpus_sents_df.loc[corpus_sents_df['parag_no'] == aparag_no]['sent_raw'].str.cat() # ['sent_clean']\n",
        "  # sentno_parag_df = corpus_sents_df[corpus_sents_df['sent_no']==asent_no]\n",
        "  # print(f'Sent #{asent_no} is in the paragraph: ')\n",
        "  # print(aparag)\n",
        "  # print(f'returning aparag_no: [{aparag_no}]: {aparag}')\n",
        "  return aparag_no, aparag_str\n",
        "\n",
        "'''\n",
        "# Testing\n",
        "asent_no = 7\n",
        "print(f'Searching for paragraph containing Sentence #{asent_no}')\n",
        "\n",
        "aparag_no, aparag_str = get_parag4sentno(asent_no)\n",
        "print(f'\\n  Found in Paragraph #{aparag_no} \\n\\n{aparag_str}')\n",
        "'''\n",
        "\n",
        "match_sents_ls = list(match_sents_df.sent_no.values)\n",
        "match_sents_ls\n",
        "\n",
        "print('\\nEmtional Valence Peaks\\n')\n",
        "print('------------------------------\\n')\n",
        "peaks_ls = list(peak_indexes)\n",
        "for i, asent_no in enumerate(peaks_ls):\n",
        "  aparag_no, aparag_str = get_parag4sentno(asent_no)\n",
        "  print(f'Peak at Sentence No. {asent_no} is in Paragraph No. {aparag_no}')\n",
        "  print('----------------------------')\n",
        "  print(f'     {aparag_str}\\n\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w78rjz3_kQz4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgF7Pxh1gcfL"
      },
      "source": [
        "# Test to see if sentiment values need to be normalized\n",
        "#   or adjusted for imbalance (e.g. VADER too many 0 polarities)\n",
        "\n",
        "corpus_cruxes_aug_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMDsMkVTfqAK"
      },
      "source": [
        "corpus_cruxes_dt['vader_nltk_norm'] = corpus_cruxes_dt['vader_nltk']*corpus_cruxes_dt['token_len']\n",
        "corpus_cruxes_dt.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIELm9NOgULA"
      },
      "source": [
        "# corpus_parags_df['vader_nltk'].lineplot()\n",
        "sns.lineplot(data=corpus_parags_df['vader_nltk_norm'].rolling(50).median())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhOqZaccUrNa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx8EqtilOvyU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utUG-rIsO0fF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "804w9PYAYP_p"
      },
      "source": [
        "# Export Manual and Automatic Sentiment Polarities and Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wnHGCQnO9aA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}